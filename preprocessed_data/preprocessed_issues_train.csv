repo,issue_text,label
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app private repo cannot give access to application # # # repro steps <number> . run application # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 0 3 5 a41c4e # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] extension build failing in windows and ubuntu # # # website or app n / a # # # repro steps been working on react devtools extension for a while now , my mac got some issue so i switched to a windows machine but currently i am unable to build the react devtool extension and run it locally here is the napshot of the error [ windows powershell 7 _28_2023 1 0 _43_05 am ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] __react_devtools_global_hook__ ? <repeated> # # # website or app <url> # # # repro steps hi , i have heard that the new versions of react will not support the react_devtools_global_hook . if there any information about this update that you can share . is there a new way to achieve the same result of using the react_devtools_global_hook but with a different method ? what is the future of react without the react_devtools_global_hook ? reactime and react inspector in the chrome store use this hook # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app local # # # repro steps open react - devtools # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - 2 4 6 8 a8735 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at f . emit ( / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at lh . e . onmessage ( / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( node : events : <number> <time> ) at e . exports . f ( / users / wangx / . config / yarn / global / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . emit ( node : events : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app localhost # # # repro steps - # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 4 6 8 a8735 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text emit <user> - extension :// a02c2c83 - f439 - 4 a45 - 9 7 2 b - 9 2 8 bb0916901 / build / main . js : <number> <time> bridge_bridge / this . _wallunlisten < <user> - extension :// a02c2c83 - f439 - 4 a45 - 9 7 2 b - 9 2 8 bb0916901 / build / main . js : <number> <time> listener <user> - extension :// a02c2c83 - f439 - 4 a45 - 9 7 2 b - 9 2 8 bb0916901 / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : app not recognized in firefox , but works in chrome # # # website or app - # # # repro steps on both firefox <number> and <number> the react devtools extension says page does not appear to be using react "" . on the most recent version of chrome it ' s working just fine . i cannot share the project since it ' s a private project unfortunately , but i suspect this is a regression introduced with the recent <url> especially since the devtools were working perfectly fine until maybe one or two weeks ago . it _does_ work in incognito mode in firefox . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : using different react instances across multiple frames throws errors # # # website or app <url> # # # repro steps <number> . visit the codepen linked above . <number> . be sure to open the debug view , so codepen does not add any additional iframes ( [ more info ] ( <url> <number> . observe : - warning in the console tab : ` invalid renderer id "" <number> "" ` . - error in the components tab : ` uncaught error : cannot add node "" <number> "" because a node with that id is already in the store ` . - after dismissing the error , all components seem to be accounted for ( you might need to adjust the "" hide components where . <repeated> "" option ) . in my actual application the error pops up for every change in the tree , making the devtools virtually unusable . note : this only seems to happen if the iframe is added some time after the initial react tree was mounted in the parent window . when i remove the ` settimeout ` so the iframe is added synchronously , i no longer see any warnings . related issues <url> - <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ~ ( automated ) ~ ( manual ) <number> . <number> ( <date> ) in google chrome version <number> . <number> ( official build ) ( <number> - bit ) # # # error message ~ ( automated ) ~ ( manual ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ~ ( automated ) ~ ( manual ) ` ` ` at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app <url> # # # repro steps <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 7 f8c501f6 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app http :// localhost : <number> / # # # repro steps <number> . accessing react dev from a local app ' ' # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 7 f8c501f6 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,[ devtools bug ] to build the extension locally # # # website or app n / a # # # repro steps i am trying to build the chrome extension for react dev tools locally to test changes as i am looking forward to fix this issue <url> but there seem to be some error with the packages looking forward to get some help ! [ windows powershell 4 _25_2023 9 _44_03 am ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_,0
facebook/react,"[ devtools bug ] : can not build react devtools for local development , instructions lead to error # # # website or app <url> # # # repro steps follow the instructions here to build local version of react devtools extension : <url> ` ` ` git clone <url> cd react yarn install yarn build - for - devtools ` ` ` stuck on this step everytime ` yarn build - for - devtools ` : ` ` ` d :\\ other \ \ react \ \ node_modules \ \ flow - parser \ \ flow_parser . js : <number> throw a } function ^ error : signedsource . signfile ( . <repeated> <sad> cannot sign file without token at object . <anonymous> ( d :\\ other \ \ react \ \ node_modules \ \ signedsource \ \ index . js : <time> ) at module . _compile ( node : internal / modules / cjs / loader : <number> <time> ) at object . module . _extensions . <repeated> js ( node : internal / modules / cjs / loader : <number> <time> ) at module . load ( node : internal / modules / cjs / loader : <number> <time> ) at function . module . _load ( node : internal / modules / cjs / loader : <number> <time> ) at module . require ( node : internal / modules / cjs / loader : <number> <time> ) at require ( node : internal / modules / cjs / helpers : <number> <time> ) at object . <anonymous> ( d :\\ other \ \ react \ \ scripts \ \ rollup \ \ packaging . js : <time> ) at module . _compile ( node : internal / modules / cjs / loader : <number> <time> ) at object . module . _extensions . <repeated> js ( node : internal / modules / cjs / loader : <number> <time> ) ` ` ` is there anything else i need to set up in order to make this work ? # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] could not find node with id "" <number> "" in commit tree # # # website or app private # # # repro steps private # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 8 ce1c171 # # # error message ( automated ) could not find node with id "" <number> "" in commit tree # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at map . foreach ( <anonymous> ) at rankedchartbuilder_getchartdata ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at profilingcache_profilingcache . getrankedchartdata ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at commitrankedautosizer ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at mf ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at qk ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at mk ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at lk ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at sj ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at commitrankedautosizer ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at div at div at div at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profiler_profiler ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not find node with id in commit tree in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] operations do not work in chrome # # # website or app <url> # # # repro steps <number> . go to components tab . <number> . for any component , try to copy the value in props , hooks etc . to clipboard . [ image ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] website or app localhost # # # repro steps in react router ' s latest version . inside the loader or action function one is not able to use reduxjs functions like useselector , usedispatch # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"bug : component is not a function when using suspense and forwardref react version : <number> . <number> - next - 3 ba7add60 - <number> # # steps to reproduce i have not been able to create a minimal example yet , if needed i will spend more time on it . however , it only seems to occur when suspending components rerender in a specific order . # # description the ` component is not a function ` error is thrown when using suspense and forwardref together in a specific way . it seems like react - reconciler does not properly handle forwardrefs in either ` renderwithhooksagain ` , ` replaysuspendedcomponentwithhooks ` , ` replayfunctioncomponent ` or ` replaysuspendedunitofwork ` . the ` component ` variable is not a function in this case , but a ` { $ $ typeof : symbol ( react . forward_ref ) , render ref ) => any } ` . ` renderwithhooksagain ` tries to execute ` component ( props , secondarg ) ` , which throws this error . i am not too familiar with react internals , if you can tell me how to trigger this codepath i can make a minimal reproduction more easily . < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",0
facebook/react,"bug : nested usetransition makes ispending of outer one always false nested ` starttransition ` call "" takes over "" and makes parent ` starttransition ` unable to track ` ispending ` . seb says it ' s a bug . repro : <url> <number> . click the button <number> . ` ispending ` in ` indexpage . js ` is ` true ` however , ` ispending ` in ` app . js ` is ` false ` . expected in ` app . js ` is also ` true ` .",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app <url> # # # repro steps log in to chatgpt # # # how often does this bug happen ? often # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 4 7 f63dc54 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text emit <user> - extension :// ce8dcc20 - 3 b2d - 4 cfe - b13e - 3 4 da2aa8e2c3 / build / main . js : <number> <time> bridge_bridge / this . _wallunlisten < <user> - extension :// ce8dcc20 - 3 b2d - 4 cfe - b13e - 3 4 da2aa8e2c3 / build / main . js : <number> <time> listener <user> - extension :// ce8dcc20 - 3 b2d - 4 cfe - b13e - 3 4 da2aa8e2c3 / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,bug ( <user> <sad> legacy ` reactdom . render ` crashes when rendering into ` document ` container < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> - next - 4 bf2113a1 - <number> # # steps to reproduce <number> . ` reactdom . render ` into a ` document ` container < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > link to code example : <url> # # the current behavior throws with ` ` ` react expected an <html> element ( document . documentelement ) to exist in the document but one was not found . react never removes the documentelement for any document it renders into so the cause is likely in some other script running on this page . ` ` ` ` reactdom . render ` clears the container before rendering into it . but with the new hostsingletons ( <url> we expect an existing ` documentelement ` . the odd part is that it seems like <url> affected the ` <user> ` release even though ` enablehostsingletons ` is disabled for that release . / cc <user> # # the expected behavior no crash like in ` react - dom <user> . <number> `,0
facebook/react,"bug : usesyncexternalstore will cause hydration missmatch in ` strictmode ` if ` serversnapshot ` is different from ` snapshot ` # # react version react version : <number> . <number> - next - b0671f9ea - <number> # # problem in ` strictmode ` , when using hydrateroot to render a component that using ` usesyncexternalstore ` it seems that useses will do hydration twice . but in second hydration process , useses does not use the result of ` getserversnapshot ` as initial state , which will cause hydration error . this problem will only happen in ` development ` mode , # # reproduce link to code example it works well in react <number> <url>",0
facebook/react,"[ devtools bug ] : can not work on devtools , instructions lead to error # # # website or app <url> # # # repro steps because react requires java , not on macos ( but assumes brew installed ) : ` ` ` brew update & & brew install java sudo ln - sfn / usr / local / opt / openjdk / libexec / openjdk . jdk / library / java / javavirtualmachines / openjdk . jdk export path =""/ usr / local / opt / openjdk / bin :$ path "" ' > > ~ / . zshrc export path =""/ usr / local / opt / openjdk / bin :$ path "" > > ~ / . zshrc export path =""/ usr / local / opt / openjdk / bin :$ path "" ` ` ` because react requires node before v17 ( but assuming you have nvm installed ! <sad> ` ` ` nvm install <number> ` ` ` then the real stuff ( directions inside folders like . / chrome / are quite wrong ) : ` ` ` git clone <url> cd react yarn install yarn build - for - devtools cd packages / react - devtools - extensions yarn build : chrome yarn build : chrome : local yarn run test : chrome ` ` ` now inside devtools : ` ` ` uncaught evalerror : refused to evaluate a string as javascript because ' unsafe - eval ' is not an allowed source of script in the following content security policy directive ' self ' ' wasm - unsafe - eval ' "" . at . / src / contentscripts / prepareinjection . js ( prepareinjection . js : <number> : <number> ) at __webpack_require__ ( prepareinjection . js : <time> ) at prepareinjection . js : <number> <time> at prepareinjection . js : <number> <time> ` ` ` what exactly are you doing that works in order to build , test , and develop this extension ? # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"error typeerror : cannot read property ' createelement ' of undefined , js engine : hermes # # # app using flipper dor react devtools # # # repro steps migrate to current version of rn - <number> . <number> using flipper enable hermes engine run the app # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) error typeerror : cannot read property ' createelement ' of undefined , js engine : hermes # # # error call stack ( automated ) ` ` ` text error typeerror : cannot read property ' createelement ' of undefined , js engine : hermes this is related to - - - > > path : node_modules / react - devtools - core / dist / backend . js function initialize ( ) { canvas = window . document . createelement ( ' canvas ' ); canvas . style . csstext = "" \ \ n xx - background - color : red ;\\ n xx - opacity : <number> ;\\ n bottom : <number> ;\\ n left : <number> ;\\ n pointer - events : none ;\\ n position : fixed ;\\ n right : <number> ;\\ n top : <number> ;\\ n z - index "" ; var root = window . document . documentelement ; root . insertbefore ( canvas , root . firstchild ) ; } ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app app # # # repro steps <number> . react - native run android <number> . react - devtools <number> . adb reverse tcp : <number> tcp : <number> <number> . open dev tools on phone <number> . error # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - 4 7 f63dc54 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at f . emit ( c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> ) at a . t ( c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . l ( c :\\ users \ \ gimpl \ \ appdata \ \ roaming \ \ npm \ \ node_modules \ \ react - devtools \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : the extension brokes some javascripts - react is not used . # # # website or app <url> # # # repro steps with the code in the sandbox ( but in a real page ) , when the extension is enabled , the tree is expanded , and cannot be collapsed anymore . [ image ] ( <url> there are no errors in the console of the page , but many in the extension . i do not know if it ' s correlated . note : this code is provided by symfony , a popular php framework . so many developer faces this issue note <number> issue is quite new i also created an [ issue in symfony ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app no source code available publically # # # repro steps working on my local react project # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 4 7 f63dc54 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app personal # # # repro steps go on a react app . open panel . observe error # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 4 7 f63dc54 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : use ( ) causes nested suspense boundaries to not reveal nested suspense boundaries should reveal as the content becomes ready . however , this does not seem to work with ` use ` . repro case : <url> expected : suspense boundaries reveal separately actual waits for everything before revealing anything",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app local app # # # repro steps working on the browser presentation api not really sure what i did to make this happen # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app <url> # # # repro steps occurred during mui datagrid setup # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app . # # # repro steps [ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - 4 4 e2ca393 # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at f . emit ( / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . l ( / users / <number> / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app chrome # # # repro steps went to check react component dev tools for state values and got error # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app <url> # # # repro steps - as soon as the < app / > component is rendered ( as soon as the application is loaded ) i get the element <number> error above , which stays consistent throughout every state change in my application start quizdata score isscored but i am not getting any console errors and the app is working as expected , but i have no access to my react components or any visibility of the state changes occurring within the app # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found | also "" element "" <number> "" not found "" / "" element "" <number> "" not found "" . <repeated> # # # website or app <url> # # # repro steps on first load i get multiple error warnings on all app components from ` <game/> ` down . they are identified with different element numbers but appear to be directed to the same issue "" the error was thrown at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> "" if you select ' begin game ' in app the error in component ` < game / > ` can be dismissed but new errors appear on all newly rendered components below ( once again all have different element numbers directed to the same location ( . <repeated> / main . js : <number> <time> ) / / this is issue is probably related to [ [ devtools bug ] element "" <number> "" not found ] ( <url> chrome is up to date ( version <number> . <number> ( official build ) (x 8 6 _64 ) ) , and restarted . i also tried reinstalling react extension ( running <number> . <number> ) . the app is using redux , which one commenter in other thread mentioned might be a commonality . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app website # # # repro steps added a useeffect to a functional component . # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] element "" <number> "" not found # # # website or app <url> # # # repro steps in branch litmus . tools - > feature / lit - <number> : i just run the app and it throws this error . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - bd2ad89a4 # # # error message ( automated ) element "" <number> "" not found # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app private # # # repro steps when we have multiple react application in the same page # # # how often does this bug happen ? often # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text emit <user> - extension :// <number> - 5 ec8 - d94c - b8d5 - 8 6 9 fc97bdf34 / build / main . js : <number> <time> bridge_bridge / this . _wallunlisten < <user> - extension :// <number> - 5 ec8 - d94c - b8d5 - 8 6 9 fc97bdf34 / build / main . js : <number> <time> listener <user> - extension :// <number> - 5 ec8 - d94c - b8d5 - 8 6 9 fc97bdf34 / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app local personal app # # # repro steps i was using global context # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] standalone launching and quitting after <number> seconds with no errors on ubuntu <number> # # # website or app <url> # # # repro steps run ` react - devtools ` in the terminal . an empty window appears for maybe <number> seconds and then disappears . the terminal does not show any messages whatsoever . there is another issue here regarding react - devtools failing silently on debian , where the submitter alleged that it was related to insufficient permissions to run electron . not sure if that ' s related to this issue , but i can run electron just fine , it ' s just react - devtools that are failing . os is ubuntu <number> ( kubuntu ) , node is v16 . <number> , react - devtools <number> . <number> ( updated to add url ) # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot read properties of undefined ( reading ' iscollapsed ' ) # # # website or app <url> # # # repro steps <number> # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot read properties of undefined ( reading ' iscollapsed ' ) # # # error call stack ( automated ) ` ` ` text at store_store . getelementatindex ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at store_store . getelementidatindex ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> at list . render ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at uj ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at sj ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at gl ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at fl ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at el ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at tl ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at list ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at div at autosizer ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at tree_tree ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> read properties of undefined ( reading ' iscollapsed ' ) in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : usereducer bail - out w / useeffect + suspense causes infinite loop ( strictmode error with next / experimental builds ) i just found our lib does not pass tests with latest build ( unreleased ones ) . <url> react version : <number> . <number> - next - 3 d443cad7 - <number> # # steps to reproduce ` ` ` jsx const component = ( ) => { const [ count , dispatch ] = usereducer ( ( prev ) => prev , <number> ); useeffect ( ( ) => { dispatch ( ); } , []); return < > { count } </>; }; const app = ( ) => ( <suspense> < component / > </suspense> ); ` ` ` > maximum update depth exceeded . this can happen when a component repeatedly calls setstate inside componentwillupdate or componentdidupdate . react limits the number of nested updates to prevent infinite loops . link to code example # # the current behavior warning in strictmode . # # the expected behavior no warning in strictmode . # <number> seems related . i am not sure if it ' s a bug or a misusage .",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app website # # # repro steps refresh page . <repeated> # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app react developer tools # # # repro steps open components in web developer tools # # # how often does this bug happen ? only once # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text emit <user> - extension :// 6 f15f1d5 - 6 0 2 a - 4 a38 - <number> - c9e5075b7456 / build / main . js : <number> <time> bridge_bridge / this . _wallunlisten < <user> - extension :// 6 f15f1d5 - 6 0 2 a - 4 a38 - <number> - c9e5075b7456 / build / main . js : <number> <time> listener <user> - extension :// 6 f15f1d5 - 6 0 2 a - 4 a38 - <number> - c9e5075b7456 / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] parsing failed "" components tab # # # website or app <url> # # # repro steps click "" components "" tab click on component click "" parse hook names "" # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) <number> . <number> # # # error message ( automated ) "" hook parsing failed "" # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app <url> # # # repro steps on inspecting comment modal # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 3 3 6 ac8ceb # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) ` ` ` text comment ` ` ` # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app <url> # # # repro steps <number> . create account <number> . redirect to verify email page <number> . copy otp from email <number> . paste it in email page <number> . redirect login <number> . keep redirecting between login page and verify email page # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 7 f673317f # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"possible suspense bug i think i have trapped a bug where – after react suspends and the data has resolved , react bails out before re - rendered some of the suspended components . ( it ' s possible the bug is in my suspense cache logic . i realize i am working with un - finalized apis . ) here is a loom walk through of the code in question : <url> here is a replay recording of the bug with annotations from myself and <user> : <url> to reproduce the bug directly checkout replay commit [ ` 2 2 a07dbb294e0381d371cb744ac1ea2031edf9d6 ` ] ( <url> <number> . in the main directory run ` yarn install ` <number> . in ` packages / bvaughn - architecture - demo ` run ` yarn dev ` <number> . open localhost : <number> / tests / object - inspector and scroll down until you see "" loading . <repeated> "" ( below the entry containing "" htmlelementwithattributes "" ) cc <user> who seemed interested in looking into this sometime ( obviously no hurry ) 😄",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app react native app # # # repro steps <number> . run react native app <number> . npx react - devtools <number> . adb reverse tcp : <number> tcp : <number> <number> . reload the app , this error should come as mentioned below # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - 7 f673317f # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at f . emit ( / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . l ( / users / <number> / . npm / _npx / <number> / lib / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] boolean from devtools component props causing loss of class functions # # # website or app <url> # # # repro steps <number> ) install webpack , react , babel with "" npm install "" in terminal . <number> ) bundle app with webpack via "" webpack - - watch - - mode = developement "" in terminal . <number> ) open up "" index . html "" in chrome and open react devtools . under components , there should be a dog component within a person component , where the person is passed as a prop to the dog . <number> ) under dog props , there should be a person object with a value of present being true . if you click the button , the value of present should change to false , the number should switch to <number> , and the checkbox should get deselected . however , if you manually select the checkbox to change between true / false , the prop seems to lose its prototype references to the original js class , as an error stating it cannot find the function will be shown . <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot remove node "" <number> "" because no matching node was found in the store . # # # website or app <number> # # # repro steps <number> # # # how often does this bug happen ? often # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 7 f673317f # # # error message ( automated ) cannot remove node "" <number> "" because no matching node was found in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) ` ` ` text <number> ` ` ` # # # github query string ( automated ) ` ` ` text <url> remove node because no matching node was found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : component tree panel becomes unresponsive after clicking on a few components # # # website or app multiple ; but you can check at <url> # # # repro steps <number> . access a website in chrome that uses react . <number> . open chrome developer tools <number> . open the react developer tools components tab / panel <number> . click on <number> - <number> components in the component tree individually to inspect them notes this started happening on all react - based websites after updating to chrome version <number> . <number> on my work macbook (x 8 6 _64 ) and my personal macbook ( arm64 ) . reverting back to chrome <number> seems to help . - i had a co - worker test as well , with the same result . - you can still select individual components using the picker , even after the panel locks up . - the lock - up seems to happen quicker when ` expand component tree by default ` is selected in the ` components ` tab in the panel settings , but will still lock up if you manually expand enough components . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> ( <date> ) # # # error message ( automated ) none # # # error call stack ( automated ) n / a # # # error component stack ( automated ) n / a # # # github query string ( automated ) n / a",0
facebook/react,"[ devtools bug ] : "" reload and profile "" aways disabled on timeline tab # # # website or app beta . reactjs . org ( local development ) # # # repro steps <number> . <url> <number> . ` yarn dev ` "" reload and profile "" is enabled for flamegraph : < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> but disabled for timeline width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> and yet i can "" reload and profile "" in flamegraph , then switch to timeline and it works . so this does not add up . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app [ expo init ] ( <url> # # # repro steps all you have to do is use ' expo init ' or ' npx create - react - native - app ' and make a fresh app with a blank template . when i open it with ' expo start ' or ' npm start ' i get this message every time in the debugger . when i press the ' dismiss ' button , i get a box that says : ' unsupported devtools backend version you are running react - devtools version <number> . <number> - d0ec283819 . this requires bridge protocol version <number> . however the current backend version uses bridge protocol version <number> . to fix this , upgrade the devtools npm package : npm i - g react - devtools @ ^ <number> . <number> ' i follow all the steps i can find to solve the issue ( including yarn resolutions , updating node . js and packages , etc . ) but nothing works at all . i cannot initialize new apps without getting the error . i really want this debugger to work again . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - d0ec283819 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at c . emit ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> ) at g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> at g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at array . foreach ( <anonymous> ) at s . gc . e . onmessage ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at s . n ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at s . emit ( events . js : <number> <time> ) at e . exports . p ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> ) at e . exports . emit ( events . js : <number> <time> ) at e . exports . datamessage ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . getdata ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . startloop ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . _write ( g :\\ workspace \ \ rn - debugger - windows - x64 \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at dowrite ( _stream_writable . js : <number> <time> ) at writeorbuffer ( _stream_writable . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] crashes the app # # # website or app local development # # # repro steps in order to reproduce it , type ` console . log ( * any variable or functionality ) ` if using console log for printing string , it works with not errors # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : when inspecting with devtools , it fails to select correct react component when there are multiple react - dom instances in the application # # # website or app <url> # # # repro steps when using devtools in the linked codesandbox , i am not able to select react components that are rendered by the micro - fe using the inspect tool . steps to reproduce go to <url> <number> . open react devtools and click on inspect icon with the "" select an element on the page to inspect it "" tooltip <number> . inspect the component with the pink background and text "" micro - fe example heading "" <number> . devtools selects ` app ` as the component ( or if the component type filter is disabled , it selects the ` div ` where the micro - fe is mounted ) ideally it would actually select either the ` microferoot ` component ( or the corresponding html node if the filter is disabled ) . i was able to get it working by doing [ the following change ] ( <url> in the ` react - devtools - backend / src / backend / agent . js ` # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : i believe that ui and ux from react devtools is a big bug # # # website or app <url> # # # repro steps hello , community ✌ i have an big question about the react devtools , why is so different in comparasion with vue devtools ? all is more hard . <repeated> see the context , indentify the state ' s , the components . <repeated> 😒 in comparasion with vue devtools , the react devtoosl do not have a pretty and functional ui and ux . the context ' s , state ' s and components do not have a ui ogarnized to easy indentification . on vue devtools there tabs to components , store ( context ' s ) , events and some other features , see below the vuex ( context ' s in react environment ) ! [ image ] ( <url> timeline of events 😍 👍 👍 ! [ image ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : treecontext error : can not access property "" id "" in undefined # # # website or app <url> # # # repro steps unfortunately i do not know how to reproduce this bug . it was just logged to sentry . it seems like there ' s a logic bug here though : <url> if ` selectedelementindex ` is null or ` elementindiceswitherrorsorwarnings ` is empty , then ` flatindex ` would be <number> still – and this statement would result in an undefined entry : ` ` ` js preventry = elementindiceswitherrorsorwarnings [ elementindiceswitherrorsorwarnings . length - <number> ]; ` ` ` # # # how often does this bug happen ? only once # # # devtools package ( automated ) react - devtools - inline # # # devtools version ( automated ) <number> . <number> # # # error message ( automated ) error : can not access property "" id "" , n is undefined # # # error call stack ( automated ) ` ` ` text react errorboundary error access property "" id "" , n is undefined at treecontextcontroller ( . / node_modules / react - devtools - inline / dist / frontend . js : <number> <time> ) at settingscontextcontroller ( . / node_modules / react - devtools - inline / dist / frontend . js : <number> <time> ) at modaldialogcontextcontroller ( . / node_modules / react - devtools - inline / dist / frontend . js : <number> <time> ) at devtools_devtools ( . / node_modules / react - devtools - inline / dist / frontend . js : <number> <time> ) at usememo ( . / src / ui / components / secondarytoolbox / reactdevtools . tsx : <number> : <number> ) at connectfunction ( . / node_modules / react - redux / es / components / connectadvanced . js : <number> <time> ) at redacted ( . / src / ui / components / redacted . tsx : <number> : <number> ) at secondarytoolbox ( . / src / ui / components / secondarytoolbox / index . tsx : <number> <time> ) at usegetshowvideo ( . / src / devtools / client / shared / components / splitter / splitbox . tsx : <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] components not marked as "" rendered "" if context changed # # # website or app <url> # # # repro steps <number> . goto <url> <number> . start profiling <number> . enter "" a "" into the input <number> . stop profiling [ forwardref - did - not - render ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) <number> . <number> - 4 6 a98cff2 # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : error in event handler : error : attempting to use a disconnected port object # # # website or app <url> # # # repro steps <number> . create a react app ` ` ` yarn create react - app test - react cd test - react yarn start ` ` ` <number> . create ` . env . development ` file in root . ` ` ` https = true port = <number> browser = none ` ` ` <number> . visit https :// localhost : <number> / in chrome v100 . <number> . <number> <number> . open react devtools by inspecting page , some times it shows ` components ` tab but in large application it does not show the ` components ` tab . if it shows the tab the error message is sent to dev tools every second . <number> . see error message in [ chrome :// extensions / ] ( chrome :// extensions / ) <number> . this is not reproducible in firefox v99 . <number> . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) ` <number> . <number> ( <date> ) ` # # # error message ( automated ) ` error in event handler : error : attempting to use a disconnected port object ` # # # error call stack ( automated ) ` build / background . js : <number> ( lone ) ` ` ` ` /* * * * * */ ( function ( modules ) { / / webpackbootstrap /* * * * * */ / / the module cache /* * * * * */ var installedmodules = {}; /* * * * * */ /* * * * * */ / / the require function /* * * * * */ function __webpack_require__ ( moduleid ) { /* * * * * */ /* * * * * */ / / check if module is in cache /* * * * * */ if ( installedmodules [ moduleid ] ) { /* * * * * */ return installedmodules [ moduleid ] . exports ; /* * * * * */ } /* * * * * */ / / create a new module ( and put it into the cache ) /* * * * * */ var module = installedmodules [ moduleid ] = { /* * * * * */ i : moduleid , /* * * * * */ l : false , /* * * * * */ exports : { } /* * * * * */ }; /* * * * * */ /* * * * * */ / / execute the module function /* * * * * */ modules [ moduleid ] . call ( module . exports , module , module . exports , __webpack_require__ ) ; /* * * * * */ /* * * * * */ / / flag the module as loaded /* * * * * */ module . l = true ; /* * * * * */ /* * * * * */ / / return the exports of the module /* * * * * */ return module . exports ; /* * * * * */ } /* * * * * */ /* * * * * */ /* * * * * */ / / expose the modules object ( __webpack_modules__ ) /* * * * * */ __webpack_require__ . m = modules ; /* * * * * */ /* * * * * */ / / expose the module cache /* * * * * */ __webpack_require__ . c = installedmodules ; /* * * * * */ /* * * * * */ / / define getter function for harmony exports /* * * * * */ __webpack_require__ . d = function ( exports , name , getter ) { /* * * * * */ if ( __webpack_require__ . o ( exports , name ) ) { /* * * * * */ object . defineproperty ( exports , name , { enumerable : true , get : getter }); /* * * * * */ } /* * * * * */ }; /* * * * * */ /* * * * * */ / / define __esmodule on exports /* * * * * */ __webpack_require__ . r = function ( exports ) { /* * * * * */ if ( typeof symbol ! = = ' undefined ' & & symbol . tostringtag ) { /* * * * * */ object . defineproperty ( exports , symbol . tostringtag , { value : ' module ' }); /* * * * * */ } /* * * * * */ object . defineproperty ( exports , ' __esmodule ' , { value : true }); /* * * * * */ }; /* * * * * */ /* * * * * */ / / create a fake namespace object /* * * * * */ / / mode & <number> : value is a module id , require it /* * * * * */ / / mode & <number> : merge all properties of value into the ns /* * * * * */ / / mode & <number> : return value when already ns object /* * * * * */ / / mode & <number> | <number> : behave like require /* * * * * */ __webpack_require__ . t = function ( value , mode ) { /* * * * * */ if ( mode & <number> ) value = __webpack_require__ ( value ) ; /* * * * * */ if ( mode & <number> ) return value ; /* * * * * */ if ( ( mode & <number> ) & & typeof value = = = ' object ' & & value & & value . __esmodule ) return value ; /* * * * * */ var ns = object . create ( null ) ; /* * * * * */ __webpack_require__ . r ( ns ) ; /* * * * * */ object . defineproperty ( ns , ' default ' , { enumerable : true , value : value }); /* * * * * */ if ( mode & <number> & & typeof value ! = ' string ' ) for ( var key in value ) __webpack_require__ . d ( ns , key , function ( key ) { return value [ key ] ; } . bind ( null , key ) ); /* * * * * */ return ns ; /* * * * * */ }; /* * * * * */ /* * * * * */ / / getdefaultexport function for compatibility with non - harmony modules /* * * * * */ __webpack_require__ . n = function ( module ) { /* * * * * */ var getter = module & & module . __esmodule ? /* * * * * */ function getdefault ( ) { return module [ ' default ' ]; } : /* * * * * */ function getmoduleexports ( ) { return module ; }; /* * * * * */ __webpack_require__ . d ( getter , ' a ' , getter ) ; /* * * * * */ return getter ; /* * * * * */ }; /* * * * * */ /* * * * * */ / / object . prototype . hasownproperty . call /* * * * * */ __webpack_require__ . o = function ( object , property ) { return object . prototype . hasownproperty . call ( object , property ) ; }; /* * * * * */ /* * * * * */ / / __webpack_public_path__ /* * * * * */ __webpack_require__ . p = "" / build / "" ; /* * * * * */ /* * * * * */ /* * * * * */ / / load entry module and return exports /* * * * * */ return __webpack_require__ ( __webpack_require__ . s = <number> ); /* * * * * */ } ) /* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */ /* * * * * */ ( { /* * */ <number> : /* * */ ( function ( module , exports , __webpack_require__ ) { "" use strict "" ; /* global chrome */ const ports = {}; const is_firefox = navigator . useragent . indexof ( ' firefox ' ) >= <number> ; chrome . runtime . onconnect . addlistener ( function ( port ) { let tab = null ; let name = null ; if ( isnumeric ( port . name ) ) { tab = port . name ; name = ' devtools ' ; installcontentscript ( + port . name ) ; } else { tab = port . sender . tab . id ; name = ' content - script ' ; } if ( ! ports [ tab ] ) { ports [ tab ] = { devtools : null , ' content - script ' : null }; } ports [ tab ] [ name ] = port ; if ( ports [ tab ] . devtools & & ports [ tab ] [ ' content - script ' ] ) { doublepipe ( ports [ tab ] . devtools , ports [ tab ] [ ' content - script ' ]); } }); function isnumeric ( str ) { return + str + ' ' = = = str ; } function installcontentscript ( tabid ) { chrome . tabs . executescript ( tabid , { file : ' / build / contentscript . js ' } , function () {}); } function doublepipe ( one , two ) { one . onmessage . addlistener ( lone ) ; function lone ( message ) { two . postmessage ( message ) ; } two . onmessage . addlistener ( ltwo ) ; function ltwo ( message ) { one . postmessage ( message ) ; } function shutdown ( ) { one . onmessage . removelistener ( lone ) ; two . onmessage . removelistener ( ltwo ) ; one . disconnect ( ); two . disconnect ( ); } one . ondisconnect . addlistener ( shutdown ) ; two . ondisconnect . addlistener ( shutdown ) ; } function seticonandpopup ( reactbuildtype , tabid ) { chrome . browseraction . seticon ( { tabid : tabid , path : { ' <number> ' : ' icons / <number> - ' + reactbuildtype + ' . png ' , ' <number> ' : ' icons / <number> - ' + reactbuildtype + ' . png ' , ' <number> ' : ' icons / <number> - ' + reactbuildtype + ' . png ' , ' <number> ' : ' icons / <number> - ' + reactbuildtype + ' . png ' } }); chrome . browseraction . setpopup ( { tabid : tabid , popup : ' popups / ' + reactbuildtype + ' . html ' }); } function isrestrictedbrowserpage ( url ) { return ! url || new url ( url ) . protocol = = = ' chrome : ' ; } function checkandhandlerestrictedpageifso ( tab ) { if ( tab & & isrestrictedbrowserpage ( tab . url ) ) { seticonandpopup ( ' restricted ' , tab . id ) ; } } / / update popup page of any existing open tabs , if they are restricted browser pages . / / we can not update for any other types ( prod , dev , outdated etc ) / / as the content script needs to be injected at document_start itself for those kinds of detection / / todo : show a different popup page ( to reload current page probably ) for old tabs , opened before the extension is installed if ( ! is_firefox ) { chrome . tabs . query ( { } , tabs => tabs . foreach ( checkandhandlerestrictedpageifso ) ); chrome . tabs . oncreated . addlistener ( ( tabid , changeinfo , tab ) => checkandhandlerestrictedpageifso ( tab ) ); } / / listen to url changes on the active tab and update the devtools icon . chrome . tabs . onupdated . addlistener ( ( tabid , changeinfo , tab ) => { if ( is_firefox ) { / / we do not properly detect protected urls in firefox at the moment . / / however we can reset the devtools icon to its loading state when the url changes . / / it will be updated to the correct icon by the onmessage callback below . if ( tab . active & & changeinfo . status = = = ' loading ' ) { seticonandpopup ( ' disabled ' , tabid ) ; } } else { / / do not reset the icon to the loading state for chrome or edge . / / the onupdated callback fires more frequently for these browsers , / / often after onmessage has been called . checkandhandlerestrictedpageifso ( tab ) ; } }); chrome . runtime . onmessage . addlistener ( ( request , sender ) => { var _request $ payload , _ports $ id ; const tab = sender . tab ; if ( tab ) { const id = tab . id ; / / this is sent from the hook content script . / / it tells us a renderer has attached . if ( request . hasdetectedreact ) { / / we use browseraction instead of pageaction because this lets us / / display a custom default popup when react is not <emphasis> detected . / / it is specified in the manifest . seticonandpopup ( request . reactbuildtype , id ) ; } else { switch ( ( _request $ payload = request . payload ) = = = null || _request $ payload = = = void <number> ? void <number> : _request $ payload . type ) { case ' fetch - file - with - cache - complete ' : case ' fetch - file - with - cache - error ' : / / forward the result of fetch - in - page requests back to the extension . const devtools = ( _ports $ id = ports [ id ] ) = = = null || _ports $ id = = = void <number> ? void <number> if ( devtools ) { devtools . postmessage ( request ) ; } break ; } } } }); /* * */ } ) /* * * * * */ }); ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot add child "" foo "" to parent "" bar "" because parent node was not found in the store . # # # website or app <url> # # # repro steps steps to reproduce : <number> . install all dependencies <number> . run the project with ` npm start ` <number> . navigate to http :// localhost : <number> / login <number> . enter any number into _phone number_ and click on * * get otp * * . <number> . enter any number into the text boxes and click on * * sign in * * . <number> . allow access to location . <number> . check the dev tools for the error # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 4 6 a98cff2 # # # error message ( automated ) cannot add child "" <number> "" to parent "" <number> "" because parent node was not found in the store . # # # error call stack ( automated ) ` ` ` text emit <user> - extension :// 3 3 a37ecb - 2 4 c1 - 4 ab5 - <number> - 2 d8a276f2472 / build / main . js : <number> <time> bridge_bridge / this . _wallunlisten < <user> - extension :// 3 3 a37ecb - 2 4 c1 - 4 ab5 - <number> - 2 d8a276f2472 / build / main . js : <number> <time> listener <user> - extension :// 3 3 a37ecb - 2 4 c1 - 4 ab5 - <number> - 2 d8a276f2472 / build / main . js : <number> <time> ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add child to parent because parent node was not found in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"console dimming on second strictmode render forces string cast < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> ( congrats on the release ☺️ ) # # steps to reproduce <number> . during rendering of a component , log something that does not naturally cast to a string ( e . g . , ` console . log ( new set ( ) ) ` ) . <number> . wrap the tree in ` strictmode ` . <number> . observe the console . link to code example : <url> note that the console dimming is not applied to the inline codesandbox dev tools , so to see the issue , you need to visit the "" fullscreen view "" here : <url> # # the current behavior in chrome : < img width = "" <number> "" alt = "" screen shot <number> - <number> - <number> at <number> <number> <number> pm "" src = "" <url> as expected , there are two console logs , one dimmed . unfortunately , the way that the dimming works forces the second log to be serialized to a string . this has two issues : <number> . it can result in two of the "" same "" logs looking very "" different "" from each other , which is confusing to developers . for example , in the screenshot above , it ' s pretty surprising that those two console lines occur from the same ` console . log ` call . <number> . it prevents browser dev tools introspection . this can make it inconvenient or impossible to compare the two values if the string cast does not include the value , as in the screenshot above . this is problematic because a key use case of printing both values is to check whether they are the same . you can kind of work around the second issue by writing your own string cast at the log callsite , but you lose the <sad> of introspection , which is pretty unfortunate especially in the case of large / deeply - nested objects , etc . easier to compare two native console values than two ` json . stringify ` dumps . # # the expected behavior while there is a new ( appreciated ! ) dev tools option to suppress logging for the second render entirely , there is no way to disable the dimming feature . any of the following options would solve the issue provide a dev tools option to disable dimming . <number> . remove the dimming feature entirely . ( so that both logs are always printed the same way . ) <number> . update the dimming implementation so that it does not force a string cast . ( guessing this is not possible . ) <number> . improve serialization of complex values . ( imo this is not a great option because it does not solve the issue of consistency / confusion , but it would be better than the current behavior if all other options are ruled out . ) thanks for considering .",0
facebook/react,"bug : [ eslint - plugin - exhaustive - deps ] hook wrongly marked as conditional ( at exact number of conditionals in fc ) < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > when using an exact number of conditionals before and after a react hook , the ` react hook "" hook_name "" is called conditionally . react hooks must be called in the exact same order in every component render ` rule is wrongly flagged as being violated . this is a really weird bug and it ' s kind of hard to explain . just take a look at the code and watch as eslint flags the hook as somehow being conditional . while this may seem like a huge edge case , this actually triggered in our code base and caused all hooks in the component to be flagged as conditional . react version : <number> . <number> ( does not seem to matter ) # # steps to reproduce <number> . check out [ this project ] ( <url> run ` yarn ` and run ` yarn eslint app / foo . tsx ` . <number> . watch as the hook is incorrectly flagged as conditional . <number> . removing * * or adding * * one of the conditionals in the return statement makes the bug go away . the same goes for removing one of the conditionals above the hook . link to code example unfortunately i could not get it to work online ( because of a lack of terminals ) # # the current behavior hook is incorrectly flagged as conditional ! [ image ] ( <url> # # the expected behavior hook should not be conditional",0
facebook/react,"bug : [ eslint - plugin - exhaustive - deps ] can not find unstable value . < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> ( not important ) # # steps to reproduce i will show as a code . < img width = "" <number> "" alt = "" 스크린샷 <number> - <number> - <number> 오전 <number> <number> <number> "" src = "" <url> link to code example : <url> < ! - - please provide a codesandbox ( <url> a link to a repository on github , or provide a minimal code example that reproduces the problem . you may provide a screenshot of the application if you think it is relevant to your bug report . here are some tips for providing a minimal example - - > # # the current behavior exhaustive - deps can not found unstable dependency . # # the expected behavior warn lint message . # # opinion i know this code is super weird but this code may be improved . something like - add more core lint rule like ' usestate is always declared as a const ' - eslint - plugin - exhaustive - deps should find reallocation value . and calculate real value . - or just stay it . i just wonder what is your opinion . thank you .",0
facebook/react,"react dom umd always warns in react <number> this is a false positive . < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",0
facebook/react,bug : no warning on infinite useeffect loop in react <number> <url> this is supposed to ` console . error ` but it does not . <number> warns,0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app <url> # # # repro steps just opening the console and going to components or profiler shows this error . i noticed that in the console there are two warnings for contentscript . js ( i am assuming this file is part of this extension ) : ﻿ contentscript . js : <number> [ violation ] ' message ' handler took 2 1 0 ms contentscript . js : <number> [ violation ] ' message ' handler took 8 9 1 ms # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 8 2 7 6 2 bea5 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : error occurs when installing react - devtools in yarn berry project # # # website or app <url> # # # repro steps i tried to install react - devtools with yarn berry , so i encountered this error log .  yarn add react - devtools - d ➤ yn0000 : ┌ resolution step ➤ yn0002 : │ <user> - native - aria / combobox <user> : <number> . <number> - alpha . <number> [ d995e ] does not provide react - dom ( pd70ba ) , requested by <user> - aria / overlays ➤ yn0002 : │ <user> - native - aria / combobox <user> : <number> . <number> - alpha . <number> [ d995e ] does not provide react - dom ( pa0a11 ) , requested by <user> - aria / live - announcer ➤ yn0002 : │ <user> - native - aria / combobox <user> : <number> . <number> - alpha . <number> [ d995e ] does not provide react - dom ( pd5c9c ) , requested by <user> - aria / combobox ➤ yn0002 : │ babel - preset - expo <user> : <number> . <number> does not provide <user> / core ( pff6e6 ) , requested by <user> / plugin - proposal - decorators ➤ yn0002 : │ babel - preset - expo <user> : <number> . <number> does not provide <user> / core ( p4f633 ) , requested by <user> / plugin - transform - react - jsx ➤ yn0002 : │ babel - preset - expo <user> : <number> . <number> does not provide <user> / core ( pa1fad ) , requested by <user> / preset - env ➤ yn0002 : │ devfeed <user> : . does not provide <user> / react ( p5404d ) , requested by native - base ➤ yn0002 : │ devfeed <user> : . does not provide react - dom ( pf66e9 ) , requested by native - base ➤ yn0002 : │ devfeed <user> : . does not provide react - dom ( pfc556 ) , requested by react - use ➤ yn0060 : │ devfeed <user> : . provides react - native - safe - area - context ( p3bb21 ) with version <number> . <number> , which does not satisfy what native - base requests ➤ yn0060 : │ devfeed <user> : . provides react - native - svg ( pc0baf ) with version <number> . <number> , which does not satisfy what native - base requests ➤ yn0002 : │ react - native - codegen <user> : <number> . <number> does not provide <user> / preset - env ( p1529d ) , requested by jscodeshift ➤ yn0000 : │ some peer dependencies are incorrectly met ; run yarn explain peer - requirements <hash> for details , where <hash> is the six - letter p - prefixed code ➤ yn0000 : └ completed ➤ yn0000 : ┌ fetch step ➤ yn0000 : └ completed in 0 s 3 9 3 ms ➤ yn0000 : ┌ link step ➤ yn0001 : │ error : while cloning / users / jihoon . lim / dev / devfeed / node_modules / string_decoder / node_modules / safe - buffer - > / users / jihoon . lim / dev / devfeed / node_modules / registry - auth - token / node_modules / safe - buffer enoent : no such file or directory , scandir ' / users / jihoon . lim / dev / devfeed / node_modules / string_decoder / node_modules / safe - buffer ' ➤ yn0000 : └ completed in 3 s 3 6 1 ms ➤ yn0000 with errors in 3 s 9 2 7 ms # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"react devtools will not connect to application running on ios simulator react devtools does not connect to the instance running in the ios simulator . react devtools screenshot [ image ] ( <url> ios simulator screenshot ! [ image ] ( <url> react native appears to have some interactivity as you can see "" devtools initialized "" and the inspector on the ios simulator is running in the condensed mode . os : macos monterey ( <number> . <number> ) chip m1 pro",0
facebook/react,"[ devtools bug ] cannot read properties of undefined ( reading ' split ' ) # # # website or app <url> # # # repro steps <number> . enter the site <number> . open react - devtools <number> . select we ( maybe suspense ' s child component ) <number> . and show following errors . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - e28a0db22 # # # error message ( automated ) cannot read properties of undefined ( reading ' split ' ) # # # error call stack ( automated ) ` ` ` text at getderivedstatefromerror ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> <number> ) at errorboundary_errorboundary . c . payload ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at gg ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at dj ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at jl ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at il ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at hl ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at wk ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at al ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at uk ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> read properties of undefined ( reading ' split ' ) in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] unsupported bridge operation "" <number> "" # # # website or app local app development # # # repro steps just install react devtools and downgrade to <number> . <number> # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - e28a0db22 # # # error message ( automated ) unsupported bridge operation "" <number> "" # # # error call stack ( automated ) ` ` ` text at / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at c . emit ( / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . l ( / users / softwaremac / desktop / users / jigneshjani / rnprojects / wifiswitch / wifiswitchv2_29_5 / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> bridge operation in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : devtools failed to load source map # # # website or app inital react app - > npm create - react - app # # # repro steps <number> . created initial react app <number> . typed "" npm start "" inside app folder <number> . check console of chrome browser this is what i get on my console in chrome "" version <number> . <number> ( official build ) ( <number> - bit ) "" as these messages are annoying , please let me know how to fix this ? # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) <number> . <number> # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] unsupported bridge operation "" <number> "" # # # website or app none # # # repro steps gaolinxiong # # # how often does this bug happen ? only once # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - e28a0db22 # # # error message ( automated ) unsupported bridge operation "" <number> "" # # # error call stack ( automated ) ` ` ` text at / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> <time> <number> at c . emit ( / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> <time> <number> ) at / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> <time> <number> at / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . l ( / usr / local / lib / node_modules / react - devtools / node_modules / _react - devtools - core <user> . <number> <user> - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) ` ` ` text <number> ` ` ` # # # github query string ( automated ) ` ` ` text <url> bridge operation in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"devtools should not crawl unmounted subtrees when profiling starts previously we crawled all subtrees , even not - yet - mounted ones , to initialize context values . this was not only unecessary , but it also caused an error to be thrown . this commit adds a test and fixes that behavior . resolves # <number> # # # test ( before ) [ screen shot <number> - <number> - <number> at <number> <number> <number> am ] ( <url> # # # test ( after ) ! [ screen shot <number> - <number> - <number> at <number> <number> <number> am ] ( <url>",0
facebook/react,[ devtools bug ] dev tools extension shows websites built with other framework as react web sites # # # website or app <url> # # # repro steps <number> . go to <url> or <url> <number> . click on react dev tools extension <number> . you should see message * * this page is using the production build of react . ✅ * * # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_,0
facebook/react,"[ devtools bug ] updates when components render . # # # website or app nothing # # # repro steps create a component list memoized ( react . memo ) , and if you only modify <number> of them , this will mark that all are rendered , but if they are memoized the report says "" not re - renders "" detected [ screenshot from <number> - <number> - <number> <number> - <number> - <number> ] ( <url> if you see near to ` typography ` all those children are memoized and they arent re - rendering ( expected ) but devtools still highliting them if u see those ` yellow boxes ` # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] : ' this page is using the react build message ' in every site # # # website or app youtube . com , github . com , gmail . com , lucidchart . app # # # repro steps <number> . open any web app not made with react ( e . g . : youtube . com , amazon . com , github . com , gmail . com , lucidchart . app ) <number> . the react icon is on and the popover message says page is using the production build of react . ✅ ' # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"react <number> : context providers are reset to initial value in ssr during rendering < - - ask a question or share feedback about the react <number> release here . - - > while testing ssr streaming in latest react <number> experimental and alpha versions , [ we noticed ] ( <url> that context providers are reset to their initial values during rendering under certain conditions . it works well when handling <number> request at a time . however , when the server gets <number> or more requests at the same time , the context providers seem to get confused . the context is correct at the beginning of the rendering for each request but it gets lost after a while . there ' s a reproduction here using <user> ' s demo : <url> to my understanding , since the react tree is wrapped in a provider in ssr , ` usecontext ` should never return ` null ` in the server . have a look at the terminal and see how it actually logs ` null ` sometimes when getting multiple requests . ` ` ` [ <number> ] this should never be null : { read : [ function : read ] } [ <number> ] this should never be null : { read : [ function : read ] } [ <number> ] this should never be null : null [ <number> ] this should never be null : null ` ` ` run the following code from the console to simulate multiple requests : ` ` ` js function dorequest ( ) { return fetch ( ' <url> { headers => r . text ( ) ) } await promise . all ( [ dorequest ( ) , dorequest ( ) ] ) ` ` ` we saw this same issue in different setups , using both webpack and vite . thanks !",0
facebook/react,"[ devtools bug ] : error not find id for fiber "" . <repeated> "" # # # website or app <url> # # # repro steps <number> . install repo <number> . run repo <number> . visit ` / playground ` in your browser of choice . <number> . navigate to the dev tools , and you should see the error # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot read properties of undefined ( reading ' push ' ) # # # website or app <url> # # # repro steps just view the components tab . this site is written in rescript . actually , this is all the rescript code you need to break devtools : ` ` ` rescript <user> . component let make = ( ) => { let _ = reactupdate . usereducer ( ( _ : unit , _ : unit ) => reactupdate . noupdate , ( ) ) react . useeffect ( ( ) => none ) < div / > } ` ` ` ` reactupdate ` is provided by [ ` rescript - react - update ` ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 0 2 2 9 baee2 # # # error message ( automated ) cannot read properties of undefined ( reading ' push ' ) # # # error call stack ( automated ) ` ` ` text at f ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at h ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at exports . inspecthooksoffiber ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at inspectelementraw ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at object . inspectelement ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at timelinecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> read properties of undefined ( reading ' push ' ) in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app app # # # repro steps <number> . run npm start for native . <number> . run android for native . <number> . run react native debugger and thats how happened . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - d0ec283819 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at c . emit ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at array . foreach ( <anonymous> ) at s . gc . e . onmessage ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at s . n ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at s . emit ( events . js : <number> <time> ) at e . exports . p ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at e . exports . emit ( events . js : <number> <time> ) at e . exports . datamessage ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . getdata ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . startloop ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at e . exports . _write ( / usr / lib / react - native - debugger / resources / app . asar / node_modules / react - devtools - core / dist / standalone . js : <time> <number> ) at dowrite ( _stream_writable . js : <number> <time> ) at writeorbuffer ( _stream_writable . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : using ` react - dom @ ^ <number> . <number> - rc . <number> ` inside a nextjs project ( ` next <user> . <number> ` ) , then opening a [ antd ] ( <url> dropdown , will cause page to be deadly frozen . after downgraded react to ` <number> . <number> ` , the issue disappears . my wild guess : this issue caused by some conflict btw dom manipulation and animation . i am happy to show detailed stuff if the react team needs . react version",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app <url> # # # repro steps accualy this bug appears only on react native debugger . the web version of debugger does not show any errors . the project is new and i just installed packages ` ` ` $ expo install react - navigation $ expo install react - navigation - stack $ expo install expo - app - loading $ expo install react - native - screens $ expo install react - native - safe - area - context ` ` ` so my dependencies are : ` ` ` json "" dependencies "" : { "" expo "" : "" ~ <number> . <number> "" , "" expo - status - bar "" : "" ~ <number> . <number> "" , "" react "" : "" <number> . <number> "" , "" react - dom "" : "" <number> . <number> "" , "" react - native "" : "" <number> . <number> "" , "" react - native - web "" : "" <number> . <number> "" , "" react - navigation "" : "" ^ <number> . <number> "" , "" react - navigation - stack "" : "" ^ <number> . <number> "" , "" expo - app - loading "" : "" ~ <number> . <number> "" , "" react - native - screens "" : "" ~ <number> . <number> "" , "" react - native - safe - area - context "" : "" <number> . <number> "" } , ` ` ` i checked and now this bug appears in every new expo init even if it is just blank project . old projects work fine . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - d0ec283819 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at c . emit ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> ) at c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> at c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> at array . foreach ( <anonymous> ) at s . gc . e . onmessage ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at s . n ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> <time> <number> ) at s . emit ( events . js : <number> <time> ) at e . exports . p ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <number> : <number> ) at e . exports . emit ( events . js : <number> <time> ) at e . exports . datamessage ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . getdata ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . startloop ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at e . exports . _write ( c :\\ users \ \ ritual \ \ appdata \ \ local \ \ react_native_debugger \ \ app - <number> . <number> \ \ resources \ \ app . asar \ \ node_modules \ \ react - devtools - core \ \ dist \ \ standalone . js : <time> <number> ) at dowrite ( _stream_writable . js : <number> <time> ) at writeorbuffer ( _stream_writable . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"react - devtools report error : cannot find module ' . / app ' react - devtools version : <number> . <number> npm - g install react - devtools react - devtools ` ` ` internal / modules / cjs / loader . js : <number> throw err ; ^ error : cannot find module ' . / app ' require stack : - / users / foo / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / bin . js at function . module . _resolvefilename ( internal / modules / cjs / loader . js : <number> <time> ) at function . resolve ( internal / modules / cjs / helpers . js : <number> <time> ) at object . <anonymous> ( / users / foo / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / bin . js : <number> <time> ) at module . _compile ( internal / modules / cjs / loader . js : <number> <time> ) at object . module . _extensions . <repeated> js ( internal / modules / cjs / loader . js : <number> <time> ) at module . load ( internal / modules / cjs / loader . js : <number> <time> ) at function . module . _load ( internal / modules / cjs / loader . js : <number> <time> ) at function . executeuserentrypoint [ as runmain ] ( internal / modules / run_main . js : <number> <time> ) at internal / main / run_main_module . js : <time> { code : ' module_not_found ' , requirestack ' / users / foo / . nvm / versions / node / v14 . <number> / lib / node_modules / react - devtools / bin . js ' ] } ` ` `",0
facebook/react,"[ devtools bug ] api cannot load webpack - internal :/// . <repeated> url scheme "" webpack - internal "" is not supported # # # website or app <url> # # # repro steps most actions in devtools ( search for component , click on component , hook parsing , profile record , etc ) cause this error . i am using next . js <number> within a nx monorepo . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app <url> # # # repro steps <number> . open chrome with <number> + tabs . <number> . run react - based website locally . <number> . open chrome dev tools to investigate [ activelink component ] ( <url> hierarchy inside [ navbar component ] ( <url> <number> . activelink component from [ next ] ( <url> was used . <number> . get uncaught error [ screenshot ] ( <url> # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 f8f60ca8 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : error checking code is skipped for async useeffect argument < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react no longer complains when the function passed to ` useeffect ` returns a promise ( either directly or by being labeled ` async ` ) . i am not in the habit of doing this , so it was only when i watched someone else make the function ` async ` and there were no errors that i realized that something had changed since <number> . <number> ( the newest version i could find that still complained ) . i can see the functionality is still present in ` commithookeffectlistmount ` in react - dom . development . js , but when i trace through it , the ` effect . tag ` is set to a different value and the test is skipped . i realize that we now have an eslint rule that provides the same message , but the lack of warning caused confusion about whether asynchronous functions were now allowed . react version : <number> . <number> # # steps to reproduce <number> . write a ` useeffect ` that has an ` async ` function <number> . open the console and observe that it does not complain < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > link to code example : <url> < ! - - please provide a codesandbox ( <url> a link to a repository on github , or provide a minimal code example that reproduces the problem . you may provide a screenshot of the application if you think it is relevant to your bug report . here are some tips for providing a minimal example : <url> - - > # # the current behavior the code runs with no complaints , and the cleaner function is ignored . # # the expected behavior the console should show the warning : ` ` ` warning : an effect function must not return anything besides a function , which is used for clean - up . it looks like you wrote useeffect ( async ( ) => . <repeated> ) or returned a promise . instead , write the async function inside your effect and call it immediately => { async function fetchdata ( ) { / / you can await here const response = await myapi . getdata ( someid ) ; / / . <repeated> } fetchdata ( ); } , [ someid ] ); / / or [ ] if effect does not need props or state ` ` `",0
facebook/react,"react <number> bug : react - dom / server "" detected multiple renderers . <repeated> "" if preceeded by react - test - renderer < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> - beta - 1 4 9 b420f6 - <number> # # steps to reproduce <number> . render a context with ` react - test - renderer ` ( wrapped in act ) <number> . render the same context with ` react - dom / server ` < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > link to code example : <url> ` ` ` js const context = react . createcontext ( null ) ; function component ( { renderer } ) { return ( < context . provider value ={ renderer } > < div / > < / context . provider > ); } let testrendererroot ; reacttestrenderer . act ( ( ) => { testrendererroot = reacttestrenderer . create ( < component renderer = "" react - test - renderer "" / > ); }); reacttestrenderer . act ( ( ) => { testrendererroot . unmount ( ); }); reactdomserver . rendertostring ( < component renderer = "" react - dom / server "" />); ` ` ` # # the current behavior ` rendertostring ` results in the console error "" warning multiple renderers concurrently rendering the same context provider . this is currently unsupported . "" # # the expected behavior no error like in react <number> ( <url> considering all renders are wrapped in their corresponding ` act ` i do not expect that i am concurrently rendering . i tried to understand when we reset the ` renderersigil ` ( responsible for checking if we "" concurrently rendering "" ) is reset and it seems like we never reset it but only initialize it when creating the context ( ` createcontext ` ) so it either seems like multiple renderers in the same module are not supported anymore or the reset is missing .",0
facebook/react,"[ devtools bug ] cannot read properties of undefined ( reading ' push ' ) # # # website or app <url> # # # repro steps i stumbled across the weirdest bug with react devtools and hooks that causes it to error out when inspecting a component . a minimal test case is documented in the code sandbox link , along with more details on the behavior and seemingly - arbitrary fixes . this does not affect the actual functionality of the app itself in any way , it behaves exactly as expected . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 f8f60ca8 # # # error message ( automated ) cannot read properties of undefined ( reading ' push ' ) # # # error call stack ( automated ) ` ` ` text at j ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at l ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at exports . inspecthooksoffiber ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at inspectelementraw ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at object . inspectelement ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at schedulingprofilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> read properties of undefined ( reading ' push ' ) in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : react instrumentation encountered an error : error : could not find id for fiber "" portal "" < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> # # steps to reproduce <number> . > when i execute ` modal . hide ( ) ` <number> . ! [ image ] ( <url> < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > link to code example : ! [ image ] ( <url> < ! - - please provide a codesandbox ( <url> a link to a repository on github , or provide a minimal code example that reproduces the problem . you may provide a screenshot of the application if you think it is relevant to your bug report . here are some tips for providing a minimal example - - > # # the current behavior # # the expected behavior",0
facebook/react,"[ devtools bug ] dispatcher . useid is not a function # # # website or app <url> # # # repro steps <number> . open <url> <number> . inspect ` app ` # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 f8f60ca8 # # # error message ( automated ) dispatcher . useid is not a function # # # error call stack ( automated ) ` ` ` text at useid ( <url> at app ( <url> at l ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at exports . inspecthooksoffiber ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at inspectelementraw ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) at object . inspectelement ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / react_devtools_backend . js : <number> : <number> ) ` ` ` # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at schedulingprofilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> is not a function in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"bug : use - sync - external - store fails to install via npm when trying to install the ` use - sync - external - store ` package via npm , i receive an error that it depends on a version of react that does not seem to exist . react version : <number> # # steps to reproduce <number> . ` mkdir example & & cd example & & npm init ` <number> . ` npm i react react - dom use - sync - external - store ` # # the current behavior npm fails to install the ` use - sync - external - store ` package with the following error : ` ` ` npm i use - sync - external - store npm err code eresolve npm err ! eresolve unable to resolve dependency tree npm err ! npm err ! while resolving : example <user> . <number> npm err ! found : react <user> . <number> npm err ! node_modules / react npm err ! react @ "" ^ <number> . <number> "" from the root project npm err ! npm err ! could not resolve dependency err ! peer react @ "" <number> . <number> - experimental - 4 5 8 9 8 dacb2 - <number> "" from use - sync - external - store <user> . <number> - experimental - 4 5 8 9 8 dacb2 - <number> npm err ! node_modules / use - sync - external - store npm err ! use - sync - external - store @ "" * "" from the root project npm err ! npm err ! fix the upstream dependency conflict , or retry npm err ! this command with - - force , or - - legacy - peer - deps npm err ! to accept an incorrect ( and potentially broken ) dependency resolution . npm err ! ` ` ` # # the expected behavior it successfully installs the package and i can use it in react v17 and the latest alpha version .",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . [ screen shot <number> - <number> - <number> at <number> <number> <number> pm ] ( <url> ! [ screen shot <number> - <number> - <number> at <number> <number> <number> pm ] ( <url> # # # website or app daily harvest # # # repro steps <number> . enable debug on the ios emulator . <number> . enable inspector . <number> . disable inspector , navigate to a different screen . <number> . try enabling inspector again . # # # how often does this bug happen ? every time # # # devtools package ( automated ) react - devtools - core # # # devtools version ( automated ) <number> . <number> - f58bbcf9a # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at c . emit ( / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> at / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> at array . foreach ( <anonymous> ) at a . e . onmessage ( / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at a . t ( / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> <time> <number> ) at a . emit ( events . js : <number> <time> ) at e . exports . f ( / users / krishnagaurav / mobile - app / node_modules / react - devtools / node_modules / react - devtools - core / dist / standalone . js : <number> : <number> ) at e . exports . emit ( events . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : blank tools localhost - react_devtools_backend . js : <number> uncaught error : could not find id for fiber # # # website or app <url> # # # repro steps react devtools - <number> . <number> react - <number> . <number> not sure if relevant but : react - scripts <number> . <number> first i get an blank devtools ( components ) but when picking select element arrow and hover over elements a lot of errors like this one shows up in the console : ` ` ` react_devtools_backend . js : <number> uncaught error : could not find id for fiber "" context . provider "" at getfiberidthrows ( react_devtools_backend . js : <number> ) at object . getfiberidfornative ( react_devtools_backend . js : <number> ) at overlay_overlay . inspect ( react_devtools_backend . js : <number> ) at showoverlay ( react_devtools_backend . js : <number> ) at onpointerover ( react_devtools_backend . js : <number> ) getfiberidthrows @ react_devtools_backend . js : <number> getfiberidfornative @ react_devtools_backend . js : <number> inspect @ react_devtools_backend . js : <number> showoverlay @ react_devtools_backend . js : <number> onpointerover @ react_devtools_backend . js : <number> ` ` ` no other errors occur . devtools works fine on deployed version . eg . <url> works fine . pr and issue that could be related ? [ issue <number> ] ( <url> ) [ pr <number> ] ( <url> ) maybe relevant comment from author and maintainer of react devtools fiber problem ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,[ devtools bug ] tools localhost only # # # website or app google . com # # # repro steps this started after last update <number> . <number> [ image ] ( <url> ! [ image ] ( <url> # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_,0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app <url> # # # repro steps not able to see # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 0 ca9b565 # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] cannot add node "" <number> "" because a node with that id is already in the store . # # # website or app http : localhost : <number> / # # # repro steps when i was using react select and dropdown . i clicked on and before opening the dropdown this error showed up and react dev tools stopped working # # # how often does this bug happen ? sometimes # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 1 7 8 a831a # # # error message ( automated ) cannot add node "" <number> "" because a node with that id is already in the store . # # # error call stack ( automated ) ` ` ` text at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at bridge_bridge . emit ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> at listener ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) ` ` ` # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) ` ` ` text <url> add node because a node with that id is already in the store . in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] could not inspect element with id "" <number> "" . error thrown : cached data for element "" <number> "" not found # # # website or app <url> # # # repro steps $ cd repo and then start the server using npm start in browser ( mozilla firefox ) when i try to add the user and age , and trying to debug the output in listusers component , i m facing this error [ screenshot from <number> - <number> - <number> <number> - <number> - <number> ] ( <url> # # # how often does this bug happen ? only once # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - 2 1 7 8 a831a # # # error message ( automated ) could not inspect element with id "" <number> "" . error thrown : cached data for element "" <number> "" not found # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) ` ` ` text inspectedelementcontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> suspense errorboundary_errorboundary <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> : <number> div inspectedelementerrorboundarywrapper <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> nativestylecontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> div div ownerslistcontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> settingsmodalcontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> components_components <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> errorboundary_errorboundary <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> : <number> div div themeprovider <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> portaledcontent <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> div div div themeprovider <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> schedulingprofilercontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> profilercontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> treecontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> settingscontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> modaldialogcontextcontroller <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> devtools_devtools <user> - extension :// 8 a220e8e - c7fe - 4 2 e7 - 9 ec5 - 0 b6e67ae5dd5 / build / main . js : <number> <time> ` ` ` # # # github query string ( automated ) ` ` ` text <url> not inspect element with id . error thrown : cached data for element not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] as visual helper produce strange symbole # # # website or app <url> # # # repro steps emoji seem supported but produce strange symbole [ image ] ( <url> to test emoji on window os , use ` [ win ] + [ . ] ` 🟩 # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",0
facebook/react,[ devtools bug ] seem supported but produce strange symbole # # # website or app ? <repeated> # # # repro steps emoji seem supported but produce strange symbole [ image ] ( <url> very low priority maybe because it affect nothing . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_,0
facebook/react,[ devtools bug ] parsing fails with fetch error # # # website or app <url> # # # repro steps have a webpack project that uses a domain mapped to your local ip such as ( appx . whenidev . net ) in my case that ' s served with https try to resolve hook names check console and observe the million errors <url> shows my console # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_,0
facebook/react,"devtools : backend console settings reportedly not synced to rn backend <user> reported that disabling the "" break on warning "" feature does not update the backend settings ( in memory , without a reload ) for react native : [ screen shot <number> - <number> - <number> at <number> <number> <number> am ] ( <url> i believe the new settings should be updated here but it sounds like they are not . we should investigate .",0
facebook/react,"[ devtools bug ] could not inspect element with id "" <number> "" . error thrown : cached data for element "" <number> "" not found # # # website or app website url ( localhost ) # # # repro steps clicked the "" reload devtools "" button to turn devtools in light mode . devtools were already in light mode . # # # how often does this bug happen ? only once # # # devtools package ( automated ) react - devtools - extensions # # # devtools version ( automated ) <number> . <number> - f58bbcf9a # # # error message ( automated ) could not inspect element with id "" <number> "" . error thrown : cached data for element "" <number> "" not found # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) ` ` ` text at inspectedelementcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at suspense at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at inspectedelementerrorboundarywrapper ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at nativestylecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at ownerslistcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingsmodalcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at components_components ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> <time> ) at errorboundary_errorboundary ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at portaledcontent ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at div at div at div at themeprovider ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at schedulingprofilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at profilercontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at treecontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at settingscontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at modaldialogcontextcontroller ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) at devtools_devtools ( chrome - extension :// fmkadmapgofadopljbjfkapdkoienihi / build / main . js : <number> : <number> ) ` ` ` # # # github query string ( automated ) ` ` ` text <url> not inspect element with id . error thrown : cached data for element not found in : title is : issue is : open is : public label : "" component tools "" repo : facebook / react ` ` `",0
facebook/react,"[ devtools bug ] : provide an icon to edge devtools ? # # # website or app website : <url> # # # repro steps <number> . open developer tools in edge <number> . go to settings - > experiment - > enable focus mode - > reload devtools <number> . we can see the react extension is loaded as below in devtools : [ image ] ( <url> <number> . since the react extension does not provide the icon when it ' s created , devtools fallback to use the default icon to represent the extension . however , we have received couple of requests from users that they ' d like to see the icon in the panel . for instance wondering if it ' s possible to provide an icon to edge in the ` chrome . devtools . panels . create ( ) ` ? thank you for your support . # # # how often does this bug happen ? every time # # # devtools package ( automated ) _no response_ # # # devtools version ( automated ) _no response_ # # # error message ( automated ) _no response_ # # # error call stack ( automated ) _no response_ # # # error component stack ( automated ) _no response_ # # # github query string ( automated ) _no response_",1
facebook/react,"bug : ` onresize ` media event is missing note : i ’ m happy to make a pull request to fix this , i just wanted to log it first to ensure there ’ s interest . - - - react ’ s [ synthetic media events ] ( <url> contain several [ existing media events ] ( <url> for instance ` onloadedmetadata ` and ` onvolumechange ` . but there is no ` onresize ` handler . [ ` resize ` is a standard media event ] ( <url> that triggers when one or both of the ` videowidth ` and ` videoheight ` attributes have just been updated . it ’ s useful for responding to resolution changes in video players . react version : <number> . <number> ( latest release as of initial issue report ) # # steps to reproduce <number> . create a ` <video> ` element with an ` onresize ` prop . <number> . check the console for the following warning : ` ` ` warning : unknown event handler property ` onresize ` . it will be ignored . ` ` ` link to code example # # the current behavior ` onresize ` handlers are ignored on ` <video> ` elements . # # the expected behavior ` onresize ` handlers are valid on ` <video> ` elements .",1
facebook/react,"feature request ( devtools ) : ` launch - editor ` for selected component # # # why : user can open the source file in editor / ide by one - click . it ' is a useful feature in ` vue - devtools ` . - - - # # # how : add a click event on here call ` ` ` fetch ( ` / __open - in - editor ? file =${ filename }:$ { linenumber } ` ) ` ` ` add [ launch - editor - middleware ] ( <url> to dev - server ( or any scaffold handle by himself ) - - - before anyone ( maybe me ) sends a pr , i want to know how do the react - team thinks about it ?",1
facebook/react,"feature proposal equivalent to createslice from redux toolkit ? would this be useful ? > a function that accepts an initial state , an object full of reducer functions , and a "" slice name "" , and automatically generates action creators and action types that correspond to the reducers and state . from [ createslice ] ( <url>",1
facebook/react,[ devtools feature request ] break on warnings it ' d be nice to have a toggle to pause the debugger when warnings fire so you can inspect the stack as it ' s happening . ` ` ` console . error = function ( ) { . <repeated> if ( isbreakon ) { debugger ; } } ` ` ` see <url>,1
facebook/react,"allow opting out of invokeguardedcallbackdev at the moment , when in development mode , react uses a special workflow for callbacks , to avoid using ` try . <repeated> catch ` . it works well . so well that several testing frameworks also get their uncaught exception handling triggered . * * example with mocha : * * ` ` ` js import react from ' react ' ; import { render } from ' <user> - library / react ' ; function mycomponent ( { dothrow } ) { if ( dothrow ) { throw new error ( ' i am bad ' ); } return <div> </div> ; } it ( ' should throw ' , function ( ) { expect ( ( ) => { render ( < mycomponent dothrow / >); } ) . to . throw ( ); }); ` ` ` * * behavior : * * - when running the test with the production build of react / react - dom , the test passes ( with the usual warning of act being unsupported in prod build ) . - when running the test with the development build of react - / react - dom , the test fails with ` error : uncaught error : i am bad ` . the root cause is ` invokeguardedcallbackdev ` runs the callback in an event to avoid using a ` try . <repeated> catch ` block … and trips mocha . js uncaught exception detector . * note : i am aware of error boundaries , i removed it from the example because the behavior is identical with it . * * note used testing - library for clarity , but using ` act ` and ` renderdom ` manually yields the same result . * * * expected behavior : * * - either make it work out of the box , or have the possibility to opt out of ` invokeguardedcallbackdev ` and force the use of the regular ` try . <repeated> catch ` implementation in development too .",1
facebook/react,"devtools : hovering "" rendered by "" list should highlight elements this list is pretty awesome width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> but always struggle to guess which component in the owner list i need to jump to . we should make hovering the owner list highlight components , just like the main tree view does . <user> - kanchev , interested ?",1
facebook/react,"add colors to component ' s name ( in component tree ) for visual feedback about type of component or node * * what is the current behavior ? * * all the components name in the component tree are of the same color * * what is the expected behavior ? * * it would be helpful if they have different colors indicating the type of component ( whether its native html node or contexts or simple react component ) i know we can filter it , but visual indication will be helpful too .",1
facebook/react,"react - refresh : add options to override $ refreshreg $ and $ refreshsig $ for better system . js integration * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature right now babel plugin emits globals : <url> ` ` ` js window . $ refreshreg $ = ( ) => {}; window . $ refreshsig $ = ( ) => type => type ; ` ` ` it would be nice to have them configurable . that would allow to use ` import . meta ` in environments like systemjs and have simpler implementation : ` ` ` js import runtime from ' react - refresh / runtime ' runtime . injectintoglobalhook ( window ) system . constructor . prototype . createcontext = function ( url ) { return { url , $ refreshsig $ : runtime . createsignaturefunctionfortransform , $ refreshreg $ : ( type , id ) => { id = url + ' ' + id runtime . register ( type , id ) } }; }; ` ` ` if you do not mind i could create pr with changes to react - refresh / babel next week . environment : ` ` ` js { "" systemjs "" : "" ^ <number> . <number> "" , "" react "" : "" ^ <number> . <number> "" , "" react - dom "" : "" ^ <number> . <number> "" , "" react - refresh "" } ` ` `",1
facebook/react,apply props / state / hooks edits on blur * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * trigger by press enter button * * what is the expected behavior ? * * trigger by out of focus or pressing enter button,1
facebook/react,"react devtools force re - render button i ' d like to have a button that forces a re - render of the selected component when clicked . for example , adding a button like this on the right of the component controls : [ image ] ( <url> this can be accomplished today by adding a new prop and changing the prop , but it ' s more work than i want to do . this would be useful for use while profiling how a component performs with unnecessary re - renders . right now i just have a button that ' s tied to a "" forcerender "" function function useforcererender ( ) { const [ , set ] = react . usestate ( ) return react . usecallback ( ( ) => set ( { } ) , [ ] ) } function filtercomponent ( ) { const forcererender = useforcererender ( ) return ( < > < button onclick ={ forcererender } > force rerender </button> {/* more jsx */} < / > ) } ` ` ` would be cool to have this built - in <happy>",1
facebook/react,"enhance react devtools "" why did this render ? "" for values nested in prop objects * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature <emphasis> * * what is the current behavior ? * * [ as demonstrated here , "" why did this render ? "" ] ( <url> does a great job reporting what prop changed , but it does not yet report which _nested_ value changed for props that are comprised of nested objects . * * what is the expected behavior ? * * the "" why did this render ? "" shows a collapsible tree with the "" leaf "" value that changed inside the prop object displayed . a couple use cases this would benefit some cases , it is most convenient creating props that are nested objects . for instance , maybe you need to pass an object to a library , and you ' d like to avoid storing the individual object items as separate prop variables such that you do not need to redefine them together as a dict later on , but changes are due to a single element in the dict that you ' d like visibility on in react devtools . in rarer cases , it is unavoidable having props that are not nested objects . for instance , how could i preserve the ` . prototype ` key of my ` props ` object without react stripping it ? if i wrap my props inside an object , that key can be preserved . but now all <emphasis> my props are considered <number> prop to the profiler and i have no visibility on which prop changed . with this change , i could expand the tree and drill down to which individual values changed . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * new feature never before released in react devtools . i am using ` <number> . <number> `",1
facebook/react,"show property type of value ( string , int , etc ) on state / props . * feature <emphasis> * * * what is the current behavior ? * * on the new developer tool you are unable to see what property type the value . you used to be able to see if the value was a string or int because of the quotation marks ( for example id : "" <number> "" ( string ) or id : <number> ( int ) ) . both string and int are shown without quotes . * * what is the expected behavior ? * * i want to see if the value inside the prop or state is an string or integer by using quotation marks on the value . ` id : "" <number> "" ` * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * i am using version ( <date> ) did this work in previous versions of react ? yes , <number> . *",1
facebook/react,"devtools v4 : where is highlight updates ? if i understood correctly , this is the correct repository for devtools v4 , right ? i just noticed that react devtool were updated . i am missing the "" highlight updates "" function . how can i activate it ? [ image ] ( <url> ! [ image ] ( <url> version ( <date> )",1
facebook/react,"new devtools cannot expand obervables * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * devtools used to be able to expand observables created by mobx . with the new update that is no longer possible . [ image ] ( <url> * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * <url> check the props of the wrappedcomponent in devtools . store . things does not expand . * * what is the expected behavior ? * * devtools should expand observables as it does any other object . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * this only seems to be an issue with observables created by mob v4 . v5 works as expected .",1
facebook/react,"[ feature request ] finish / normalize portal api < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the expected behavior ? * * portals are in a weird state . the core ` react ` knows about them but you can only create them from other libs ( e . g . ` reactdom ` ) . you have to branch your code because they can ’ t be server - side rendered . and so on . this is just a pre - rfc to brainstorm ways that portals can become first - class citizens : * extend ` react . createref ( ) ` to allow an optional renderer - specific argument ( e . g . the dom element ) , matching ` useref ( ) ` * add ` react . createportal ( child , ref ) ` ( * note the use of ` ref ` rather than a e . g . a direct dom element <wink> * deprecate ` reactdom . createportal ( ) ` now with some thought into the structure of your app with modals , they could be e . g . server - side rendered with : ` ` ` function app ( props ) { const modal = useref ( null ) return ( <div> < modalcontext . provider value ={ modal } > <div> { props . content } </div> < / modalcontext . provider > < div ref ={ modal } / > </div> ) } function modal ( props ) { const modal = usecontext ( modalcontext ) return react . createportal ( props . children , modal , ) } / / somewhere in { props . content } tree . <repeated> return ( < > { visible & & ( <modal> hello , world ! </modal> ) } < / > ) ` ` ` after ` reactdom . createportal ( . <repeated> ) ` is deprecated , legacy web or those with no need to ssr portals can simply upgrade with this . el = react . createref ( document . createelement ( ' div ' ) ) ` ` ` for the same behavior . as an initial version , react can error if ` ref . current ` is null when it goes to mount the portal . some sort of dirty flag could be considered separately if proven necessary , but with the pattern proposed above , it shouldn ’ t be necessary as the portal host would always <emphasis> be mounted first as a natural consequence of how react works .",1
facebook/react,"allow reactnode as a type for the child of <option/> * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * currently , the options element only allows types number and string . * * what is the expected behavior ? * * an option should allow for a reactnode as a child in addition to a number + string . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * all versions . all browser types . to the best of my knowledge , no . p . s . this is my first feature request here , so let me know if i need to adjust the feature request in any way .",1
facebook/react,"provide withhooks hoc to decouple hooks and components < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * currently the recommended way to use hooks involves coupling them with components . by this i mean that components are aware of the hooks they consume and depend on them explicitly . this [ issue thread on the recompose repo ] ( <url> discusses this issue in some detail and how recompose favored keeping components dumb , but wraps them in hoc ' s to make them smart . hooks promote baking the smartness right into the component itself . * * what is the expected behavior ? * * react should offer a way to decouple components from the hooks they consume . i suggest a ` withhooks ` hoc that maps hooks to props . this will be a familiar model for those who have used redux with react . ` ` ` jsx const withhooks = maphookstoprops => wrappedcomponent => { return props => { let hookprops = maphookstoprops ( props ) ; return < wrappedcomponent { . <repeated> hookprops } { . <repeated> props } />; }; }; const counter = props => { return ( <div> <div> counter : { props . counter } </div> < button onclick ={ props . increment } > increment </button> < button onclick ={ props . decrement } > decrement </button> </div> ); }; const maphookstoprops = props => { let [ counter , setcounter ] = usestate ( <number> ); return { counter , increment : ( ) => setcounter ( prev => prev + <number> ) , decrement : ( ) => setcounter ( prev => prev - <number> ) }; }; const enhancedcounter = withhooks ( maphookstoprops ) ( counter ) ; ` ` ` demo : <url> some reasons why this is nice : <number> ) it decouples components from the things that make them smart . some examples of things that could make dumb components smart include hooks , redux , and good ol ' parent components . by mapping hooks to props , we make it very easy to swap a dumb component ' s hook - powered ' brain ' for a new ' brain ' , say a redux - powered ' brain ' . <number> ) not sure if it ' s a good idea , but i know several people trying to replace redux with hooks in their applications . for these people , migrations from redux to hooks would be dead simple because they could replace ` mapstatetoprops ` and ` mapdispatchtoprops ` with ` maphookstoprops ` and would not have to worry about touching the underlying component . <number> ) testing is also easier because we can test the component in isolation without the hooks baked in . <number> ) it makes prop overrides possible . in the case of our enhancedcounter , we could override the counter prop by doing ` < enhancedcounter counter ={ <number> } / > ` . a real world example of hooks making things harder to override includes material - ui ' s [ new styling approach via hooks ] ( <url> because classes are provided via hook and no longer via props , we would need custom logic to override classes via props with the new hook - based approach export default function hook ( props ) { let classes = usestyles ( ); classes = { . <repeated> classes , . <repeated> props . classes } ; return < button classname ={ classes . root } > hook </button> ; } ` ` ` * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * for versions of react >= <number>",1
facebook/react,"feature request : export of reactdom libraries < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * it ' s a feature <emphasis> . * * what is the current behavior ? * * [ internal shared libraries of reactdom ] ( <url> is not exported . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * n / a * * what is the expected behavior ? * * it is great that some shared libraries such as [ ` iscustomcomponent ` ] ( <url> or [ ` domnamespaces ` ] ( <url> are exported from ` react - dom ` and available externally . they are useful to know what types of html tags are regarded to be valid one by react , for example , with static analysis issued at <url> * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * n / a",1
facebook/react,"add option in shallow renderer to run effects / componentdidupdate / componentdidmount * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * the shallow renderer does not run componentdidupdate , componentdidmount , or useeffect functions . ( i will call them effect functions for short ) * * what is the expected behavior ? * * see [ this enzyme issue ] ( <url> for more details about where this request is coming from . but the general idea is that it is often nice to run effect functions even when shallow rendering , rather than having to use full rendering on those specific tests . if the shallow renderer provided an option to run the effect functions , it would allow people who test with shallow rendering to more easily test their components . enzyme currently supports this in class components by calling componentdidupdate / mount directly on the component instance , but this would be a much harder thing to do for hooks , since they are usually anonymous . enzyme used to not allow this at all , but then added an option to turn on this behavior in their shallow renderer , before finally turning it on by default and then adding an option to turn it off . it did not seem to cause too many issues for them , so i think this approach could work well for the react shallow renderer as well . obviously i am only asking for an option to turn it on now , not to change the default or anything . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * all and no it was never supported afaik",1
facebook/react,"using context to pass down mount order to children with concurrent * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature / use case * * what is the current behavior ? * * it seems currently there is no way to get the order of children from the children themselves with context . * * what is the expected behavior ? * * i have run into this now in three use cases when building a style system : breadcrumbs , segmented views , and text rhythm / spacing . example <number> , breadcrumbs . you want to show an arrow on all but the last breadcrumb , but they may be deeply nested : ` ` ` ts let breadcrumborder = createcontext ( { index : - <number> , total : - <number> } ) let button = props => { let order = usecontext ( breadcrumborder ) return < div style ={{ borderright : order . index > total ? ' 1 px solid red ' : ' none ' } } { . <repeated> props } / > } let myview = ( ) => { return ( <breadcrumbprovider> < button / > < button / > <div> < button / > </div> </breadcrumbprovider> ) } ` ` ` where myview should provide the ordering so that the sub - views can access ` total ` and ` index ` and properly style . the other use cases are basically identical , but for different patterns . one is for joining together buttons that are in a row in the interface ( segmented ) , and the other is for collapsing margins when you have text elements in a vertical column . really this ticket encompasses more of a question or request for documentation clarity here . i am not sure how it is not pre - concurrent , but i am assuming mount - order will be non - deterministic if not now then shortly . is there any reference to a pattern that works for this use case ? namely children can access their mount order / total children . i can do it now usereducer / context , but i have seen it mount in a weird order at least once and think it was due to a suspense type thing , and would be curious a better practice for this .",1
facebook/react,"[ usecontext ] throw error if ' usecontext ' is used outside function components * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature ( need better errors ) * * what is the current behavior ? * * consider the following functional component ` ` ` import react , { usecontext } from "" react "" const myfunctioncomponent = props => <div> hello usecontext </div> ` ` ` the immediate reaction for most of us ( newbies to hooks ) to refactor the above code to accomodate ` usecontext ` is as follows ` ` ` import react , { usecontext } from "" react "" import mycontext from "" . / mycontext "" / / react does not throw error const { mycontextvalue } = usecontext ( mycontext ) const myfunctioncomponent = props => <div> hello usecontext - { mycontextvalue } </div> ` ` ` the way to actually refactor is to explictly convert the arrow function return expression into a function body and then accomodate ` usecontext ` inside along with a return statement , like this ` ` ` import react , { usecontext } from "" react "" import mycontext from "" . / mycontext "" const myfunctioncomponent = props => { const { mycontextvalue } = usecontext ( mycontext ) return ( <div> hello usecontext - { mycontextvalue } </div> ) } ` ` ` not only , react * * does not throw error * * , react app actually compiles , while the component in question fails to load with no information . this is very difficult to pin the reason to this specific issue . * * what is the expected behavior ? * * react should ideally throw some kind of error , when ` usecontext ` is used outside of function components . this lack of error really bites us for people who are refactoring function components without a return statement . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react ` <number> . x ` with hooks support",1
facebook/react,"[ eslint ] hardcore rule allowing default hooks using only inside custom ones < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * there is no rule * * what is the expected behavior ? * * we have some hardcore plugins like ` eslint - plugin - lodash - fp ` . why not have eslint hardcore rule allowing default hooks using only inside custom ones ?",1
facebook/react,"pass dependencies to ` usememo ` callback as arguments < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * the ` usememo ` factory function does not receive any arguments . * * what is the desired behavior ? * * the ` usememo ` factory function would receive the dependencies as arguments . * * why ? * * this would allow more compact syntax for memoizing components because of implicit returns and desctructuring . this came to mind after experiencing some of the issues in # <number> . there may be other potential use cases too * * example of current behavior * * ` ` ` jsx const avatar = ( ) => { const [ src ] = usesomeglobalstate ( [ state => state . user . avatar . src ]); return usememo ( ( ) => < img src ={ src } / > , [ src ] ) } ` ` ` * * example of proposed behavior * * ` ` ` jsx const avatar = ( ) => usememo ( ( src ) => < img src ={ src } / > , usesomeglobalstate ( [ state => state . user . avatar . src ] ) ); ` ` ` * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> . <number>",1
facebook/react,"unhelpful warning for ` act ` for react - dom <user> . <number> < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature / improvement * * what is the current behavior ? * * if there is test code that should be wrapped in ` act ( . <repeated> ) ` then the current warning is given : ` ` ` console . error node_modules / react - dom / cjs / react - dom . development . js : <number> warning : an update to null inside a test was not wrapped in act ( . <repeated> ) . when testing , code that causes react state updates should be wrapped into act ( . <repeated> ) => { /* fire events that update state */ }); /* assert on the output */ this ensures that you are testing the behavior the user would see in the browser . learn more at <url> ` ` ` when upgrading a large code base , this is basically useless . * * what is the expected behavior ? * * provide at least the test name if not the line number of code that triggered the warning . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <user> . <number> react - dom <user> . <number>",1
facebook/react,"adding if directive if you added if directive to any element for showing the element or not that would be better than making a js expression in my opinion * * regular way * * ` ` ` js const app = ( props ) => { reactif ={ true } let name = props . name ; return ( < div > { name = = = ' koko ' ? < div classname = "" yousef "" > { name } </div> : null } </div> ) } reactdom . render ( < app name = "" koko "" / > , document . getelementbyid ( ' app ' ) ) ` ` ` * * my way * * ( i edited the react file btw and it worked ) ` ` ` js const app = ( props ) => { let name = props . name ; return ( < div > < div classname = "" yousef "" reactif ={ name = = = ' koko ' } > { name } </div> </div> ) } reactdom . render ( < app name = "" koko "" / > , document . getelementbyid ( ' app ' ) ) ` ` ` * * m <elongated> * * i hope you talk that into consideration i mean less than 1 kb will not make difference : "" d * * react code * * ` ` ` js var reserved_props = { key : true , ref : true , __self : true , __source : true , reactif : true }; if ( config = null ) { if ( hasvalidref ( config ) ) { ref = config . ref ; } if ( hasvalidkey ( config ) ) { key = ' ' + config . key ; } / / i added that if ( config . reactif ) { reactif = config . reactif if ( reactif = = = false ) { return null } else if ( reactif ! = = true || reactif ! = = false ) { console . error ( ' reactif expression didn \ \ ' t return bolean value ' ) } } self = config . __self = = = undefined ? null : config . __self ; source = config . __source = = = undefined ? null / / remaining properties are added to a new props object for ( propname in config ) { if ( hasownproperty <money> . call ( config , propname ) & & ! reserved_props . hasownproperty ( propname ) ) { props [ propname ] = config [ propname ] ; } } } ` ` `",1
facebook/react,"support proxy as child < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * objects are not valid as a react child * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * n / a * * what is the expected behavior ? * * i am trying to build a system that auto - detects if data is used in a react component . to do this , i detect usage during ` render ( ) ` by using proxies , which can register all access . this data , in turn , is used to prevent needless re - renders . a parent component can pass a proxy which represents a string , for example , to a child component , which , without knowing it is a proxy , can use this value in a calculation ( e . g . ` props . value + <number> ` or ` ` ` the value is ${ props . value } ` ` ` ) . this can be handled with ` proxy [ symbol . toprimitive ] ( ) ` , which is called by js automatically when used in this sort of expression . however , this does not work if the child now passes the proxy directly to react as a child : ` ` ` jsx return ( <span> { this . props . childvalue </span> ); ` ` ` react will do a ` typeof ` on the child , find it is an object , and report ` objects are not valid as a react child ` . unfortunately , ` typeof ` cannot be spoofed with proxy , and it will always return ` ' object ' ` . the relevant code is found [ here ] ( <url> the alternative could be for react to check if there ' s a ` child [ symbol . tpprimitive ] ` and , if so , evaluate it to find the intended value , and to retry with this value as the child . it could be as simple as inserting the following code [ here ] ( <url> if ( typeof symbol ! = = ' undefined ' ) { const toprimitive = children [ symbol . toprimitive ] ; if ( toprimitive ) { const value = toprimitive ( ' string ' ); if ( typeof value ! = = ' object ' ) { return traverseallchildrenimpl ( value , namesofar , callback , traversecontext , ) } } } ` ` ` it would be great if react would support this use - case . the alternative is for parent components to resolve the value , which would attribute the access to the parent component , instead of the child , where it ' s really being used . this in turn would cause the parent to be rerendered when only the child is being changed . if the child is aware it may be receiving proxy objects , it can resolve the value itself . however , the whole point of using proxies is to automate all the refresh logic ( like ` shouldcomponentupdate ` ) . also , not needing to resolve the value would allow the value to propagate through to descendant components which may be wholly unaware of the proxy . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * all version , never worked before .",1
facebook/react,"make it easier to debug when context uses defaultvalue accidentally due to no provider i just spend several hours debugging app blaming everything except me ofc . i am using this usetheme hook . ` ` ` ts import react from ' react ' ; import themecontext from ' . <repeated> / contexts / themecontext ' ; const usetheme = ( ) => { const theme = react . usecontext ( themecontext ) ; / / if ( theme = = null ) / / throw error ( ' usetheme provide themecontext value . ' ); return theme ; }; export default usetheme ; ` ` ` some styles were light while other dark . very strange . then i found the bug in my code , ` themecontext . provider ` was sometimes used after using usetheme . themecontext had an initial value different than provided . while it ' s probably fine that react allows us to use default context value without a parent provider , it can lead to hard to find bugs . therefore , i decided to never provide default context value and throw an exception in usefoocontext hook to warn about it . because of <sad> , react should reconsider default / initial context values . in my humble opinion .",1
facebook/react,"eslint - plugin - react - hooks should report errors inside unnamed functions i want to report a bug for the hooks plugin . * * what is the current behavior ? * * there was no error report after running eslint , but the component failed when running in the browser . from the chrome dev console it reported "" uncaught error : rendered fewer hooks than expected . this may be caused by an accidental early return statement . "" * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . here is a link to the github repo : <url> * * what is the expected behavior ? * * followed the hooks api guide which says react hooks provides a linter plugin to enforce these rules automatically . therefore it should have reported a usage violation when the eslint hooks plugin is specified . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * using window <number> os and chrome browser . <number> . first ran npx create - react - app hooks <number> . installed react <number> . <number> - alpha . <number> and react - dom <number> . <number> - alpha . <number> <number> . installed eslint dev dependencies : "" babel - eslint "" : "" <number> . <number> "" , "" babel - loader "" : "" <number> . <number> "" , "" eslint "" : "" <number> . <number> "" , "" eslint - config - airbnb "" : "" <number> . <number> "" , "" eslint - loader "" : "" <number> . <number> "" , "" eslint - plugin - import "" : "" <number> . <number> "" , "" eslint - plugin - jsx - a11y "" : "" <number> . <number> "" , "" eslint - plugin - react "" : "" <number> . <number> "" , "" eslint - plugin - react - hooks "" : "" <number> . <number> "" <number> . created the . eslintrc . json following the instructions from the hooks api doc then ran package script lint as follows : "" npm run lint "" no errors reported . then ran package script start as follows start "" the react component counthooks calls usestate incorrectly and reports error in the browser dev console .",1
facebook/react,"feature idea : useerror hook * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature idea * * what is the current behavior ? * * currently there is no hook for dealing with errors in a component ' s sub - tree . * * what is the expected behavior ? * * have a hook that allows for functional components to act as error boundaries . example function myerrorboundary ( ) { const caughterror = useerrorcatching ( ); if ( caughterror = = null ) { return < errorhandler error ={ caughterror } />; } return < regularcontent />; } ` ` ` * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * n / a",1
facebook/react,react - is memo feature request * * what is the current behavior ? * * ` react - is ` now does not have method to check if element is ` memo ` ( like ` isforwardref ` ) . maybe there are some reasons why it ' s not implemented ?,1
facebook/react,"support reporting suspense loading indicator outside of the suspended tree cryptic title i can imagine , but i am not aware that something like this would have been mentioned anywhere so far . i have a page showing some statistics and it ' s split into two even panels . the left panel is showing some numbers and contains a form to set necessary filters . the right panel is showing some other details about filtered data . initially , only filter form is visible , nothing else . the user sets the filter and hits the "" filter "" button to send out a request . there is a requirement to show a text loader in the left panel and the right panel should be showing content loader animation . too many loaders perhaps ? well , it kinda makes sense in this context <happy> now my confusion is how to achieve that . obviously , i do not want each panel to run the same query on its own . i would like to do that once in the upper level . i can surely pass down the ` isloading ` prop to both panels . however , i am not too happy about it , because once there will be a fully fledged data fetching relying on the suspense , it would mean that for such scenarios i will need to fall back to a regular solution . am i misunderstanding something in here ?",1
facebook/react,"synthetic keyboardevent should support keyboardevent . code * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * the current synthetic keyboard event does not support the [ ` keyboardevent . code ` ] ( <url> property . * * what is the expected behavior ? * * the synthetic keyboard event should pass along the [ ` keyboardevent . code ` ] ( <url> property . this is currently in the wd of dom events but is part of replacing ` keycode ` and ` charcode ` and is much more consistent and easy to use . this is currently only supported by ff and chrome ( [ caniuse ] ( <url> so it may be a bit premature to fully integrate . however ` keycode ` , ` charcode ` and ` which ` are being deprecated so this will eventually need to be added . edit spoke too quickly , caniuse shows that ff , chrome , safari and opera support it . ie , edge and most mobile browsers do not .",1
facebook/react,"provide a way to trigger useeffect from tests hello , i tried testing components that use the cool new hooks api , but ` useeffect ` does not seem to work with the test renderer . here ' s a small failling jest test : ` ` ` js import react , { useeffect } from "" react "" ; import { create as render } from "" react - test - renderer "" ; it ( "" calls effect "" , ( ) => { return new promise ( resolve => { render ( < effectfulcomponent effect ={ resolve } />); }); }); function effectfulcomponent ( { effect } ) { useeffect ( effect ) ; return null ; } ` ` ` and here ' s a minimal reproducing repo > note that other _use_ apis seemed to work ( eg . ` usecontext ` ) .",1
facebook/react,"<number> contexttype + getderivedstatefromprops * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * context not passed into getderivedstatefromprops static getderivedstatefromprops ( props , state , context ) { } ` ` ` just curious with the new ` static contexttype ` , it would save a lot of nesting if i could access context now from getderivedstatefromprops when using this pattern . i gave it a shot assuming it may work already but i get undefined from the third argument . just curious if there ' s been any discussion on this .",1
facebook/react,"conditional components < - - note : if the issue is about documentation or the website , please file it at : <url> - - > feature * * what is the current behavior ? * * often we will use expressions like this : ` { ! x ? null title ={ x . title } / > } ` * * what is the desired behavior ? * * we would like to have expressive components such as ` ` ` < when c ={ x}> < component title ={ x . title } / > </when> ` ` ` in many cases this generic syntax is preferred over an explicit child component or pure function which knows about what it is supposed to render . the issue is that this will evaluate the children , even if they ultimately are not returned ( since they are passed as children to the when component ) . what is desired is for the syntax above to be able to behave exactly like a conditional expression , in that the contained children are not actually processed at all unless a condition is met . in reality the component could be something else entirely such as ` < superuser / > ` which will only process and render the children if the current user is a super user , or ` < morning / > ` to only process and render items between <number> am and <number> am ( or similar ) . the key thing is that we do not want the props passed into the children , or the children themselves produced , unless a condition has been satisfied . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * presumably any version of react .",1
facebook/react,"is it possible to use profiler server side ? i am trying to use the profiler server side rendering with ` rendertostring ` but the onrender callbacks are not getting called . is there a way to do it ? i am using <number> . <number> , node_env = = = "" development "" . my code looks like this , works fine client side : ` ` ` javascript import react , { unstable_profiler } from ' react ' ; const profilercallback = ( id , phase , actualtime , basetime , starttime , committime ) => { console . log ( ` ${ id } ' s ${ phase } phase : ` ); console . log ( ` actual time : ${ actualtime } ` ); console . log ( ` base time : ${ basetime } ` ); console . log ( ` start time : ${ starttime } ` ); console . log ( ` commit time }; const mycomponent = ( ) => ( < profiler id = "" card "" onrender ={ profilercallback } > . <repeated> </profiler> ) ` ` `",1
facebook/react,"cache provider : add hooks to read and preload in dev mode this topic originally came up in a ` react - devtools ` discussion * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * the package ` simple - cache - provider ` does not presently provide hooks to understand when we have pulled a resource for the first time , hit the cache , or dropped something from the cache due to ` max_size ` . due to this , tools such as ` react - devtools ` cannot provide an interface around our cached resources which makes debugging / inspection harder . * * what is the expected behavior ? * * it would be great if there was a way to expose callbacks / events for when the cache resource has resolved . an idea would be to fire these callbacks / events in the existing switch statement in ` read ` / ` preload ` if we are in ` __dev__ ` mode . with this information we could have a "" redux - devtools "" - esque interface to better understand where our data is coming from and when we are hitting the cache in dev mode . tools like this would also be useful to people new to the suspend api to visually see their resources transition between the various record states .",1
facebook/react,"provide a ` testinstance . context ` property to facilitate context testing < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature request * * what is the current behavior ? * * ` testinstance ` provides a ` props ` property , but does not provide a ` context ` property allowing for context tests . * * what is the expected behavior ? * * ` testinstance . context ` should return an object containing the instance ' s current context * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * at least <number> . <number>",1
facebook/react,"ability to use return value of react . children . map with react . children . only as stated in the docs and shown in # <number> , the return value of ` react . children . map ` is incompatible with the ` react . children . only ` function , and will return with the error message : > invariant violation : invariant violation expected to receive a single react element child . if used . this incompatibility causes some fairly annoying restrictions , however . from what i understand , this means that you can not edit the properties of a component ' s children at runtime if they contain any component that requires a single child . ( eg . you can not dynamically set the disabled prop of a child ` touchablenativefeedback ` component in the parent ) . i believe this is due to the type differences between the ` this . props . children ` parameter and the ` react . children . map ` return value , the former being a valid element and the latter not ( see # <number> and <url> i ' d like to make a request to have the return type of ` react . children . map ` be the same as the type of ` this . props . children ` , which will fix these quirks and be much more intuitive . a separate static function that converts the return type of ` react . children . map ` to the same type as ` this . props . children ` would also solve the issue . it feels like some sort of fix is in order , since it ' s a broad and strange restriction on the components you can use .",1
facebook/react,"identify different instances of react component in performance measures < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * * feature <emphasis> * can fiber . _debugid also be part of performance measure [ label ] ( <url> along with component name / displayname ? to distinguish / track multiple instances of a react component in performance measures , please let me know if there is any other way already available to achieve this .",1
facebook/react,"add a way to compare relative positions of deep children hey , it seems like there is currently no way to take two mounted component instances and tell which one of them is coming earlier in the application structure ( they could have been mounted asynchronously , and tracking the instantiation / render / mount time is not enough ) . ideologically i cannot traverse the application tree , that is understandable . though , the relative positions are needed sometimes . my use case — i track focusable elements in the application using context ( each focusable element reports of its existence to a focus manager which is provided by the context ) . this is needed to be able to limit the focusablility / accessibility of all elements which are outside of the currently shown modal dialog / popup so that the focus is trapped inside . for the sake of better accessibility , i need to automatically focus first focusable in the modal dialog when i am in the keyboard navigation mode . so , i have the references to all focusable elements inside the dialog , but i cannot tell which one comes first using public react api . for web there is a workaround to finddomnode and comparedocumentposition ( ) , but that does not work with react native . can we have something similar to comparedocumentposition ( ) but for react component instances ? thanks",1
facebook/react,"synthetic ( keyboard ) events do not implement the . code property * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * ` <event> . code ` is undefined * * what is the expected behavior ? * * ` . code ` is a very useful part of the keyboardevent spec * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * i am on react <number> , but i saw no mention of this in today ' s <number> changelog .",1
facebook/react,"provide hoc for new context api i find myself needing a higher - order component every once in a while when using render props , so i can get stuff from context in my lifecycle methods . it ' s pretty easy to create a hoc from a render prop , so i was wondering how open you all would be to adding a hoc to the new context api ? we can already do this in userland with a little ` withcontext ` helper : ` ` ` js function withcontext ( context , component ) { return props => { return ( < context . consumer > { context => < component { . <repeated> props } context ={ context } / > } < / context . consumer > ); } } const appwithcontext = withcontext ( mycontext , app ) ; ` ` ` it would be useful to have this built - in to the new context api , something like : ` ` ` js const mycontext = react . createcontext ( ); const appwithcontext = mycontext . provide ( app , ' optionalnameoftheprop ' ); ` ` ` the second argument to ` provide ` ( the prop name ) could default to ` context ` . anyway , just thought i ' d open this up for discussion before making an actual pr that adds this . thanks for your consideration 😅 [ edit example using ` this . context ` ]",1
facebook/react,"cursor jumps to end of input when onchange does not call setstate [ edit ] : * * i am asking for a feature * * . * * current behaviour * * an input ` onchange ` function that returns a value equalling the prior value causes the cursor to jump to the end of the input . this is the same as [ this comment from # <number> ] ( <url> formally raised as a feature request . repro sandbox : <url> that same code : ` ` ` javascript import react from "" react "" ; import { render } from "" react - dom "" ; class input extends react . component { state = { value : "" typeanumber "" }; onchange = e => { let nextvalue = e . target . value ; if ( / [ <number> - <number> ] / . test ( nextvalue ) ) { nextvalue = this . state . value ; } this . setstate ( { value : nextvalue }); }; render ( ) { return ( < input type = "" text "" value ={ this . state . value } onchange ={ this . onchange } / > ); } } render ( < input / > , document . getelementbyid ( "" root "" )); ` ` ` * * what is the expected behavior ? * * i ' d like the cursor not to jump in the special case where the returned changed value is a rejected change i . e . the ' noop ' change . i understand fully that react cannot predict cursor position if the value is _changed_ in ` onchange ` , ~ however i cannot currently find an npm module that allows free - length regex filters ( vs a fixed length mask ) ~ or a way to implement a filter myself , without the cursor jumping in this case . [ edit ] raising i now fully see this as a feature request for handling a special case of a behaviour that indeed is not a bug , differently . it would be a nice to have as it would allow very straightforward implementation of filters . regarding the non - clarity of how to deal with the general case of non - jumping cursors i think a modernized best practice example would be ideal , but that discussion still lives at # <number> . i ' d be totally fine with this issue being closed by assisting instead with the education of handling the general case . though , this would still be a nice to have for the api , if possible .",1
facebook/react,"allow to specify displayname for createcontext ( ) providers and consumers < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * in react tree name of a context must be like its name in code * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * ` ` ` js const mycontext = react . createcontext ( null ) ; ` ` ` ` ` ` js < mycontext . consumer > { data => . <repeated> } < / mycontext . consumer > ` ` ` let us have a look at react tree in chrome extention ' s page here is context - not mycontext * * what is the expected behavior ? * * expexted to see mycontext * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number>",1
facebook/react,"do not call getderivedstatefromprops on a purecomponent if props are the same ? < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * ` getderivedstatefromprops ` is called on a purecomponent even if the props have not changed * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * as is * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * all i was hoping ` getderivedstatefromprops ` would not be called on a purecomponent if the props had not changed . any reason why this should not / could not be the case ?",1
facebook/react,"support hydration after html minification * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * report a bug * * what is the current behavior ? * * ` react . hydrate ` replaces the dom after a ssr page is served , and two sibling links in the page have the ` href ` attributes wrongly set . i did a little repl to replicate this behaviour [ here ] ( <url> when the server responds , the html is correct : [ screen shot <number> - <number> - <number> at <number> <number> <number> ] ( <url> but right after hydration , the first ` href ` is changed shot <number> - <number> - <number> at <number> <number> <number> ] ( <url> and whitespace artifacts are added . * * what is the expected behavior ? * * the first ` href ` should not be changed . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react v16 . <number> chrome <number> macos",1
facebook/react,"provide a better error on react . cloneelement ( null / undefined ) < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * * * what is the current behavior ? * * * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * ! [ image ] ( <url>",1
facebook/react,"issues with reactcontrolledvalueproptypes * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * see below . * * what is the current behavior ? * * <number> . ( bug / inconsistency ) ` ` ` jsx < input type = "" radio "" checked ={ false } / > ` ` ` no warning . <number> . ( feature request ) ` ` ` jsx < input type = "" radio "" checked ={ true } onchange ={ undefined } / > ` ` ` ` warning : failed prop type : you provided a ' checked ' prop to a form field without an ' onchange ' handler . this will render a read - only field . if the field should be mutable use ' defaultchecked ' . otherwise , set either ' onchange ' or ' readonly ' . ' ` <number> . ( bug ? ) ` ` ` jsx < select value = "" foo "" readonly ={ true } > . <repeated> </select> ` ` ` no warning . * * what is the expected behavior ? * * <number> . passing a falsy ` value ` or ` checked ` attribute will not trigger a warning , but a truthy value does . <number> . if onchange is passed as undefined ( or null ? ) this should be considered as an acknowledgement and silence the warning . i have a case where i split the render method from the component and use it as a preview . when the component is interactive i use the component . when doing a preview i pass undefined as my change handler . a warning is shown to tell me i "" forgot "" it , but i intended it to be this way . <number> . going along with <number> , i can pass ` readonly ={ handlechange } ` , but according to definitelytyped this is not a valid attribute for select . i can not use disabled because it changes the appearance of the field . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number> chrome <number> * other <emphasis> * a proposed "" fix "" would be to change : <url> to : ` ` ` js if ( ! ( propname in props ) || / / fixes <number> hasreadonlyvalue [ props . type ] || "" onchange "" in props || / / fixes <number> props . readonly || props . disabled ) { ` ` ` and : <url> to if ( ! ( propname in props [ propname ] ) || / / fixes <number> "" onchange "" in props || / / fixes <number> props . readonly || props . disabled ) { ` ` `",1
facebook/react,"production reconciler instrumentation _apologies if this is documented somewhere , but i was unable to find anything related after a pretty exhaustive search of docs + code . _ are there any production instrumentation hooks for the reconciler ? specifically , i am looking for callbacks / events that would allow me to track overall reconciliation time spans ( nothing more granular ) . e . g . equivalent to the ` ( react tree reconciliation ) ` span . as best i can tell , there are ` performance . timings ` spans reported as of fiber ( and ` reactperf ` prior ) , but those are only enabled in development mode .",1
facebook/react,"does react still require non - toplevel submit handler ? * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug ? maybe . * * what is the current behavior ? * * using non - delegated handler for submit event . * * what is the expected behavior ? * * after ie9 , at least i know , submit event bubbled up . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number>",1
facebook/react,"extracting a context stack a useful feature of context is creating a custom stack of things to see what your component is embedded in side . i think the primary use case is logging explicitly . currently that is pretty expensive to maintain just * in case * you need it . see # <number> as an example . we could provide an api that lazily extracts a whole path of contexts from the tree . ` ` ` js < foocontext . provider value ={ "" foo "" } > < foocontext . provider value ={ "" bar "" } > < foocontext . provider value ={ "" baz "" } > < app / > < / foocontext . provider > < / foocontext . provider > < / foocontext . provider > ` ` ` ` ` ` js class app extends react . component { log ( ) { var stack = this . getcontextstack ( foocontext ) ; logtoserver ( stack ) ; / / [ "" foo "" , "" bar "" , "" baz "" ] } render ( ) { return < div onclick ={ this . log } />; } } ` ` ` it would basically synchronously rerender the shortest path to recreate the context at the time of the call to create the stack lazily . cc <user>",1
facebook/react,"provide a way to perform a synchronous render into another root during the commit phase * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * reentrancy checks prevent synchronous ` reactdom . render ` in a nested react component . this used to work before react <number> , and [ seems related to this issue about nested reactdom renders ] ( <url> * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * here ' s a [ jsfiddle ] ( <url> that documents the problem , with a simulation of the external dependency where this manifests . * * what is the expected behavior ? * * i am running into what i think is a [ similar problem to this one ] ( <url> with a nested ` reactdom . render ` , except where the difference is that i do not think we can use portals to address our use - case . we have a component which manages the dom tree for all nodes below it outside of react — it ' s a contenteditable node and uses the best - in - class [ prosemirror ] ( <url> library to manage its children . the component looks something like this : ` ` ` javascript class prosemirror extends component { componentdidmount ( ) { / / prosemirror manages the dom for all nodes below this . el . } setref ( el ) { this . el = el ; } render ( ) { return ( < div ref ={ this . setref } />); } } ` ` ` as part of its render cycle , our configuration of prosemirror ends up calling : ` ` ` javascript reactdom . render ( < crucialsubcomponent / > , somedivmanagedbyprosemirror ) ; ` ` ` to render an isolated child node of ` < prosemirror / > ` , and wants to be able to immediately afterwards be able to leverage this . el . queryselector ( ' . my - subcomponent ' ) ` ` ` . <repeated> but this piece of the dom is no longer available synchronously , and it looks like this is because of the re - entrancy change that came about in react <number> . portals do not work for us , because the site where the ` reactdom . render ` is being called is not itself directly part of the root react tree ( this is [ clearer to observe in the fiddle ] ( <url> the hierarchy is something like ` < prosemirror / > - - - > ( opaque prosemirror rendering code ) - - > < crucialsubcomponent / > ` . is there a way to skip these re - entrancy checks in these cases where there ' s an isolated react render happening in a grandchild of a component , but where the react tree is not the immediate parent ? * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * querying the dom immediately after reactdom . render worked in versions prior to react <number> . we are excited about the async possibilities for our main react tree , but curious if there are workarounds where we can ignore the reentrancy checks for these isolated renders .",1
facebook/react,"add oldprops as additional argument to getderivedstatefromprops ? < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * ` getderivedstatefromprops ` only receives the nextprops and previousstate as arguments . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * the deprecated ` componentwillreceiveprops ( nextprops ) ` used to allow code like ` this . props . foo ! = = nextprops . foo ` . with the new ` getderivedstatefromprops ` function , there ' s no choice ( because it is a static method ) but to constantly copy ` nextprops . foo ` into state in order to access it later . this is illustrated in the example posted to twitter by <user> : <url> * * what is the expected behavior ? * * ideally ( if it ' s not difficult to implement ! ) the ` getderivedstatefromprops ` would also take the current ( previous / old ) props as an argument , something like prevstate , prevprops ) ` this would eliminate the need to constantly assign props to state purely for comparison purposes . <repeated> a quick look at the source does not make it clear to me how easy this would be though . <repeated> <url> * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number>",1
facebook/react,can react support feature like keep - alive in vue ? i found this issue and <user> said that react never reuses an instance after it ' s been unmounted . does it means that react will never support feature like keep - alive in vue ? or there is other way to maintain component ' s state ?,1
facebook/react,"have fragments support dangerouslysetinnerhtml the addition of the ` fragment ` in <number> . <number> is fantastic and helps keep our html semantic and clean . unfortunately there is still no way to inject html without a wrapping tag . ` ` ` jsx const html = <span> hello world </span> ; < div key ={ id } dangerouslysetinnerhtml ={ { __html : html } } / > ` ` ` which will render : ` ` ` html <div> <span> hello world </span> </div> ` ` ` it would be mostly helpful for rendering html from jsx on the back end rather than in the spa context . to me ` fragment ` seems to be the ideal candidate to support ` dangerouslysetinnerhtml ` so that you may inject html without wrapping elements . ` ` ` jsx const html = <span> hello world </span> ; < fragment key ={ id } dangerouslysetinnerhtml ={ { __html : html } } / > ` ` ` would render <span> hello world </span> ` ` ` simple , obvious and aligned with the current api .",1
facebook/react,"symbols as keys in children as arrays or iterators * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * i want to request a feature * * what is the current behavior ? * * using ` symbols ` as element keys throws a type error . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * when using ` key ={ symbol ( ' mykeysymbol ' ) } ` we get the following ` typeerror convert a symbol value to a string at object . reactelement . createelement ` [ codesandbox here ] ( <url> * * what is the expected behavior ? * * using ` symbols ` as keys should work seamlessly , in my opinion element keys are a perfect use - case for ` symbols ` . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * using react ` <number> . <number> ` and the browsers affected are safari , chrome and firefox on osx , but i am pretty sure this is not browser dependent but a matter of implementation . ~ ~ ~ thanks .",1
facebook/react,"add react . createref ( ) as the migration path for string refs creating this issue to track <url> i intend to close the pr as it ' s outdated , but we probably want to turn it into a real rfc and potentially get it in during <number> . x .",1
facebook/react,"lifecycle method to build initial state for classes * * feature request * * we need to have a way to build the initial state of reactcomponent in case of usage of classes . * * what is the current behavior ? * * warning in case this . state modified in willcomponentmount . access to partially constructed object in case of somewhat complex state building logic in the constructor . for example , if we have a hierarchy of classes with the _buildstate method called in the constructor to generate an initial state . derived class overrides the _buildstate method to have a richer state . in result , part of the object related to the derived class will not be constructed yet and can not be accessed in the _buildstate . the buildstate approach is currently used in resub framework * * what is the expected behavior ? * * rather no warning in case of this . state modification or new lifecycle method which is called right after constructor call which returns the state . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> + is affected .",1
facebook/react,"feature request : global state at the render / hydrate level this is a feature request . it relates to <url> this has been a pain point for me , and i am sure we can do better . the issue : how to have some state available to all components without passing everything down on props through the component tree . i am speaking about global state . so why not use context ? i have a couple of issue with it . <number> . it ' s not the root component ' s job to receive and disseminate state . the root component is just some component . it should not care if it ' s root or not . maybe you have ( example using express ) : ` res . render ( ' homepage . jsx ' , {}); ` but some other page might just as well have : res . render ( ' otherpage . jsx ' , { } ) where otherpage has : <div> < something / > < homepage / > </div> components can be composed however you like , they should not care what depth they are at . <number> . above is talking about a multipage app . the thinking around react seems to be so skewed toward to the special case of single page app . spa is sometimes the right choice . if you are google maps it ' s clearly right . if you are something broader in scope like amazon . com it ' s clearly not . the general case is much more interesting to solve . <number> . having every root component wrapped in some ` <provider> ` that publishes context downwards , and every non - root component wrapped in some "" receiver "" higher level component that declares context is just boilerplate . it ' s not expressive , it ' s bookkeeping . <number> . alternatives : there are not any as far as i know . each component only knows about the props ( and possibly context ) passed in , it does not have any handle on data for the current render ( ) call . simply importing global state through commonjs or es6 modules is impossible on the serverside for anything request scoped ( like query params , route params , cookies , headers , anything generated by middleware based on these things ) . - - proposal - - why not just handle global state at a higher level than props passed component to component ? why not extend reactdom . render to take a 4 th argument for global context ? it would just make everyone ' s life easier . do not even worry about changes firing componentwillreceiveprops , at least as a first version . think of it as analogous to express and the "" req "" object that ' s passed around . one should always have access to some "" per request "" object . so many things would be useful to put in there ( authenticated user , geolocation , "" store "" from redux , query and route params , etc , etc ) . it would look like : reactdom . render ( rootcomponent , somediv , callback , { . <repeated> request specific data . <repeated> } ) where request specific data might have things that are truly request specific and also things that are the same across requests . the latter can be done with imports but it ' s a bit messy - - you end up with code like ( typeof window = ' undefined ' ) { / / client , get value from window object } else { / / server , get value from filesystem or wherever } just having global data available to all components in the tree simplifies things a lot . do not you think ?",1
facebook/react,"add hooks to reactdomserver to support caching * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * react - dom ssr performance could be improved using server - side cache , but currently the reactpartialrenderer is currently not accessible from the ` react - dom ` package . * * desired behavior * * on the * server <emphasis> * only , it would be nice if _plugins_ could be used to improve render performance . currently the only way to do this would be to externally maintain a renderer implementation . however , the ` reactpartialrenderer ` already contains all the behavior required to support plugins - with a little refactoring . i have refactored the ` reactpartialrenderer ` and created a * * proof of concept * * for supporting _plugins_ for react server side rendering . you can see the [ reactpartialpluginrenderer ] ( <url> in this fork , its comprised of <number> different commits : * [# <number> ] ( <url> strict refactoring of ` reactpartialrenderer ` , the only addition is exporting the ` reactpartialrenderer ` from the react - dom server package * [# <number> ] ( <url> create the ` reactpartialpluginrenderer ` by extending the refactored ` reactpluginrenderer ` , and introduce a plugin interface * [# <number> ] ( <url> of concept plugin implementations and application example . you can [ view instructions for running the example in the repo ] ( <url> i understand that exporting ` reactpartialrenderer ` exposes the internal api , which is far from ideal . is there any scenario in which ` reactpartialrenderer ` would be made to be accessible from the ` react - dom ` package ? or would a plugin implementation similar to above be required to maintain its own forked ` reactpartialrenderer ` ? maintaining the plugin renderer in its own repo is not a problem , but it would be great if plugins could be used without needing to maintain the core server renderer . thanks , adam .",1
facebook/react,"stopimmediatepropagation is not available for synthetic events in order to call this method , you ' d have to access it via ` event . nativeevent . stopimmediatepropagation ( ) ` : <url> it ' d be awesome if this was supported on ` event ` itself . i did see a [ prior issue ] ( <url> for this , but it was for a use case that sip wasn ' t required for . for a specific use case you want to kill hover events for touch input , ` ontouchstart ` will trigger ` onmouseover ` and ` stopimmediatepropagation ` cancels that internal cascade .",1
facebook/react,"add a way to opt out of user timing api calls * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug or v16 feature ( dont know ) * * what is the current behavior ? * * when building in dev environment , performance timeline measures appear by default , ie . without ` ? react_perf ` query string as in v15 described [ here ] ( <url> ` react <user> . <number> ` if it ' s not a bug , then what ' s the way to disable ` react ` perf measures ? need a way to clear timeline to focus on my own custom perf measures .",1
facebook/react,"release a tool for statistical perf analysis ( a replacement for reactperf ) * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * a feature * * what is the current behavior ? * * n / a * * what is the expected behavior ? * * it ' s great that react <number> integrates with the timeline in browsers dev tools and that is very helpful in cases where you are trying to fix the performance of a specific thing . however i do miss the perf tool from previous versions and could not find any issue tracking a re - implementation of such a tool . what was great in the perf tool that is not covered as well with timeline integration ? - easily see which components render needlessly and therefore should be easily eliminated with scu ( and what impact it will give ) - easily see the collective render time of a component . i might for example have a component that renders very fast but which have very many instances and thus contribute to a significant render time anyway . - give an overview of which components are slow by themselves and which are fast by themselves but render slow components . the flamegraph shows this , but i find it to specific in some scenarios with too much detail that can distract . basically the wasted , exclusive and inclusive tables . the dom table i feel is much better represented by the timeline integration . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> , yes the perf addon in <number> was good .",1
facebook/react,"add ` code ` property to ` synthetickeyboardevent ` * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * improvement * * what is the current behavior ? * * [ ` synthetickeyboardevent ` ] ( <url> does not currently support the ` code ` property . ` code ` ( [ mdn ] ( <url> is nice to have when you want to write key - specific handling — rather than input - specific ( dependent on layout and modifier keys ) handling . * * what is the expected behavior ? * * ` synthetickeyboardevent ` already exposes a ` keycode ` property . it should have a ` code ` property as well . currently , if you want to use the ` keyboardevent ` ' s ` code ` , you must access it through ` synthetickeyboardevent ` ' s ` nativeevent ` . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * affects react <number> and earlier ( i assume ) .",1
facebook/react,"async componentwillreceiveprops to allow state update * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * an ( almost ) immediate re - render is due after ` componentwillreceiveprops ` is called , unless ` shouldcomponentupdate ` says otherwise . * * what is the expected behavior ? * * if new props in ` componentwillreceiveprops ` cause an async call that ' s soon going to update the state anyway , will not it be cool if react might as well wait for that async call to do it ' s thing ( which calls ` setstate ` ) and do one render instead of two ? potential solution can see if ` componentwillreceiveprops ` returns a ` promise ` . if it does it defers the re - render until it ` resolves ` . ` ` ` javascript async componentwillreceiveprops ( nextprops ) { const { postid } = nextprops ; const posttitle = await fetch ( ` <url> this . setstate ( { posttitle }); return ; } ` ` `",1
facebook/react,"react16 not compatible with x3 dom * * this is both bug / feature <url> i used to be able to use x3 dom with react . it was great . * * react ^ <number> i was able to use the * is <emphasis> * property to generate custom elements and custom attributes that could be picked up by x3 dom i . e . < shape is render = "" true "" / > now with react16 i get many warning messages for all of the custom x3 dom tag elements . likewise it seems that certain attributes are not getting rendered either . for instance if i do x3 dom elements like so size = "" <number> "" / > ` i will get ` <fontstyle/> ` output without the size attribute . i do not think react should have to know what x3 dom tags are , nor should they be hard - coded into react . there has got to be a way to have react output custom tags without throwing warning messages <elongated> . why not just re - introduce the * is <emphasis> * attribute to indicate that it ' s a custom tag with custom attributes ?",1
facebook/react,"loosen up type requirements for event handlers * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * when adding event handlers , it is common practice to do something like : ` ` ` js const mybutton = ( { canclick , onclick } ) => < div onclick ={ canclick & & onclick } > </div> ` ` ` this was fine in react <number> . x , but in <number> it reports a warning , which is technically correct : > expected ` onclick ` listener to be a function , instead got a value of ` boolean ` type . however , this now forces you to use the more verbose variant : ` ` ` js const mybutton = ( { canclick , onclick } ) => < div onclick ={( canclick & & onclick ) ? onclick ` ` ` * * what is the expected behavior ? * * i think it makes sense to allow ` null ` , ` false ` , and ` undefined ` in addition to function types for event handlers . or just anything "" falsy "" , although that may be too much to ask . i definitely understand the rationale from a type safety perspective , but this does make it less pragmatic . i am personally a huge fan of how js evaluates ` null ` , ` <number> ` , ` "" "" ` and ` undefined ` to ` false ` , and it reduces the amount of boilerplate needed to conditionally wire up handlers . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> , all browsers . this did not emit a warning in react <number> and below .",1
facebook/react,"production check in react - test - renderer when upgrading to <number> . <number> from <number> . <number> , react - test - renderer started failing on my tests with the message : > test renderer is not available in production mode . i have in fact been running unit tests using this renderer during my production build . i am trying to understand was this restriction introduced ? i saw the change was made in # <number> but could not find any explanation on the pull request or the documentation as to why production mode is now disallowed for this renderer . thanks , alex",1
facebook/react,""" did not expect server html to contain the text node "" due to whitespace in react <number> we have updated our react v15 application to v16 . everything seems to work fine instead the fact that this error appears : ` warning not expect server html to contain the text node "" "" in <div> . ` we are using reactdom . hydrate and our app was completely ssr ready in v15 . i have found an old issue on stackoverflow where someone wrote that this could be a problem with the markup which is send from server - > client , but as far as we can see the html code is the same without any markup problem .",1
facebook/react,"attach third - party tools to monitor component state updates * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature , that exists in previous releases of react * * what is the current behavior ? * * feature is not implemented * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> , worked in previous versions hey , we would really like to be able to use reactinstrumentation with fiber . i have found <user> ' s pull request , but work on it was stopped . would like to know why ? and if we can pick this up and implement it . <url> ( i have also asked why in the pull request , but figured it might get ignored since it is a closed pull request ) omer",1
facebook/react,"` select ` should warn if ` value ` is not available imo , ` ` ` < select value = "" foo "" onchange ={ . <repeated> } > < option value = "" yes "" > yes </option> < option value = "" no "" > no </option> </select> ` ` ` should warn because there is no "" foo "" option . it should probably also warn if ` value ` is not specified , because there is no "" empty "" option either . one could also argue that the change handler should be called with { value when the component renders and the first option is selected instead of an invalid / non - existing one .",1
facebook/react,accept ref objects as refs ? [ reason react ] ( <url> uses first class ocaml refs to store mutable values ( instead of on ` this ` instances ) . these are basically just an object with a mutable ` contents ` property . these can be updated with callback refs ` n => ref . contents = n ` but it would be a nice convenience feature to just have that built - in . we could also make these first class objects on isomorphic react . ` ` ` js react . createref = ( ) => ( { contents : null }); ` ` ` ` ` ` js class foo extends react . component { state = { mydiv : react . createref ( ) }; componentdidmount ( ) { if ( mydiv . contents ) { mydiv . contents . focus ( ); } } render ( ) { return < div ref ={ this . state . mydiv } />; } } ` ` ` basically the implementation would just be if ( typeof ref = = = ' function ' ) { ref ( newvalue ) ; } else if ( typeof ref = = = ' object ' ) { ref . contents = newvalue ; } else if ( typeof ref = = = ' string ' ) { owner . refs [ ref ] = newvalue ; } ` ` ` this is something that needs to be implemented in the core runtime and not as part of any particular component api since refs cross that boundary . cc <user>,1
facebook/react,"async top - level hook before commit _ ( this is not needed for <number> . ) _ i think we are missing a top - level api . if you want to integrate with non - react code around you in an async way , then you probably have some parents around you . it ' s not always the case that you want to show those parents before react is done . maybe you can hide them and then show them at the callback time . however , it would be better if you could start building the tree async with react , and then get a callback before <emphasis> we trigger life - cycles so that you can insert the tree into the actual dom and do whatever manipulation you need . only after that do we trigger the life - cycles . that way they will have the css and layout information available to them by virtue of being in the document already . i see two possible routes we just call out for this hook and then commit immediately after . b ) we invoke a callback and pass another function . that function , when invoked , does the actual commit . this approach has precedence in the dom with "" append async "" and offscreen canvas . the second option is probably preferable but we should only do that if we can do it efficiently and cleanly .",1
facebook/react,"[ feature suggestion ] publish react also as es2015 code * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature <emphasis> * * what is the current behavior ? * * react is published to npm only as es5 code * * what is the expected behavior ? * * publish react also in es2015 , with es2015 entry point in the package . json motivation leverage the targeted client native es features its part of the angular <number> package format <url> <url>",1
facebook/react,"nicer formatting of ssr validation the new validation in # <number> only issues a warn for the first difference found in a html hydration scenario . ideally it should instead queue up all the differences and then at the end ( commit ) issue a single warning with a nicely formatted diff . <number> ) instead of warning add [ these warn calls ] ( <url> to a global buffer ( array , map , set , whatever ) . <number> ) inside [ prepareforcommit ] ( <url> issue all the currently batched up warnings as a single message . <number> ) format that message in terms of a jsx diff in a nicely formatted way . with only the relevant nodes ( parent and child with changes ) . irrelevant child content can be replaced with ellipsis . e . g . ` ` ` . <repeated> < div classname = "" unchanged "" > - < div classname = "" foo "" / > + < div classname = "" bar "" > … </div> + < span / > </div> . <repeated> < div classname = "" another_unchanged "" > - < span / > </div> . <repeated> ` ` ` this strategy will not yield perfect results because if we are asynchronously hydrating , and it gets interrupted by another tree , we will flush a warning before the actual hydrating particular tree is flushed . so we might show a partial diff in that case . this is probably . it ' s just a warning .",1
facebook/react,"feature request : support server - side rendering of non - standard dom attribute names ( eg . amp ' s [ prop ] = "" value "" ) * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * request a feature i am working on project to build amp page with react server side rendering . i am having an issue to add custom attribute to built - in amp element . in order to be able to use [ amp - bind ] ( <url> we need to be able to output “ bindings ” , which are special attributes of the form ` [ attribute ] ` , eg . ` [ slide ] = "" selectedslide "" ` . ` ` ` < amp - carousel layout ={ layout } height ={ height } width ={ width } [ slide ] ={ slide } > . <repeated> < / amp - carousel > ` ` ` here is amp carousel example that work with [ amp - bind ] ( <url> * * what is the current behavior ? * * - parsing error token [ ( fatal ) * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * - all for more information , you can read all the discussion in this [ pr ] ( <url>",1
facebook/react,"feature request : rendertypes * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * request a feature per some discussion today with <user> and <user> , i ' d like a non - flow mechanism to annotate what type ( s ) of elements a component expects to render . here ' s some examples , with flow types for comparison ( that i realize may not be currently checked in flow , yet ) : ` ` ` jsx function foo ( { yes } ) { return yes ? < bar / > : < div />; } foo . rendertypes = [ bar , ' div ' ]; class bar extends react . component { static rendertypes = [ button ] ; render ( ) { return < button />; } } ` ` ` ` ` ` jsx function foo ( { yes }): react . element < bar | ' div ' > { return yes ? < bar / > : < div />; } class bar extends react . component { render ( <sad> react . element <button> { return < button />; } } ` ` ` inside <user> , we have lots of use cases where we have container components in a separate package - say , a ` <buttonrow> ` , and we have intentionally restrictive proptypes on its ` children ` prop , to only allow a ` button ` ( also in the same package ) . however , in an app that consumes this component library package , a dev may want to create a ` < specialproductbutton / > ` that in turn renders a ` <button> ` - however , they are unable to pass it into ` buttonrow ` ( our proptype warnings fail tests ) , even though conceptually it should be permitted . having ` . rendertypes ` would allow us to widen our ` children ` proptype to allow for either a ` <button> ` , or * anything that renders a ` <button> ` * , which helps us maintain separation of concerns ( the package does not have to know about ` <specialproductbutton> ` to accept it ) as well as maintain strictness ( the package does not have to allow any wacky element inside ` <buttonrow> ` ) . i imagine the implementation to be : <number> . when render ( ) is called or an sfc is invoked , ( in async rendering , it ' d be when the component resolves , i suppose ) <number> . in development only and if ` . rendertypes ` exists on the component <number> . evaluate the equivalent of [ ` elementtype ` ] ( <url> children } , ' children ' , . <repeated> ) ` , <number> . just like proptypes , log the error if one is returned ( cc <user> )",1
facebook/react,add react . children . find i think a ` react . children . find ` method would be really useful in situations where you need to iterate over your children and find just one that satisfies a particular condition . i am running into this situation more and more often in my react code . a few examples in [ react router ] ( <url> the ` <switch> ` component [ iterates over its children ] ( <url> to figure out which ` <route> ` matches the url . - in [ a ` <select> ` component we use in our training workshops ] ( <url> we need to iterate over the ` <option> ` s to figure out which label we should show in the select box . i wonder if there would be any interest from others in seeing something like this .,1
facebook/react,"warn when ` static proptypes / static defaultprops ` in es6 class is a function * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * ` ` ` class testwrongproptypes extends component { static proptypes ( ) { return { children : proptypes . string , missing : proptypes . string . isrequired }; } static defaultprops ( ) { return { children : ' default props via static function ' }; } render ( ) { return <p> { this . props . children } </p> ; } } ` ` ` in this example react will silently skip ` proptypes ` checking and default props setting for ` testwrongproptypes ` component . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem via <url> or similar ( template <url> <url> * * what is the expected behavior ? * * i know that in order to work , proptypes definition should be ` static get proptypes = { . <repeated> } ` or ` testwrongproptypes . proptypes = { . <repeated> } ` . but i ' d like to have warning like "" proptypes / defaultprops is function but should be either property or getter "" to prevent such errors . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * discovered in react <number> . x , but probably the same behavior will be also in earlier versions .",1
facebook/react,"[ rfc ] add traversal utility to reacttestutils matching totree shape # # # problem the ` reacttestrenderer ` now supports a new api , [ ` totree ` ] ( <url> which returns an easily traversed tree representing the internal state of the instance and its rendered nodes . the existing solution for this in ` testutils ` is ` findallinrenderedtree ` which traverses the root and lets you provide a test function it will call to reduce a tree based on a predicate . the predicate function is passed the public instance for the node . the issue with that is that it makes the predicate function polymorphic and requires every consuming utility to check if it ' s being passed a component instance or a dom node . # # # solution i propose that we implement a new api similar to ` findallinrendererdtree ` that calls the predicate function with the result of calling ` totree ` on the internal instance instead of the public instance . this would mean : * the predicate function will always be called with the same data structure ( a tree node ) * third - party traversal utilities can be shared between ` reacttestrenderer ` and ` reacttestutils ` , since they would operate on the same tree structure * you could actually assert on functional components , which currently have no instance and just call the predicate with ` null ` this new api could just be a pure traversal utility , leaving it up to the consumer to accumulate results . ` ` ` js import { renderintodocument , traversetree } from ' react - dom / test - utils ' ; var root = renderintodocument ( < app />); var results = []; traversetree ( root , node => { if ( somepredicate ( node ) { results . push ( node ) } } ) ` ` ` # # implementation with a brief review , it looks like we could just export the ` totree ` function that is currently inlined in ` reacttestrenderer ` and provide a way to change : ` ` ` js var publicinst = node . statenode ; if ( test ( publicinst ) ) { ret . push ( publicinst ) ; } ` ` ` to var treenode = totree ( node ) ; if ( test ( treenode ) ) { ret . push ( treenode ) ; } ` ` ` cc <user> <user> <user>",1
facebook/react,"expose dev - mode warnings in devtools ui react has made recent developer experience improvements to lifecycle error handling and logging . errors thrown during render can be recovered from using ~ ~ ` unstable_handleerror ` ~ ~ ` componentdidcatch ` . errors thrown during lifecycle methods are also automatically logged to the console with the component stack location to make them easier to identify ( see # <number> ) . the team has discussed similar improvements for warnings - such as using a "" yellow box "" approach to make warnings stand out more in developer mode ( see prs # <number> and # <number> ) . there has been some pushback though to the idea of react modifying the dom for warning purposes ( see issue # <number> ) . i am not sure how that will play out yet - but in the meanwhile , could we improve at least some of these use cases by making better use of the devtools ? for example , what if react exposed a new api that allowed associating a warning with one or more components in the devtools panel ? ( eg a method that - when called - recorded the current component stack and notified devtools if present ) react could use this for things like missing or non - unique keys . 3 rd party libraries may also benefit from this ( eg react - virtualized could use this to warn about missing or incomplete positioning styles for cells ) . here ' s a rough outline of some of the features : * components with warnings could be highlighted in some emphasized way ( eg yellow background ) in devtools to make them easier to spot . * a new toolbar option could be added to "" show all warnings "" ( eg filter the tree view to show only components with warnings ) . * warning message could be shown inline in the settings panel for the selected component . here ' s a rough mockup : [ screen shot <number> - <number> - <number> at <number> <number> <number> pm ] ( <url> limitations not everyone uses devtools so this would not help a lot of people . * this would not address issues like using the development mode of react for production sites . thoughts ? ideas ? suggestions ?",1
facebook/react,"rfc : reactfiberreconciler release artifact this issue is intended to be a discussion for how to distribute the reactfiberreconciler . js file and dependencies for custom renderers . currently first - class renderers within the react codebase / facebook ecosystem do not have any concerns for this because of haste / access to the react . js build + publish tooling within this codebase . 3 rd party renderers are currently adding ` react - dom ` to their dependency list and requiring ` react - dom / lib / reactfiberreconciler . js ` to build and expose their custom renderer . ideally , ` reactfiberreconciler . js ` would be distributed with the ` react ` package or as a standalone ` react - fiber - reconciler ` package . whether this is at ` react / reconciler . js ` or ` react / lib / reconciler . js ` idk . i think at the root is better to continue the discouragement of looking in ` react / lib / * ` for anything . i understand flat bundles are coming , too . i don ’ t know if that should block this or if this could be a flat bundle , or any other alternative . is this something the team is ready to commit to and support if we came to a decision on approach and i put together a pr ? related issues # <number> create separate copies of each renderer * # <number> include flow type definitions ( flow type distribution proposal at <url> + <user> <user>",1
facebook/react,"provide a way for external tools to list nodes with event info i would like to add react event bubbles to the markup tree of firefox developer tools . to do this i need a way to get a list of nodes and their attached event listeners . if somebody can provide a way for me to do this it will be added within a day . is this possible at the moment and , if not , what can be done to make this possible ?",1
facebook/react,"[ rfc ] idea and forcedeepupdatewithscope ( scope ) _just going to put it out there for feedback . <repeated> _ # # motivation subscription management comes with a cost and that eats into the wins of async rendering since it needs to be managed synchronously . not just managing the direct subscriptions themselves but managing the dynamic dependency graph so that it can be invalidated . meanwhile , most of what subscriptions are used for is data that will never update . at least in our apps . it is a pure loss . the use case is when you are connecting to third party systems that are not as easily connected to the top level data tree . # # proposal ` ` ` js this . forcedeepupdate ( ); ` ` ` same use case as ` forceupdate ` , if you are reading from global mutable state for some reason , you can use this to by - pass ` shouldcomponentupdate ` in an entire subtree . basically rerender everything . when combined with fiber this can be a low - priority update so it ' s not so bad for things that change a lot of things . a good example would be changing the locale . regardless if you read a global mutable locale ( like airbnb does ) or a context locale ( like yahoo ) does , this lets you change it when you need to . without needing to manage subscriptions for all those cases when you do not need it . ` ` ` js this . forcedeepupdatewithscope ( scope ) ; ` ` ` ` ` ` js class foo extends react . component { shouldcomponentupdateforscope ( scope ) { return scope . store = = = userstore & & scope . id = = = this . props . userid ; } render ( ) { . <repeated> } } ` ` ` ` forcedeepupdatewithscope ` would traverse the subtree and only start rendering if ` shouldcomponentupdateforscope ` returns ` true ` for the arbitrary ` scope ` argument . this allows for a bit more of a targeted update with some convenience overhead . additionally , react would cache the pair of ` scope ` and ` components ` that responded . for some number of scopes back . if a new component gets mounted with a ` shouldcomponentupdateforscope ` we might check it against the cache to see if we need to add it to the cache . effectively this creates * * lazy subscriptions * * . the use case is something like typing into an input field that then updates some global store which immediately displays in a completely different place on the page . the first character might be a bit slower but still with responsive levels and the subsequent characters are fast to update . # # caveat the major downside of this proposal is that it relies on mutation . as we know , react does not really like mutation for many more reasons than just ` shouldcomponentupdate ` . the effect in fiber for example , is that any component that gets a higher priority update will start using the new value . components that rely on mutable state effectively become up - prioritized which is not good . <number> ) it can temporarily show inconsistent data . <number> ) the point of making this kind of update lower priority is because it is likely to be large . larger updates will stall the page if they take the same priority as higher priority updates . thereby defeating the benefits of fiber anyway . i ' d like to try to come up with a variant of this api that does not rely on mutation .",1
facebook/react,"reacttestrenderer custom depth rendering * * do you want to request a _feature_ or report a _bug_ ? * * feature * * what is the current behavior ? * * ` ` ` jsx / / mycomponent . js import react from "" react "" ; import thirdpartycomponent from ' third - party - component ' ; function myinternalcomponent ( ) { return <div> test </div> ; } export default function mycomponent ( ) { return <div> <myinternalcomponent> </myinternalcomponent> < thirdpartycomponent someprop ={ true } > </thirdpartycomponent> </div> ; } / / test . js import reacttestrenderer from "" react - test - renderer "" ; import react from "" react "" ; import mycomponent from "" . / mycomponent . js "" ; const renderer = reacttestrenderer . create ( <mycomponent/> ); console . log ( renderer . tojson ( )); ` ` ` this renders whole tree of dom which is actually expected behaviour . the problem is , that i dont want render ` thirdpartycomponent ` , only ` myinternalcomponent ` . shallow renderer isnt answer because shallow would not render ` myinternalcomponent ` at all . which is problem because it ' s hard to divide component into smaller , internal chunks . i have done some work in order to achieve this in # <number> . it was fully working patch . it was done as part of shallowrenderer but now we have testrenderer . besides this patch is pretty old so resolving conflicts would be very hard . * * what is the expected behavior ? * * i would love to provide "" blacklist "" of components which i do not want to render : ` ` ` js import thirdpartycomponent from ' third - party - component ' ; / / . <repeated> const renderer = reacttestrenderer . create ( <mycomponent/> , { dontrender : [ thirdpartycomponent ] } ); ` ` ` this would return jsx js <div> <div> test </div> < thirdpartycomponent someprop ={ true } > </thirdpartycomponent> </div> ` ` ` so we can test props returned for ` thirdpartycomponent ` and internal logic . it ' s combine of full and shallow renderer . i have some ideas of implementation but i dont want waste my time writing code which will not be marged into master anyway . this possibly would allow to resolve <url>",1
facebook/react,"shouldcomponentupdate does not work well if component accepts children react ' s shouldcomponentupdate based performance improvements work great for improving the performance of medium - weight components with large numbers of instances . they even work well with event handlers , as you can ignore event handler changes and instead pass a locally bound method that ' ll access ` this . props . on * ` on demand . however this all fails apart you start passing react elements to pure components . ` ` ` js ' use strict ' ; import react , { component , proptypes } from ' react ' ; import reactdom from ' react - dom ' ; import shallowequal from ' recompose / shallowequal ' ; / / shallowequalexcluding : fictional function that works like shallowequal , but ignores changes to a list of props passed as the third argument class button extends component { static proptypes = { icon : proptypes . node . isrequired , onclick : proptypes . func }; onclick = ( e ) => { this . props . onclick ( e ) ; }; shouldcomponentupdate ( nextprops ) { / / <user> does not actually work return shallowequalexcluding ( this . props , nextprops , [ ' onclick ' ]); } render ( ) { const { icon } = this . props ; return ( < button onclick ={ this . onclick } > { icon } { / * react . cloneelement ( icon , { ref : ( icon ) => this . iconref = icon } )* / } </button> ); } } class icon extends component { static proptypes = { name : proptypes . string . isrequired , color : proptypes . string }; shouldcomponentupdate ( nextprops ) { return shallowequal ( this . props , nextprops ) ; } render ( ) { const { name , color } = this . props ; return getsvgicon ( name , color ) ; } } const nilclick = ( ) => {}; reactdom . render ( < button onclick ={() => alert ( ' clicked ' ) } icon ={< icon name = ' done ' } / > , document . queryselector ( ' <hashtag> container </hashtag> ' )); reactdom . render ( < button onclick ={ nilclick } icon ={< icon name = ' done ' } / > , document . queryselector ( ' <hashtag> container </hashtag> ' )); / / 2 nd invovation reactdom . render ( < button onclick ={ nilclick } icon ={< icon name = ' cancel ' } / > , document . queryselector ( ' <hashtag> container </hashtag> ' )); / / 3 rd invovation ` ` ` given this sample ; a ` < button / > ` component that expects an icon to be passed as an ` icon ` prop and a simple ` < icon / > ` . both are pure components and button is also coded to not re - render when ` onclick ` is changed . pretend that button actually has a heavy ` render ( ) ` but its props and state do not change frequently . on the second invocation , icon should not require any prop change or render and button should have its ` onclick ` prop changed but not require a render . on the third invocation , icon should require a render while button itself does not need to render except for the change to icon . however in practice button will always re - render , including during the second invocation when nothing changes . this is because ` < icon / > ` will always result in a new instance and will never be the same . normally you could work around this within the component itself , without telling users they have to store ` < icon / > ` in a variable until they think they need to change its props ; for functions you could pass a function that will use ` this . props . * ` itself and for objects you can do a deep comparison if you know the structure of the object . but for react elements , even though ` shouldcomponentupdate ` allows react to know if the current component has a render dependency on a sub - component , you do not have access to this information so button cannot tell if icon requires a render . in practice this can turn out to be a problem when you are writing some libraries rather than an application . notably [ material ui ] ( <url> suffers from this problem in production . ` enhancedswitch ` ' s ` render ( ) ` is not light ; ` enhancedswitch ` is used by ` radiobutton ` and ` checkbox ` ; both use a ` checkedicon ` and ` uncheckedicon ` react element prop ; you can reasonably have <number> checkboxes on one page ; even if they were pure , they cannot identify whether an icon requires an update ; as a result , a render of the component containing the checkboxes to check a single checkbox will result in the ` render ( ) ` of all <number> ` enhancedswitch ` instances . i can think of a few ideas on what type of api could be added to react to solve this issue . # # shouldcomponentupdate helper the most obvious api would be a top - level react function that given the instance context , old reactelement , and new reactelement would return the result of a component ' s ` shouldcomponentupdate ` . then heavy parent components can use that to implement a ` shouldcomponentupdate ` that is aware of render dependencies in its children . ( as a bonus , theoretically you could temporarily remember this while you are walking the current tree ; then instead of calling ` shouldcomponentupdate ` multiple times for every ( potentially nested ) component the result is simply that ` shouldcomponentupdate ` calls are raised up to the highest level where a component is render - dependent on them ) ` react . shouldcomponentupdate ( this , this . props . icon , nextprops . icon ) ` however i expect the problem we have with this is that ` shouldcomponentupdate ` is also responsible for state dependent updates and ` this ` is supposed to be a rendered instance , not a reactelement instance . while you know ` context ` from passing the current instance , you do not have a reference to the state from either of the props . # # ref based shouldcomponentupdate helper the second most obvious api would be a ` shouldcomponentupdate ` helper that instead uses a ref . ` react . shouldcomponentupdate ( this . iconref , nextprops . icon ) ` the downside to this is that to get a ref for a component you did not create , you inevitably have to use ` react . cloneelement ` . # # render passthrough the next idea i had was a render passthrough . a way during the render process for a component to say "" i do not need a render ( ) / update , but these children of mine may "" which would tell react to skip render ( ) and then run ` shouldcomponentupdate ` on the instances deeper in the tree . however those components only know if they need updates if you pass them the new props ; so a passthrough will not work . we ' d instead need a way to tell react that it should not run ` render ( ) ` but do pass on an update to a specific component instance ` this . renderref ( this . iconref , nextprops . icon ) ; ` the advantage of this over using ` shouldcomponentupdate ` is that instead of only allowing medium components wrapping light components to only ` render ( ) ` when a child requires it ; we also allow heavy components to never ` render ( ) ` unless they themselves require it , while still allowing them to permit their light children to update . # partial renders that ` shouldcomponentupdate ` based ` renderref ` only applying updates to a component child feels somewhat awkward and forced though . so a more robust idea might be a partial render lifecycle that optionally runs when ` shouldcomponentupdate => false ` and can call for the render of a sub - tree that belongs to the current component . ` ` ` js class heavycomponent extends component { shouldcomponentupdate ( nextprops ) { / / ignore icon and children return nextprops . text ! = = this . props . text ; } render ( ) { const { text , icon , children } = this . props ; text = dosomethingabsurdlycpuintensiveandhardtofactoroutofthiscomponent ( text ) ; return ( <div> <h2> { react . cloneelement ( icon , { ref : ( icon ) => this . iconref = icon } ) } { text } </h2> < wrapper ref = ' subtree ' > { children } </wrapper> ); } componentskippedrender ( nextprops / * , nextstate */) { this . subrender ( this . iconref , react . cloneelement ( icon , { ref => this . iconref = icon } )); this . subrender ( this . refs . subtree , < wrapper ref = ' subtree ' > { children } </wrapper> ); } } ` ` ` though ` this . subrender ` probably has potential for conflicts , so i expect the most react - line way to name that would be something like ` react . rendersubtreeintocomponent ( parentcomponent , nextelement , component ) ` which would be invoked using ` react . rendersubtreeintocomponent ( this , /* subtree */ , this . refs . subtree ) ; ` . the ` <wrapper> ` i used would be a really light component that probably would just render its children . it ' s there because ` react . rendersubtreeintocomponent ` should probably not accept dom refs ; this should be part of react lifecycle / walker , not part of client side browser only react - dom like ` reactdom . unstable_rendersubtreeintocontainer ` .",1
facebook/react,"is there a way to let users know which invalid type was returned ? this is the error message in question : ` ` ` labelbutton ( . <repeated> ) valid react element ( or null ) must be returned . you may have returned undefined , an array or some other invalid object . ` ` ` not very helpful . is it technically possible to _show_ what was returned ?",1
facebook/react,"clean up top - level event listeners after unmounting all roots * * do you want to request a _feature_ or report a _bug_ ? * * bug - maybe intended behaviour . * * what is the current behavior ? * * _background_ i have an app that needs to be embedded by other apps ( other customers ) . the idea being "" our "" react app has its javascript loaded in an iframe , but the "" main "" window hosts dom elements from the customers and our react app . that bit works fine . as time goes on "" our "" react ui is no longer needed , and then react root is removed , and the iframe destroyed . these apps are often long lived so there will be times when the react app needs to appear again , and the iframe is recreated and everything reloaded . this can and will happen many times . _goal_ we would like to not keep the iframe around when its not actually needed , but rather re - create just in time when it is needed . this app is used by customers and they would like to embed our "" react "" app , without interference with their "" app "" and all its javascript , which is why we are doing the iframe thing . _problem_ it is evident by watching the chrome dev tools "" timeline "" memory graph that memory always increases each time a new iframe is created and the react ui is init ' d . unmounting and destroying the iframe , never causes the memory to drop to "" near "" original before load value . repeating this process multiple times slowly show an increase memory . this also causes a more immediate problem , in that react is throwing exceptions on every event ( click , type etc ) because the window of the iframe is now null . _proof : first symptom - event exceptions ( only happens in my app ) _ these exceptions only happen in my ( cant share ) app , i cant repo them , but parts of this apply to all react apps . please read thru - it will all make sense when you get to the end and if you examine my poc . destroying the iframe , leaves react and its event dispatching system in memory . i have a mixture of x - tag , webcomponents which are used to "" create "" the iframe and load the react app . after the custom element is used ( lets call it < embed - react > ) , the console starts showing exceptions all within react code . this is a side effect of the react dispatchevent still being active and trying to do stuff . ` ` ` javascript uncaught typeerror : cannot read property ' nodename ' of null shouldusechangeevent @ vm1068_embeddedapp . js : <number> extractevents @ vm1068_embeddedapp . js : <number> extractevents @ vm1068_embeddedapp . js : <number> handletoplevel @ vm1068_embeddedapp . js : <number> handletoplevelimpl @ vm1068_embeddedapp . js : <number> perform @ vm1068_embeddedapp . js : <number> batchedupdates @ vm1068_embeddedapp . js : <number> batchedupdates @ vm1068_embeddedapp . js : <number> dispatchevent @ vm1068_embeddedapp . js : <number> ` ` ` i know about ` reacteventlistener . dispatchevent ` ( snip below ) where i can disable react ( i havent actually tried ) to avoid the exceptions , but that would leave the memory leak . <url> ` ` ` javascript dispatchevent : function ( topleveltype , nativeevent ) { if ( reacteventlistener . _enabled ) { return ; } ` ` ` its rather easy to prove that react remains in memory , simply goto the compiled app , find the ` react dispatchevent ` and insert a console . log and watch as it continues to "" print "" stuff after unmounting the last component , even though there are no listeners . in my case the exception is caused because all ` extractevents ` eventually default to "" window "" as the "" target "" . there are multiple copies of the same basic idea in various react functions , where it tries to get a target that it assumes will never be null . if one doesnt load react in an iframe , then window is always defined . ` ` ` javascript var targetnode = targetinst ? reactdomcomponenttree . getnodefrominstance ( targetinst ) : window ; ` ` ` later the ` shouldusechangeevent ` tries to read the nodename of the now "" undefined "" window , because its iframe has been destroyed , but that now results in an exception ( null pointer etc ) . <url> . <repeated> ` ` ` javascript function shouldusechangeevent ( elem ) { var nodename = elem . nodename & & elem . nodename . tolowercase ( ); return nodename = = = ' select ' || nodename = = = ' input ' & & elem . type = = = ' file ' ; } ` ` ` * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem via <url> or similar ( template * * what is the expected behavior ? * * there are probably two possible solutions , that work in tandem . <number> ) firstly react should provide an api that will remove all its global event listeners . naturally it could complain if there are any active components that remain mounted . this api may be internal / private ( not public ) , if # <number> was implemented . it might be called something like ` react . shutdownall ` because everything is gone , the next react render would setup all its globals again . <number> ) react should dispose of all its global event handlers when the last or "" root "" component is unmounted . this would call the _new api_ mentioned in <number> . either option solves my problem , where i wish to either let react shutdown gracefully . with this in mind i could . - unmount iframe powered react ui component . - call react . disposeglobals ( mentioned above ) . if unmounting auto calls an internal ` react . shutdownall ` then this step is skipped . - destroy iframe . _proof <hashtag> 2 </hashtag> goto your compiled out , locate the ` dispatchevent ` and add a console . log , notice even after the last / root container is unmounted stuff will continue to be printed because the event listeners are still active . i did a very quick scan of the abstraction around adding listeners , and i couldnt see the remove function being stored and then called to cleanup . _proof <hashtag> 3 </hashtag> look at my last section below where i have a proof of concept form of the popular todomvc react example . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> . <number> react - dom <number> . <number> react - redux <number> . <number> ( might be useful to know ) * * reproducable use case * * sorry i tried but decided that using the facebook jsfiddle wasnt really a smart thing for the following reasons . - the compile the "" jsx "" content means loading babel etc to compile ( babel , jsfiddle etc too many moving parts ) - its "" hard "" to get the "" root component "" that is inserted into the "" output "" box and - its even just too "" hard "" to put the jsx compiled output into somewhere for the iframe src = to "" load "" . i have forked the popular todomvc app and added a few minor edits to recreate , reload , render + unmount x100 , destroy everything about the app , and try again in a loop separated by a sleep . - <url> ( todomvc main site ) - <url> ( todomvc github ) - <url> ( my fork - with comments and snapshots of chrome dev tools timeline memory graph ) hopefully we can trust the todomvc guys are doing the right thing , no dumb memory leaks . if you examine it should be obvious the only thing im adding is support for my horrible create app , run app , render + unmount many times , render , unmount , sleep a bit and then loop again until counter exhausted . sorry if this is boring but as a convenience i will list the basic instructions to "" run "" the react version of my branch on your local machines . <repeated> <number> . clone <url> <number> . in the root , run "" gulp "" , to compile everything . <number> . run something like "" python - m simplehttpserver "" 4 a . navigate to <url> 4 b . navigate to <url> / / / examples / react corresponds to the dist / examples / react directory that gulp built into . my poc supports <number> concepts . - re - run todomvc over and over again in "" same "" window . - create iframe , load todomvc js in the iframe but render to outer window , unmount , destroy iframe , try 2 0 x - create custom element , webcontainer creates iframe and load todomvc js in the iframe but render to outer window , unmount , destroy custom element , try 2 0 x if you look at my p / r against todomvc you will see many helpful pictures with memory leak graphs from chrome dev tools for each of the <number> described scenarios and some commentary .",1
facebook/react,"expose react build mode / flags <user> has expressed concern that people will forget to set the node_env when building their react application , which will result in babel producing bloated builds , due to our new dev - mode transforms . the ` __source ` and ` __self ` should never be set in production , and having them set on every element would introduce substantial bloat that you would not want in a production environment . we probably want to be able to warn when this happens . in this case , i think we might want to expose ` react . mode = __dev__ ` or something , such that the transform could add runtime checks to verify that react is in dev mode , and warn if not . i know we are thinking about switching to real build flags for the various features , and maybe we want to expose all those individual flags . anyway , opening the issue so we have a place to discuss and track .",1
facebook/react,"improve error messages for invalid states i think we should create a test suite that throws errors in different lifecycle methods , and make sure we have relatively sensible invariants as early as possible , preferably with component names . # <number> is an example of this , but we ’ ll keep regressing until we actually test for something like this explicitly . cc <user> <user> <user> / react - core",1
facebook/react,"provide an opt - in way to easily manage ` this ` in event handlers there ' s no shortage of places in react where a dev will write something like ` ` ` onclick ={() => this . setbookssubjects ( ) } ` ` ` or alternatively ` ` ` onclick ={ this . setbookssubjects . bind ( this ) } ` ` ` it ' s not ideal to re - create these functions on each render , so the alternative would be some form of auto - binding in the class ' s constructor , all of which re - create these functions once per instance , and require boilerplate . i ' d love to see the react team add some way of opting in to having a handler ` call ` ed with the current component set as this . by "" current component "" i mean the component whose ` render ` created the element . i do not know what it should be called , but , for example , if it were called "" ownclick "" it would look like this ` ` ` < button ownclick ={ this . foo } > click me </button> ` ` ` and so when that button is clicked , ` foo ` would be called , with the object that owns the ` render ` method which rendered the button set as ` this ` .",1
facebook/react,"warn on inline style update with a bad value if set an inline style attribute for a component , such as ` backgroundcolor : ' yellow ' ` , and then update the state with a bad value to make the inline style like this ` backgroundcolor : ' non - exist - color ' ` . this currently takes no effect on the component , and the backgroundcolor will remain yellow . ( live example ) i suppose this is not the correct behavior , the old value should be override by new value , even the value is non - standard , so it can fallback to use the parent style just like plain html . if leave the previous style as is , the behavior of component will lose connection with component state , the style will become chaotic and unpredictable .",1
facebook/react,"<datalist> support not a _ton_ of browser support for this right now , but it appears to be on the horizon for webkit . <url> <url> right now , i am not getting any dom events fired from it in chrome . wonder if syntheticevent could prollyfill . <url>",1
facebook/react,"have react ignore a specific dom element as per my chat [ here ] ( <url> with <user> i am filing an issue to discuss this further . i would like to avoid a specific element that i rendered on the server from beeing further updated by react once it reaches the client . one specific use case is rendering ad server tags that are kind of a pain in the ars , using things like ` document . write ` , etc . i render them using ` dangerouslysetinnerhtml ` but sometimes when react is doing the reconciliation client side they get re - render so they stop executing . now i managed to track down some of these cases by fixing render differences between server and client which would trigger dom patching but it still seems to happen . any advice ?",1
facebook/react,"add comments / attribute indicating which component was rendered as per the discussion today . <repeated> sometimes you are developing on a platform that does not have devtools ( safari , etc ) . the problem is that you are looking at a whole pile of markup , and can not tell which components rendered it . without devtools , the output markup is really hard to navigate . it would be cool if we had comment nodes ( or a ` data - reactcomponent ` attribute ) that helps users navigate the output . these nodes would be rendered only in dev mode or with some flag turned on or something .",1
facebook/react,"allow specifying multiple fallback values for inline styles ( e . g . for vendor prefixing with ssr ) i have been using the "" string "" hack to override css values in react components . for example , if you want to have ` display ` with different values , you do ` ` ` styleobj . display = ' - webkit - box ; display : - moz - box ; display : - ms - flexbox ; display : - webkit - flex ; display ` ` ` i have an npm module for poly - filling my styles in my react components . <url> all of that worked perfectly in v0 . <number> . no warnings no nothing . updated to v15 . <number> today , and everything is breaking . a ) normally there are deprecation warnings . i did not notice any warnings on that change . b ) in the changelog i cannot see anything related to that , to understand what changed . am i missing something ?",1
facebook/react,"add support for css variables in style attributes css variables is now supported in chromium , which we use for rendering . they enable us to write cleaner , more flexible code . sadly , i cannot seem to use them in react without resorting to various tricks . ideally , i would like to be able to use them like ` < div style ={{ "" - - color "" : "" hotpink "" } } / > ` , which would make the variable available inside the scope of the div . i am able to add them using the following syntax ` < div style ={{[ "" - - color "" ] / > ` , but then they are not updated if i try assigning a new value — which ruins much of the point of using a variable . i am able to add and remove them using reactdom and ` reactdom . finddomnode ( this ) . style . setproperty ( "" - - color "" , "" hotpink "" ) ` , but that gets it out of sync with the dom updates , in addition to not being pretty . if there are any questions on the usefulness of css variables i will be more than happy to explain .",1
facebook/react,support dom nodes as children it would be nice to be able to do the equivalent of ` <div> { document . createelement ( ' div ' ) } </div> ` . it seems entirely doable now with our new fancy renderer i think ? obviously it would not be supported for ssr though so you would have to provide your own fallback if necessary .,1
facebook/react,question about react . fragment and dangerouslysetinnerhtml react version react . fragment why not dangerouslysetinnerhtml property,2
facebook/react,"bug : data is losing during page refresh in reactjs - redux ; tried using ' redux - persist ' and localstorage , but not working . in our project , forms ( login , signup , logout , etc . <repeated> ) were built in django and through this we are getting the authorization details and this was stored as redux - data and was used in the rest of the application which is built in react . there was no refresh issue during that time . evenif the store was getting disapper but we are getting it back . now , we shifted all that we done in django into react and used the same redux storage method in login , but we are facing the data losage during refresh , store is not getting restore and we are having <number> status for <number> apis for getting the user details . this was not happening in former case . we used redux - persist package to avoid this data losage . <repeated> and also tried using localstorage persisting method ( loadstate ( ) , savestate ( ) ) . but , still facing the issue . store . js ` ` ` import { createstore , applymiddleware , compose } from ' redux ' import thunk from ' redux - thunk ' import rootreducer from ' . / reducers ' import { persiststore , persistreducer } from ' redux - persist ' import storage from ' redux - persist / lib / storage ' ; const persistconfig = { key : "" root "" , storage , } const composeenhancers = window . __redux_devtools_extension_compose__ || compose const persistedreducer = persistreducer ( persistconfig , rootreducer ) const store = createstore ( persistedreducer , composeenhancers ( applymiddleware ( thunk ) ) ) const persistor = persiststore ( store ) ; export default ( store ) export { persistor } ` ` ` action . js : ` ` ` import axios from ' axios ' import { set_profile , set_feature_toggles } from ' . / actiontypes ' import { client_request_data } from ' . <repeated> / config ' ; const redirecttologin = ( ) => { delete axios . defaults . headers . common . authorization if ( window . location . href . indexof ( ' / accounts / ' ) = = - <number> ) { window . location . href = ' / accounts / login ' } } export const fetchuserprofile = ( ) => dispatch => { axios . post ( ` / accounts / user_profile / ` , { client_request_data : client_request_data } ) . then ( resp => dispatch ( { type : set_profile , payload : resp . data , } ) , ) . catch ( e => { / / todo figure out what do do here if ( e . response ? . <repeated> status = = = <number> ) { redirecttologin ( ) } } ) } export const fetchfeaturetoggles = ( ) => dispatch => { axios . post ( ` / api / study / v1 / feature_toggle / ` , { client_request_data : client_request_data } ) . then ( resp => dispatch ( { type : set_feature_toggles , payload : resp . data , } ) , ) . catch ( e => { / / todo figure out what do do here if ( e . response ? . <repeated> status = = = <number> ) { redirecttologin ( ) } } ) } ` ` ` reducers : <number> . featuretoggle . js ` ` ` import { set_feature_toggles } from ' . <repeated> / actiontypes ' const intialstate = { } export default ( state = intialstate , action ) => { switch ( action . type ) { case set_feature_toggles : return action . payload default : return state } } ` ` ` <number> . userprofile . js ` ` ` import { set_profile } from ' . <repeated> / actiontypes ' const intialstate = { } export default ( state = { } , action ) => { switch ( action . type ) { case set_profile : return action . payload default : return state } } ` ` ` app . js : ` ` ` import react , { useeffect , suspense } from ' react ' import { connect } from ' react - redux ' import cssbaseline from ' <user> - ui / core / cssbaseline ' import { themeprovider } from ' <user> - ui / styles ' import muithemeprovider from ' <user> - ui / core / styles / muithemeprovider ' import { provider } from ' react - redux ' import { browserrouter , switch , route } from ' react - router - dom ' import theme from ' . / theme / muitheme ' import ' . / i18n ' import home from ' . / screens / home ' import * as actions from ' . / redux / actions ' import userservice from ' . / services / userservice ' import { base_url } from ' . / config ' import login from ' . / login ' import signup from ' . / signup ' import logout from ' . / logout ' import resetpassword from ' . / resetpassword ' import resetsuccess from ' . / resetsuccess ' import store from ' . / redux / store ' const app = props => { const { userprofile , featuretoggles , fetchuserprofile , fetchfeaturetoggles , } = props useeffect ( ( ) => { fetchuserprofile ( ) fetchfeaturetoggles ( ) } ) return ( < suspense fallback ={< span > </span> } > <browserrouter> <switch> < route exact path =""/ "" render ={() => { return ( userprofile = = = null || featuretoggles = = = null ? <login/> : < home / > ) } } / > </switch> </browserrouter> </suspense> ) } const mapstatetoprops = state => ( { userprofile : state . userprofile , featuretoggles : state . featuretoggles , } ) export default connect ( mapstatetoprops , actions ) ( app ) ` ` ` index . js : ` ` ` import promisefinally from ' promise . prototype . finally ' import react , { suspense } from ' react ' import reactdom from ' react - dom ' import ' . / index . css ' import cssbaseline from ' <user> - ui / core / cssbaseline ' import { themeprovider } from ' <user> - ui / styles ' import muithemeprovider from ' <user> - ui / core / styles / muithemeprovider ' import { provider } from ' react - redux ' import * as serviceworker from ' . / serviceworker ' import app from ' . / app ' import theme from ' . / theme / muitheme ' import store , { persistor } from ' . / redux / store ' import ' . / i18n ' ; import home from ' . / screens / home ' import login from ' . / login ' import signup from ' . / signup ' import logout from ' . / logout ' import { persistgate } from ' redux - persist / integration / react ' promisefinally . shim ( ) reactdom . render ( < provider store ={ store } > < persistgate loading ={ null } persistor ={ persistor } > < muithemeprovider theme ={ theme } > < themeprovider theme ={ theme } > < cssbaseline / > <suspense> < app / > </suspense> </themeprovider> </muithemeprovider> </persistgate> </provider> , document . getelementbyid ( ' root ' ) , ) serviceworker . unregister ( ) ` ` ` also tried with localstorage : localstorage . js ( in redux ) ` ` ` export const loadstate = ( ) => { try { const serializedstate = localstorage . getitem ( "" state "" ); if ( serializedstate = = = null ) { return undefined ; } return json . parse ( serializedstate ) ; } catch ( err ) { return undefined ; } }; export const savestate = ( state ) => { try { const serializesstate = json . stringify ( state ) ; localstorage . setitem ( "" state "" , serializesstate ) ; } catch ( err ) { console . log ( err ) ; } }; ` ` ` corresponding store . js : ` ` ` import { createstore , applymiddleware , compose } from ' redux ' import thunk from ' redux - thunk ' import rootreducer from ' . / reducers ' import { persiststore , persistreducer } from ' redux - persist ' import storage from ' redux - persist / lib / storage ' ; import { fetchfeaturetoggles } from ' . / actions ' ; import { loadstate , savestate } from ' . / localstorage ' ; import { throttle } from ' lodash ' ; const persistconfig = { key storage , } const composeenhancers = window . __redux_devtools_extension_compose__ || compose const persistedstate = loadstate ( ); const persistedreducer = persistreducer ( persistconfig , rootreducer ) const store = createstore ( persistedreducer , persistedstate , composeenhancers ( applymiddleware ( thunk ) ) ) store . subscribe ( throttle ( ( ) => { savestate ( store . getstate ( )); } , <number> )); const persistor = persiststore ( store ) ; export default store export { persistor } ` ` `",2
facebook/react,react developer tools i successfully installed the extensions but i am not able to see the the news tabs ( components and react,2
facebook/react,"question : starttransition behavior i am very sorry that i am using the issue tracker to ask this question , but i think others might ask the same question after reading <url> ( i do not have commenting rights ) . ` ` ` js function handleinputchange ( e ) { const input = e . target . value setinputvalue ( input ) starttransition ( ( ) => { setsearchquery ( input ) }); } ` ` ` what happens if the user types "" ab "" ? i . e . : ` ` ` js / / pseudocode representing the first event handler triggered by keystroke "" a "" setinputvalue ( "" a "" ) starttransition ( ( ) => setsearchquery ( "" a "" ) ) / / pseudocode representing the second event handler triggered by keystroke "" b "" setinputvalue ( "" ab "" ) starttransition ( ( ) => setsearchquery ( "" ab "" ) ) ` ` ` from my understanding ` setinputvalue ( "" a "" ) ` and ` setinputvalue ( "" ab "" ) ` will batch generating a single rerender , the callback ` ( ) => setsearchquery ( "" a "" ) ` passed to the first ` starttransition ` will be cancelled , and only the second callback ` ( ) => setsearchquery ( "" ab "" ) ` passed to the second ` starttransition ` will be executed . i . e . / / pseudocode representing the final logic after keystrokes "" a "" and "" b "" setinputvalue ( "" a "" ) / / will batch with ` setinputvalue ( "" ab "" ) ` starttransition ( ( ) => setsearchquery ( "" a "" ) ) / / noop , this callback will be cancelled setinputvalue ( "" ab "" ) starttransition ( ( ) => setsearchquery ( "" ab "" ) ) ` ` ` am i correct ? i think this question is important because if the first callback passed to ` starttransition ` really gets cancelled and i try to do more work inside it , i should have in mind that the additional work will not happen . cc <user> <user>",2
facebook/react,"experimental createroot method not available in react <number> i am using react and react - dom v17 . <number> and i was trying to use the unstable_createroot method to render my app but looks like that method is not even present in react v17 . <number> . can someone comment if this is expected or not . thanks , arpit",2
facebook/react,"bug : strict mode , initial state changes , and usememo ( with dependency ) does not seem to be recomputed < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> i ran in to this interesting dynamic that i cannot explain today . <number> ) the code sets some initial state , which is a random number , and logs this initial value . <number> ) it converts the number to a string using ` usememo ` , which logs the number it is converting , and is dependant on that number ( not the string ) . and <number> ) when the component is mounted , i force a re - render by changing a boolean state once . the result is that the initial value of the number is changed ( and the log is output ) so you see the number being set twice to two different values - this only happens in ` strictmode ` . it is surprising . is it expected ? you also see the ` usememo ` being computed once - only the first time . so seemingly it does not recompute a new string value . but , the output in the html is the correct and shows the stringified value of the _second_ number . so what is happening here ? is the console log swallowed ? is ` usememo ` behaving correctly , and more importantly , is the ` initialvalue ` of ` usestate ` supposed to change like that ? # # steps to reproduce ` ` ` typescript export default function app ( ) { const [ rand ] = usestate ( math . random ( )); const [ , setstate ] = usestate ( false ) ; console . log ( ` number : ${ rand } ` ); useeffect ( ( ) => { setstate ( true ) ; } , []); const text = usememo ( ( ) => { console . log ( ` computing text from ${ rand } ` ); return rand . tostring ( ); } , [ rand ] ); return ( < div classname = "" app "" > <h1> rand : { rand } </h1> <h1> text : { text } </h1> </div> ); } ` ` ` and wrap your app in ` strictmode ` # # link to code example # # the current behavior initial state computed twice . usememo seemingly not run twice # # the expected behavior initial state computed once ? or usememo runs twice ?",2
facebook/react,"question there any way to retrieve react devtool performance data ? hi guys , i want to get react devtool performance / profiler results , probably a json data and send to our local server , i read through the react - devtools - core but unfortunately do to get any clue how to do this ?",2
facebook/react,"question about getting the latest state value in the concurrent mode < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > react version : <number> . <number> # # steps to reproduce <number> . enable strict mode for checking for possible issues in the future concurrent mode <number> . create the below component and run the code ` ` ` import { usecallback , usestate } from "" react "" ; const example = ( { onincrement } ) => { const [ count , setcount ] = usestate ( <number> ); const incrementhandler = usecallback ( ( ) => { onincrement ( count , count + <number> ); / / is count guaranteed to be the latest state here due to including count in the usecallback dependency array ? setcount ( ( count ) => count + <number> ); } , [ count , onincrement ] ); return ( < > <span> { count } </span> < button onclick ={ incrementhandler } > increment </button> < / > ); }; const parent = ( ) => ( < example onincrement ={( currentcount , incrementedcount ) => alert ( ` count before incrementing : ${ currentcount } , after increment : ${ incrementedcount } ` ) } / > ); export default parent ; ` ` ` < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > < ! - - link to code example : - - > < ! - - please provide a codesandbox ( <url> a link to a repository on github , or provide a minimal code example that reproduces the problem . you may provide a screenshot of the application if you think it is relevant to your bug report . here are some tips for providing a minimal example : <url> - - > # # the current behavior in this simple example everything seems to be fine but in a more complicated situation full of event handlers that change the count or async callbacks that may change the count ( like data fetching callbacks ) the count value is not guaranteed to be the latest state and if i change the ` incrementhandler ` function like below : ` ` ` const incrementhandler = usecallback ( ( ) => { setcount ( ( count ) => { onincrement ( count , count + <number> ); return count + <number> }); } , [ count , onincrement ] ); ` ` ` then the ` onincrement ` will run twice in development while in strict mode and may run twice in production in concurrent mode according to documentation . and if you suggest running the ` onincrement ` in ` useeffect ` callback with ` count ` and ` onincrement ` in effect ' s dependencies array how can i know that the ` onclick ` event of the increment button has caused the effect and not another event for example decrement or anything else . you may say by setting another state that shows what is responsible for the effect , then i may need the previous state which unlike this example may be impossible to recalculate . you may suggest using a ref for storing the previous state ( count ) then i will end up with * * one extra state or ref for storing what is responsible for the effect to run * * , * * one extra ref for storing the previous state * * , and * * a useeffect hook to run the onincrement click event handler * * # # the expected behavior providing a second callback argument to ` setstate ` like in class components that will run after this state update so we can save the current and next state and use it in the callback like below const incrementhandler = usecallback ( ( ) => { let prevcount , nextcount ; setcount ( ( count ) => { prevcount = count ; nextcount = count + <number> ; return nextcount ; } , ( ) => onincrement ( prevcount , nextcount ) ); } , [ onincrement ] ); ` ` ` in my humble opinion , this does not collide with the async nature of ` setcount ` and can be implemented . unlike below ` getstate ` proposals that if it will be asynchronous it may not return the desired state . and if it will be synchronous it will not return the latest state too because ` setstate ` is not executed yet . * * wrong solution : * * ` ` ` const [ count , setcount , getcount ] = usestate ( <number> ); const incrementhandler = usecallback ( ( ) => { setcount ( ( count ) => count + <number> ); const currentcount = getcount ( ); const nextcount = currentcount + <number> ; onincrement ( currentcount , nextcount ) } , [ onincrement ] ); ` ` ` or providing a third array to ` usecallback ` for accessing the latest state can not be implemented due to the same problem with ` getstate ` and async nature of setstate . please tell me if i am missing something or i have misunderstood things . if not , please tell me if there is a simple solution for this scenario or similar ones , or tell me the best practices for running a callback or event handler with the latest state . thank you !",2
facebook/react,"bug : it seems that the default value in functional react component gets updated after render . < - - please provide a clear and concise description of what the bug is . include screenshots if needed . please test using the latest version of the relevant react packages to make sure your issue has not already been fixed . - - > it seems that the default value in functional react component gets updated after render . react version : <number> . <number> # # steps to reproduce i created a question on stackoverflow : <url> but also repeat it here : ` ` ` const mycomponent = ( ) => { / / initialise data with a random value : const [ data , setdata ] = react . usestate ( ( ) => { const data = _ . samplesize ( _ . range ( <number> ) , <number> ) / / print data on initialisation : console . log ( ' init data in default :', data ) return data } ) react . useeffect ( ( ) => { / / print data after the component is rendered : console . log ( ' init data after render :', data ) } ) return ( <div> { data } </div> ); }; ` ` ` the output in console is : ` ` ` [ log ] init data in default : – [ <number> , <number> , <number> ] ( <number> ) [ log ] init data after render : – [ <number> , <number> , <number> ] ( <number> ) ` ` ` my understanding is that before the component is rendered , the function under ` usestate ` is called . the value returned by the function is assigned to ` data ` , and the ` data ` values is used to render the component on the screen . the function under ` usestate ` is called only once and we never call ` setdata ` , so the value should be the same . maybe i miss something ? < ! - - your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . issues without reproduction steps or code examples may be immediately closed as not actionable . - - > link to code example : < ! - - please provide a codesandbox ( <url> a link to a repository on github , or provide a minimal code example that reproduces the problem . you may provide a screenshot of the application if you think it is relevant to your bug report . here are some tips for providing a minimal example - - > <url> # # the current behavior the output in console before and after render is different . # # the expected behavior the output in console before and after render should be the same .",2
facebook/react,"dev tools chrome extension only works properly in incognito window hi folks , at some point w / in the last <number> months or so , the dev tools chrome extension stopped working properly for me in * non <emphasis> * - incognito chrome windows . it ' s a bit hard to explain , but i will try : when i use the extension , react components show up , but they are basically just high level wrapper components that we use in our app . for example , i cannot inspect a button component , a container component , an input component , etc , in the ui . this issue disappears in incognito mode , however , and i can use the extension just fine . chrome version : <number> . <number> extension version : <number> . <number> react version i have been unable to find anyone else with this issue . please let me know if there is other information i can provide you with . thank you .",2
facebook/react,"question mode and useeffect cleanups since cm mode now runs useeffects ' cleanups async . are we still guaranteed that they will resolve in order ? by that i mean , if a component get ' s cleanup , re - rendered and cleaned up again . that the first cleanup will resolve , before the second ? componenta ( <number> ) - > thrown away - > componenta ( <number> ) - > thrown away . will that ( <number> ) effectively "" await "" on the ( <number> ) to cleanup first .",2
facebook/react,"some questions about lanes . first of all , thank you for reading and patience . i have been studying the principle of react lanes recently , and its implementation is interesting to me , but i still do not know what the specific problems it solves . > this constraint was designed before suspense was a thing , and it made some sense in that world . when all your work is cpu bound , there ' s not much reason to work on tasks in any order other than by their priority . but when you introduce tasks that are io - bound ( i . e . suspense ) , you can have a scenario where a higher priority io - bound task blocks a lower - priority cpu - bound task from completing . from the explanation of <user> , it seems to solve the blocking problem of io operation on low priority tasks . but i could not figure out what asynchronous io blocked ？ ` ` ` js <a/> <suspense> <b/> </susepsne> <c/> ` ` ` based on the above example , before lanes , where is blocked , and where is the problem solved after lanes . or do you have a better demo to explain ? for developers , the new technology related information is too little , binary is also very abstract , thank you again for your patience .",2
facebook/react,"why does react warn about multiple renderers using the same context provider ? i am currently developing a web app that uses both [ react - pixi ] ( <url> and [ react - babylonjs ] ( <url> both of these libraries use ` react - reconciler ` and have a custom renderer . i also use redux in my project , so they share the same context in the two libraries . it displays a warning on console after every redux state updating , but everything works well , both renderers can trigger an update . i want to know if there is any risk in doing this , or is this just a false warning ? react version : <number> . <number> # # steps to reproduce <number> . using multiple react renderers <number> . using the same context provider between that react renderers link to code example : <url> # # the current behavior it will throw a warning message after every state updating : > warning multiple renderers concurrently rendering the same context provider . this is currently unsupported . but everything works well , both renderers can trigger an update . # # the expected behavior do not show any warning .",2
facebook/react,"showing an array in string format in ui in js , array rendered with ' , ' in between each element e . g . [ ' piyush ' , ' sinha ' ] / / piyush , sinha / / but in react array rendered without ' , ' in between each element e . g . [ ' piyush ' , ' sinha ' ] / / piyushsinha / /",2
facebook/react,"[ micro react apps ] - need to render another react app into existing react app hi , i need to render another react app ( i . e . app2 ) into existing react app ( i . e . app1 ) on run time . i have hosted my "" app2 "" on a remote server . i read "" asset - manifest . json "" file from it and on runtime i append those . js chunks in our head tag , this overall code i call from my "" app1 "" to load my "" app2 "" on runtime . but i am not able to trigger "" app2 "" . how should i triggered app2 component inside app1 component ? my application have lot of other internal dependencies , such redux , redux - thunk etc .",2
facebook/react,"devtool api request : add api for customize renderer inspect element i am wirte a custimse renderer for render element in canvas ( like react - pixi ) , i want intergrate with react dev tool ; i can hightlight element when click element in react dev tool compoent panel use code : ` ` ` ts __react_devtools_global_hook__ ? . <repeated> reactdevtoolsagent ? . <repeated> _bridge . addlistener ( ' highlightnativeelement ' , ( eleinfo => { const { id , rendererid } = eleinfo ; const renderer = __react_devtools_global_hook__ ? . <repeated> rendererinterfaces . get ( rendererid , ); const node_list = ( renderer . findnativenodesforfiberid ( id ) as sprite [ ] ) || []; / / . <repeated> customise render engin hightlight code }); ` ` ` i want hightlight ele when mouse move in canvas , i can use ` __react_devtools_global_hook__ . rendererinterfaces . get ( <number> ) . getfiberidfornative ( node ) ` find node fiber id , i can use ` _bridge ` send hightlint msg to backend just like the code <url> but i can not get store object and get enugh infomation send to backend . maybe dev tool can expose proper api for this function",2
facebook/react,"selenium integration hello . i am not sure if this is an issue , but i would like to know a little more about how react developer tools work . i want to get reacts props with selenium in order to make easier the debugging of a website . i ' d be thankful with any kind of help you can give me . best regards",2
facebook/react,"react hooks will render multiple times after await ` ` ` const [ html , sethtml ] = usestate ( ' ' ); const [ script , setscript ] = usestate ( ' ' ); const update = ( script , html ) => { setscript ( script ) ; sethtml ( html ) ; }; update ( ' a ' , ' b ' ); ` ` ` the above code works fine , react hooks will render once and combine setscript & sethtml ; ` ` ` const [ html , sethtml ] = usestate ( ' ' ); const [ script , setscript ] = usestate ( ' ' ); const update = async ( script , html ) => { await new promise ( resolve => settimeout ( resolve , <number> )); setscript ( script ) ; sethtml ( html ) ; }; update ( ' a ' , ' b ' ); ` ` ` the above code does not work anymore , react hooks will render twice and it does not combine setscript & sethtml . i can change to the code to : ` ` ` const [ state , setstate ] = usestate ( { html : ' ' , script : ' ' }); const update = async ( script , html ) => { await new promise ( resolve => settimeout ( resolve , <number> )); setstate ( { script , html }); }; update ( ' a ' , ' b ' ); ` ` ` the above code only renders once but it has a new bug cursor in the textarea ( where script and html go ) will move to the end of the textarea instead of staying at where it is .",2
facebook/react,extention react non définie sur chrome react developer tools <number> . <number> google chrome version <number> . <number> l ' outil de développement react est inactif sur la console google chrome,2
facebook/react,"question : why not usecallback always return static value without deps ? ` ` ` tsx function userefcallback < t extends ( . <repeated> args : any ) => void > ( callback : t ) { const ref = useref <t> ( callback ) ; ref . current = callback ; return usecallback ( function ( this : any , . <repeated> args { return ref . current . apply ( this , args ) ; } as t , []); } ` ` ` i think userefcallback is safe to replace usecallback in any code , and it ' s better than usecallback because it will never cause recalculation .",2
facebook/react,"question to remove dynamic children from parent state ? i am using react hook ` usecontext ` . i have two identical components ( siblings ) , each that use the same context that is a list . * scenario <emphasis> * <number> . the first sibling is created , calls ` usecontext ` , and then pushes something into the list . <number> . the second sibling is then created , using the same ` usecontext ` , and then pushes something into the list . * issue <emphasis> * the second sibling has the current state of list , which has two items , _but the first sibling state is not updated with the second item that was pushed in by the second sibling_ * expected <emphasis> * that each component that is using the same ` usecontext ` will be updated amongst all components that use the same context . is this a bug or am i misusing this ? any help or guidance is appreciated 🙇",2
facebook/react,"webpack can not find reactdom . createroot hi , i am kinda in doubt if this is an error or is just me doing something wrong . i am trying the new react experimental in a very simple existing app i have . although i had installed the react experimental versions in my package . json , it seems it does not recognize reactdom . createroot . when i try to run my project i receive the error message from the console : ` ` ` uncaught typeerror : react_dom__webpack_imported_module_1___default . a . createroot is not a function at module . <repeated> / src / client / index . js ( main . chunk . js : <number> ) at __webpack_require__ ( runtime . bundle . js : <number> ) at fn ( runtime . bundle . js : <number> ) at object . <number> ( main . chunk . js : <number> ) at __webpack_require__ ( runtime . bundle . js : <number> ) at checkdeferredmodules ( runtime . bundle . js : <number> ) at array . webpackjsonpcallback [ as push ] ( runtime . bundle . js : <number> ) at main . chunk . js : <number> ` ` ` this only happens when i try using ` ` reactdom . createroot ` ` . using ` ` reactdom . render ` ` everything works perfectly . any idea why this is happening ? ` ` ` "" dependencies "" : { "" chalk "" : "" ^ <number> . <number> "" , "" compression "" : "" ^ <number> . <number> "" , "" express "" : "" ^ <number> . <number> "" , "" morgan "" : "" ^ <number> . <number> "" , "" uuid "" : "" ^ <number> . <number> "" } , "" devdependencies "" : { "" <user> / cli "" : "" ^ <number> . <number> "" , "" <user> / core "" : "" ^ <number> . <number> "" , "" <user> / node "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - class - properties "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - decorators "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - export - namespace - from "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - function - bind "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - nullish - coalescing - operator "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - optional - chaining "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - pipeline - operator "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - private - methods "" : "" ^ <number> . <number> "" , "" <user> / plugin - proposal - throw - expressions "" : "" ^ <number> . <number> "" , "" <user> / plugin - syntax - dynamic - import "" : "" ^ <number> . <number> "" , "" <user> / preset - env "" : "" ^ <number> . <number> "" , "" <user> / preset - react "" : "" ^ <number> . <number> "" , "" <user> / preset - typescript "" : "" ^ <number> . <number> "" , "" <user> - loader / react - dom "" : "" ^ <number> . <number> "" , "" <user> - library / jest - dom "" : "" ^ <number> . <number> "" , "" <user> - library / react "" : "" ^ <number> . <number> "" , "" autoprefixer "" : "" ^ <number> . <number> "" , "" babel - eslint "" : "" ^ <number> . <number> - beta . <number> "" , "" babel - jest "" : "" ^ <number> . <number> "" , "" babel - loader "" : "" ^ <number> . <number> "" , "" babel - plugin - dynamic - import - node "" : "" ^ <number> . <number> "" , "" babel - plugin - styled - components "" : "" ^ <number> . <number> "" , "" case - sensitive - paths - webpack - plugin "" : "" ^ <number> . <number> "" , "" circular - dependency - plugin "" : "" ^ <number> . <number> "" , "" clean - webpack - plugin "" : "" ^ <number> . <number> "" , "" connected - react - router "" : "" ^ <number> . <number> "" , "" copy - webpack - plugin "" : "" ^ <number> . <number> "" , "" core - js "" : "" ^ <number> . <number> "" , "" css - hot - loader "" : "" ^ <number> . <number> "" , "" css - loader "" : "" ^ <number> . <number> "" , "" deep - freeze "" : "" ^ <number> . <number> "" , "" eslint "" : "" ^ <number> . <number> "" , "" eslint - loader "" : "" ^ <number> . <number> "" , "" eslint - plugin - babel "" : "" ^ <number> . <number> "" , "" eslint - plugin - import "" : "" ^ <number> . <number> "" , "" eslint - plugin - jsx - a11y "" : "" ^ <number> . <number> "" , "" eslint - plugin - ramda "" : "" ^ <number> . <number> "" , "" eslint - plugin - react "" : "" ^ <number> . <number> "" , "" eslint - plugin - react - hooks "" : "" ^ <number> . <number> "" , "" eslint - plugin - redux - saga "" : "" ^ <number> . <number> "" , "" eslint - watch "" : "" ^ <number> . <number> "" , "" file - loader "" : "" ^ <number> . <number> "" , "" hard - source - webpack - plugin "" : "" ^ <number> . <number> "" , "" history "" : "" ^ <number> . <number> "" , "" html - webpack - plugin "" : "" ^ <number> . <number> "" , "" immer "" : "" ^ <number> . <number> "" , "" jest "" : "" ^ <number> . <number> "" , "" jest - styled - components "" : "" ^ <number> . <number> "" , "" lodash "" : "" ^ <date> "" , "" mini - css - extract - plugin "" : "" ^ <number> . <number> "" , "" moment "" : "" ^ <number> . <number> "" , "" nock "" : "" ^ <number> . <number> "" , "" normalizr "" : "" ^ <number> . <number> "" , "" npm - run - all "" : "" ^ <number> . <number> "" , "" open "" : "" ^ <number> . <number> "" , "" optimize - css - assets - webpack - plugin "" : "" ^ <number> . <number> "" , "" pm2 "" : "" ^ <number> . <number> "" , "" postcss - flexbugs - fixes "" : "" ^ <number> . <number> "" , "" postcss - loader "" : "" ^ <number> . <number> "" , "" prop - types "" : "" ^ <number> . <number> "" , "" ramda "" : "" ^ <number> . <number> "" , "" react "" : "" ^ <number> . <number> - experimental - 2 4 1 c4467e "" , "" react - dom "" : "" ^ <number> . <number> - experimental - 2 4 1 c4467e "" , "" react - hooks - testing - library "" : "" ^ <number> . <number> "" , "" react - hot - loader "" : "" ^ <date> "" , "" react - is "" : "" ^ <number> . <number> "" , "" react - redux "" : "" ^ <number> . <number> "" , "" react - router "" : "" ^ <number> . <number> "" , "" react - router - dom "" : "" ^ <number> . <number> "" , "" react - test - renderer "" : "" ^ <number> . <number> "" , "" redux "" : "" ^ <number> . <number> "" , "" redux - actions "" : "" ^ <number> . <number> "" , "" redux - devtools - extension "" : "" ^ <number> . <number> "" , "" redux - logger "" : "" ^ <number> . <number> "" , "" redux - saga "" : "" ^ <number> . <number> "" , "" redux - saga - test - plan "" : "" ^ <number> . <number> - rc . <number> "" , "" regenerator - runtime "" : "" ^ <number> . <number> "" , "" reselect "" : "" ^ <number> . <number> "" , "" source - map - loader "" : "" ^ <number> . <number> "" , "" style - loader "" : "" ^ <number> . <number> "" , "" styled - components "" : "" ^ <number> . <number> "" , "" stylelint "" : "" ^ <number> . <number> "" , "" stylelint - bare - webpack - plugin "" : "" ^ <number> . <number> "" , "" stylelint - config - recommended "" : "" ^ <number> . <number> "" , "" stylelint - config - standard "" : "" ^ <number> . <number> "" , "" stylelint - config - styled - components "" : "" ^ <number> . <number> "" , "" stylelint - custom - processor - loader "" : "" ^ <number> . <number> "" , "" stylelint - order "" : "" ^ <number> . <number> "" , "" stylelint - processor - styled - components "" : "" ^ <number> . <number> "" , "" stylelint - selector - bem - pattern "" : "" ^ <number> . <number> "" , "" thread - loader "" : "" ^ <number> . <number> "" , "" typescript "" : "" ^ <number> . <number> "" , "" url - loader "" : "" ^ <number> . <number> "" , "" webpack "" : "" ^ <number> . <number> "" , "" webpack - dev - middleware "" : "" ^ <number> . <number> "" , "" webpack - hot - middleware "" : "" ^ <number> . <number> "" , "" webpack - manifest - plugin "" : "" ^ <number> . <number> "" , "" webpack - merge "" : "" ^ <number> . <number> "" , "" webpack - pwa - manifest "" : "" ^ <number> . <number> "" , "" workbox - webpack - plugin "" } ` ` `",2
facebook/react,"bug : nested setstate and unstable_batchedupdates ( are they ignored ? ) nested setstate and unstable_batchedupdates ( are them ignored ? ) react version # # steps to reproduce <url> open profile after click , you will see <number> commit . it seems that even if we use unstable_batchedupdates , nested setstates called on didupdate / layouteffect do not get batched .",2
facebook/react,"while using useref , some data has been updated by context value from reducers , when the context value was updated from some other user event , variable used with useref also updated , then how to use instance variables in hooks ? const createnotificationbase = ( props ) => { const [ state , dispatch ] = usecontext ( store ) ; const draftdata = useref ( {}); useeffect ( ( ) => { if ( state . notificationdetails . draftid ) { draftdata . current = state . notificationdetails ; } } , []); useeffect ( ( ) => { debugger } , [ draftdata . current ] ) } when value in store context changes useeffect of draftdata . current also called . please suggest hwo to resolve .",2
facebook/react,"question for my app test . thanks . 🚨 this issue tracker is not for questions . 🚨 as it happens , support requests that are created as issues are likely to be closed . we want to make sure you are able to find the help you seek . please take a look at the following resources . # # coding questions if you have a coding question related to react and react dom , it might be better suited for stack overflow . it ' s a great place to browse through frequent questions about using react , as well as ask for help with specific questions . <url> # # talk to other react developers there are many online forums which are a great place for discussion about best practices and application architecture as well as the future of react . <url> # # proposals if you ' d like to discuss topics related to the future of react , or would like to propose a new feature or change before sending a pull request , please check out the discussions and proposals repository . <url>",2
facebook/react,"question : react apollo hooks fails after adding react - native to monorepo i am trying to create a react web app and react - native app with monorepo by using yarn workspaces . so i created web and controllers and it works fine . i was able to make graphql queries to my apollo - express server . but , after adding react - native application i see this : [ [ enter image description here ] [ <number> ] ] [ <number> ] [ <number> <sad> <url> i am <percent> that i am not breaking any react hooks rules because before adding react - native application it was work fine . is there any way how can i solve it ? apollo controller import { usequery } from "" <user> / react - hooks "" ; import gql from "" graphql - tag "" ; export const usehelloquery = ( ) => usequery ( gql ` { hello } ` ) react component test ( ) { const data = usehelloquery ( ); return ( <text> awesoe </text> ); } before adding react - native it was exactly same",2
facebook/react,"question : react lib context overrides app context hi , i believe that this can potentially be an issue , though i am not sure . i was wondering about good patterns concerning the react context . let us get a public library , for example ` react - intl ` that exposes a provider ( intlprovider ) . let us create a library ` barlib ` that uses ` react - intl ` to manage translations internally and that also exposes a ` provider ` . now let us imagine i create a web app and use both ` barlib ` and ` react - intl ` . i do not expect the ` barlib ` to ever override my react - intl context , because i am not aware it uses internally the ` react - intl ` lib . but the ` barlib ` can accidentally override the react - intl context => <url> this kinda breaks the isolation of libs imo . i see several solutions do not use an other lib context in the ` barlib ` ( pretty extreme ) - check that there is not already an intl context in the ` barlib ` , if so merge the context values ? i am not convinced with either solutions , what do you guys think about it ?",2
facebook/react,"question : why cann ' t i set echarts instance using usestate ? # # coding questions i try to store the echart instance using usestate , but after ` setinstance ` , ` instance ` is always undefined ` ` ` const [ instance , setinstance ] = usestate < echarts | undefined > ( undefined ) ; const chartinstance = echarts . init ( root . current ) ; setinstance ( prev => { console . log ( ' prev instance ' , prev ) ; / / first time : undefined , then echarts instance return prev || chartinstance ; }); console . log ( ' instance instance , chartinstance ) ; / / instance is always undefined ` ` `",2
facebook/react,"help ~ when i use react . createelement directly and how to pack the component ? hello all . i was met a problem when i published a react component , i need to help and discuss with you all i am going to write a component to load ` react component from cdn ` and make it like a wrapper component . ` ` ` jsx import react , { component } from ' react ' ; import scriptjs from ' scriptjs ' ; const dev_script = ' <url> const prod_script = ' <url> class phoenixheaderwrapper extends component { constructor ( props ) { super ( props ) ; this . type = null ; this . scripturl = props . scripturl || ( props . env = = ' prod ' ? dev_script : prod_script ) ; this . state = { cmp : null , }; } componentdidmount ( ) { scriptjs ( this . scripturl , ( ) => { this . type = window . phoenixheader ; this . createorupdatecomponent ( ); }); } createorupdatecomponent ( ) { const { scripturl , . <repeated> otherprops } = this . props ; if ( ! this . type ) { console . error ( ' load component failed ' ); return ; } const cmp = react . createelement ( this . type , otherprops || {}); / / this is point this . setstate ( { cmp }); } render ( ) { const { cmp } = this . state ; return cmp ; } } phoenixheaderwrapper . defaultprops = { scripturl : ' ' , env : ' prod ' , }; export default phoenixheaderwrapper ; ` ` ` and then , i do it just like do a normal react component that i was set the webpack config like below : ` ` ` js / / phoenixheader output : { publicpath : ' . / ' , filename : ' phoenix - header . js ' , path : paths . appbuild , library : ' phoenixheader ' , librarytarget : ' commonjs2 ' , libraryexport : ' default ' , } , mode : ' production ' , externals : { react : { commonjs : ' react ' , commonjs2 : ' react ' , amd : ' react ' , root : ' react ' , } , [ ' react - dom ' <sad> { commonjs : ' react - dom ' , commonjs2 : ' react - dom ' , amd : ' react - dom ' , root : ' reactdom ' , } , } , ` ` ` and then , i run the ` npm run build & & npm publish ` to publish it . in my own object which use the phoenixheader component as a npm package . ` ` ` jsx / / app . jsx import react from ' react ' ; import phoenixheader from ' phoenix - header ' ; import ' . / app . css ' ; function app ( ) { return ( < div classname = "" app "" > < phoenixheader / > < header classname = "" app - header "" > <p> edit <code> src / app . js </code> and save to reload . </p> < a classname = "" app - link "" href = "" <url> target = "" _blank "" rel = "" noopener noreferrer "" > learn react </a> </header> </div> ); } export default app ; ` ` ` after ` npm start ` , it show me a error : ! [ image ] ( <url> * * that it is my confusion is , why ` createelement ` not in this scope . * * so i try to make react set into global window . react = react ; ` ` ` yes , it ' s work for me . * * but someone would like to tell me why need to make react set into global ? * * * * and am i need to build this wrapper component which do not external the react when i was build ? * * let us discuss or give me more suggest about this wrapper component please 🙏",2
facebook/react,"rendering react component on server to take a screenshot of it my react application has a list of to do lists like this : < img width = "" <number> "" alt = "" figma_canvases "" src = "" <url> > replace the figma canvas preview images with todo lists above . in the list of todo lists , i ' d like to include a preview of each list as well as its name . here ' s what i thought of to achieve this whenever a todo list is updated , run a aws lambda ( node . js environment ) job which generates html for the todo list server - side ( using ` reactdomserver . renderstaticmarkup ` . - then , serve that html locally , visit the page using puppeteer , and take a screenshot of the page however , this seems like a lot of work . is there an easier way of achieving this ? is there anything in the steps above that will not work ? i have looked at ` repng ` [ <number> ] so far but it has not worked for the components i tested it with . [ <number> ] <url>",2
facebook/react,"i can not get the latest status value in the initialization method i try to get the latest state value in the initialization method , but the result is not satisfactory 。 all this happens in function components 。 <number> . react version is <number> . <number> ； <number> . react - dom version is <number> . <number> ； this is my demo code import react , { usestate , useeffect } from ' react ' ; const app =() =>{ / * button click */ const btnclick = ()=> { console . log ( "" initbutton getting state is = = ==> "" , nowstate ) ; / / can not get now state , all is init value } / * init a button */ const inittitle = ()=> { return ( < button onclick ={ btnclick } > click </button> ) } const [ title , settitle ] = usestate ( ); / * now state */ const [ nowstate , setnowstate ] = usestate ( <number> ); console . log ( "" now state is = = ==> "" , nowstate ) ; useeffect ( ()=> { settitle ( inittitle ( )); } , [ ] ) return ( < > { title } < button onclick ={() => setnowstate ( nowstate + <number> ) } > change now state </button> < / > ); } export default app ; ` ` `",2
facebook/react,"[ typescript ] is there any way to define the state variables when using functional component ? < - - note : if the issue is about documentation or the website , please file it at - - > if there is a lot of state variables , i guess it is hard to get a whole picture the state variables verse class components * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * * * what is the current behavior ? * * * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * *",2
facebook/react,getderivedstatefromprops is executed after setstate # <number> ideally getderivedstatefromprops should not be called due to current component ' s setstate . but it is behaving so . can someone explain ? could not find solution in [# <number> ] ( <url>,2
facebook/react,"why only one component can be render at root div ? i called two render methods to same root div . ` ` ` renderdom . render ( < navigation / > , document . getelementbyid ( ' root ' )); renderdom . render ( < app / > , document . getelementbyid ( ' root ' )); ` ` ` and what i get rendered on my screen is only a app component . just want to know that the one render method override the previous render method ?",2
facebook/react,"bug : react table - element type is invalid : expected a string ( for built - in components ) or a class / function ( for composite components ) but got : undefined . * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * hello , i am new in react and i am trying to create a simple react table to display data from db with login for user authentication . until i added the login , it worked . momentally , it still throws the same mistake , no matter what . none of the existing solutions helped me , i tried to repair imports and exports , i reinstalled nodejs , reinstalled node_moduls . first , i verify the user by logging in to redirect me to the records page after verification . verification is ok , but then it crashes . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * edit : login , password admin <url> <url> [ [ edit black - voice - 2 z6y5 ] ( <url> ! [ error_message ] ( <url> * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * nodejs version v8 . <number> npm version <number> . <number> ubuntu / chrome please give me any advice . thank you in advance <happy>",2
facebook/react,"how do suspense and subscriptions interact ? * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * ask a question * * what is the current behavior ? * * with ` suspense ` , a component throws a ` promise ` when it encounters something that ' s not ready . however , our company ' s data fetching is subscription - oriented : at a very high level , the following happens : <number> . component renders , and calls ` usefoo ( id , ' name ' , ' amount ' , ' discounts ' ); ` <number> . internally , the hook adds a callback to the ` fooloader ` which is responsible for batching and sending async requests . the callback will invoke the setter for a ` usestate ` inside the hook to force a rerender of the consuming component . <number> . the hook returns a [ ` remotedata < pick < foo , ' name ' | ' amount ' | ' discounts ' > > ` ] ( <url> which could contain the data if it was locally available , or is just a constant that says , "" i have not asked for this data yet "" <number> . when the data becomes available , or the request for the data fails , the ` fooloader ` invokes the hook - passed - in callback method , which triggers the rerender , which presents the new component state . at no point in there does the component have a ` promise ` . as a result , i am not sure it ' s possible for it to throw anything ; it ' s depending on the ` usestate ` inside ` usefoo ` to trigger rerender when data is available , but if we throw then that never happens and we would not ever complete . throwing a ` promise ` from inside the hook does not seem right , as it would be very difficult to combine multiple data dependencies in a single component . the only alternative seems to be a pretty sizable plumbing change to return a ` promise ` instead of the loading state constant , but that seems to force a lot of allocations and boilerplate code to handle the "" maybe not ready the first time "" case . note in response to [ this tweet ] ( <url> by <user>",2
facebook/react,"why include refs as a feature ? - - seems broken and unnecessary refs never seem to work , is this a broken feature ? why not remove them and just let people use document . queryselector ( ' <hashtag> i d </hashtag> ' ) . action like normal people . also , what does this mean ? function components cannot have refs . did you mean to use react . forwardref ( ) ?",2
facebook/react,"devtools component filter does not work with location * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * report a bug . * * what is the current behavior ? * * in devtools when a component filter is added with field set to ` location ` and regex set to ` . * ` , nothing is filtered out . ( btw , no documentation in this subject is available anywhere . ) * * what is the expected behavior ? * * at least some components be filtered out . it would also be awesome if the ` location ` was shown somewhere for the selected component , so that users can know what kind of regex they should put together . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react devtools <number> . <number> on firefox <number> . <number> .",2
facebook/react,"why is useeffect hook not activating when a component is reloaded after previously throwing an error ? i am learning react and redux within a typescript environment . i have managed to implement a container that dispatches a fetch action and subscribes to corresponding fetch success and error state notifications from a redux store . the source code is listed below : * container <emphasis> * ` ` ` typescript import react , { useeffect } from ' react ' ; import { connect } from ' react - redux ' ; import grid from ' <user> - ui / core / grid ' ; import { gridspacing } from ' <user> - ui / core / grid ' ; import course from ' . <repeated> / components / course / course ' ; import { coursemodels } from ' . <repeated> / redux / features / course ' ; import { courseselectors } from ' . <repeated> / redux / features / course ' ; import { fetchcoursesasync } from ' . <repeated> / redux / features / course / actions ' ; import { rootstate } from ' reduxtypes ' ; type errorreport = { haserror : boolean ; error ? : error }; type stateprops = { isloading : boolean ; courses : coursemodels . course [ ]; error : errorreport ; }; /* * * redux dispatch and state mappings */ const dispatchprops = { fetchcourses : fetchcoursesasync . request , }; const mapstatetoprops = ( state : rootstate ) : stateprops => ( { isloading : state . courses . isloadingcourses , courses : courseselectors . getreduxcourses ( state . courses ) , error : courseselectors . getreduxcourseserror ( state . courses ) , }); /* * * component property type definitions */ type props = returntype < typeof mapstatetoprops > & typeof dispatchprops ; /* * * courselist component */ const courselist = ( { courses = [ ] , error , fetchcourses , isloading , <sad> propas ) : jsx . element => { / / fetch course action on mount useeffect ( ( ) => { console . log ( ' courselist fetching courses ' ); fetchcourses ( ); } , [ fetchcourses ] ); if ( isloading ) { return <p> loading . <repeated> </p> ; } if ( error & & error . haserror & & error . error ) { throw error . error ; / / if throw an error then encapsulating error boundary catches and displays . / / however when the container is loaded again via clicking on a navbar link the useeffect / / action does not trigger . / / alternatively , if the error is rendered inside the container then the useeffect hook is / / still activated if the container is loaded again ( e . g . via clicking on a navbar link ) . / / return <p> { json . stringify ( error . error , null , <number> ) } </p> ; } return ( < div style ={{ margintop : <number> , padding } } > { < grid container spacing ={ <number> as gridspacing } justify = "" center "" > { courses . map ( element => ( < grid item key ={ element . courseid } > < course course ={ element } / > </grid> ) ) } </grid> } </div> ); }; /* * * exports */ export default connect ( mapstatetoprops , dispatchprops , ) ( courselist ) ; ` ` ` if i throw an error within the container then the encapsulating error boundary catches and displays it . however , when the container is reloaded via clicking on a navbar link the useeffect action does not trigger . subsequently , the fetchcourses action is not dispatched . why is the _useeffect_ hook not triggered on second load after it previously threw an error ? my errorboundary component includes a home button for navigating to ' / ' . however , after clicking home , if i then click on link to display my courselist container the errorboundary is again displayed . i do not see the console log message displayed from useeffect . when navigating back to ' / courses ' should not this recreate the courselist container ? is this not happening because the error was thrown in render previously , so the container is being reused ? what is best practice for resetting a component that threw an error for surrounding errorboundary ?",2
facebook/react,"utilize suspense to express app init loader with concurrent mode getting finalized , i went to try to solve an old problem i have . the app needs to run a series of init steps before it can do anything viable . each steps depends on the previous one . this is expressed as nested components . the fairly elaborate animated logo is shown for that process , but it ' s being restarted for each step and the experience is not that nice . i have prepared a demo with some experiments the first one that ' s active initially is what we currently have . the second is what we would like to have , but with less naive coding of decreasing counter . the last one is my attempt to use suspense , but i definitely missing something out here , because it behaves very oddly and i cannot seem to figure out why . i wonder if the new ` usetransition ` should be used somehow or what ' s going on here .",2
facebook/react,"weird behavior with functional components and usestate , a bug or "" another rule "" < ! - - note : if the issue is about documentation or the website , please file it at : <url> - - > ` ` ` react sample code ` ` ` javascript import react from "" react "" ; import reactdom from "" react - dom "" ; function optionone ( ) { return <div> blue pill </div> } const optiontwo = react . memo ( function optiontwo ( ) { return <div> red pill </div> }); function app ( ) { const [ option , setoption ] = react . usestate ( null ) ; return ( <div> <div> < button onclick ={ e => setoption ( optionone ) } > option one </button> < button onclick ={ e => setoption ( optiontwo ) } > option two </button> </div> { option & & <option/> } </div> ); } const rootelement = document . getelementbyid ( "" root "" ); reactdom . render ( < app / > , rootelement ) ; ` ` ` well the expected behavior was option one wasn ' t suppose to throw an error ; setstate in class components did not mind if a key value was a pure functional component or a class component even , but it seams the usestate hook is sensitive to the value passed using the setter . if its another functional component it throws a confusing error , worst if the pure functional component passed to setvlaue of usestate hook uses hooks of its own , then you get multiple misleading hooks errors to debug . so is the usestate hook working like its suppose to "" a rule not to pass a pure functional component as a value to the setvalue of the usestate hook "" or a bug ?",2
facebook/react,"i was trying to pass through hook effect , and the codes used are declared no where . like chatapi ` ` ` js import react , { usestate , useeffect } from ' react ' ; function friendstatus ( props ) { const [ isonline , setisonline ] = usestate ( null ) ; function handlestatuschange ( status ) { setisonline ( status . isonline ) ; } useeffect ( ( ) => { chatapi . subscribetofriendstatus ( props . friend . id , handlestatuschange ) ; return ( ) => { chatapi . unsubscribefromfriendstatus ( props . friend . id , handlestatuschange ) ; }; }); if ( isonline = = = null ) { return ' loading . <repeated> ' ; } return isonline ? ' online ' } ` ` `",2
facebook/react,"why does parent ' s componentdidmount gets called first , then child ' s ? i thought it was always the case , that first , child ' s ` componentdidmount ` would be called , and then , parent ' s . however , in this example : <url> first parent ' s ` componentdidmount ` is called , then child ' s ( check the logs ) . this happens _after_ i use this in child export default compose ( withstyles ( styles ) , withwidth ( ) ) ( child ) ; ` ` ` does anyone have explanation why this happens ?",2
facebook/react,"read latest state value in event handler i have this question i could have asked on stack overflow , but i think it is more suitable for people from react team or more experienced users . so i will give it a try to ask it here . imagine i have event handler below where i * * want to read latest state value and do something with it ( however , not do a new ` setstate ` )* * : onclick ( ) { let data = this . state . data ; api . makerequest ( "" url "" , data ) ; } [ here ] [ <number> ] dan abramov says it is safe to read state in event handler ( in the sense it will be up to date ) . however he says this applies to react <number> . otherwise he suggests to use functional ` setstate ` to get current state . i have question : what if i want to read latest state value in event handler , however such that it works also in later react versions ( <number> + , without hooks ) ? one option imho would be to use functional ` setstate ` , but what if like in the beginning of question i said i do not want to ` setstate ` , just read the state value - and do something with it like network request . so using functional ` setstate ` would not be right for me , as it would force me to update state , right ? or i could return empty object from functional ` setstate ` ( to avoid updating state ) and put the network request code inside it , like this : onclick ( ) { this . setstate ( ps =>{ / / using this form only to read current state in event handler which works for <number> and onwards versions probably / / but this breaks purity of this function api . makerequest ( "" url "" , ps . data ) ; return {}; } ) } but then i would violate that the function passed to functional ` setstate ` must be pure . is there a solution to the question i asked ? [ <number> ]",2
facebook/react,"devtools tree navigation * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * where is no way to collapse / expand component tree or it subtrees . if i pick element with "" select element "" button - i got all the tree expanded , not the only one subtree , where selected element came from ( probably bug ? ) and i do not found way to collapse tree . so if you pick something from page your component tree is basically is always expanded on all levels . maybe we can have selected row context menu like the one in chrome devtools with options to collapse / expand . [ image ] ( <url> ! [ image ] ( <url> also i guess it maybe good place to add <url>",2
facebook/react,"there is no ' rendered by ' section in my extension < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * * * what is the current behavior ? * * * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * *",2
facebook/react,react devtools always launches in chrome hi . i have the extension installed on both chrome and firefox . my default browser is firefox . each time i try to debug a react native app remotely it launches chrome to use the debugger there . how do i change the default browser for devtools to firefox ?,2
facebook/react,"limitations of context api compared to legacy implementation this is not technically a bug , but a limitation of the new context api implementation and also a question on whether this should be fixed by react , or if i should implement a custom , in - house , solution instead . i have a fairly "" edgy "" use - case with a component library that provides several parent - child components ( e . g . tabs , accordions etc . ) . this looks like a standard context use - case so far . however , the library has to work in a plugin - based environment , meaning the parent and child components are rendered by two completely independent apps ( host app and plugins respectively ) . example <accordion> < - - provided by host app - - > < ! - - provided by separate plugin - - > < accordionitem label = "" item <number> "" > content </accordionitem> / / . <repeated> </accordion> ` ` ` thus , two separate instances of the component library are created , one for the host app and one in the plugin scope , but they still have to communicate with each other , behind the scenes , while keeping it transparent to the user . this worked seamlessly with the legacy context api , but now with the new one , it breaks . that is because ` react . createcontext ` returns an object that now has to be explicitly shared between the components , by means of an import . but because of the decoupled architecture of the app , and multiple instance of the library being used , this seems impossible to do . as stated in the docs , the legacy context api is deprecated and will be removed in future versions of react ( <number> + ? ) , so i ' d like to avoid being stuck on react <number> , when that happens .",2
facebook/react,"error message when calling work . commit in commit phase this was me just toying around with unstable apis . i do not know what these can be used for . just tried to make sense of them from their names . * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * error message of ` batch . commit ( ) ` is confusing * * what is the current behavior ? * * i was experimenting with the ` unstable_create ( sync ) root ` apis and just tried to piece together what goes where . while looking through the source i found that the ` work ` returned from ` createroot ` has a parameter called ` oncommit ` . for me this implied it ' s called after the commit [ which does not seem to be intended ] ( <url> however given the code below i get ` uncaught invariant violation : work . commit ( <sad> cannot commit while already rendering . this likely means you attempted to commit from inside a lifecycle method . ` ` ` ` js function app ( ) { return ( < div classname = "" app "" > <h1> hello codesandbox </h1> <h2> start editing to see some magic happen </h2> </div> ); } const rootelement = document . getelementbyid ( "" root "" ); const root = reactdom . unstable_createroot ( rootelement ) ; root . render ( < app / > ) . then ( ( ) => { const batch = root . createbatch ( ); batch . render ( < app />); / / bad batch . commit ( ); settimeout ( ( ) => { / / good / / batch . commit ( ) } , <number> ); batch . then ( ( ) => { / / good / / batch . commit ( ); }); }); ` ` ` three things i noticed : <number> . seems like i need to call batch . commit after the batch is complete . changing it to ` ` ` batch . then ( ( ) => { / / good batch . commit ( ); }); ` ` ` got rid of the error . but i noticed that the original code is used throughout the internal tests . in fact moving createbatch and batch . commit the batch well after ( long timeout ) the initial root . render call seemed to allow sync ` batch . render ; batch . commit ` . <number> . "" lifecycle "" should be replaced with something different before these apis get stable since we try to get away from this mental model . for example [ ` applies setstate in componentdidmount synchronously in a batch ` ] ( <url> could be converted to a sync test and still pass while i would have expected [ ` can defer a commit by batching it ` ] ( <url> to fail with the invariant violation from above . <number> . not all lifecycles are "" during rendering "" as far as i know . ` componentdidupdate ` is called during commit . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * <url> * * what is the expected behavior ? * * i am basically asking when it ' s safe to call ` batch . commit ` [ ] during render phase * [ ] during commit phase * [ ] outside of these phases e . g . in some ` oncomplete ` callback * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * ` react ( - dom ) <user> . <number> `",2
facebook/react,"devtools v4 are removed ? * * request for feature * * the older devtools used to show a breadcrumbs / component hierarchy , at the bottom , it is really useful to navigate the parent component , i could not find it on the new devtool , did we remove it ? any reason to remove it ? will we add it back ?",2
facebook/react,"why does devtool chrome extension need access to history ? i woke up today and the devtool extension for chrome asked for additional permissions . more specifically access to history . i could not find any explanations or reference in the changelog , so i opened this issue .",2
facebook/react,"suspense + concurrent mode immediately shows fallback when updated from onchange < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * reporting a bug , or at least trying to understand some spooky behavior 👻 * * what is the current behavior ? * * i have a simple component which reads from a toy suspense - enabled cache . the dummy cache simply waits 1 0 0 ms before responding to anything . the cache key is based on some component state ( managed with the usestate hook ) . this component is wrapped with ` <suspense> ` and rendered in a react root with concurrent mode enabled . ` ` ` javascript function myapp ( ) { let [ text , settext ] = react . usestate ( ' stuff ' ) return <div> data < button onclick ={ e => settext ( ' b - a - n - a - n - a - s ' ) } > update text from button </button> < input type = "" text "" value ={ text } onchange ={ e => settext ( e . target . value ) } / > </div> } reactdom . unstable_createroot ( document . getelementbyid ( ' root ' ) ) . render ( < react . suspense fallback ={< div > cause i ain ' t no fall - a - back div </div> } > < myapp / > < / react . suspense > ) ` ` ` the component has a text field whose value is set to the current cache key ( with an onchange handler that updates the usestate hook when the text changes ) . it also has a button which updates the state to some fixed string "" asdf "" when clicked . clicking the button does what i would expect — the page does not respond for a fraction of a second ( while the data is being "" fetched "" ) and then updates with a view of the loaded data . * * editing the text however ( for instance , typing a single letter in the field ) immediately causes the fallback ui to load and unfocuses the text input * * . this happens even if the ` settimeout ` is changed to 0 ms , or ` requestanimationframe ` , or a ` setimmediate ` polyfill . rather than directly calling ` settext ` within the ` onchange ` handler — if i call it within a ` settimeout ( . <repeated> , <number> ) ` , it behaves the way i would expect ( i . e . without unfocusing the field and loading fallback ) . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . * * <url> * * what is the expected behavior ? * * i would expect that it should not really matter whether i am updating state from an ` onchange ` versus an ` onclick ` . i would expect that the fallback ui does not show up until its max duration is met . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * i am using react <number> . <number> . i tested it on chrome <number> on macos <number> .",2
facebook/react,"[ eslint - plugin - react - hooks ] : can not call hooks on component returned from function wrapping a component in a function , returning the component . tl ; dr ` ` ` javascript export function home ( ) { return function ( ) { const [ items , setitems ] = usestate ( [ ] ) return <div> nothing </div> } } const root = document . getelementbyid ( ' root ' ) reactdom . render ( react . createelement ( home ( ) ) , root ) ` ` ` results in the following error : ` ` ` react hook "" usestate "" cannot be called inside a callback . react hooks must be called in a react function component or a custom react hook function ` ` ` however this works export function home ( ) { const [ items , setitems ] = usestate ( ' ' ) return <div> nothing </div> } const root = document . getelementbyid ( ' root ' ) reactdom . render ( react . createelement ( home ) , root ) ` ` `",2
facebook/react,"useeffect for synchronizing state and props hi . i have a recurring scenario that i ’ ve been struggling with since the good old days of ` componentwillreceiveprops ` , and now i ’ ve pretty much run into the same issue with hooks , so i was hoping i could get some guidance as to what the idiomatic way of solving this and similar cases in react is . # # # problem description - starting point i have a list of items . every item has an edit button next to it . clicking it opens an “ editor ” , where one can change all the fields and either confirm or cancel . ( confirming would send an api call to save the data , but this part is not relevant to the problem i am having . ) the “ parent ” component would render the list with the edit buttons , and have an ` itemunderedit ` property that would be null from the start . clicking on “ edit ” for a specific item would set the ` itemunderedit ` to the clicked item . [ usecase ] ( <url> here is the full example with all <number> solutions on codesandbox : <url> # # # solution <number> make the “ editor ” component stateless and controlled - it takes in change handlers for every field as props with the parent tracking every change . this solution appeals to me , since i like pure stateful components that are a one - to - one mapping of props to html - they are simple to reason about etc etc . this kind of goes against the commonly heard “ keep your state close to where it is used ” advice , which also seems reasonable , since i don ’ t really need to know in the parent what the user is typing , i am only interested to know when they are done at the end . this stateless solution also introduces a lot of props , since i need one event handler per field ( onnamechanged , ondescriptionchanged in the example , but it could as well be <number> fields ) , which is a lot of props . # # # solution <number> make the “ editor ” component stateful and only get an event when editing is done : ` onconfirm ( itemtosave ) ` or ` oncancel ( ) ` . this seems like the “ react ” way and is in line with the advice of keeping state close to where it is used . since i am only interested to know when the user clicks ` confirm ` , a stateful “ blackbox ” - component that tracks its own state seems reasonable . in order to achieve this , however , i need to copy my props to the state , which , according to <user> , is a bad idea : ` ` ` const [ name , setname ] = usestate ( props . item . name ) ; const [ description , setdescription ] = usestate ( props . item . description ) ; ` ` ` moreover , this solution is buggy from the start , since clicking on edit for a different item doesn ’ t “ re - sync ” the props with the state - it only works if i close the editor and then reopen it : ! [ stateful_editor1 ] ( <url> which brings us to solution <number> . # # # solution <number> this one has been one of my biggest pain - points with stateful components in react ( which is why i prefer stateless components with a state container , but those i widely demonized nowadays , so i am yet again trying to understand the idiomatic react way of doing this ) . the “ old ” ways were to sync in ` componentwillreceiveprops ` and later with ` getderivedstatefromprops ` . now i can do this with ` useeffect ` , where i specify ` props . item ` as the “ dependency ” , since i want to run it when the item changes . ` ` ` useeffect ( ( ) => { if ( props . item . name ! = = name ) { setname ( props . item . name ) ; } if ( props . item . description ! = = description ) { setdescription ( props . item . description ) ; } } , [ props . item ] ); ` ` ` this seems to work as expected , but i get the linter warning : ` react hook useeffect has missing dependencies : ' description ' and ' name ' . either include them or remove the dependency array react - hooks / exhaustive - deps ` . obviously if i were to add those to the dependency list , i wouldn ’ t be able to change anything in the inputs , so how come i get this warning ? # # # summary this is a question in two parts : first one about an idiomatic solution in react , as well as feedback to the react team scenario is simple and common , but * it ’ s difficult to know how to implement correctly and safely in a consistent way * . lifting state up and making the problematic component stateless is good advice that solves the problem , but every time it seems like a “ temporary ” solution . it also leads to painful refactoring every time something has to be moved around the component tree , so relying on it in the long run is extremely brittle . the second part of the question is whether the solution with ` useeffect ` is viable at all , and in this case - why do i get the linter warning ? clearly i want to run it * only <emphasis> * when a certain prop changes . is there an edge - case where this would result in an unexpected bug ?",2
facebook/react,"` static getderivedstatefromprops ( ) ` does not works same as componentwillreceiveprops < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * ` static getderivedstatefromprops ( ) ` is not a replacement for ` componentwillreceiveprops ` * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> hi i am trying to implement ` toasternotificationcards ` which will be displayed when a user save an item , stating ` item saved successfully ` below is my code i am using ` componentwillreceiveprops ` which is depreciated i tried ` static getderivedstatefromprops ( ) ` but it did not work how can i removed ` componentwillreceiveprops ` , assuming the close button should not be in parent component ( whichever is calling ` notificationcomponent ` ) [ jsfiddle working example ] ( <url> requirement : on click of the button show ` notificationcard ` on click on close hide ` notificationcard ` ` ` ` jsx class notification extends react . component { constructor ( props ) { super ( props ) ; this . state = { open : true }; } componentwillreceiveprops ( props ) { this . setstate ( { open : props . show }); / / settimeout ( this . handleclick . bind ( this ) , <number> ); } handleclick ( ) { this . setstate ( { open : false }); } componentdidmount ( ) { / / settimeout ( this . handleclick . bind ( this ) , <number> ); } render ( ) { if ( ! this . state . open ) { return null ; } return ( <div> < br / > <div> item saved successfully </div> < div classname = "" cls - - btn "" onclick ={() => this . handleclick ( ) } > & # <number> ; </div> </div> ); } } class test extends react . component { handleclick ( ) { this . setstate ( { show }); } render ( ) { return ( <div> < button onclick ={ this . handleclick . bind ( this ) } > click </button> < notification show ={ true } / > </div> ); } } reactdom . render ( < test name = "" world "" / > , document . getelementbyid ( "" container "" )); ` ` `",2
facebook/react,"declarative vs . imperative coding style using hooks _if this should be asked on stack overflow instead , please let me know and feel free to close the issue . _ consider a component that fetches some data in a custom hook , saves the fetched data in a state hook , and notifies the user that data has been fetched using a prop callback . notifying the user can be done imperatively : ` ` ` js function component ( props ) { const [ data , setdata ] = usestate ( null ) ; useapi ( "" / api / data "" ) . then ( setdata ) . then ( props . onfetched ) ; return datatoelements ( data ) ; } ` ` ` or declaratively using an effect hook function component ( props ) { const [ data , setdata ] = usestate ( null ) ; useapi ( "" / api / data "" ) . then ( setdata ) ; useeffect ( ( ) => { if ( data ) { props . onfetched ( ); } } , [ data ]); return datatoelements ( data ) ; } ` ` ` react seems to promote a declarative approach . but what i have found is that when components grow large and complex , using declarative effect hooks makes the flow of data and actions quite hard to follow . if you are not careful , a lot of things start to depend on a lot of other things , and the predicted results become non - intuitive and hard to wrap your head around . i would like to know other peoples ' opinions on this matter , and whether or not an imperative approach might sometimes be better .",2
facebook/react,"how to test multiple state changes with act ? * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * with the new ` act ` function , i am unsure how to test state transitions that occur _during_ an event handler processing . for example if i have this handler that is called on form submission const [ issubmitting , setissubmitting ] = usestate ( false ) ; const handlesubmit = async ( ) => { setissubmitting ( true ) ; await fetcher ( ); setissubmitting ( false ) ; }; ` ` ` then i want to be able to test that ` issubmitting ` state is indeed set to true before ` fetcher ` is called . due to the nature of ` act ` ( i believe it defers all state changes until after its provided function has been run ) i am not sure that this is currently possible ? previously i have been testing using ` await new promise ( settimeout ) ` to flush the current runtime task queue , which works fine for this use case . i _have_ found a way to make this work without triggering the ` act ` warning , but it feels like a hack . i have to wrap ` act ` around my expectation , not the submit . ` ` ` it ( ' displays indicator when form is submitting ' , async ( ) => { reacttestutils . simulate . submit ( form ( )); await act ( async ( ) => { expect ( container . queryselector ( ' . submittingindicator ' ) ) . not . tobenull ( ); }); }); ` ` ` i have provided this test in a repo together with a couple of other tests which complete the feature - see the link below . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * <url> * * what is the expected behavior ? * * there ' s a way for me to test this which does not feel like a hack . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number> - alpha . <number>",2
facebook/react,"[ question ] about "" . _owner . alternate "" it seems that in react <user> every react element children contains cyclic property ` . _owner . alternate . alternate . alternate . alternate ` . <repeated> [ image ] ( <url> so it is easy to cause "" maximum call stack exceeded "" error when developer compares ` props . children ` in deep way ( like [ deep - equal ] ( <url> this caused bugs for community libraries like [ react - helmet ] ( <url> <url> i am extremely curious about react has ` _owner . alternate ` , and what it stands for ? as it is named as “ _owner ” , could it be better if we make “ owner ” not enumerable ? i tried to search source code and but still can ’ t find any clue . thanks in advance .",2
facebook/react,"usestate causing children to re - mount ? bug <emphasis> * * what is the current behavior ? * * i have a hook that is supposed to call a callback after a ` settimeout ` . when i render a list of children with this hook , the callback behaves differently when its in a function component with ` usestate ` than it does if its in a class component . to see this in action , checkout the codesandbox below . the demo should show children components being added to the page and then after <number> seconds the child is mounted , it should be removed . both the examples use the same child components , only difference is the parent component being a class vs functional component . [ codesandbox ] ( <url> * * what is the expected behavior ? * * the function component should behave like the class component . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react v16 . <number> latest chrome unsure if this worked with previous versions of hooks .",2
facebook/react,"onanimationend / ontransitionend issues * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * # # # onanimationend creates unexpected behavior in the following example , i am creating a simple notification component from an array . each element in the array is used to generate a html element with a class that has a css animation on it . the generated div has an "" onanimationend "" binding which will remove them from the array of notifications . the basic idea is that the notification will fade away ( using a css animation ) and then be removed from the array , or alternatively , i am also allowing the user to manually click on the notification element to remove it . the interesting bug is as follows . if you add two notifications , the first one will trigger its onanimationend and remove itself . the remaining notification will suddenly jump to the end of its css animation and never trigger its own onanimationend . even more curiously if you add four notifications , exactly two of them will have the above bug , while the other two function properly . in fact exactly half of the added elements will trigger onanimationend properly , while the other half will not . the onclick functionality to remove the notifications from the array does not cause any unexpected behavior , and thus i am forced to conclude that the fault lies with onanimationend , since they both run the exact same function . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * <url> click the "" add notification "" button . then press it again before the first element has faded ( <number> seconds ) . you will notice that the second notification element gets "" fast forwarded "" to the end of its animation when the first notification is removed by its onanimationend trigger . the second notification will then be stuck there , never triggering its own onanimationend . * * what is the expected behavior ? * * onanimationend should trigger for each element in the loop properly . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * current version of <number> . <number> displays this behavior .",2
facebook/react,"useeffect how to solve conditional paging list < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * useeffect how to solve conditional paging list reset the page number to <number> when the condition changes ! [ image ] ( <url> <number> . request to return the current page and keep the current state ! [ image ] ( <url> * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * properly handle paging and conditions * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> previously treated in class form",2
facebook/react,"mimic componentdidmount but with hooks we are trying to use hooks and we want to mimic componentdidmount but our eslint is responding with "" react hook react . useeffect has a missing dependency : ' _checkuser ' . either include it or remove the dependency array react - hooks / exhaustive - deps "" if we add checkuser to the dependency in the useeffect we get : "" the ' _checkuser ' function makes the dependencies of useeffect hook change on every render . move it inside the useeffect callback . alternatively , wrap the ' _checkuser ' definition into its own usecallback ( ) hook react - hooks / exhaustive - deps "" . if we change the _checkuser to use usecallback eslint is responding with : "" react hook react . usecallback has a missing dependency either include it or remove the dependency array react - hooks / exhaustive - deps "" . what is the correct way to solve this ? or should we ignore the warnings ? ` ` ` javascript function _handleuserstate ( respons ) { / / some code } function _checkuser ( ) { const response = someexternalfunction ( ); _handleuserstate ( response ) ; } react . useeffect ( ( ) => { _checkuser ( ); } , [ ] / / run once ); ` ` `",2
facebook/react,"why function as child is not considered as children ? after digging deeper with [ this question ] ( <url> i have found ` function as a child ` is not being considered as * children <emphasis> * . take this example class countexpression extends react . component { render ( ) { const children = react . children . toarray ( this . props . children ) console . log ( children ) return <p> { react . children . count ( children ) } </p> } } <countexpression> { ' one ' } { ' two ' } { ( ) => <p> still , this will be ignored as child . why ? </p> } <p> this will be included in array - that ' s fine </p> </countexpression> ` ` ` so , i would like know why is it so ? is it a bug or an expected behavior ?",2
facebook/react,"× maximum update depth exceeded in controled way * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * - maybe bug ? * * what is the current behavior ? * * my case is that i want to update state n times by adding different types of fields to the state and then render them . every updating of dom ( adding new field ) i have to check whether i should do something more by checking the hight of added elements , for example , add a break between elements . fields are represented by the tree structure of data , so i created the class which helps me with going through the tree of fields and i keep it in the class property . i know how many times i have to modify the dom so it will be a fully controlled way of state update and it will finish after all fields are added to dom but before i render all fields i am getting the error "" maximum update depth exceeded "" . do you know how i can handle with such a problem ? i found a solution by adding settimeout on the way but i am not sure that is the clean solution ? ( commented code ) is it ok that i keep in component class property more complex class to manage structure of data ? i am providing a simple example in codepage which shows the problem and throw the error . <url>",2
facebook/react,"should setting state inside discrete events cause cleanup to run ? this bug is pretty confusing : <url> i think it happens because ` fn ` scheduled by ` setinterval ( fn , <number> ) ` jumps in front of the ` [ running ] ` effect cleanup caused by ` setrunning ( false ) ` . so the interval still fires , overwriting ` setlapse ( <number> ) ` that happened during the event with its ` setlapse ( somevalue ) ` . this reminds me of the issue described in <url> or at least a part of it fact , this problem exists even for regular react keystrokes ( and other “ discrete ” events ) . the solution to that would be to flush passive effects before we get a discrete event . but here , it seems like this wouldn ’ t be sufficient because the effect flips * as a result * of the click , not before it . so should ` setstate ` inside a discrete event also <emphasis> flush passive effect ? seems like not . ( that would defeat the purpose of delaying them . ) so this is working as designed , and the fix is just ` uselayouteffect ` when the timing matters ? or the raf solution ?",2
facebook/react,"uselayouteffect in ssr hi , i do not understand the situation with this hook a bit . i use this hook to perform the animation synchronously with the state update , if i use useeffect , then i have jumps in the animation , because the animation library does not have time to start . also , the documentation states that uselayouteffect runs on the same phase as componentdidmount ( that is , on the client side ) , and here my server issues complaints to me about my code . why is that ? <url> _originally posted by <user> in <url>",2
facebook/react,"hooks : usestate one - off callbacks * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * question / feature i have been trying to handle a case where i need to execute a piece of code right after the state is set at a particular place in the code . i do understand i am supposed to use ` useeffect ` to respond to changes in state , like so const [ val , setval ] = usestate ( null ) ; useeffect ( ( ) => { /* handle changes to val here */ } , [ val ] ) ` ` ` but the problem is , it will run on all changes made to ` val ` anywhere in the code . without the second argument of ` setval ` being a callback that ' ll run after the state is set , how can i execute something after a specific ` setval ` function call sets the state ?",2
facebook/react,"prop reassignment in components < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * prop reassignment in components * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * <number> . have a parent component that passes a prop to a child <number> . have a child assign the prop to the state default <number> . unpack the state value and use a ` . push ` to alter it <number> . watch in horror as both the prop and the state changes . <url> check child . js * * what is the expected behavior ? * * props should not be reassignable within mounted components * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * all browsers all os ' s react",2
facebook/react,"under what circumstances , unstable_shouldyield will return true ？ in scheduler . js , ` ` ` function unstable_shouldyield ( ) { return ( currentdidtimeout & & ( ( firstcallbacknode ! = = null & & firstcallbacknode . expirationtime < currentexpirationtime ) || shouldyieldtohost ( ) ) ); } ` ` ` unstable_shouldyield ( ) return true when currentdidtimeout is false and shouldyieldtohost ( ) return true , but why ? ` ` ` shouldyieldtohost = function ( ) { return framedeadline <= getcurrenttime ( ); }; ` ` ` shouldyieldtohost ( ) return true means there ' s no time left in this idle period currentdidtimeout is false means the schedule is not timeout what relationship between them , why does unstable_shouldyield ( ) depend on them ?",2
facebook/react,"act cannot detect secondary updates < - - note : if the issue is about documentation or the website , please file it at - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * if a component performs a second ( non user triggered ) update , ` act ` cannot detect it and warns about the update . for example , a button is clicked and updates its text . after a second , the button resets and its text reverts to its original state . <url> ( the reproduction is a bit contrived , but demonstrates the issue . ) * * what is the expected behavior ? * * the test runs without warning about being wrapped in ` act ` . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react & react dom @ ` <number> . <number> `",2
facebook/react,"unable to use usecontext hook inline in context . provider * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * not sure if it is a bug or a feature yet . this relates to hooks . it could also be that this is all expected behaviour and one of the workarounds mentioned is required . * * what is the current behavior ? * * i have a hook that depends on the ` usecontext ` hook . using it as follows works perfectly : ` ` ` const myhookedcomponent = ( ) => { const contextvalue = usecontext ( democontext ) ; return ( / / do something with contextvalue ) } const mycontextprovidercomponent = ( ) => { return ( < democontext . provider value ={ somecontextvalue } > < myhookedcomponent / > < / democontext . provider > ) } ` ` ` what if i want to use the ` getcontext ` hook inline in the same component that declares the ` democontext . provider ` ? ` ` ` const mycontextprovidercomponent = ( ) => { const contextvalue = usecontext ( democontext ) ; / / of course this fails due to the context hierarchy . return ( < democontext . provider value ={ somecontextvalue } > / / do something with contextvalue < / democontext . provider > ) } ` ` ` i seem to be unable to get this working . * * please note * * i have a very good reason for solving my issue with context and not passing props . - the implementation i show above looks trivial and dumb but it is the simplest way to illustrate what the use case is . in my implementation the ` provider ` sits in a complex component that does a lot of data management which i really want to happen at this level . - the usual way to use this will be the first working version i noted above , it is only in the case where the user would want to use the hook inline inside the ` provider ` . - i have searched for a couple of hours and tried various configurations without success , so my apologies if this is a duplicate of another issue . * * what is the expected behavior ? * * any method to consume context inline in the provider using the same re - usable hook without having to revert back to render props . i know i can solve this with * * render props * * but i am trying to convert an implementation using render props to hooks . i also know i can hoist the context provider higher up but in my implementation it would quadruple the code complexity to develop and maintain while introducing extra complexity into the user interface . also , by extracting the body inside the ` provider ` to a new component i can also solve this but ideally i would not like to have a user do this for this use case .",2
facebook/react,react - router can not pass hooks this is actually an issue of react - router <url> but since it ' s the major routing lib i want to make you aware that there are issues when you want to pass custom hooks via props through your component tree . since routing ( with react - router ) is a regular use case and react hooks is an easy / elegant way to manage state through the app they should work together . otherwise react hooks is about creating hooks limited to just one component and you cannot share state / hooks component - wide . maybe i did not get something right and there is a way . <repeated>,2
facebook/react,how will react solve nested contexts ? ` ` ` js < context1 . provider value ={ value1 } > < context2 . provider value ={ value2 } > < context3 . provider value ={ value3 } > < context4 . provider value ={ value4 } > < context5 . provider value ={ value5 } > < / context5 . provider > < / context4 . provider > < / context3 . provider > < / context2 . provider > < / context1 . provider > ` ` ` ` ` ` js < context1 . consumer > { value1 => < context2 . consumer > { value2 => < context3 . consumer > { value3 => < context4 . consumer > { value4 => < context5 . consumer > { value5 => ( null ) } < / context5 . consumer > } < / context4 . consumer > } < / context3 . consumer > } < / context2 . consumer > } < / context1 . consumer > ` ` `,2
facebook/react,"unable to catch error emitted in componentdidmount * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug report . * * what is the current behavior ? * * error boundary handles ` error ` emitted in ` componentdidmount ` and somewhy * rethrows <emphasis> * it . ` ` ` javascript class errorboundary extends react . component { constructor ( props ) { super ( props ) ; this . state = { error : null , errorinfo : null } ; } componentdidcatch ( error , errorinfo ) { this . setstate ( { error : error , errorinfo : errorinfo , }); } render ( ) { if ( this . state . errorinfo ) { return ( <div> ive handled an error </div> ); } return this . props . children ; } } class mycomponent extends react . component { componentdidmount ( ) { this . setstate ( ()=> { throw new error ( ' this error somewhy was rethrown ! ' ) }); } render ( ) { return ( <div> this component is awesome </div> ); } } reactdom . render ( ( <errorboundary> <mycomponent/> </errorboundary> ) , document . getelementbyid ( ' approot ' )); ` ` ` <url> * * what is the expected behavior ? * * it should not rethrow the error . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react : <number> . <number> os : win <number> x64 browser <number> . <number>",2
facebook/react,"usecallback / useeffect support custom comparator currently we can pass an array as second argument when using ` usecallback ` or ` useeffect ` like below : ` ` ` js usecallback ( ()=> { dosth ( a , b ) } , [ a , b ] ) / / how to do deep equal if a is an object ? ` ` ` the problem is it only compare array items with ` = = = ` , it there any way to compare complex object ? support custom comparator as third argument looks not bad usecallback ( ()=> { dosth ( a , b ) } , [ complexobject ] , ( item , previousitem ) => { / / custom compare logic , return true || false here } ) ` ` `",2
facebook/react,"setstate hook inside useeffect can cause unavoidable warning can not perform a react state update * bug <emphasis> * * * what is the current behavior ? * * example : <url> clicking on ok button cause ` warning : can not perform a react state update on an unmounted component . ` the problem that unsubscribe is called during b event ` setvisible ( v => false ) ; ` call , see logs : ` ` ` set visible before unsubscribe warning perform a react state update on an unmounted component . this is a no - op , but it indicates a memory leak in your application . to fix , cancel all subscriptions and asynchronous tasks in a useeffect cleanup function . in child ( created by holder ) set visible after ` ` ` in our case we have this even without raf call , but on ` transitionend ` dom event . ( it ' s occurred randomly and rare in our codebase as transitionend event should be called exactly at needed time , but example showed what happens ) seems like it occurred only if you have a ` setstate ` call during useeffect callback like ` setrefresh ( v => v + <number> ); ` ( _inside provided code_ ) ( _after rewriting our codebase to avoid setstate calls in useeffect the error has gone_ ) code ` ` ` javascript import react from "" react "" ; import reactdom from "" react - dom "" ; import mitt from "" mitt "" ; const emitter = mitt ( ); const child = ( ) => { const [ visible , setvisible ] = react . usereducer ( ( s , a ) => a , true ) ; react . useeffect ( ( ) => { const handle = ( ) => { console . log ( "" set visible before "" ); setvisible ( v => false ) ; / / < - - - this call causes unsubscribe and warning about state console . log ( "" set visible after "" ); }; emitter . on ( "" b "" , handle ) ; return ( ) => { console . log ( "" unsubscribe "" ); emitter . off ( "" b "" , handle ) ; }; } , []); return <div> { visible & & <h1> child text </h1> } </div> ; }; const holder = ( ) => { const [ refresh , setrefresh ] = react . usestate ( <number> ); const visible = react . useref ( true ) ; react . useeffect ( ( ) => { if ( refresh = = = <number> ) { visible . current = false ; setrefresh ( v => v + <number> ); / / < - - - this state change from effect caused problems } const handle = ( ) => { setrefresh ( v => v + <number> ); }; emitter . on ( "" a "" , handle ) ; return ( ) => { emitter . off ( "" a "" , handle ) ; }; }); return <div> { visible . current & & < child / > } </div> ; }; function app ( ) { return ( <div> < holder / > < button onclick ={() => { emitter . emit ( "" a "" , {}); requestanimationframe ( ( ) => { emitter . emit ( "" b "" , {}); }); } } > ok </button> </div> ); } const rootelement = document . getelementbyid ( "" root "" ); reactdom . render ( < app / > , rootelement ) ; ` ` ` * * what is the expected behavior ? * * do not provide warning if unsubscription is called during "" setstate "" call . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> . <number> - alpha . <number>",2
facebook/react,"getderivedstatefromprops is user - hostile so i think ` getderivedstatefromprops ` as designed turns out to be user - hostile . the short summary is that it makes the pattern described in the docs , <url> impossible for non - trivial cases . imagine implementing an hoc that listens to change events on a datasource , but where a simple shallowequal ( ) on the resultant state is not sufficient to prevent rendering . i . e . , what if you need more complex logic to determine if new data from the datasource should be applied ? in my case , i have a datasource that does not guarantee that it will give out objects of the same identity , so a ` = = = ` check will always return false and cause a re - render . but , i can do a little bookkeeping on the side and determine whether the data is actually new and should therefore be loaded into state . this leads to some problems : <number> ) because ` getderivedstatefromprops ` is static , it ' s not possible to store some internal bookkeeping data on the component instance that gets used to determine whether to query the datasource . worst case , this bookkeeping information could be stuffed in the state object , but that ' s unnecessarily constraining and a bit ugly from a code organization perspective . <number> ) more importantly , it ' s not possible to determine why ` getderivedstatefromprops ` is being called . is it because of a props change ? or is it because of a "" change "" event firing on the ` datasource ` ? this matters because if props have changed , data must be fetched and the component re - rendered unconditionally , but if the props have not changed , then it ' s possible all the data loaded is the same as last time , and so ` render ( ) ` can be skipped . here ' s an example using the deprecated api that ' s no longer possible , lightly edited from the hoc example i mention above function withdataloading ( component , datasource , getdatafunc ) { return class extends react . purecomponent { componentdidmount ( ) { / / subscribe to changes datasource . addchangelistener ( this . handlechange ) ; } componentwillunmount ( ) { / / clean up listener datasource . removechangelistener ( this . handlechange ) ; } componentwillreceiveprops ( props ) { / / unconditionally update state and rerender since the props changed const possiblynewdata = getdatafunc ( datasource , props , this ) this . setstate ( possiblynewdata ) ; / / remember some metadata about what we fetched for next time this . bookkeeping = datasource . bookkeepingdata ( ); } handlechange ( ) { / / update component state whenever the data source changes const possiblynewdata = getdatafunc ( datasource , this . props , this ) if ( datasource . didifetchnewdata ( this . bookkeeping ) ) { / / the datasource had updated data in it , so rerender this . setstate ( possiblynewdata ) ; / / remember some metadata about what we fetched for next time this . bookkeeping = datasource . bookkeepingdata ( ); } else { / / no new data was fetched , so do not update state and do not rerender / / do nothing … } } render ( ) { < component data ={ this . state } / > } } } ` ` ` notice that the behavior is different between ` componentwillreceiveprops ` and ` handlechange ` . there ' s no way to make that distinction with the new api .",2
facebook/react,"cases where hooks do not currently provide a good answer vs hoc ( unsure if this is the right place , so trying it out ) i have noticed that the new react hooks feature is aiming at providing an alternative composition pattern to hoc and render functions , but i believe that many of the use cases solved by hoc ( at the framework level ) cannot currently be addressed by the new hooks api . specifically , there is not way to incorporate react hooks with react . memo . unless i am incorrect , this means that any system that would like to implement optimisations based on external context , such as the react - redux ` connect ` function ( that uses ` mapstatetoprops ` to implement an efficient ` shouldcomponentupdate ` ) will still need to rely on a hoc / render - prop to automate this optimisation . the reason i am bringing this up is because one of the main benefits stated in the documentation is to reduce framework level use of hoc that "" pollute "" the tree , of which the react - redux connect hoc is probably the most prevalent use case . additionally redux ( and ` useredux ` ) are specifically brought up as an exemplary use case , although with the current system it will cause large optimisation issues ( since with no optimised ` shouldcomponentupdate ` , every "" connected "" component will re - render on every state change ) . ( although this might fit into the documentation repo , this is a discussion / opinion and i do not feel it is a "" mistake "" that i should report , but rather a discussion on importance ) . an example solution for this could be if there was a way to use contexts in ` react . memo ` ( which unless i am incorrect only have access to ` props ` and ` prevprops ` )",2
facebook/react,[ npm : create - react - class ] no way to implement getderivedstatefromprops there seems to be no way to implement class methods such as ` getderivedstatefromprops ` and a few others . am i overlooking something ? there is no mention of it here either,2
facebook/react,"element attributes be removed silently in frameset tag * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * in some historical reason , i am still using obsolete tag ` frameset ` in my project . and when i render my component which includes ` frameset ` tag in react , some attributes are ignored , like ` rows ` , ` cols ` . could anyone tell me why and how to solve it by no hacking way ? thx . * input <emphasis> * ` ` ` javascript export default class obsoleteelment extends react . component { render ( ) { return ( <html> <head> < meta httpequiv = "" content - type "" content = "" text / html ; charset = utf - <number> "" / > <title> old page </title> </head> < frameset id = "" frame - container "" rows = ' <number> , * ' frameborder = "" <number> "" border = "" <number> "" cols = "" <number> , *""> </frameset> </html> ) } } ` ` ` * output <emphasis> * ` ` ` javascript <html> <head> < meta http - equiv = "" content - type "" content = "" text / html ; charset = utf - <number> "" / > <title> old page </title> </head> < frameset id = "" frame - container "" frameborder = "" <number> "" border = "" <number> "" > </frameset> </html> ` ` ` * expect <emphasis> * ` ` ` javascript <html> <head> < meta http - equiv = "" content - type "" content = "" text / html ; charset = utf - <number> "" / > <title> old page </title> </head> < frameset id = "" frame - container "" rows = ' <number> , * ' frameborder = "" <number> "" border = "" <number> "" cols = "" <number> , *""> </frameset> </html> ` ` ` * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * * * react version * * [ image ] ( <url> * * node environment * * ! [ image ] ( <url> ! [ image ] ( <url> * os <emphasis> * ! [ image ] ( <url>",2
facebook/react,"access react component underlying dom tree * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature * * what is the current behavior ? * * form component has "" form field "" children ( input ( ) , checkbox ( ) , etc ) that are validated against some custom rules when form is submitted . validation works fine . * * what is the expected behavior ? * * if the form is invalid , i would like to scroll the page to the first invalid element after the form is submitted but unfortunately components do not expose the underlying dom three ( except via the fiber object ) and i cannot get the "" ref "" of any component , which makes scrolling not possible since i do not have the dom reference . i am trying to avoid ref forwarding on "" form field "" components because i would like the form to handle all validation related code ( for encapsulation purposes and in case i want to publish this as a library , for instance ) . * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react is there any way to achieve this ?",2
facebook/react,"question on reconciliation i think i understood reconciliation in react however there is one thing i would like to clarify . let us say on first render , we render : ` ` ` <ul> <li> duke </li> / / key <number> <li> villanova </li> / / key <number> </ul> ` ` ` and on next render ` ` ` <ul> <li> connecticut </li> / / key <number> <li> duke </li> / / key <number> <li> villanova </li> / / key <number> </ul> ` ` ` if i understood reconciliation correctly on the second render , react will check that * * types ( e . g . ` <li> ` ) and keys of first two items from first and second render match * * , hence it will add only the new item with key <number> to the new output , hence generate following result <ul> <li> duke </li> / / key <number> <li> villanova </li> / / key <number> <li> villanova </li> / / key <number> </ul> ` ` ` however i think this is not what react will generate and it seems then there is some issue with my understanding ( especially the bold part in the previous paragraph ) . can someone explain what i missed in my understanding ?",2
facebook/react,get keys in react - reconciler i am struggling to implement reusable views on mobile platforms and to make it i need to get key values ( explicitly set and generated ) in react - reconciler to match two different view trees somewhere outside react . is there a way to achieve this ? thanks,2
facebook/react,"how map ? render an array of elements * * what is the current behavior ? * * react16 + support return array components , for example : render ( ) { return [ 《 li 》 <number> 《 / li 》 《 li 》 <number> 《 / li 》 《 li 》 <number> 《 / li 》 ] } export default arraydemo - - - - - - - - - - - - - - - - i want to map it with react . children . map , for example arraydemo from ' x <elongated> / x <elongated> ' ; . <repeated> react . children . map ( arraydemo , ( item , index ) => { return 《 col span ={ <number> } 》 { item } 《 / col 》 } ) . <repeated> - - - - - - - - - - - - - - - - - - - but arraydemo has become a single element ; react . children . toarray got [ <number> ] only ; * * what is the expected behavior ? * * i expected since react can return array ; how can i map it ? for sometimes i want to wrap each element in array ; * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - or am i missing something important ? thx a lot",2
facebook/react,"avoid reconciliation , alternative component interface hello . i want to ask a question about a way to avoid reconciliation process . today i can see the following process : <number> . component wants to re - render . <number> . component render method provides new virtual dom . <number> . some react diff library tries to find some non - optimal way to morph old virtual dom into new one . please fix me if i am wrong , i am not familiar with react codebase . i can see an information in [ docs ] ( <url> > you don ’ t have to worry about exactly what changes on every update but your solution has complexity about o ( n ) or even worse , so user should care about what changes sometimes . when user knows what changed he will be able to provide o ( log n ) or even o ( <number> ) solution . for example i am working with huge data list and i am receiving information from websocket about how to morph my list : append / prepend , remove , swap items , etc . i do not want to render huge component list and run reconciliation process for each mutation . i can tell virtual dom how to morph efficiently . [ append ] ( <url> is there a way for user to provide morph method ? i can imagine some api like : ` ` ` / / render is not defined morph ( component ) { if ( . <repeated> ) { component . append ( < item />); } else { ( < item / > ) . prependto ( component . find ( { key })); } } ` ` ` do you have any plans to implement it ? thank you . please feel free to ask any questions .",2
facebook/react,"componentwillupdate discussion < - - note : if the issue is about documentation or the website , please file it at : <url> - - > * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * feature discussion * * what is the current behavior ? * * the current behavior calls the function before re - rendering the render target however there is no indication for when the re - render method should be called * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem . your bug will get fixed much faster if we can run your code and it does not have dependencies other than react . paste the link to your jsfiddle ( <url> or codesandbox ( <url> example below : * * * * what is the expected behavior ? * * the expected behavior should call the function before re - rendering the render target if there is no returned conditional statement and if there is , true should trigger the re - render and false should not * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * <number> . <number> i have a few questions before i try to solve this problem and it gets rejected . i am assuming that the team has decided to move in the direction of removing "" will - updates "" from react all together and have labeled them "" unsafe "" . i have seen a lot of issues with those functionalities before as well . i was curious if providing a conditional statement inside those functions would satisfy the unexpected renders . "" shouldcomponentupdate "" does trigger when to call these functions however in those functions that "" will "" run before the render can always be controlled with a conditional statement of it ' s own . i am getting comfortable with the code base however i am curious if the team is removing that functionality all together and using the "" static "" function "" getderivedstatefromprops "" instead . has a react lover it is a little confusing and may break a lot of legacy code when updating versions . i believe that may solve the problem of unexpected renders after a "" will "" change lifecycle event . i was curious if that was the case , and removing "" will "" lifecycles is the next steps for react then how will the "" did "" lifecycles work instead ? i love react and i am just curious about the future for react as a whole . i ' d love to tackle any bugs or feature requests when i have time . i will be writing clean code and create tests for new features or bugs as well .",2
facebook/react,"suspense : timeout expiration and siblings rendering issues * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug * * what is the current behavior ? * * code for reproducing is [ here ] ( <url> i have also deployed an example to zeit now this is a basic example of using react suspense and simple cache provider . postponing text rendering and showing loading spinners when it ' s necessary . i can see my use of ` < timeout / > ` does not really care about ` ms ` i am passing — it always become expired right after the render . you can open a page and the loading bar appears immediately despite ` 1 0 0 0 ms ` delay that it has . there is content that is placed next to an async component ( the one that ' s going to be suspended ) . <url> looking at react suspense tests , it is assumed that sibling elements can be rendered in any way . * * what is the expected behavior ? * * ` < timeout / > ` component only shows placehold when expired , sibling content is shown even if an async component was suspended . i built sources for ` react ` , ` react - dom ` , and ` simple - cache - provider ` from the current master , updating ` enablesuspense ` flag . i also used ` < unstable_asyncmode / > ` but it did not seem to make any difference . i overall was really satisfied with this feature . i hope i did the code correctly so it shows the real bug .",2
facebook/react,"use and set a component ' s state within setinterval function this is a question , so i apologize if this is not the best place to ask . i am trying to work with a component ' s internal state within a setinterval function and i know there are some asynchronous problems i need to be aware of . i also know that i can pass setstate a function which would help me protect against async problems . <repeated> but i can not because i need to use the changed state within the setinterval function ? is what i am doing safe or should i be working a different way around this problem . i have seen the following that nearly answers my question and am having trouble visualizing / adapting this ( or maybe the behavior i have is fine ? <sad> [ stackoverflow answer ] ( <url> ` ` ` sectiontimerhandler = ( ) => { const elapsedsectiontimems = this . state . elapsedsectiontimems + <number> ; / / do some stuff with elapsedsectiontimems this . setstate ( { elapsedsectiontimems } ); } restartsectiontimer = ( ) => { if ( this . sectiontimeintervalid = = <number> ) { window . clearinterval ( this . sectiontimeintervalid ) ; } this . sectiontimeintervalid = window . setinterval ( this . sectiontimerhandler , <number> ); this . setstate ( { elapsedsectiontimems : <number> }); } ` ` ` or would something like the following be better / safer even though i am using ` this . state . elapsedsectiontimems + <number> ` earlier in the function ? ` ` ` this . setstate ( ( prevstate : assessmentstate ) => { return { elapsedsectiontimems + <number> }; }); ` ` `",2
facebook/react,"context api bitmask related questions i am playing with context api bitmask feature to bail out unwanted re - render . i have a dynamic model ( a json object ) as context value . by dynamic i meant , the number of keys and structure of the json object is unknown . but when the json object changed , i know which key is changed . it seems difficult to turn the unknown keys to static pre - defined bitmasks . but i thought such use case is very common , and the bail - out feature should handle it easily by just matching the key . also i notice default changedbits and observedbits is max_signed_31_bit_int . does this mean it has a limitation up to <number> type of context change ?",2
facebook/react,"[ question ] context provider state initialisation . consider a component wrapping a context provider : ` ` ` js class valuewrapper extends react . component { constructor ( props ) { super ( props ) ; this . state = { value : null , setvalue : this . setvalue , }; } setvalue = ( value ) => { this . setstate ( { value }); }; render = ( ) => ( < context . provider value ={ this . state } > { this . props . children } < / context . provider > ); } ` ` ` a consumer then might want to set a default value when it is first mounted . the only way i can see to do this using the new api is to check for an existing value on first render : ` ` ` js class valueupdater extends react . component { state = { inputvalue : <number> , }; handleinputchange = e => { this . setstate ( { inputvalue : e . target . value }); }; render = ( ) => ( < context . consumer > { ( { value , setvalue } ) => { if ( value ) { setvalue ( this . state . inputvalue ) ; } return ( <div> < input type = "" text "" value ={ value } onchange ={ handleinputchange } / > < button onclick ={() => setvalue ( this . state . inputvalue ) } > update value </button> </div> ); } } < / context . consumer > ) }; ` ` ` but this seems to break the golden rule of updating state in the render method ( ` setvalue ( this . state . inputvalue ) ` ) , as this would immediately cause a re - render . ideally i would be able to call the ` setvalue ` from the context in the ` componentdidmount ` method of the ` valueupdater ` component , but with context as a render prop , that ' s not possible , as far as i can tell . the docs suggest passing props down to another component render = ( ) => ( < context . consumer > { ( { value , setvalue } ) => ( < valueupdaterinput setvalue ={ setvalue } value ={ value } / > ) } < / context . consumer > ) ` ` ` but if i tried to do the initialisation in the ` valueupdaterinput ` component ' s ` componentdidmount ` method , it would be called on every render , surely , as ` valueupdaterinput ` would be re - rendered each time ? is there a better pattern than this , or am i trying to use context inappropriately ?",2
facebook/react,is there a way to access new context api within componentdidmount ? we are building a react mapbox gl module and we use clone and inject props today . we were looking into using the <number> . <number> context api but i saw that it will have a new one on <number> . <number> but i can ’ t seem to find a way to read context details on componentdidmount lifecycle ( which makes sense for me to use on the map implementation ) . is there a way around this ?,2
facebook/react,"potential future bug in geteventkey for edge with synthetic event normalization . * bug <emphasis> * * * current behavior * * react currently normalizes keyboard events cross - browser by falling back on the native keyboardevent . keycode property and using a dictionary object to normalize the key . as noted in the source , geteventkey is used for "" normalization of deprecated html5 ` key ` values "" . edge currently does not implement the correct key values and this normalization will fall back to keyboardevent . keycode . keyboardevent . keycode is deprecated and may be dropped at any time . microsoft as per <url> have fixed this issue , but it has not yet been released to insider preview releases . this means there is the potential for the keyboardevent . keycode to be removed from edge before the fix is made public . it may be a good idea to not rely on keycode in this instance . refer to <url> line <number> * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react - dom version <number> . <number> , potentially affects future versions of edge .",2
facebook/react,"state change in td element displaying tabular rows , when setting data in td element , on state change it displays only the currently updated td element data . <repeated> all the other ones go missing from screen . when the same thing is placed within text box in each td element things work fine . following is sample the code : working code : ` < tr key ={ id } > ` ` <td> < input value ={ this . state . price [ id ] } / > </td> ` failing code key ={ id } > ` ` <td> { this . state . price [ id ] } </td> `",2
facebook/react,"getting started docs are terrible i just came back to react after about a year or so of not using it and now the getting started docs are pretty much not helpful in the slightest . all the docs are now is just links to other websites without any instructions on how to get a react project running . i remember a year ago back in react <number> the docs had a step by step walkthrough for how to set up and get a project running from scratch and now they are just links to websites without react specific instructions . what happened to the docs , when did they become so useless ? there needs to be a step by step guide added back to the docs , a guide that will get you from having nothing to running it in the browser instead of just links to other websites that do not have anything to do with react . i remember back in react <number> , you could run through the guide and have a hello world app running within <number> minutes no problems , i have been trying to figure it out with these new docs for over an hour now and still <number> luck getting this running . the instructions used to actually be instructions , basically saying , install this , then that , then run this command in terminal , then do this and so on . the docs now just say , well you can use this , then has a link to a site that has nothing to do with react , there is not even any sort of instruction on the react website on what to do with that , just the link without any context around it . can these docs please be reverted to the old versions or actually create a proper getting started page to get you from nothing to running ?",2
facebook/react,"concating react with other resources hi . i want to use many libraries and frameworks in my project . can i concat react with other framework like angular , vue , ember ? . <repeated>",2
facebook/react,"can not edit the wiki hello , i can not edit the wiki to add my website to the list there , is editing disabled ?",2
facebook/react,"typeerror : cannot read property ' state ' of undefined with create - react - class i am currently attempting to create a button which is red with the text "" yes "" that when you click on it the button changes to a green color with the text "" confirm ? "" before the final stage in which an action takes place . where i am currently at is defining ` buttoncolor ` as a state which changes on the click of the button ; the initial color should be ` <hashtag> fd8f83 </hashtag> ` and the final color after the click should be ` <hashtag> a4d87c </hashtag> ` . however , i am currently getting the error "" typeerror : cannot read property ' state ' of undefined "" pointing to the ` style ={{ backgroundcolor : this . state . buttoncolor } } ` line whenever the code is compiled on the webpage . * * defining initial state and behavior on click : * * ` ` ` getinitialstate : function ( ) { return { buttoncolor : "" <hashtag> fd8f83 </hashtag> "" }; } , handleclick ( color ) { this . setstate ( { buttoncolor : color }); } ` ` ` * * code inside table in render ( <sad> * * ` ` ` <td> < button classname = "" removebutton "" style ={{ backgroundcolor : this . state . buttoncolor } } onclick ={ function ( ) { this . handleclick ( "" <hashtag> a4d87c </hashtag> "" ) } } > yes </button> </td> ` ` ` does anyone have any ideas why this is ? i am brand new to react so i apologize if it ' s obvious . i also learned react using createclass so i have been trying to piece together how to make this work with the new ` create - react - class ` package . any advice is greatly appreciated react : ` ^ <number> . <number> ` chrome <number> . <number> ( official build ) ( <number> - bit ) `",2
facebook/react,"sluggish scrolling when rendering table with large dataset in react so i am creating an application with real - time streaming data in a table using react ( v16 . <number> ) . the number of rows shown at a time can be rather large ( <number> + rows ) and when those situations occur , the scrolling behavior on browsers ( desktop & mobile ) really takes a hit and staggers when attempting to browse the content . has anybody experienced this kind of scrolling behavior when rendering large data sets in a table ? i am expecting react to be able to efficiently draw / redraw these elements with ease but that does not appear to be the case . i tried using jsfiddle to recreate the scenario but did not have much success .",2
facebook/react,"forceupdate recalls all ref callbacks * * do you want to request a feature <emphasis> or report a bug <emphasis> ? * * bug , maybe * * what is the current behavior ? * * on calling forceupdate ref callbacks called again . so , doing forceupdate in ref callback makes infinite loop . * * if the current behavior is a bug , please provide the steps to reproduce and if possible a minimal demo of the problem via <url> or similar ( template for react <number> : <url> template for react <number> <url> * * what is the expected behavior ? * * do not recall refs * * which versions of react , and which browser / os are affected by this issue ? did this work in previous versions of react ? * * react <number> . <number> chrome <number>",2
facebook/react,can not access discuss react i know this is not related to react directly but did not know where to turn to . am i the only one who can not access / login to discuss . reactjs . org ? ( for quite some time already ) i get error : _this page isn ’ t working . discuss . reactjs . org is currently unable to handle this request . http error 5 0 0 _ and sometimes when i retry this error there was an error authorizing your account . perhaps you did not approve authorization ? _,2
tensorflow/tensorflow,"add support <elongated> for vedv ( <url> please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : - * * python version * * : - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : - * * gpu model and memory * * : - * * exact command to reproduce * * : you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"can not run bert_vocab_from_dataset without typeerror : tensor is unhashable # # # issue type support # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? this is the code from you [ manual ] ( <url> i really do not understans that i get this error . why is it ? if i add tf . compat . v1 . disable_eager_execution ( ) tf . compat . v1 . disable_v2_behavior ( ) i get runtimeerror : input_dataset : attempting to capture an eagertensor without building a function . # # # standalone code to reproduce the issue ` ` ` shell data = tf . data . textlinedataset ( [ sentences_path , tags_path ] ) from tensorflow_text . tools . wordpiece_vocab import bert_vocab_from_dataset as bert_vocab tokens = bert_vocab . bert_vocab_from_dataset ( data , # the target vocabulary size vocab_size = <number> , # reserved tokens that must be included in the vocabulary reserved_tokens =[ "" [ pad ] "" , "" [ unk ] "" , "" [ start ] "" , "" [ end ] "" ] , # arguments for ` text . berttokenizer ` bert_tokenizer_params = dict ( lower_case = true ) , # arguments for ` wordpiece_vocab . wordpiece_tokenizer_learner_lib . learn ` learn_params ={} , ) ` ` ` # # # relevant log output ` ` ` shell typeerror is unhashable . instead , use tensor . ref ( ) as the key . ` ` `",2
tensorflow/tensorflow,"float16 mixed precision training # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version python <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory a6000 # # # current behavior ? enabled float16 training by setting the mixed precision policy , but why i still need to manually cast the y tensors to float16 before calculating the loss ? error when no manual cast the tensor # # # standalone code to reproduce the issue ` ` ` shell confidential . ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"ckpt to tflite how can i convert ckpt file to tf lite , while i have only . ckpt file . no meta - file present",2
tensorflow/tensorflow,"when converting tensorflow model to tflite model , is there any way to fix the output order during inference using tflite as facing an issue of output order of tflite inference on meraki custom cv i took a pretrained model ( ssd mobilenet 3 2 0 x320 ) for object detection from the tensorflow zoo and configured / tuned it according to my data . i trained a tensorflow model which detects <number> labels . i have used the latest checkpoint to save the model , then froze it , and finally performed tf lite conversion . i did this because i need to upload the tf lite model only to a cisco camera . i am facing an issue with the output order during tf lite inference , as the output arrays get jumbled / rearranged . i need help on how to convert the tensorflow model to tf lite efficiently . my tensorflow version is <number>",2
tensorflow/tensorflow,"distributed training with parameter servers example using a single binary hello everyone i am sorry if this is a duplicate issue but from my considerable search - i could not find a single end - to - end distributed parameter - server example to run using tensorflow ( using the keras api with ` . fit ( ) ` method ) . also , for some reason - the documentation for parameter - server strategy seems a lot more confusing and difficult to get started with , compared to multi - worker strategy . i have been running training jobs using the estimator api before and now trying to update it to tf2 . x style distributed training job with parameter - server training strategy using a single binary file for all workers and parameter - servers . i started with the example in documentation here ( <url> and modified the code to be used as a single binary . code : ` ` ` import tensorflow_datasets as tfds import tensorflow as tf import os cluster_resolver = tf . distribute . cluster_resolver . tfconfigclusterresolver ( ) if cluster_resolver . task_type in ( "" worker "" , "" ps "" <sad> # start a tensorflow server and wait . server = tf . distribute . server ( cluster_resolver . cluster_spec ( ) , job_name = cluster_resolver . task_type , task_index = cluster_resolver . task_id , protocol = cluster_resolver . rpc_layer or "" grpc "" , start = true ) server . join ( ) else : # # parameter - server strategy = tf . distribute . parameterserverstrategy ( cluster_resolver = cluster_resolver ) global_batch_size = <number> x = tf . random . uniform ( ( <number> , <number> ) ) y = tf . random . uniform ( ( <number> , ) ) dataset = tf . data . dataset . from_tensor_slices ( (x , y ) ) . shuffle ( <number> ) . repeat ( ) dataset = dataset . batch ( global_batch_size ) dataset = dataset . prefetch ( <number> ) with strategy . scope ( ) = tf . keras . models . sequential ( [ tf . keras . layers . dense ( <number> ) ] ) model . compile ( tf . keras . optimizers . legacy . sgd ( ) , loss = "" mse "" , steps_per_execution = <number> ) working_dir = "" . / my_working_dir "" log_dir = os . path . join ( working_dir , "" log "" ) ckpt_filepath = os . path . join ( working_dir , "" ckpt "" ) backup_dir = os . path . join ( working_dir , "" backup "" ) callbacks = [ tf . keras . callbacks . tensorboard ( log_dir = log_dir ) , tf . keras . callbacks . modelcheckpoint ( filepath = ckpt_filepath ) , tf . keras . callbacks . backupandrestore ( backup_dir = backup_dir ) , ] model . fit ( dataset , epochs = <number> , steps_per_epoch = <number> , callbacks = callbacks ) ` ` ` to my understanding , all workers and paramter - servers will start and wait for chief to assign the tasks . chief or coordinator ( documentation uses them interchangeably but is there any difference between the two ? ) will automatically divide the work based on the information it gets from ` cluster_resolver ` ( let me know if that ' s wrong interpretation ) . in any case , i would highly appreciate if someone can point out what i am doing wrong in this example because i have not been able to get it to work !",2
tensorflow/tensorflow,"when building from source code , i always end up with a python <number> whl file , instead of a python3 . <number> whl file . # # # issue type build / install # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version <number> # # # bazel version <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version <number> / <number> # # # gpu model and memory gtx <number> ti 4 gb # # # current behavior ? when i try to build the source code from my machine i end up always with a wheel for python <number> , although i specified the python path for python3 . <number> and i do not even have python3 . <number> installed . the generated wheel is called tensorflow - <number> . <number> - cp310 - cp310 - linux_x86_64 . whl can you guide why this is happening and how to solve it ? # # # standalone code to reproduce the issue ` ` ` shell just trying to build the source code following the steps from this two sites <url> ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"issues running transformer model example with estimator api hello everyone i am trying to run an image classification training example with vision transformer from keras examples ( <url> everything ran perfectly when i ran it as it is but i started facing issues when i switched training from ` model . fit ( ) ` to ` tf . estimator . train_and_evaluate ( ) ` ( ofcourse i made the appropriate changes to first convert model to estimator ) . from what i understand . <repeated> the problem lies with saving and reloading the model which is done by the estimator api . the model has custom classes : ` ` ` class patches ( layers . layer ) : def __init__ ( self , patch_size , * * kwargs ) : super ( ) . __init__ ( * * kwargs ) self . patch_size = patch_size def call ( self , images ) : batch_size = tf . shape ( images ) [ <number> ] patches = tf . image . extract_patches ( images = images , sizes =[ <number> , self . patch_size , self . patch_size , <number> ] , strides =[ <number> , self . patch_size , self . patch_size , <number> ] , rates =[ <number> , <number> , <number> , <number> ] , padding = "" valid "" , ) patch_dims = patches . shape [ - <number> ] patches = tf . reshape ( patches , [ batch_size , - <number> , patch_dims ] ) return patches # # personal addition def get_config ( self ) : base_config = super ( ) . get_config ( ) base_config . update ( { ' patch_size ' : self . patch_size , } ) return base_config class patchencoder ( layers . layer ) : def __init__ ( self , num_patches , projection_dim , * * kwargs ) : super ( ) . __init__ ( * * kwargs ) self . num_patches = num_patches self . projection = layers . dense ( units = projection_dim ) self . position_embedding = layers . embedding ( input_dim = num_patches , output_dim = projection_dim ) def call ( self , patch ) : positions = tf . range ( start = <number> , limit = self . num_patches , delta = <number> ) encoded = self . projection ( patch ) + self . position_embedding ( positions ) return encoded # # personal addition def get_config ( self ) : base_config = super ( ) . get_config ( ) base_config . update ( { ' num_patches ' : self . num_patches , ' projection ' : self . projection , ' position_embedding ' : self . position_embedding } ) return base_config ` ` ` from looking at some related issues , i found how we need to provide a ` get_config ( ) ` method to save and reload the model with custom classes so i made small personal modifications but now its sort of giving me a different issue i am unable to understand . error log : ` ` ` warnings . warn ( x_train shape : ( <number> , <number> , <number> , <number> ) - y_train shape : ( <number> , <number> ) x_test shape : ( <number> , <number> , <number> , <number> ) - y_test shape : ( <number> , <number> ) warning : tensorflow : from train . py : <number> : runconfig . __init__ ( from tensorflow_estimator . python . estimator . run_config ) is deprecated and will be removed in a future version . instructions for updating : use tf . keras instead . / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / backend . py : <number> : userwarning : ` tf . keras . backend . set_learning_phase ` is deprecated and will be removed after <number> - <number> - <number> . to update it , simply pass a true / false value to the ` training ` argument of the ` __call__ ` method of your layer or model . warnings . warn ( traceback ( most recent call last ) : file "" train . py "" , line <number> , in <module> history = run_experiment ( vit_classifier ) file "" train . py "" , line <number> , in run_experiment model_est = keras . estimator . model_to_estimator ( keras_model = model , model_dir ='. ' , config = run_config ) file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / estimator / __init__ . py "" , line <number> , in model_to_estimator_v2 return keras_lib . model_to_estimator ( file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / tensorflow_estimator / python / estimator / keras_lib . py "" , line <number> , in model_to_estimator warm_start_path = _save_first_checkpoint ( keras_model , custom_objects , file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / tensorflow_estimator / python / estimator / keras_lib . py "" , line <number> , in _save_first_checkpoint model = _clone_and_build_model ( modekeys . train , keras_model , file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / tensorflow_estimator / python / estimator / keras_lib . py "" , line <number> , in _clone_and_build_model clone = tf . compat . v2 . keras . __internal__ . models . clone_and_build_model ( file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in clone_and_build_model clone = clone_model ( model , input_tensors = input_tensors ) file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in clone_model return _clone_functional_model ( file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in _clone_functional_model model_configs , created_layers = _clone_layers_and_model_config ( file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in _clone_layers_and_model_config config = functional . get_network_config ( file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / engine / functional . py "" , line <number> , in get_network_config layer_config = serialize_layer_fn ( layer ) file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in _copy_layer created_layers [ layer . name ] = layer_fn ( layer ) file "" / home / nearchus / . local / lib / python3 . <number> / site - packages / keras / src / models / cloning . py "" , line <number> , in _clone_layer return layer . __class__ . from_config ( layer . get_config ( ) ) file "" train . py "" , line <number> , in from_config return cls ( * * config ) typeerror : __init__ ( ) missing <number> required positional argument ` ` ` i thought it might be because of ` patchencoder ` class constructor has custom objects as argument - so i tried to do serialization / deserialization but to no vail . in any case , i would highly appreciate if someone can guide me as to where i am going wrong in this !",2
tensorflow/tensorflow,"tesla v100 tensorflow cuda support # # # issue type support # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution rhel <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version <number> # # # cuda / cudnn version <number> / <number> , <number> / <number> # # # gpu model and memory tesla v100 2 gb vram # # # current behavior ? attempting to fetch value instead of handling error internal : failed to get device attribute <number> for device <number> : cuda_error_unknown : unknown error . nvidia - smi give the following output <sad> nvidia - smi <number> . <number> driver version : <number> . <number> cuda version : <number> nvcc - v the following output : nvcc ( r ) cuda compiler driver copyright ( c ) <number> - <number> nvidia corporation built on fri_feb__8_ <time> _pst_2019 cuda compilation tools , release <number> , v10 . <number> # # # standalone code to reproduce the issue ` ` ` shell doesnt happen with windows or ubuntu systems . ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"parse output of ` mobile_ssd_v2_float_coco . tflite ` # # # issue type support # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version v2 . <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device android # # # python version _no response_ # # # bazel version <number> . <number> # # # gcc / compiler version <number> # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i am trying to use the model [ mobile_ssd_v2_float_coco . tflite ] ( <url> on a c + + application , i am able to execute the inference and get the results . based on the netron app i see that its output is but i could not find an example code showing how to parse this output . i tried to look into <url> and <url> but the output of the model is different from the one provided [ here ] ( <url> do you have any example code available in java , python , or even better in c + + to parse this model output ? # # # standalone code to reproduce the issue ` ` ` shell no example code is available to parse the output of mobile_ssd_v2_float_coco . tflite . ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"error installing impoerting tf model into node red hello , i am getting a tensor flow to install on my node red . it seems like there is and install issue and i keep on getting the error pictured below . i have also tried import an model from teachable machine too and i get the same error please help . [ capture ] ( <url> ! [ capture1 ] ( <url>",2
tensorflow/tensorflow,"support for empty gpu batches during distributed training # # # issue type support # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? when doing ` distributed training ` using ` mirroredstrategy ` , one may encounter empty gpu batches when ` drop_remainder = false ` during dataset construction . from # <number> , it seems it is a long - standing issue . for the last few batches of data it is possible that some replica workers receive an empty tensor as input ( see [ here ] ( <url> for an example ! <repeated> ) . either one must set ` drop_remainder = true ` or consider not using ` <user> . function ` because you would have to add a conditional statement in the ` train_step ` function which will not work with ` tf . function ` . so , from the point of efficiency , ` drop_remainder = true ` seems the only option . in some applications or critical experiments , one would not like to drop the remainder data for very precise and reproducible quantitative analysis . so , can there be a support for handling empty gpu batches in distributed mode ? # # # standalone code to reproduce the issue ` ` ` shell ` tf . data . dataset . range ( <number> ) . batch ( <number> ) ` over <number> replicas , results in the following output : batch <number> : replica <number> : [ <number> , <number> ] replica <number> : [ <number> , <number> ] replica <number> : [ ] batch <number> : replica <number> : [ <number> , <number> ] replica <number> : [ <number> , <number> ] replica <number> without ` drop_remainder = true ` this will cause an incompatible shape error when doing a forward pass . ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,convert tf . tensor into tensorflow : : tensor # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behavior ? is there any way to convert tf . tensor from python into tensorflow : : tensor c + + ? # # # standalone code to reproduce the issue ` ` ` shell actually using pybind11 . i can get pyobject * from tf . tensor and have no idea how to get tensorflow : : tensor * from pyobject * ` ` ` # # # relevant log output _no response_,2
tensorflow/tensorflow,"how to use keras layers to augment both image and label data in image segmentation tasks ？ # # # issue type others # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version python <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? how to use tool a to enhance both image and label data in image segmentation tasks . i saw an example in the official document of using keras layer for data augmentation , which is very useful in the training process of classification models because the data augmentation does not require synchronized operations on labels . however , in the segmentation task , if i embed the data enhancement layer into the model structure , i cannot do operations like rotate 、 zoom . etc on the label and image at the same time , because the fit method only feed the original image into the model for inference , which results in the label being isolated from the inference process , so it is impossible to do synchronize affine transformations with the original image . thanks . # # # standalone code to reproduce the issue ` ` ` shell i cannot copy my code from the company computer . i just want to know how to do efficient data augmentation that can apply gpu acceleration and distribute strategy . thanks . ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"savedmodel is not deterministic when saved with enable_op_determinism = true # # # issue type support # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i have a model with monte - carlo sampling layers . at train time it is ok for sampling to be random and i do not set a seed . at inference this model must output deterministic results - so the sampled values must be reproducible . i need to save this model after training using ` tf . keras . models . save_model ( ) ` and then load it with ` tf . keras . models . load_model ( ) ` when training is complete i save the model like so : ` ` ` tf . keras . utils . set_random_seed ( <number> ) tf . config . experimental . enable_op_determinism ( ) model . seed = <number> tf . keras . models . save_model ( model , save_path , save_format = "" tf "" ) ` ` ` to load it : ` ` ` import tensorflow as tf tf . keras . utils . set_random_seed ( <number> ) tf . config . experimental . enable_op_determinism ( ) model = tf . keras . models . load_model ( save_path ) for _ in range ( <number> <sad> output = model ( input_tensor , training = false ) ` ` ` i expect output to be always the same for the same input - within one python process and when reloading , however it is not . this is what my model looks like ( only the relevant portions ) : ` ` ` class custommodel ( tf . keras . model ) : def __init__ ( self , * args , * * kwargs ) : self . _seed = none self . mc_layer = custommontecarlosamplinglayer ( ) <user> . keras . utils . register_keras_serializable ( name = "" seed "" ) <user> def seed ( self ) : return self . _seed <user> . setter def seed ( self , value ) : if is_op_determinism_enabled ( <sad> lgr . info ( "" setting random seed "" ) self . _seed = value tf . random . set_seed ( value ) else : lgr . info ( "" tf op determinism not set "" ) def call ( self , inputs , training = true ) : return self . mc_layer ( inputs , seed = self . _seed ) ` ` ` internally in ` custommontecarlosamplinglayer ` i use the seed value like so tf . random . set_seed ( seed ) samples = dist . sample ( num_samples , seed = seed ) ` ` ` where dist is a distribution from tensorflow_probability . when ` self . _seed = none ` which is the case in training , ` dist . sample ` will randomly generate values . when ` self . _seed ` is set then i need ` dist . sample ` to be reproducible . the loaded model , even with op determinism enabled during saving and loading , is not reproducible . # # # standalone code to reproduce the issue ` ` ` shell see above ` ` ` # # # relevant log output _no response_",2
tensorflow/tensorflow,"error : input <number> of layer ' batch_normalization ' is incompatible with the layer : expected ndim = <number> , found ndim = <number> . estoy intentando entrenar un modelo gan utilizando keras en tensorflow , pero estoy encontrando el siguiente error : ` ` ` input <number> of layer ' batch_normalization ' is incompatible with the layer : expected ndim = <number> , found ndim = <number> . full shape received : ( none , none , none , <number> ) ` ` ` el error ocurre cuando intento entrenar el modelo con el siguiente código history = model . fit ( train_generator , epochs = <number> , callbacks =[ checkpoint , tensorboard ] ) ` ` ` ¿ alguien podría ayudarme a entender qué está causando este error y cómo puedo solucionarlo ? gracias .",2
tensorflow/tensorflow,"hi <user> , thank you for reporting the issue hi <user> , thank you for reporting the issue ! you are seeing this error because colab has python version <number> . tensorflow quantum <number> . <number> is compatible with python <number> , <number> , <number> and does not support python <number> . please refer to the gist where i was able to install tensorflow quantum sucessfully [ here ] ( <url> thank you ! _originally posted by <user> in <url>",2
tensorflow/tensorflow,"using c api and library * * system information * * - windows if possible ) : - tensorflow lite in play windows - google play services version * * standalone code to reproduce the issue * * hi : i have compiled tflite ' s static library "" libtensorflow - lite . a "" and "" libtensorflowite_c . so "" using cmake according to the official document . however , when i introduced this library and used c to call it , the following error occurred reference to ` __imp_tflitemodelcreatefromfile ' "" , undefined reference to ` __ imp_ tfliteinterpreteroptionscreate ' do you know what caused it , or are there any relevant cases thanks",2
tensorflow/tensorflow,"what phone support tensorflow lite gpu delegate ? <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution android <number> # # # mobile device android <number> # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? i tested multiple mobile phones ( including xiaomi 1 2 s ultra , oneplus <number> , honor nova <number> , oppo reno <number> ) using the example android apk at <url> . on all of the devices , the app tells "" gpu is not supported "" , but nnapi and cpu is ok . anything i can do to enable the gpu delegate on these qualcomm snapdragon devices ? # # # standalone code to reproduce the issue ` ` ` shell the example android apk at <url> ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"about using c to call tflite * * system information * * - risc - v * * standalone code to reproduce the issue * * hi i want to compile tflite into a library and then use c to call it . how can i effectively optimize and crop it to make the compiled tflite library file smaller . because for our model , tflite micro is not supported by many operators",2
tensorflow/tensorflow,"tensorflow . map hangs randomly when using for num_parallel_calls a value > <number> # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : yes - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : wsl2 on windows - * * tensorflow installed from ( source or binary ) * * : binary - * * tensorflow version ( use command below ) * * : <number> . <number> - * * python version * * : <date> - * * cuda / cudnn version * * : <number> - * * gpu model and memory * * : rtx a5000 2 4 gb # # # describe the problem i am using tensorflow in a docker container on wsl2 for training neural networks but unfortunately everything freezes during the image import if done with the tf . map function with the parameter "" num_parallel_calls "" set to a value > <number> . below is a minimal example of the code which causes the console to freeze at a random step and the cpu - usage to drop to a minimal level while the ram is still occupied . after everything is frozen , the docker container is unresponsive and has to be restarted by stopping the process . this might even take half a day - > a few hundred thousand iterations but it allways happens at some point . # # # source code / logs ` import tensorflow as tf def parse_function ( filename ) : return tf . io . read_file ( filename ) filenames = [ ] for i in range ( <number> <sad> filenames . append ( "" / d / test . png "" ) dataset_train = tf . data . dataset . from_tensor_slices ( ( filenames ) ) dataset_train = dataset_train . map ( parse_function , num_parallel_calls = <number> ) dataset_train = dataset_train . batch ( <number> ) for epoch in range ( <number> <sad> for step , (x _batch_train ) in enumerate ( dataset_train ) epoch , "" start train step :"", step ) ` this issue is similar to others like # <number> but i still can not find a fix . bypassing the map - function by loading everything at once works - > definitly during mapping if parallel calls are allowed",2
tensorflow/tensorflow,"same error as # <number> - - w tensorflow / core / kernels / data / generator_dataset_op . cc : <number> ] error occurred when finalizing generatordataset iterator : failed precondition : python interpreter state is not initialized . the process may be terminated . [ [ { { node pyfunc } } ] ] i am reproducing the [ code ] ( <url> , i followed there requirement but when code reached to <number> / <number> epoch i got this error : ` <number> - <number> - <number> <time> . <number> : w tensorflow / core / kernels / data / generator_dataset_op . cc : <number> ] error occurred when finalizing generatordataset iterator : failed precondition : python interpreter state is not initialized . the process may be terminated . [ [ { { node pyfunc } } ] ] ` . i tried to to update tensorflow but it stop working because of keras version . code requirements : ` package version - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - absl - py <number> . <number> astor <number> . <number> certifi <number> . <number> charset - normalizer <number> . <number> colorama <number> . <number> filelock <number> . <number> fsspec <number> . <number> gast <number> . <number> google - pasta <number> . <number> grpcio <number> . <number> h5py <number> . <number> huggingface - hub <number> . <number> idna <number> importlib - metadata <number> . <number> joblib <number> . <number> keras <number> . <number> keras - applications <number> . <number> keras - bert <number> . <number> keras - embed - sim <number> . <number> keras - layer - normalization <number> . <number> keras - multi - head <number> . <number> keras - pos - embd <number> . <number> keras - position - wise - feed - forward <number> . <number> keras - preprocessing <number> . <number> keras - self - attention <number> . <number> keras - transformer <number> . <number> markdown <number> . <number> markupsafe <number> . <number> mock <number> . <number> numpy <number> . <number> packaging <number> pip <number> . <number> protobuf <number> . <number> pyyaml <number> regex <number> . <number> requests <number> . <number> scikit - learn <number> . <number> scipy <number> . <number> setuptools <number> . <number> six <number> . <number> sklearn <number> . post5 tensorboard <number> . <number> tensorflow - estimator <number> . <number> tensorflow - gpu <number> . <number> termcolor <number> . <number> threadpoolctl <number> . <number> tokenizers <number> . <number> torch <number> . <number> tqdm <number> . <number> transformers <number> . <number> typing_extensions <number> . <number> urllib3 <number> . <number> werkzeug <number> . <number> wheel <number> . <number> wincertstore <number> wrapt <number> . <number> zipp <number> . <number> ` and run . py code is : ` # - * - coding : utf - <number> - * - from data_loader import data_generator , load_data from model import e2emodel , evaluate from utils import extract_items , get_tokenizer , metric import os , argparse os . environ [ "" cuda_visible_devices "" ] = "" <number> "" from keras import backend as k if ( k . backend ( ) = = ' tensorflow ' <sad> import tensorflow as tf from keras . backend . tensorflow_backend import set_session config = tf . configproto ( ) config . gpu_options . allow_growth = true sess = tf . session ( config = config ) <hashtag> tried </hashtag> these lines too but not useful ' ' ' config = tf . compat . v1 . configproto ( gpu_options = tf . compat . v1 . gpuoptions ( allow_growth = true ) ) sess = tf . compat . v1 . session ( config = config ) ' ' ' parser = argparse . argumentparser ( description = ' model controller ' ) parser . add_argument ( ' - - train ' , default = true , type = bool , help = ' to train the hbt model , python run . py - - train = true ' ) parser . add_argument ( ' - - dataset ' , default = ' vkg ' , type = str , help = ' specify the dataset from [ "" nyt "" , "" webnlg "" , "" ace04 "" , "" nyt10 - hrl "" , "" nyt11 - hrl "" , "" wiki - kbp "" ] ' ) args = parser . parse_args ( ) if __name__ = = ' __main__ ' : # pre - trained bert model config bert_model = ' cased_l - 1 2 _h - 7 6 8 _a - <number> ' bert_config_path = ' pretrained_bert_models / ' + bert_model + ' / bert_config . json ' bert_vocab_path = ' pretrained_bert_models / ' + bert_model + ' / vocab . txt ' bert_checkpoint_path = ' pretrained_bert_models / ' + bert_model + ' / bert_model . ckpt ' dataset = args . dataset train_path = ' data / ' + dataset + ' / train_triples . json ' dev_path = ' data / ' + dataset + ' / dev_triples . json ' <hashtag> test path </hashtag> = ' data / ' + dataset + ' / test_split_by_num / test_triples_5 . json ' # [ ' <number> ' , ' <number> ' , ' <number> ' , ' <number> ' , ' <number> ' ] <hashtag> test path </hashtag> = ' data / ' + dataset + ' / test_split_by_type / test_triples_seo . json ' # [ ' normal ' , ' seo ' , ' epo ' ] test_path = ' data / ' + dataset + ' / test_triples . json ' # overall test rel_dict_path = ' data / ' + dataset + ' / rel2id . json ' save_weights_path = ' saved_weights / ' + dataset + ' / best_model . weights ' lr = 1 e - <number> tokenizer = get_tokenizer ( bert_vocab_path ) train_data , dev_data , test_data , id2rel , rel2id , num_rels = load_data ( train_path , dev_path , test_path , rel_dict_path ) subject_model , object_model , hbt_model = e2emodel ( bert_config_path , bert_checkpoint_path , lr , num_rels ) if args . train : batch_size = <number> epoch = <number> max_len = <number> steps = len ( train_data ) / / batch_size data_manager = data_generator ( train_data , tokenizer , rel2id , num_rels , max_len , batch_size ) evaluator = evaluate ( subject_model , object_model , tokenizer , id2rel , dev_data , save_weights_path ) hbt_model . fit_generator ( data_manager . __iter__ ( ) , steps_per_epoch = steps , epochs = epoch , callbacks =[ evaluator ] ) else : hbt_model . load_weights ( save_weights_path ) test_result_path = ' results / ' + dataset + ' / test_result . json ' isexactmatch = true if dataset = = ' wiki - kbp ' else false if isexactmatch : print ( "" exact match "" ) else match "" ) precision , recall , f1_score = metric ( subject_model , object_model , test_data , id2rel , tokenizer , isexactmatch , test_result_path ) print ( f ' { precision } \ \ t { recall } \ \ t { f1_score } ' ) ` # <number>",2
tensorflow/tensorflow,"tensorflow object detection project <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version tf1 . x # # # custom code no # # # os platform and distribution macos ventura # # # mobile device macbook air <number> i3 # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? so i am making an object detection projetc for school and i need help whenever i run this code this is the error that pops up . please help it is due in a few days i have installed all neccesary modules , i think # # # standalone code to reproduce the issue ` ` ` shell # ! / usr / bin / env python # coding : utf - <number> "" "" "" detect objects using your webcam = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = "" "" "" # % % # this demo will take you through the steps of running an "" out - of - the - box "" detection model to # detect objects in the video stream extracted from your camera . # % % # create the data directory # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # the snippet shown below will create the ` ` data ` ` directory where all our data will be stored . the # code will create a directory structure as shown bellow : # # . <repeated> code - block : : bash # # data # └ ─ ─ models # # where the ` ` models ` ` folder will will contain the downloaded models . import os <hashtag> os </hashtag> . chdir ( ' / users / akulthota / desktop / object detection ' ) data_dir = os . path . join ( os . getcwd ( ) , ' data ' ) models_dir = os . path . join ( data_dir , ' models ' ) for dir in [ data_dir , models_dir ] : if not os . path . exists ( dir ) : os . mkdir ( dir ) # % % # download the model # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # the code snippet shown below is used to download the object detection model checkpoint file , # as well as the labels file ( . pbtxt ) which contains a list of strings used to add the correct # label to each detection ( e . g . person ) . # # the particular detection algorithm we will use is the ` ssd resnet101 v1 fpn 6 4 0 x640 ` . more # models can be found in the ` tensorflow <number> detection model zoo < <url> # to use a different model you will need the url name of the specific model . this can be done as # follows : # # <number> . right click on the ` model name ` of the model you would like to use ; # <number> . click on ` copy link address ` to copy the download link of the model ; # <number> . paste the link in a text editor of your choice . you should observe a link similar to ` ` download . tensorflow . org / models / object_detection / tf2 / y <elongated> / x <elongated> . tar . gz ` ` ; # <number> . copy the ` ` x <elongated> ` ` part of the link and use it to replace the value of the ` ` model_name ` ` variable in the code shown below ; # <number> . copy the ` ` y <elongated> ` ` part of the link and use it to replace the value of the ` ` model_date ` ` variable in the code shown below . # # for example , the download link for the model used below is : ` ` download . tensorflow . org / models / object_detection / tf2 / <number> / ssd_resnet101_v1_fpn_640x640_coco17_tpu - <number> . tar . gz ` ` import tarfile import urllib . request # download and extract model model_date = ' <number> ' model_name = ' ssd_resnet101_v1_fpn_640x640_coco17_tpu - <number> ' model_tar_filename = model_name + ' . tar . gz ' models_download_base = ' <url> model_download_link = models_download_base + model_date + ' / ' + model_tar_filename path_to_model_tar = os . path . join ( models_dir , model_tar_filename ) path_to_ckpt = os . path . join ( models_dir , os . path . join ( model_name , ' checkpoint / ' ) ) path_to_cfg = os . path . join ( models_dir , os . path . join ( model_name , ' pipeline . config ' ) ) if not os . path . exists ( path_to_ckpt ) : print ( ' downloading model . this may take a while . <repeated> ' , end = ' ' ) urllib . request . urlretrieve ( model_download_link , path_to_model_tar ) tar_file = tarfile . open ( path_to_model_tar ) tar_file . extractall ( models_dir ) tar_file . close ( ) os . remove ( path_to_model_tar ) print ( ' done ' ) # download labels file label_filename = ' mscoco_label_map . pbtxt ' labels_download_base = \ \ ' <url> path_to_labels = os . path . join ( models_dir , os . path . join ( model_name , label_filename ) ) if not os . path . exists ( path_to_labels ) : print ( ' downloading label file . <repeated> ' , end = ' ' ) import ssl ssl . _create_default_https_context = ssl . _create_unverified_context urllib . request . urlretrieve ( labels_download_base + label_filename , path_to_labels ) print ( ' done ' ) # % % # load the model # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # next we load the downloaded model os . environ [ ' tf_cpp_min_log_level ' ] = ' <number> ' # suppress tensorflow logging import tensorflow as tf from object_detection . utils import label_map_util from object_detection . utils import visualization_utils as vis_util from object_detection . utils import visualization_utils as viz_utils from object_detection . builders import model_builder tf . get_logger ( ) . setlevel ( ' error ' ) # suppress tensorflow logging ( <number> ) # enable gpu dynamic memory allocation gpus = tf . config . experimental . list_physical_devices ( ' gpu ' ) for gpu in gpus : tf . config . experimental . set_memory_growth ( gpu , true ) # load pipeline config and build a detection model configs = config_util . get_configs_from_pipeline_file ( path_to_cfg ) model_config = configs [ ' model ' ] detection_model = model_builder . build ( model_config = model_config , is_training = false ) # restore checkpoint ckpt = tf . compat . v2 . train . checkpoint ( model = detection_model ) ckpt . restore ( os . path . join ( path_to_ckpt , ' ckpt - <number> ' ) ) . expect_partial ( ) <user> . function def detect_fn ( image ) : "" "" "" detect objects in image . "" "" "" image , shapes = detection_model . preprocess ( image ) prediction_dict = detection_model . predict ( image , shapes ) detections = detection_model . postprocess ( prediction_dict , shapes ) return detections , prediction_dict , tf . reshape ( shapes , [ - <number> ] ) # % % # load label map data ( for plotting ) # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # label maps correspond index numbers to category names , so that when our convolution network # predicts ` <number> ` , we know that this corresponds to ` airplane ` . here we use internal utility # functions , but anything that returns a dictionary mapping integers to appropriate string labels # would be fine . category_index = label_map_util . create_category_index_from_labelmap ( path_to_labels , use_display_name = true ) # % % # define the video stream # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # we will use ` opencv < <url> to capture the video stream # generated by our webcam . for more information you can refer to the ` opencv - python tutorials < <url> import cv2 cap = cv2 . videocapture ( <number> ) # % % # putting everything together # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # the code shown below loads an image , runs it through the detection model and visualizes the # detection results , including the keypoints . # # note that this will take a long time ( several minutes ) the first time you run this code due to # tf . function ' s trace - compilation - - - on subsequent runs ( e . g . on new images ) , things will be # faster . # # here are some simple things to try out if you are curious : # # * modify some of the input images and see if detection still works . some simple things to try out here ( just uncomment the relevant portions of code ) include flipping the image horizontally , or converting to grayscale ( note that we still expect the input image to have <number> channels ) . # * print out ` detections [ ' detection_boxes ' ] ` and try to match the box locations to the boxes in the image . notice that coordinates are given in normalized form ( i . e . , in the interval [ <number> , <number> ] ) . # * set ` ` min_score_thresh ` ` to other values ( between <number> and <number> ) to allow more detections in or to filter out more detections . import numpy as np while true : # read frame from camera ret , image_np = cap . read ( ) # expand dimensions since the model expects images to have shape : [ <number> , none , none , <number> ] image_np_expanded = np . expand_dims ( image_np , axis = <number> ) # things to try : # flip horizontally # image_np = np . fliplr ( image_np ) . copy ( ) # convert image to grayscale # image_np = np . tile ( # np . mean ( image_np , <number> , keepdims = true ) , ( <number> , <number> , <number> ) ) . astype ( np . uint8 ) input_tensor = tf . convert_to_tensor ( np . expand_dims ( image_np , <number> ) , dtype = tf . float32 ) detections , predictions_dict , shapes = detect_fn ( input_tensor ) label_id_offset = <number> image_np_with_detections = image_np . copy ( ) viz_utils . visualize_boxes_and_labels_on_image_array ( image_np_with_detections , detections [ ' detection_boxes ' ] [ <number> ] . numpy ( ) , ( detections [ ' detection_classes ' ] [ <number> ] . numpy ( ) + label_id_offset ) . astype ( int ) , detections [ ' detection_scores ' ] [ <number> ] . numpy ( ) , category_index , use_normalized_coordinates = true , max_boxes_to_draw = <number> , min_score_thresh = . <number> , agnostic_mode = false ) # display output cv2 . imshow ( ' object detection ' , cv2 . resize ( image_np_with_detections , ( <number> , <number> ) ) ) if cv2 . waitkey ( <number> ) & 0 xff = = ord ( ' q ' <sad> break cap . release ( ) cv2 . destroyallwindows ( ) ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" / users / akulthota / desktop / object / object_detection_camera . py "" , line <number> , in <module> from object_detection . utils import label_map_util file "" / library / frameworks / python . framework / versions / <number> / lib / python3 . <number> / site - packages / object_detection / utils / label_map_util . py "" , line <number> , in <module> from object_detection . protos import string_int_label_map_pb2 file "" / library / frameworks / python . framework / versions / <number> / lib / python3 . <number> / site - packages / object_detection / protos / string_int_label_map_pb2 . py "" , line <number> , in <module> _descriptor . fielddescriptor ( file "" / library / frameworks / python . framework / versions / <number> / lib / python3 . <number> / site - packages / google / protobuf / descriptor . py "" , line <number> , in __new__ _message . message . _checkcalledfromgeneratedfile ( ) typeerror : descriptors cannot not be created directly . if this call came from a _pb2 . py file , your generated code is out of date and must be regenerated with protoc >= <number> . <number> . if you cannot immediately regenerate your protos , some other possible workarounds are : <number> . downgrade the protobuf package to <number> . x or lower . <number> . set protocol_buffers_python_implementation = python ( but this will use pure - python parsing and will be much slower ) . more information ` ` ` </details>",2
tensorflow/tensorflow,"typeerror : cannot convert <number> to eagertensor of dtype int64 <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution googlecolab # # # mobile device _no response_ # # # python version <date> # # # bazel version colab # # # gcc / compiler version colab # # # cuda / cudnn version colab # # # gpu model and memory colab # # # current behaviour ? getting this error when i execute the code present in tensowflow website to implement tf . keras . optimizers . schedules . learningrateschedule error : typeerror : cannot convert <number> to eagertensor of dtype int64 ` ` ` python class mylrschedule ( tf . keras . optimizers . schedules . learningrateschedule ) : def __init__ ( self , initial_learning_rate ) : self . initial_learning_rate = initial_learning_rate def __call__ ( self , step ) : return self . initial_learning_rate / ( step + <number> ) optimizer = tf . keras . optimizers . sgd ( learning_rate = mylrschedule ( <number> ) ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell typeerror : cannot convert <number> to eagertensor of dtype int64 ` ` ` # # # relevant log output typeerror traceback ( most recent call last ) [ < ipython - input - <number> - 8 6 d045432fd5 > ] ( https :// localhost : <number> /# ) in < cell line : <number> > ( ) <number> return self . initial_learning_rate / ( step + <number> ) <number> - - - - > <number> optimizer = tf . keras . optimizers . sgd ( learning_rate = mylrschedule ( <number> ) ) <number> frames [ / usr / local / lib / python3 . <number> / dist - packages / keras / optimizers / sgd . py ] ( https :// localhost : <number> /# ) in __init__ ( self , learning_rate , momentum , nesterov , weight_decay , clipnorm , clipvalue , global_clipnorm , use_ema , ema_momentum , ema_overwrite_frequency , jit_compile , name , * * kwargs ) <number> * * kwargs <number> ) - - > <number> self . _learning_rate = self . _build_learning_rate ( learning_rate ) <number> self . momentum = momentum <number> self . nesterov = nesterov [ / usr / local / lib / python3 . <number> / dist - packages / keras / optimizers / optimizer . py ] ( https :// localhost : <number> /# ) in _build_learning_rate ( self , learning_rate ) <number> # create a variable to hold the current learning rate . <number> current_learning_rate = tf . convert_to_tensor ( - - > <number> learning_rate ( self . iterations ) <number> ) <number> self . _current_learning_rate = tf . variable ( [ < ipython - input - <number> - 8 6 d045432fd5 > ] ( https :// localhost : <number> /# ) in __call__ ( self , step ) <number> <number> def __call__ ( self , step ) : - - - - > <number> return self . initial_learning_rate / ( step + <number> ) <number> <number> optimizer = tf . keras . optimizers . sgd ( learning_rate = mylrschedule ( <number> ) ) [ / usr / local / lib / python3 . <number> / dist - packages / tensorflow / python / util / traceback_utils . py ] ( https :// localhost : <number> /# ) in error_handler ( * args , * * kwargs ) <number> except exception as e : <number> filtered_tb = _process_traceback_frames ( e . __traceback__ ) - - > <number> raise e . with_traceback ( filtered_tb ) from none <number> finally : <number> del filtered_tb [ / usr / local / lib / python3 . <number> / dist - packages / tensorflow / python / framework / constant_op . py ] ( https :// localhost : <number> /# ) in convert_to_eager_tensor ( value , ctx , dtype ) <number> dtype = dtypes . as_dtype ( dtype ) . as_datatype_enum <number> ctx . ensure_initialized ( ) - - > <number> return ops . eagertensor ( value , ctx . device_name , dtype ) <number> <number> typeerror convert <number> to eagertensor of dtype int64 </details>",2
tensorflow/tensorflow,"unable to compile tensorflow c + + code using cmake i followed this steps <number> ) i cloned the repo <number> ) cd tensorflow <number> ) mkdir examples in examples folder i created helloworld . cc file and cmakelist . txt this is cmakelist . txt file cmake_minimum_required ( version <number> . <number> ) project ( examples ) # specify the path to the tensorflow source directory set ( tensorflow_source_dir "" d <annoyed> github_issues / tensorflow / tensorflow "" ) # add the tensorflow source directory to the cmake module path list ( append cmake_module_path "" ${ tensorflow_source_dir } / cmake "" ) # add the tensorflow include directories include_directories ( ${ tensorflow_source_dir } ) include_directories ( ${ tensorflow_source_dir } / tensorflow / cc ) include_directories ( ${ tensorflow_source_dir } / tensorflow / core ) # build the hello - world executable add_executable ( hello - world hello - world . cc ) target_link_libraries ( hello - world tensorflow ) after running the command cmake - - build . - - config release i got this error d :\\ github_issues \ \ tensorflow \ \ examples \ \ hello - world . cc ( <number> <sad> fatal error c1083 : cannot open include file : ' tensorflow / cc / client / cl ient_session . h ' such file or directory [ [ d :\\ github_issues \ \ tensorflow \ \ examples \ \ build \ \ hello - world . vcxproj ] ] [ <number> ] ( <url> ! [ <number> ] ( <url> ! [ <number> ] ( <url> ! [ <number> ] ( <url>",2
tensorflow/tensorflow,"shuffle flag is true in make_dataset function <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? just replicating an example of time series forecasting from link [ tsf ] ( <url> there is a function make_dataset which uses timeseries_dataset_from_array . the function timeseries_dataset_from_array shuffle flag is set to true which is not allowed in time series forecasting . # # # standalone code to reproduce the issue ` ` ` shell def make_dataset ( self , data ) = np . array ( data , dtype = np . float32 ) ds = tf . keras . utils . timeseries_dataset_from_array ( data = data , targets = none , sequence_length = self . total_window_size , sequence_stride = <number> , shuffle = true , batch_size = <number> , ) ds = ds . map ( self . split_window ) return ds windowgenerator . make_dataset = make_dataset ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"building from source with clang and nvcc ? <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution debian gnu / linux <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version gcc9 / clang12 # # # cuda / cudnn version <number> # # # gpu model and memory nvidia t4 # # # current behaviour ? the docs say that tf <number> is supported with gcc <number> , but clang / llvm builds significantly faster . how can i use bazel to at least try to build using clang and nvcc at the same time ? changing gcc_host_compiler_path =""< clang - path > "" fails . using my own crosstool - top with clang seems to work , but i am not sure if this ends up building the cuda kernels with clang as well . # # # standalone code to reproduce the issue ` ` ` shell # from . bazelrc build - - crosstool_top =// toolchain : clang_suite build : cuda - - repo_env tf_need_cuda = <number> # build : cuda - - crosstool_top = <user> / / crosstool : toolchain build : cuda - - <user> / / : enable_cuda build : tf_gpu - - action_env python_bin_path =""/ opt / conda / bin / python3 "" build : tf_gpu - - action_env python_lib_path =""/ bin "" build : tf_gpu - - python_path =""/ opt / conda / bin / python3 "" build : tf_gpu - - action_env pythonpath =""/ home / axlui / p3achygo / python <annoyed> usr / lib / llvm - <number> / bin <annoyed> home / axlui / . local / bin <annoyed> usr / local / cuda / bin <annoyed> opt / conda / bin <annoyed> opt / conda / condabin <annoyed> usr / local / bin <annoyed> usr / bin <annoyed> bin <annoyed> usr / local / games <annoyed> usr / games <annoyed> usr / local / go / bin "" build : tf_gpu - - define = with_xla_support = true build : tf_gpu - - action_env tf_cuda_version = "" <number> "" build : tf_gpu - - action_env tf_cudnn_version = "" <number> "" build : tf_gpu - - action_env cuda_toolkit_path =""/ usr / local / cuda - <number> "" build : tf_gpu - - action_env cudnn_install_path =""/ usr / local / cuda "" build : tf_gpu - - action_env tf_cuda_compute_capabilities = "" <number> "" build : tf_gpu - - action_env ld_library_path =""/ usr / local / cuda / lib64 <annoyed> usr / local / nccl2 / lib <annoyed> usr / local / cuda / extras / cupti / lib64 "" build : tf_gpu - - action_env gcc_host_compiler_path =""/ usr / bin / x86_64 - linux - gnu - gcc - <number> "" # build : tf_gpu - - action_env cc =""/ usr / lib / llvm - <number> / bin / clang "" # build : tf_gpu - - action_env cxx =""/ usr / lib / llvm - <number> / bin / clang + + "" # build : tf_gpu - - action_env gcc_host_compiler_path =""/ usr / lib / llvm - <number> / bin / clang "" build : tf_gpu - - config = cuda ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"issue when importing pix2pix in google colab hello , i wanted to use pix2pix in google colab and here is the command i used to import it : pip install git + <url> i also tried ! pip install - q git + <url> but for both request , i get this error : looking in indexes : <url> <url> collecting git + <url> cloning <url> to / tmp / pip - req - build - z5dheb37 running command git clone - - filter = blob : none - - quiet <url> / tmp / pip - req - build - z5dheb37 resolved <url> to commit 1 ca61321294cd2e97efc021ff1b3700b42befd0b error : subprocess - exited - with - error × python setup . py egg_info did not run successfully . │ exit code : <number> ╰ ─ > see above for output . note : this error originates from a subprocess , and is likely not a problem with pip . preparing metadata ( setup . py ) . <repeated> error error : metadata - generation - failed × encountered error while generating package metadata . ╰ ─ > see above for output . note : this is an issue with the package mentioned above , not pip . hint : see above for details . could you please repair it ? best regards p . s . there is any way for me to use pix2pix in an alternative way , could you please indicate me on how to do it because i did not found an alternative .",2
tensorflow/tensorflow,"getting error with using coco - ssd model with the latest tensorflow <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution windows # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? when i try to use the net . detect ( frame ) , it throws me error : typeerror : _tensorflow_tfjs_core__webpack_imported_module_0__ . util . convertbackendvaluesandarraybuffer is not a function at mathbackendcpu . readsync ( backend_cpu . js : <number> : <number> ) at engine . readsync ( engine . js : <number> : <number> ) at tensor . datasync ( tensor . js : <number> : <number> ) at d . infer ( coco - ssd . es2017 . esm . min . js : <number> : <number> ) even though i am using the latest tensorflow js libraries . # # # standalone code to reproduce the issue ` ` ` shell i am trying to use the coco - ssd model to do object detection . in my package . json i have : "" <user> - models / coco - ssd "" : "" ^ <number> . <number> "" , "" <user> / tfjs "" : "" ^ <number> . <number> "" , "" <user> / tfjs - backend - cpu "" : "" ^ <number> . <number> "" , "" <user> / tfjs - backend - webgl "" : "" ^ <number> . <number> "" , for some reasons , i want to use the latest tensorflow libraries , as my project uses other things also . i have the following code : const tf = require ( ' <user> / tfjs ' ); const _tfcpubackend = require ( ' <user> / tfjs - backend - cpu ' ); const _tfwebglbackend = require ( ' <user> / tfjs - backend - webgl ' ); const cocossd = require ( ' <user> - models / coco - ssd ' ); i have also set the tf . setbackend ( ' webgl ' ) , and tf . ready ( ) before cocossd . load ( ) but when i try to use the net . detect ( frame ) , it throws me error : typeerror is not a function at mathbackendcpu . readsync ( backend_cpu . js : <number> : <number> ) at engine . readsync ( engine . js : <number> : <number> ) at tensor . datasync ( tensor . js : <number> : <number> ) at d . infer ( coco - ssd . es2017 . esm . min . js : <number> : <number> ) even though i am using the latest tensorflow js libraries . ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"how does average pooling function work in tensorflow ? let us assume a tensor like this : ` ` ` x = tf . constant ( [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] ) ` ` ` to apply the average pooling function , i will do this : ` ` ` x = tf . reshape ( x , [ <number> , <number> , <number> , <number> ] ) avg_pool_2d = tf . keras . layers . averagepooling2d ( pool_size =( <number> , <number> ) , strides =( <number> , <number> ) , padding = ' same ' ) avg_pool_2d ( x ) ` ` ` the result is : ` ` ` < tf . tensor : shape =( <number> , <number> , <number> , <number> ) , dtype = float32 , numpy = array ( [ [ [ [ <number> . ] , [ <number> ] ] , [ [ <number> ] , [ <number> . ] ] ] ] , dtype = float32 ) > ` ` ` i can follow the logic above : ` ` ` ( <number> + <number> + <number> + <number> ) / <number> = <number> ( <number> + <number> ) / <number> = <number> ( <number> + <number> ) / <number> = <number> ( <number> / <number> ) = <number> ` ` ` * * i think the logic is : * * the pooling filter is usually situated inside the tensor to perform the pooling operator . but when the entire filter does not situate inside the tensor ( see the below figure for an example ) , we need to specify the number of elements of the filter that are situated inside the tensor ( a ) . the following figure illustrates the logic for a <number> by <number> tensor , with pooling filter and stride sizes of <number> by <number> , and padding the same . [ image ] ( <url> however , it is not always like this . for example , suppose the following tensor : ` ` ` y = tf . constant ( [ [ <number> . , <number> . , <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . , <number> . , <number> . ] ] ) ` ` ` then , i do this : ` ` ` y = tf . reshape ( y , [ <number> , <number> , <number> , <number> ] ) avg_pool_2d = tf . keras . layers . averagepooling2d ( pool_size =( <number> , <number> ) , strides =( <number> , <number> ) , padding = ' same ' ) avg_pool_2d ( y ) ` ` ` the result is like this : ` ` ` < tf . tensor : shape =( <number> , <number> , <number> , <number> ) , dtype = float32 , numpy = array ( [ [ [ [ <number> ] , [ <number> . ] ] ] ] , dtype = float32 ) > ` ` ` if i wanted to follow the logic for the first example , i expected the result to be like this ( <number> + <number> + <number> + <number> + <number> + <number> + <number> + <number> ) / <number> = <number> ( <number> + <number> ) / <number> = <number> ` ` ` i am using tensorflow <number> . <number> . what mistake am i making ?",2
tensorflow/tensorflow,"xla_sharding in new version hi what is the replacement for ` from tensorflow . compiler . xla . experimental . xla_sharding import xla_sharding ` in tf - <number> . <number> ? for example , the function is ` xla_sharding . split ( ) ` .",2
tensorflow/tensorflow,"we do not have your exam ready right now i had purchased the tensorflow exam and tried to start it , but it is showing that we do not have your exam ready right now . but in the candidate portal it is showing resume . so i am gonna post the screenshot of my problem < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> <number> "" src = "" <url>",2
tensorflow/tensorflow,"unequal strides support recently removed for depthwiseconv2d please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : yes - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : ubuntu - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : <number> and <number> - * * python version * * : <number> - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : <number> and <number> - * * gpu model and memory * * : - * * exact command to reproduce * * : import tensorflow as tf import numpy as np layer1 = tf . keras . layers . depthwiseconv2d ( depth_multiplier = <number> , kernel_size =( <number> ) , strides =( <number> ) ) print ( layer1 ( np . ones ( ( <number> , <number> , <number> , <number> ) , dtype = np . float32 ) ) ) you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with : ` ` ` bash python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . the command would run without any error when i was using it with tf2 . <number> version installed previously . on the newly installed tf2 . <number> version , however , the code throws an error as : invalidargumenterror : exception encountered when calling layer ' depthwise_conv2d_1 ' ( type depthwiseconv2d ) . { { function_node __wrapped__depthwiseconv2dnative_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } current implementation only supports equal length strides in the row and column dimensions . [ op : depthwiseconv2dnative ] call arguments received by layer ' depthwise_conv2d_1 ' ( type depthwiseconv2d ) inputs = tf . tensor ( shape =( <number> , <number> , <number> , <number> ) , dtype = float32 ) seeing the documentation , the error is expected for version <number> , but should work for versions <number> and <number> <url> <url> <url> is it that the implementation existing in the earlier versions has been removed recently for all previous and current versions ? # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,data and ml module missing <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution mac # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? a bug happened ! # # # standalone code to reproduce the issue ` ` ` shell from data import bodypart from ml import movenet ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"training data format for efficientnet i would like to train a efficientnetb7 model from scratch to classify 2 d arrays into two <number> classes , but it seems like i did not prepare my data in the correct format . currently my ` x_train ` is a list of float64 arrays with a 6 0 0 x600 size , my ` y_train ` is a list of integers that are either <number> or <number> . of course ` x_train ` and ` y_train ` have the same length . this is what i have so far : ` ` ` from tensorflow . keras . layers import dense , flatten from tensorflow . keras . applications import efficientnetb7 from tensorflow . keras . models import sequential base_model = efficientnetb7 ( include_top = false , weights = none , input_shape =( <number> , <number> ) , classes = <number> ) base_model . trainable = true model = sequential ( ) model . add ( base_model ) model . add ( flatten ( ) ) model . add ( dense ( <number> , activation = ' sigmoid ' ) ) model . compile ( optimizer = ' adam ' , loss = ' binary_crossentropy ' , metrics =[ ' accuracy ' ] ) history = model . fit ( x_train , y_train , batch_size = <number> , epochs = <number> ) ` ` ` the last line currently gives me an error : ` ` ` valueerror : failed to find data adapter that can handle input ' list ' > containing values of types { "" < class ' numpy . ndarray ' > "" } ) , ( < class ' list ' > containing values of types { "" < class ' int ' > "" } ) ` ` ` what would be the right format for training data ? any help is appreciated",2
tensorflow/tensorflow,issue with obtaining files from this repo via google colab [ screenshot <number> - <number> - <number> <number> ] ( <url>,2
tensorflow/tensorflow,"tf . gradienttape . gradients ( ) does not support graph control flow operations like tf . cond or tf . while at this time . use tf . gradients ( ) instead please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : - * * python version * * : - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : - * * gpu model and memory * * : - * * exact command to reproduce * * : you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"tensorflow lite library is crashing in wasm library at 3 rd inference <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution emscripten , ubuntu <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hello ! i have c + + code that i want to deploy as wasm library and this code contains tflite library . i have compiled tflite library with xnnpack support using emscripten toolchain quite easy , so no issue there . i have a leight - weight convolution + dense model that runs perfectly on desktop , but i am starting having problems in the browser . in <percent> of cases i have an error on the third inference : uncaught runtimeerror : memory access out of bounds through some trivial debugging i have found out that the issue comes from _interpreter - > invoke ( ) method . does not matter if i put any input or not , i just need to call invoke ( ) three times and i have a crash . first thing first : i decided to add more memory to my wasm library by adding this line to cmake "" ${ cmake_cxx_flags } - s total_stack = <number> - s total_memory = <number> "" ) set ( cmake_cxx_flags "" ${ cmake_cxx_flags } - s total_stack = <number> - s total_memory = <number> "" ) <number> mb and <number> mb in total for <number> mb model - i think this is more than enough . and on top of that , i am allowing memory growth . but unfortunately , i have exactly the same issue . i am beating on this problem for <number> weeks straight and at this stage i have no clue how to fix it . also i have tried to set custom allocation using tflitecustomallocation but in this case i have a crash on the very first inference . i guess i was not using it right , but unfortunately i could not find even one tutorial describing how to apply custom allocation in tflite . i said that i have a crash in <percent> of cases . there was one time when wasm library worked and inference worked as well . it happens just randomly once , and i could not reproduce it anymore . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell here is the code that does tflite inference <hashtag> include </hashtag> <cstdlib> <hashtag> include </hashtag> "" tflite_model . h "" <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> "" tensorflow / lite / interpreter . h "" <hashtag> include </hashtag> "" tensorflow / lite / util . h "" namespace tracker { <hashtag> if def </hashtag> emscripten void tflitemodel : : init ( std : : stringstream & stream ) { std : : string img_str = stream . str ( ); std : : vector <char> img_model_data ( img_str . size ( )); std : : copy ( img_str . begin ( ) , img_str . end ( ) , img_model_data . begin ( )); _model = tflite : : flatbuffermodel : : buildfrombuffer ( img_str . data ( ) , img_str . size ( )); <hashtag> else </hashtag> void tflitemodel : : init ( const std : : string & path ) { _model = tflite : : flatbuffermodel : : buildfromfile ( path . c_str ( )); <hashtag> end if </hashtag> tflite : : ops : : builtin : : builtinopresolver resolver ; tflite : : interpreterbuilder ( * _model , resolver ) ( & _interpreter ) ; _interpreter - > allocatetensors ( ); / * for ( int i = <number> ; i < _interpreter - > tensors_size ( ); i + + ) { tflitetensor * tensor = _interpreter - > tensor ( i ) ; if ( tensor - > allocation_type = = ktflitearenarw || tensor - > allocation_type = = ktflitearenarwpersistent ) { int aligned_bytes = tensor - > bytes + ( tflite : : kdefaulttensoralignment - tensor - > bytes % tflite : : kdefaulttensoralignment ) % tflite : : kdefaulttensoralignment ; tflitecustomallocation customalloc ; int result = posix_memalign ( & customalloc . data , tflite : : kdefaulttensoralignment , tensor - > bytes ) ; if ( result ! = <number> || customalloc . data = = null ) { std : : cout < < "" posix_memalign does not work ! \ \ n "" ; } tflitestatus st = _interpreter - > setcustomallocationfortensor ( i , customalloc ) ; std : : cout < < "" status = "" < < st < < std : : endl ; if ( tensor - > bytes % tflite : : kdefaulttensoralignment ! = <number> ) { std : : cout < < "" bad ! i "" < < i < < "" , size "" < < tensor - > bytes < < std : : endl ; } _allocations . push_back ( customalloc ) ; } } exit ( <number> ); */ } void tflitemodel : : forward ( const cv : : mat & img_input , const std : : vector <float> & lms_input ) { float * model_in = _interpreter - > typed_input_tensor <float> ( <number> ); std : : memcpy ( model_in , img_input . data , img_input . total ( ) * img_input . elemsize ( )); float * lms_in = _interpreter - > typed_input_tensor <float> ( <number> ); std : : memcpy ( lms_in , lms_input . data ( ) , sizeof ( float ) * lms_input . size ( )); _interpreter - > invoke ( ); } float * tflitemodel : : out ( ) { return _interpreter - > typed_output_tensor <float> ( <number> ); } std : : vector <int> tflitemodel : : getoutputshape ( ) const { tflitetensor * outtensor = _interpreter - > output_tensor ( <number> ); tfliteintarray * dims = outtensor - > dims ; std : : vector <int> sh ; for ( int i = <number> ; i < dims - > size ; i + + ) { sh . push_back ( dims - > data [ i ] ); } return sh ; } } ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"how to evaluate a pretrained tf mobilenet_v2 saved_model for accuracy on test dataset # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : no - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : ubuntu <number> - * * tensorflow installed from ( source or binary ) * * : binary - * * tensorflow version ( use command below ) * * : <number> - * * python version * * : <number> # # # describe the problem how can i use a pretrained saved_model and find its accuracy on a test dataset ? i have mobilenet_v2 saved model which is sourced from <url> i have an imagenet validation dataset consisting of <number> images , and a labels . txt file consisting of ground truth labels for those <number> images . i also have imagenetlabels . txt sourced from <url> consisting of <number> imagenet classes . how do i preprocess this data so that i can run evaluate ( ) function to find test_data loss and accuracy of this pretrained model ? i am currently using the below script , but it does not seem to work : ` ` ` import tensorflow as tf import tensorflow_hub as hub import numpy as np import os m = tf . keras . sequential ( [ hub . keraslayer ( "" <url> output_shape =( <number> , ) ) ] ) m . build ( [ none , <number> , <number> , <number> ])# batch input shape . images = ' / home / downloads / ilsvrc2012_img_val ' classes = ' / home / documents / imagenetlabels . txt ' labels ='/ home / documents / val . txt ' with open ( labels , ' r ' ) as f : label_name = [ line . strip ( ) for line in f . readlines ( ) ] class_map = { } with open ( classes , ' r ' ) as f : classes = [ line . strip ( ) for line in f ] for i , class_name in enumerate ( classes ) : class_map [ class_name ] = i test_labels =[] for label in label_name : if label in class_map : test_labels . append ( class_map [ label ] ) else : print ( f "" label ' { label } not found in class_map "" ) image_paths = [ os . path . join ( images , filename ) for filename in os . listdir ( images ) ] dataset = tf . data . dataset . from_tensor_slices ( ( image_paths , test_labels ) ) def preprocess_image ( image_path ) : image = tf . io . read_file ( image_path ) image = tf . image . decode_jpeg ( image , channels = <number> ) image = tf . image . resize ( image , [ <number> ] ) image = tf . image . convert_image_dtype ( image , tf . float32 ) image /= <number> return image dataset = dataset . map ( lambda image_path , label : ( preprocess_image ( image_path ) , label ) ) dataset = dataset . batch ( batch_size = <number> ) m . compile ( loss = ' categorical_crossentropy ' , optimizer = ' rmsprop ' , metrics =[ ' accuracy ' ] ) loss , accuracy = m . evaluate ( dataset ) print ( ' loss : ' , loss ) print ( ' accuracy : ' , accuracy ) ` ` ` i get the below error here : > traceback ( most recent call last ) : > file "" test1234 . py "" , line <number> , in <module> > loss , accuracy = m . evaluate ( dataset ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py "" , line <number> , in error_handler > raise e . with_traceback ( filtered_tb ) from none > file "" / tmp / __autograph_generated_filemqiwcebs . py "" , line <number> , in tf__test_function > retval_ = ag__ . converted_call ( ag__ . ld ( step_function ) , ( ag__ . ld ( self ) , ag__ . ld ( iterator ) ) , none , fscope ) > valueerror : in user code : > > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in test_function * > return step_function ( self , iterator ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in step_function * * > outputs = model . distribute_strategy . run ( run_step , args =( data , ) ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in run_step * * > outputs = model . test_step ( data ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in test_step > self . compute_loss ( x , y , y_pred , sample_weight ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in compute_loss > return self . compiled_loss ( > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / engine / compile_utils . py "" , line <number> , in __call__ > loss_value = loss_obj ( y_t , y_p , sample_weight = sw ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / losses . py "" , line <number> , in __call__ > losses = call_fn ( y_true , y_pred ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / losses . py "" , line <number> , in call * * > return ag_fn ( y_true , y_pred , * * self . _fn_kwargs ) > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / losses . py "" , line <number> , in categorical_crossentropy > return backend . categorical_crossentropy ( > file "" / home / mtk / . local / lib / python3 . <number> / site - packages / keras / backend . py "" , line <number> , in categorical_crossentropy > target . shape . assert_is_compatible_with ( output . shape ) > > valueerror ( none , <number> ) and ( none , <number> ) are incompatible i assume something is wrong in the way i preprocess my data though , but not sure how to go about it . some insights would be nice thanks",2
tensorflow/tensorflow,"docs do not mention discontinuation of support for python <number> in tf <number> . <number> <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version tf <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell the release notes for tensorflow <number> . <number> state that support for python <number> has been discontinued . however , it seems that python <number> support has also been removed from tensorflow <number> . <number> , despite this not being mentioned in the release notes or documentation . as a result , to maintain compatibility , we were forced to limit our application to using tensorflow <number> . <number> . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell na ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"jvp using tf . autodiff . forwardaccumulator becomes none under graph execution i am new to tensorflow . i am trying to compute a jacobian - vector product using tf . autodiff . forwardaccumulator with a train function looks something like the code below . the jvp looks fine under eager execution . however , the jvp becomes a list of nones when activate graph execution using <user> . function . what could be the issue here ? # # # tensorflow version tf <number> . <number> # # # python version <number> # # # train code ` ` ` shell <user> . function def train ( self , data ) : with tf . gradienttape ( ) as upper_tape : loss1 = self . loss1 ( data ) grad1 = upper_tape . gradient ( loss1 , self . net1 . variables ) with tf . autodiff . forwardaccumulator ( primals = self . net1 . variables , tangents = grad1 ) as acc : with tf . gradienttape ( ) as lower_tape = self . loss2 ( data ) grad2 = lower_tape . gradient ( loss2 , self . net2 . variables ) final_grad = acc . jvp ( grad2 ) self . optimizer . apply_gradients ( zip ( final_grad , self . net2 . variables ) ) ` ` ` grad1 and grad2 are correctly computed under both eager mode and graph mode . the only problem is with the jvp .",2
tensorflow/tensorflow,"spam removed please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : - * * python version * * : - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : - * * gpu model and memory * * : - * * exact command to reproduce * * : you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"build tensorflow lite for ios failed ! <repeated> please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . ` bazel build - - config = ios_arm64 - c opt - - cxxopt = - - std =c + + <number> \ \ / / tensorflow / lite / ios : tensorflowlitec_framework ❯ bazel build - - incompatible_run_shell_command_string = false - - verbose_failures - - config = ios_arm64 - c opt / / tensorflow / lite / ios : tensorflowlitecmetal_framework info : options provided by the client : inherited ' common ' options : - - isatty = <number> - - terminal_columns = <number> info : reading rc options for ' build ' from / users / thao / desktop / tensorflow / . bazelrc : inherited ' common ' options : - - experimental_repo_remote_exec info : reading rc options for ' build ' from / users / thao / desktop / tensorflow / . bazelrc : ' build ' options : - - define framework_shared_object = true - - define tsl_protobuf_header_only = true - - define = use_fast_cpp_protos = true - - define = allow_oversize_protos = true - - spawn_strategy = standalone - c opt - - announce_rc - - define = grpc_no_ares = true - - noincompatible_remove_legacy_whole_archive - - enable_platform_specific_config - - define = with_xla_support = true - - config = short_logs - - config = v2 - - define = no_aws_support = true - - define = no_hdfs_support = true - - experimental_cc_shared_library - - experimental_link_static_libraries_once = false info : reading rc options for ' build ' from / users / thao / desktop / tensorflow / . tf_configure . bazelrc : ' build ' options : - - action_env python_bin_path <annoyed> users / thao / miniforge3 / bin / python - - action_env python_lib_path <annoyed> users / thao / miniforge3 / lib / python3 . <number> / site - packages - - python_path <annoyed> users / thao / miniforge3 / bin / python info : reading rc options for ' build ' from / users / thao / desktop / tensorflow / . bazelrc : ' build ' options : - - deleted_packages = tensorflow / compiler / mlir / tfrt , tensorflow / compiler / mlir / tfrt / benchmarks , tensorflow / compiler / mlir / tfrt / jit / python_binding , tensorflow / compiler / mlir / tfrt / jit / transforms , tensorflow / compiler / mlir / tfrt / python_tests , tensorflow / compiler / mlir / tfrt / tests , tensorflow / compiler / mlir / tfrt / tests / ir , tensorflow / compiler / mlir / tfrt / tests / analysis , tensorflow / compiler / mlir / tfrt / tests / jit , tensorflow / compiler / mlir / tfrt / tests / lhlo_to_tfrt , tensorflow / compiler / mlir / tfrt / tests / lhlo_to_jitrt , tensorflow / compiler / mlir / tfrt / tests / tf_to_corert , tensorflow / compiler / mlir / tfrt / tests / tf_to_tfrt_data , tensorflow / compiler / mlir / tfrt / tests / saved_model , tensorflow / compiler / mlir / tfrt / transforms / lhlo_gpu_to_tfrt_gpu , tensorflow / core / runtime_fallback , tensorflow / core / runtime_fallback / conversion , tensorflow / core / runtime_fallback / kernel , tensorflow / core / runtime_fallback / opdefs , tensorflow / core / runtime_fallback / runtime , tensorflow / core / runtime_fallback / util , tensorflow / core / tfrt / common , tensorflow / core / tfrt / eager , tensorflow / core / tfrt / eager / backends / cpu , tensorflow / core / tfrt / eager / backends / gpu , tensorflow / core / tfrt / eager / core_runtime , tensorflow / core / tfrt / eager / cpp_tests / core_runtime , tensorflow / core / tfrt / gpu , tensorflow / core / tfrt / run_handler_thread_pool , tensorflow / core / tfrt / runtime , tensorflow / core / tfrt / saved_model , tensorflow / core / tfrt / graph_executor , tensorflow / core / tfrt / saved_model / tests , tensorflow / core / tfrt / tpu , tensorflow / core / tfrt / utils info : found applicable config definition build : short_logs in file / users / thao / desktop / tensorflow / . bazelrc : - - output_filter = dont_match_anything info : found applicable config definition build : v2 in file / users / thao / desktop / tensorflow / . bazelrc : - - define = tf_api_version = <number> - - action_env = tf2_behavior = <number> info : found applicable config definition build : ios_arm64 in file / users / thao / desktop / tensorflow / . bazelrc : - - config = ios - - cpu = ios_arm64 info : found applicable config definition build : ios in file / users / thao / desktop / tensorflow / . bazelrc : - - apple_platform_type = ios - - apple_bitcode = embedded - - copt = - fembed - bitcode - - copt = - wno - c + + <number> - narrowing - - noenable_platform_specific_config - - copt = - w - - cxxopt = - std =c + + <number> - - host_cxxopt = - std =c + + <number> - - define = with_xla_support = false info : build option - - cxxopt has changed , discarding analysis cache . error : / private / var / tmp / _bazel_thao / 2 6 d40dc75f2c247e7283b353a9ab184f / external / local_config_cc / build : <number> <time> : in cc_toolchain_suite rule <user> / / : toolchain : cc_toolchain_suite ' <user> / / : toolchain ' does not contain a toolchain for cpu ' ios_arm64 ' error : / private / var / tmp / _bazel_thao / 2 6 d40dc75f2c247e7283b353a9ab184f / external / local_config_cc / build : <number> <time> : analysis of target ' <user> / / : toolchain ' failed error : analysis of target ' / / tensorflow / lite / ios : tensorflowlitecmetal_framework ' failed ; build aborted : info : elapsed time : <number> . 4 5 5 s info : <number> processes . failed : build did not complete successfully ( <number> packages loaded , <number> targets configured ) ` * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information macos - m1max : <number> tensorflow : <number> . <number> python # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"thread - safety for ` tensorflow : : tensor ` <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution debian gnu / linux <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version llvm <number> # # # cuda / cudnn version <number> # # # gpu model and memory t4 # # # current behaviour ? ` ` ` shell are there any methods on ` tensorflow : : tensor ` objects that are thread - safe ? specifically , any of subslice ( ) flat ( ) , unaligned_flat ( ) , shaped ( ) . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell std : : vector <tensor> output ; mutex mu ; bool ready = false ; void eval ( ) { output = { tensor ( datatype : : dt_half , { <number> , <number> }); / / one entry per thread nn_evaluator_ . infer ( <some_input> , <some_names> , & output ) ; mu . lock ( ); ready = true ; mu . unlock ( ); } void readresult ( int thread_id ) { mu . lock ( ); mu . await ( condition ( & ready ) ); mu . unlock ( ); / / can we read tensor result now ? auto res = output [ <number> ] . subslice ( thread_id ) . unaligned_flat < eigen : : half > (); return res ( <number> ); } ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"if we have any detail information of each ci in tensorflow github ? <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hello , i have been studying the actions ( ci ) of various open source projects on github recently . i noticed that tensorflowhas a well - established ci , so i would like to further understand its composition and structure . do you have any relevant materials that i can study and refer to ? thank you very much . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell none ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"plot training and validation losses of object detection model hello . i am using the following notebook to train my dataset with * * efficientdet - lite0 * * model . <url> i can see for each epoch we get the information of training loss "" loss "" and validation loss "" val_loss "" . i would like to plot them like : loss = model . history [ ' loss ' ] val_loss = model . history [ ' val_loss ' ] plt . plot ( loss , label = ' training loss ' ) plt . plot ( val_loss , label = ' validation loss ' ) plt . legend ( loc = ' upper right ' ) plt . ylabel ( ' cross entropy ' ) plt . ylim ( [ <number> . <number> ] ) plt . title ( ' training and validation loss ' ) plt . xlabel ( ' epoch ' ) plt . show ( ) and unfortunately getting the following error * * attributeerror object has no attribute ' history ' * * can you help me to figure out how to plot training and validation losses on the same graph . thanks in advance",2
tensorflow/tensorflow,"type int32 ( <number> ) not supported . node add ( number <number> ) failed to invoke with status <number> . node while ( number <number> ) failed to invoke with status <number> * * system information * * - os platform : window10 - tensorflow installed from ( source or binary ) : pip - tensorflow version ( or github sha if from source ) : <number> . <number> - board : arduino nano <number> ble sense i tried to run tensorflow lite for microcontrollers with arduino nano <number> ble sense , the model is my custom lstm . and the result int32 ( <number> ) not supported . node add ( number <number> ) failed to invoke with status <number> . node while ( number <number> ) failed to invoke with status <number> . invoke failed . but it has output values . [ model1 tflite ] ( <url>",2
tensorflow/tensorflow,"how to find accuracy of a pretrained tflite model # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : no - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : - ubuntu <number> - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : none - * * tensorflow installed from ( source or binary ) * * : binary - * * tensorflow version ( use command below ) * * : v2 . <number> - * * python version * * # # # describe the problem i have a pretrained tf mobilenetv2 model downloaded from tfhub which i converted to tflite using tflite interpreter converter . the original model was trained on imagenet dataset , but i want to find out the accuracy of the converted tflite model . how do i do that ? as of now , the eval function is specific to models generated from modelmaker . what is the best way to find the model accuracy ? thanks",2
tensorflow/tensorflow,"how to benchmark tflite object detection model it seems that only classification models can be benchmarked . there is no explanation for how to run benchmarking on ` tflite ` object detection models under <url> could anybody point me into the right direction to get the modifications needed to run the ` tflite ` benchmark on my custom ` tflite ` object detection model ? after running : ` ` ` bash ➜ tensorflow git <sad> master ) adb shell am start - s \ \ - n org . tensorflow . lite . benchmark / . benchmarkmodelactivity \ \ - - es args ' "" - - graph <annoyed> data / local / tmp / my_custom_object_detection_float32 . tflite \ \ - - num_threads = <number> "" ' ` ` ` when i check logcat : ` ` ` bash ➜ tensorflow git <sad> master ) adb logcat | grep "" inference timings in us "" ` ` ` i see : ` ` ` shell <number> - <number> <time> . <number> <number> <number> i tflite : inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> . 5 3 0 2 5 e + <number> , inference ( avg ) : <number> . 4 9 7 1 2 e + <number> <number> - <number> <time> . <number> <number> <number> i tflite : inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> , inference ( avg ) : <number> ` ` ` basically , my model inference fails and it falls back onto the classification model <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? # # # standalone code to reproduce the issue ` ` ` shell the app falls back onto the classification model once the object detection one fails : <number> - <number> <time> . <number> <number> <number> i tflite : inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> . 5 3 0 2 5 e + <number> , inference ( avg ) : <number> . 4 9 7 1 2 e + <number> <number> - <number> <time> . <number> <number> <number> i tflite : inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> , inference ( avg ) ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"typeerror : variablemetaclass . _variable_v1_call ( ) got an unexpected keyword argument ' experimental_enable_variable_lifting ' <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version v2 . <number> - rc0 - <number> - g0d8efc960d2 <number> . <number> - rc1 # # # custom code no # # # os platform and distribution windows <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell self . model = keras . sequential ( [ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ file "" c :\\ python311 \ \ lib \ \ site - packages \ \ tensorflow \ \ python \ \ trackable \ \ base . py "" , line <number> , in _method_wrapper result = method ( self , * args , * * kwargs ) ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^ file "" c :\\ python311 \ \ lib \ \ site - packages \ \ keras \ \ utils \ \ traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" c :\\ python311 \ \ lib \ \ site - packages \ \ tensorflow \ \ python \ \ ops \ \ variables . py "" , line <number> , in __call__ return cls . _variable_v1_call ( * args , * * kwargs ) ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^ typeerror got an unexpected keyword argument ' experimental_enable_variable_lifting ' ` ` ` # # # standalone code to reproduce the issue ` ` ` shell self . model = keras . sequential ( [ keras . layers . dense ( <number> , input_dim = self . degree ) , keras . layers . dense ( <number> ) ] ) self . model . compile ( optimizer = optimizer , loss = loss ) self . model . summary ( ) ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"resource exhausted error during training resource exhausted error during training i would like to train the deep learning task semantic segmentation . specifications with <number> million parameters image size ( minimum 1 0 2 4 x1024 ) , batch size <number> framework - tensorflow keras <number> total image samples <number> , label mask samples <number> does colab pro plus allow the training for this specification with out oom error ? issue currently facing is even with <number> number of images and 5 1 2 x512 size for batch size <number> results in out of memory error in colab pro subscription .",2
tensorflow/tensorflow,"` unsupported object type numpy . ndarray ` on multi - input ` dataset ` <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution kaggle kernel # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i am trying to return tuple to a dataset to be used in a multi - input model , but it ' s throwing an error saying : ` unsupported object type numpy . ndarray ` . previosuly i was setting shape , with ` set_shape ` , but right now i simply do not know how to set shape for a different input shape . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell def read_image ( path , rel ) : # blah blah blah , read somehow return image def read_image1 ( path1 , filter0 ) : # blah blah blah , read somehow return image def preprocess ( x , y ) : def func ( x , y ) : x = json . loads ( x ) x_img1 = read_image ( x [ ' path ' ] , x[ ' rel ' ] ) # 3 d image x_img2 = read_image ( x [ ' path - fork ' ] , x[ ' filter ' ] ) <hashtag> 2 d </hashtag> image with different shape # image resizing will lose data y = tf . keras . utils . to_categorical ( y , num_classes = len ( set ( df [ ' label ' ] . values ) ) ) # todo : yeah , i will optimize it never return (x _img1 , x_img2 ) , y _x , _y = tf . numpy_function ( func , [x , y ] , [ tf . float32 , tf . float32 ] ) # _x . set_shape ( [ <number> , <number> , <number> ] ) < - - - previously i used to do this _y . set_shape ( [ <number> ] ) return _x , _y # here ` x ` is an array of string , and those strings are actually json / dictionary def tf_dataset ( x , y , batch = <number> <sad> dataset = tf . data . dataset . from_tensor_slices ( (x , y ) ) dataset = dataset . shuffle ( buffer_size = <number> ) dataset = dataset . map ( preprocess ) dataset = dataset . batch ( batch ) dataset = dataset . prefetch ( <number> ) return dataset ` ` ` # # # relevant log output ` ` ` shell internalerror : graph execution error : unsupported object type numpy . ndarray [ [ { { node pyfunc } } ] ] [ [ iteratorgetnext ] ] [ op : __inference_train_function_30745 ] ` ` ` here is what i am trying to do input model with tf . data . dataset ] ( <url> </details>",2
tensorflow/tensorflow,"failing to run hexagon delegates on android <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> # # # mobile device qcs6125 # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell problem with hexagon delegates , the hexagon libs from hexagon delegates page are set into arm64 - v8a folder . > cpu architecture : <number> > adb shell getprop ro . board . platform trinket > adb shell getprop ro . product . device trinket ` ` ` # # # standalone code to reproduce the issue ` ` ` shell implementation ' org . tensorflow : tensorflow - lite : <number> . <number> ' implementation ' org . tensorflow : tensorflow - lite - hexagon : <number> . <number> ' running tensorflow example from quickstart for android . ` ` ` # # # relevant log output ` ` ` shell / tflite to fetch hexagon nn version . this might be because you are using incompatible versions of libhexagon_interface and libhexagon_nn_skel . you must use compatible versions . refer to tensorflow lite hexagon delegate guide . ` ` ` </details>",2
tensorflow/tensorflow,"tensorflow import error <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i cannot import the tensorflow package in my jupyter notebook . i have checked the installations in conda and pip , but everything seems to work . so whenever i try to import it i get the following error : "" importerror : traceback ( most recent call last ) : file "" c :\\ users \ \ rury \ \ anaconda3 \ \ lib \ \ site - packages \ \ tensorflow \ \ python \ \ pywrap_tensorflow . py "" , line <number> , in <module> from tensorflow . python . _pywrap_tensorflow_internal import * importerror : dll load failed while importing _pywrap_tensorflow_internal dynamic link library ( dll ) initialization routine failed . "" is it a problem of the environment or a problem with package itself ? i am starting now to study dl so i would appreciate a lot a quick response . thank you ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"running tensorflow distributed on multiple workers <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution linux hpc # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> . <number> # # # gpu model and memory <number> rtx <number> ti workers each having <number> gpus # # # current behaviour ? ` ` ` shell i am trying to run tensorflow distributed training code on multiple worker nodes . i initially tried it using the mirrored strategy using a single worker with multiple gpus and the code was working fine and the training process was getting distributed among multiple gpus . when i have tried it with multiple worker nodes , the training process is actually executing seperately on different workers rather than the load getting distributed among the workers . my linux systems has <number> worker nodes with each node having <number> 2 0 8 0 ti gpus and the gpus are connected through pcie system . i also not sure how to configure the tf_config . can anyone help me on this ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell if __name__ = = "" __main__ "" : # get mpi rank from getonehot import getonehot from mpi4py import mpi comm = mpi . comm_world rank = comm . get_rank ( ) # load in the parameter files from json import load as loadf with open ( "" params . json "" , ' r ' ) as infile : params = loadf ( infile ) # get data files and prep them for the generator import tensorflow from tensorflow import distribute as d callbacks = [ ] devices = getdevices ( ) print ( devices ) set_tf_config_mpi ( ) strat = d . experimental . multiworkermirroredstrategy ( communication = d . experimental . collectivecommunication . nccl ) # create network from sys import argv resume_training = false print ( argv ) if "" resume_latest "" in argv : resume_training = true with strat . scope ( <sad> # scheduler if isinstance ( params [ "" learning_rate "" ] , str ) : # get the string for the importable function lr = params [ "" learning_rate "" ] from tensorflow . keras . callbacks import learningratescheduler # use a dummy learning rate params [ "" learning_rate "" ] = <number> # model = create_model ( * * params ) # get the importable function lr = lr . split ( "" . "" ) baseimport = __import__ ( lr [ <number> ] , globals ( ) , locals ( ) , [ lr [ <number> ] ] , <number> ) lr = getattr ( baseimport , lr [ <number> ] ) # make a schedule lr = learningratescheduler ( lr ) callbacks . append ( lr ) # resume model ? model_name = none if resume_training : initial_epoch , model_name = getinitialepochsandmodelname ( rank ) if model_name is none : initial_epoch = <number> model = create_model ( * * params ) resume_training = false else : from tensorflow . keras . models import load_model model = load_model ( model_name ) # load data from disk import numpy if "" root "" in params . keys ( <sad> root = params [ ' root ' ] else : root = "" . / "" if "" filename "" in params . keys ( <sad> filename = params [ "" filename "" ] else : filename = "" 1 5 0 mev_all_shuffled_normed . csv "" restricted = [ ' euc1 ' , ' e1 ' , ' x1 ' , ' y1 ' , ' z1 ' , ' euc2 ' , ' e2 ' , ' x2 ' , ' y2 ' , ' z2 ' , ' euc3 ' , ' e3 ' , ' x3 ' , ' y3 ' , ' z3 ' , ] x , y = getonehot ( "" { } / { } "" . format ( root , filename ) , restricted = restricted , * * params ) # val_filename = "" 1 5 0 mev_180kmumin - stdcc_stitched_triples_dtot_trip_only . csv "" # val_x , val_y = getonehot ( "" { } / { } "" . format ( root , val_filename ) , restricted = restricted ) val_x , val_y = none , none params [ "" gbatch_size "" ] = params [ ' batch_size ' ] * len ( devices ) print ( "" x . shape ="", x . shape ) print ( "" y . shape ="", y . shape ) print ( "" epochs ="", params [ ' epochs ' ] , type ( params [ ' epochs ' ] ) ) print ( "" batch ="", params [ ' batch_size ' ] , type ( params [ ' batch_size ' ] ) ) print ( "" gbatch ="", params [ ' gbatch_size ' ] , type ( params [ ' gbatch_size ' ] ) ) # load data into a distributed dataset # dataset object does nothing in place : # <url> from tensorflow . data import dataset data = dataset . from_tensor_slices ( (x , y ) ) # create validation set v = params [ ' validation ' ] if val_x is not none : vrecord = val_x . shape [ <number> ] val = dataset . from_tensor_slices ( ( val_x , val_y ) ) validation = val # data . take ( vrecord ) else : vrecord = int ( x . shape [ <number> ] * v ) validation = data . take ( vrecord ) validation = validation . batch ( params [ ' gbatch_size ' ] ) validation = validation . repeat ( params [ ' epochs ' ] ) # validation - - need to do kfold one day # this set should not be distributed vsteps = vrecord / / params [ ' gbatch_size ' ] if vrecord % params [ ' gbatch_size ' ] ! = <number> : vsteps + = <number> data = data . batch ( params [ ' gbatch_size ' ] ) data = data . repeat ( params [ ' epochs ' ] ) records = x . shape [ <number> ] # - vrecord steps = records / / params [ ' gbatch_size ' ] if records % params [ ' gbatch_size ' <sad> steps + = <number> print ( "" steps ="", steps ) # note that if we are resuming that the number of _remaining_ epochs has # changed ! # the number of epochs * steps is the numbers of samples to drop print ( "" initial cardinality = "" , data . cardinality ( ) ) print ( "" initial v cardinality = "" , data . cardinality ( ) ) data = data . skip ( initial_epoch*steps <censored> ) validation = validation . skip ( initial_epoch*vsteps <censored> ) print ( "" final cardinality = "" , data . cardinality ( ) ) print ( "" final v cardinality = "" , data . cardinality ( ) ) # data = strat . experimental_distribute_dataset ( data ) # split into validation and training callbacks = createcallbacks ( params , callbacks , rank , resume_training ) print ( callbacks ) print ( "" fitting model "" ) print ( data ) import tensorflow as tf options = tf . data . options ( ) options . experimental_distribute . auto_shard_policy = tf . data . experimental . autoshardpolicy . off train_data = data . with_options ( options ) val_data = validation . with_options ( options ) history = model . fit ( train_data , epochs = params [ ' epochs ' ] , batch_size = params [ ' gbatch_size ' ] , steps_per_epoch = steps , verbose = <number> , initial_epoch = initial_epoch , validation_data = val_data , validation_steps = vsteps , callbacks = callbacks ) print ( "" fitting model done "" ) if rank = = <number> : model . save ( "" model - final "" ) else : model . save ( "" checkpoints / model - tmp "" ) # # # # # # # # # # # # # # # slurm script <hashtag> s batch </hashtag> - - job - name = job1 # job name <hashtag> s batch </hashtag> - - mem = <number> # job memory request <hashtag> s batch </hashtag> - - gres = gpu : <number> # number of requested gpu ( s ) <hashtag> s batch </hashtag> - - time = <number> - <time> # time limit days - hrs : min : sec <hashtag> s batch </hashtag> - - constraint = rtx_2080 # specific hardware constraint <hashtag> s batch </hashtag> - - error = slurm . err # error file name <hashtag> s batch </hashtag> - - output = slurm . out # output file name <hashtag> s batch </hashtag> - - nodes = <number> <hashtag> s batch </hashtag> - - ntasks - per - node = <number> <hashtag> s batch </hashtag> - - cpus - per - task = <number> <hashtag> s batch </hashtag> - - array = <number> - <percent> <number> if [ - d "" model - final "" ] then scancel $ slurm_array_job_id else module load anaconda3 / <number> module load tensorflow / <number> . <number> - foss - 2 0 2 1 a - cuda - <number> . <number> mpirun python - u main . py resume_latest fi ` ` ` # # # relevant log output ` ` ` shell i log each and every epoch in a csv file . i see two csv files created and each one has different workers running . ` ` ` </details>",2
tensorflow/tensorflow,tensorflow grad <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf <number> . x # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hi i want to slimming network ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i want to ask how i can get the gradient value of each iteration in the model training process . ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"tensorflow static library of windows : missing few files please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : its tensorflow provided code as given in <url> - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : windows <number> - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : laptop - * * tensorflow installed from ( source or binary ) * * : static library from <url> - * * tensorflow version ( use command below ) * * : <number> - * * python version * * : na - * * bazel version ( if compiling from source ) * * : na - * * gcc / compiler version ( if compiling from source ) * * : <number> - * * cuda / cudnn version * * : na - * * gpu model and memory * * : na - * * exact command to reproduce * * : took static library from tensorflow ebsite from link below : <url> have taken windows version of <url> ( cpu version ) unzipped and created hello_tf . c as mentioned in same webpage . compiled the hello_tf . c as below : gcc hello_tf . c - i d :\\ tensorflow_lib \ \ include - ld :\\ tensorflow_lib \ \ lib \ \ tensorflow - o hello_tf got error as _in file included from hello_tf . c : <number> : d :\\ tensorflow_lib \ \ include / tensorflow / c / c_api . h : <time> : fatal error : tensorflow / c / tf_buffer . h : no such file or directory <number> | <hashtag> include </hashtag> "" tensorflow / c / tf_buffer . h "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . _ then took tf_buffer . h from github repo , to see if the error is only dependent on tf_buffer . h , but then got another error : _d :\\ tensorflow_lib > gcc hello_tf . c - i d :\\ tensorflow_lib \ \ include - ld :\\ tensorflow_lib \ \ lib \ \ tensorflow - o hello_tf in file included from d :\\ tensorflow_lib \ \ include / tensorflow / c / tf_tstring . h : <number> , from d :\\ tensorflow_lib \ \ include / tensorflow / c / c_api . h : <number> , from hello_tf . c : <number> : d :\\ tensorflow_lib \ \ include / tensorflow / core / platform / ctstring . h : <time> : fatal error : tensorflow / tsl / platform / ctstring . h : no such file or directory <number> | <hashtag> include </hashtag> "" tensorflow / tsl / platform / ctstring . h "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . _ you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with : ` ` ` bash python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . took static library from tensorflow ebsite from link below : <url> have taken windows version of <url> ( cpu version ) unzipped and created hello_tf . c as mentioned in same webpage . compiled the hello_tf . c as below : gcc hello_tf . c - i d :\\ tensorflow_lib \ \ include - ld :\\ tensorflow_lib \ \ lib \ \ tensorflow - o hello_tf got error as _in file included from hello_tf . c : <number> : d :\\ tensorflow_lib \ \ include / tensorflow / c / c_api . h : <time> : fatal error : tensorflow / c / tf_buffer . h : no such file or directory <number> | <hashtag> include </hashtag> "" tensorflow / c / tf_buffer . h "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . _ then took tf_buffer . h from github repo , to see if the error is only dependent on tf_buffer . h , but then got another error : _d :\\ tensorflow_lib > gcc hello_tf . c - i d :\\ tensorflow_lib \ \ include - ld :\\ tensorflow_lib \ \ lib \ \ tensorflow - o hello_tf in file included from d :\\ tensorflow_lib \ \ include / tensorflow / c / tf_tstring . h : <number> , from d :\\ tensorflow_lib \ \ include / tensorflow / c / c_api . h : <number> , from hello_tf . c : <number> : d :\\ tensorflow_lib \ \ include / tensorflow / core / platform / ctstring . h : <time> : fatal error : tensorflow / tsl / platform / ctstring . h such file or directory <number> | <hashtag> include </hashtag> "" tensorflow / tsl / platform / ctstring . h "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . _ # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"what is the final training result of asynchronous synchronous parallel distributed training ？ <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell when distributed training adopts asynchronous synchronous parallel , the parameters updated by each worker are inconsistent . is the final result of distributed training the slowest parameter updated by workers ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell when distributed training adopts asynchronous synchronous parallel , the parameters updated by each worker are inconsistent . is the final result of distributed training the slowest parameter updated by workers ? ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"warning : absl : found untraced functions such as _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op while saving ( showing <number> of <number> ) . these functions will not be directly callable after loading . <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell the warning appears : warning : absl : found untraced functions such as _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op while saving ( showing <number> of <number> ) . these functions will not be directly callable after loading . ` ` ` can you please , explain , what is meant by this warning and how to fix it ? i have the following accompanying questions to it : * function traces are basically graph representations of functions or something else ? * if these jit traces are not found in a loaded model , will they be generated again ? is model compilation needed for regeneration ? * are these functions needed only for training or also for prediction / evaluation ? i have found in keras docs ( <url> that i can disable saving the traces with the following note : "" disabling this will decrease serialization time and reduce file size , but it requires that all custom layers / models implement a get_config ( ) method . "" * why would the ` get_config ( ) ` method be only needed when i disable saving the traces ? thanks in advance for any explanations given ! ` ` ` shell # # # reproducer it ' s not my code but the warnings appear e . g . here ` ` ` ` ` ` # # # relevant log output warning : absl : found untraced functions such as _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op while saving ( showing <number> of <number> ) . these functions will not be directly callable after loading . ` ` ` </details>",2
tensorflow/tensorflow,"how to use exported automl tensorflow model for tabular data in python <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution linux ubuntu # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hello , i followed the instructions from google cloud to export and load a trainen automl model ( training was done with data from bigquery ) : <url> as we want to integrate this model in our spark flow i need to load it and make predictions on it in python directly . i tested the model with the tensorflow serving docker image as described in the documentation . this works perfectly fine . however , when i try to use the model directly i get an erro "" error : data_loss : failed skipping unrequested field "" . input must be a string tensor . i tried several formats ( json , csv ) but none of them seemed to be correct . can anybody give me a hint on how to convert the tabular data into a string tensor in a way that the model accepts the data and returns a prediction ? thanks in advance ! <repeated> kay ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import struct2tensor import tensorflow as tf import json import requests model_path = ' / home / kay / tmp / models / ml_cr_2022_12 ' predict_sample = { ' animals ' : ' <number> ' , ' apparel ' : ' <number> ' , ' arts ' : ' <number> ' , ' baby ' : ' <number> ' , ' business ' : ' <number> ' , ' cameras ' : ' <number> ' , ' electronics ' : ' <number> ' , ' food ' : ' <number> . 4 7 4 9 0 5 4 1 3 0 6 1 5 e - <number> ' , ' furniture ' : ' <number> ' , ' hardware ' : ' <number> ' , ' health ' : ' <number> ' , ' home ' : ' <number> ' , ' luggage ' : ' <number> . 6 5 9 8 0 4 4 8 8 3 3 7 9 6 e - <number> ' , ' mature ' : ' <number> ' , ' media ' : ' <number> ' , ' office ' : ' <number> ' , ' religious ' : ' <number> ' , ' software ' : ' <number> ' , ' sporting ' : ' <number> ' , ' toys ' : ' <number> ' , ' vehicles ' : ' <number> . 2 5 5 2 4 3 4 8 2 0 2 5 7 1 e - <number> ' , ' aov_bench ' : ' <number> ' , ' aov_rep ' : ' <number> ' , ' avgprice ' : ' <number> ' , ' bouncerate ' : ' <number> ' , ' brand_search_volume ' : ' <number> ' , ' brand_search_volume_share ' : ' <number> ' , ' category ' : ' adult ' , ' competition ' : ' <number> ' , ' cpc ' : ' <number> ' , ' cr_bench ' : ' <number> ' , ' date ' : ' <number> - <number> - <number> ' , ' directshare ' : ' <number> ' , ' displayshare ' : ' <number> ' , ' domain_cat_1 ' : ' <number> ' , ' domain_cat_2 ' : ' <number> ' , ' domain_search_volum_share ' : ' <number> ' , ' domain_search_volume ' : ' <number> ' , ' ekps_aov ' : ' <number> ' , ' ekps_cr ' : ' <number> ' , ' iib_bench ' : ' <number> ' , ' int64_field_0 ' : ' <number> ' , ' lineid ' : ' <number> ' , ' mailshare ' : ' <number> ' , ' maincountry ' : ' germany ' , ' maincountryshare ' : ' <number> ' , ' no_cat ' : ' <number> ' , ' organicshare ' : ' <number> ' , ' pageviews ' : ' <number> ' , ' paidsearchshare ' : ' <number> ' } # request to tensorflow_serving docker container , works perfectly response = requests . post ( ' http :// localhost : <number> / predict ' , json ={ ' instances ' : [ predict_sample ] } ) # prints { "" predictions "" : <number> } print ( response . text ) model = tf . saved_model . load ( model_path ) infer = model . signatures [ "" serving_default "" ] # no matter what i tried as input here , i always get error "" error consuming . error : data_loss skipping unrequested field "" print ( infer ( tf . constant ( ' how to feed model with one string tensor ? <repeated> ' ) ) ) ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"tflite minimal example failing on latest tensorflow repo when i run latest tensorflow lite example minimal and it is failing on linux machine with the below error followed steps mentioned here and ran on x86_64 gnu / linux <url> , , , <percent> ] linking cxx executable minimal / usr / bin / ld : tensorflow - lite / libtensorflow - lite . a ( register . cc . o ) : in function ` tflite : : ops : : builtin : : builtinopresolver : : builtinopresolver ( <sad> register . cc <sad> . text + 0x 1 cd ) : undefined reference to ` tflite : : ops : : builtin : : register_abs ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 ed ) : undefined reference to ` tflite : : ops : : builtin : : register_hard_swish ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 0 7 ) : undefined reference to ` tflite : : ops : : builtin : : register_relu ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 2 7 ) : undefined reference to ` tflite : : ops : : builtin : : register_relu_n1_to_1 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 4 1 ) : undefined reference to ` tflite : : ops : : builtin : : register_relu_0_to_1 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 5 b ) : undefined reference to ` tflite : : ops : : builtin : : register_relu6 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 7 b ) : undefined reference to ` tflite : : ops : : builtin : : register_tanh ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 9 b ) : undefined reference to ` tflite : : ops : : builtin : : register_logistic ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 bb ) : undefined reference to ` tflite : : ops : : builtin : : register_average_pool_2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 db ) : undefined reference to ` tflite : : ops : : builtin : : register_max_pool_2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 2 fb ) : undefined reference to ` tflite : : ops : : builtin : : register_l2_pool_2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 3 1 5 ) : undefined reference to ` tflite : : ops : : builtin : : register_conv_2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 3 3 5 ) : undefined reference to ` tflite : : ops : : builtin : : register_depthwise_conv_2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 3 5 5 ) : undefined reference to ` tflite : : ops : : builtin : : register_svdf ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 3 7 5 ) : undefined reference to ` tflite : : ops : : builtin : : register_rnn ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 3 9 5 ) : undefined reference to ` tflite : : ops : : builtin : : register_bidirectional_sequence_rnn ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x3 b5 ) : undefined reference to ` tflite : : ops : : builtin : : register_unidirectional_sequence_rnn ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x3 d5 ) : undefined reference to ` tflite : : ops : : builtin : : register_embedding_lookup ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x3 f5 ) : undefined reference to ` tflite : : ops : : builtin : : register_embedding_lookup_sparse ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 0 f ) : undefined reference to ` tflite : : ops : : builtin : : register_fully_connected ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 2 f ) : undefined reference to ` tflite : : ops : : builtin : : register_lsh_projection ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 4 9 ) : undefined reference to ` tflite : : ops : : builtin : : register_hashtable_lookup ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 6 3 ) : undefined reference to ` tflite : : ops : : builtin : : register_softmax ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 8 3 ) : undefined reference to ` tflite : : ops : : builtin : : register_concatenation ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 a3 ) : undefined reference to ` tflite : : ops : : builtin : : register_add ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 c0 ) : undefined reference to ` tflite : : ops : : builtin : : register_space_to_batch_nd ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 4 e0 ) : undefined reference to ` tflite : : ops : : builtin : : register_batch_to_space_nd ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 0 0 ) : undefined reference to ` tflite : : ops : : builtin : : register_mul ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 2 0 ) : undefined reference to ` tflite : : ops : : builtin : : register_l2_normalization ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 4 0 ) : undefined reference to ` tflite : : ops : : builtin : : register_local_response_normalization ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 5 a ) : undefined reference to ` tflite : : ops : : builtin : : register_lstm ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 7 a ) : undefined reference to ` tflite : : ops : : builtin : : register_bidirectional_sequence_lstm ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 9 a ) : undefined reference to ` tflite : : ops : : builtin : : register_unidirectional_sequence_lstm ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 ba ) : undefined reference to ` tflite : : ops : : builtin : : register_pad ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 da ) : undefined reference to ` tflite : : ops : : builtin : : register_padv2 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 5 fa ) : undefined reference to ` tflite : : ops : : builtin : : register_reshape ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 1 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_resize_bilinear ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 3 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_resize_nearest_neighbor ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 5 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_skip_gram ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 6 e ) : undefined reference to ` tflite : : ops : : builtin : : register_space_to_depth ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 8 e ) : undefined reference to ` tflite : : ops : : builtin : : register_depth_to_space ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 ae ) : undefined reference to ` tflite : : ops : : builtin : : register_gather ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 ce ) : undefined reference to ` tflite : : ops : : builtin : : register_transpose ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 6 ee ) : undefined reference to ` tflite : : ops : : builtin : : register_mean ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 0 e ) : undefined reference to ` tflite : : ops : : builtin : : register_div ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 2 e ) : undefined reference to ` tflite : : ops : : builtin : : register_sub ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 4 e ) : undefined reference to ` tflite : : ops : : builtin : : register_split ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 6 e ) : undefined reference to ` tflite : : ops : : builtin : : register_split_v ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 8 e ) : undefined reference to ` tflite : : ops : : builtin : : register_squeeze ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 ae ) : undefined reference to ` tflite : : ops : : builtin : : register_strided_slice ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 ce ) : undefined reference to ` tflite : : ops : : builtin : : register_exp ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 7 e8 ) : undefined reference to ` tflite : : ops : : builtin : : register_topk_v2 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 0 8 ) : undefined reference to ` tflite : : ops : : builtin : : register_log ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 2 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_log_softmax ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 4 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_cast ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 6 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_dequantize ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 8 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_prelu ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 9 c ) : undefined reference to ` tflite : : ops : : builtin : : register_maximum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 bc ) : undefined reference to ` tflite : : ops : : builtin : : register_minimum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 dc ) : undefined reference to ` tflite : : ops : : builtin : : register_arg_max ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 8 fc ) : undefined reference to ` tflite : : ops : : builtin : : register_arg_min ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 1 c ) : undefined reference to ` tflite : : ops : : builtin : : register_greater ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 3 c ) : undefined reference to ` tflite : : ops : : builtin : : register_greater_equal ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 5 c ) : undefined reference to ` tflite : : ops : : builtin : : register_less ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 7 c ) : undefined reference to ` tflite : : ops : : builtin : : register_less_equal ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 9 c ) : undefined reference to ` tflite : : ops : : builtin : : register_floor ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 b6 ) : undefined reference to ` tflite : : ops : : builtin : : register_ceil ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 d0 ) : undefined reference to ` tflite : : ops : : builtin : : register_round ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 9 ea ) : undefined reference to ` tflite : : ops : : builtin : : register_neg ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa04 ) : undefined reference to ` tflite : : ops : : builtin : : register_select ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa24 ) : undefined reference to ` tflite : : ops : : builtin : : register_select_v2 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa3e ) : undefined reference to ` tflite : : ops : : builtin : : register_slice ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa5e ) : undefined reference to ` tflite : : ops : : builtin : : register_sin ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa78 ) : undefined reference to ` tflite : : ops : : builtin : : register_cos ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xa92 ) : undefined reference to ` tflite : : ops : : builtin : : register_transpose_conv ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xab2 ) : undefined reference to ` tflite : : ops : : builtin : : register_tile ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xad2 ) : undefined reference to ` tflite : : ops : : builtin : : register_sum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xaf2 ) : undefined reference to ` tflite : : ops : : builtin : : register_reduce_prod ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xb12 ) : undefined reference to ` tflite : : ops : : builtin : : register_reduce_max ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xb32 ) : undefined reference to ` tflite : : ops : : builtin : : register_reduce_min ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xb52 ) : undefined reference to ` tflite : : ops : : builtin : : register_reduce_any ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xb6c ) : undefined reference to ` tflite : : ops : : builtin : : register_reduce_all ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xb86 ) : undefined reference to ` tflite : : ops : : builtin : : register_expand_dims ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xba0 ) : undefined reference to ` tflite : : ops : : builtin : : register_sparse_to_dense ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xbc0 ) : undefined reference to ` tflite : : ops : : builtin : : register_equal ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xbe0 ) : undefined reference to ` tflite : : ops : : builtin : : register_not_equal ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc00 ) : undefined reference to ` tflite : : ops : : builtin : : register_sqrt ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc1a ) : undefined reference to ` tflite : : ops : : builtin : : register_rsqrt ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc3a ) : undefined reference to ` tflite : : ops : : builtin : : register_shape ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc54 ) : undefined reference to ` tflite : : ops : : builtin : : register_rank ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc6e ) : undefined reference to ` tflite : : ops : : builtin : : register_pow ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xc88 ) : undefined reference to ` tflite : : ops : : builtin : : register_fake_quant ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xca8 ) : undefined reference to ` tflite : : ops : : builtin : : register_pack ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xcc8 ) : undefined reference to ` tflite : : ops : : builtin : : register_one_hot ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xce2 ) : undefined reference to ` tflite : : ops : : builtin : : register_logical_or ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xcfc ) : undefined reference to ` tflite : : ops : : builtin : : register_logical_and ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xd16 ) : undefined reference to ` tflite : : ops : : builtin : : register_logical_not ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xd30 ) : undefined reference to ` tflite : : ops : : builtin : : register_unpack ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xd50 ) : undefined reference to ` tflite : : ops : : builtin : : register_floor_div ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xd70 ) : undefined reference to ` tflite : : ops : : builtin : : register_square ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xd8a ) : undefined reference to ` tflite : : ops : : builtin : : register_zeros_like ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xda4 ) : undefined reference to ` tflite : : ops : : builtin : : register_floor_mod ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xdbe ) : undefined reference to ` tflite : : ops : : builtin : : register_range ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xdd8 ) : undefined reference to ` tflite : : ops : : builtin : : register_leaky_relu ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xdf8 ) : undefined reference to ` tflite : : ops : : builtin : : register_squared_difference ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xe18 ) : undefined reference to ` tflite : : ops : : builtin : : register_fill ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xe38 ) : undefined reference to ` tflite : : ops : : builtin : : register_mirror_pad ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xe58 ) : undefined reference to ` tflite : : ops : : builtin : : register_unique ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xe72 ) : undefined reference to ` tflite : : ops : : builtin : : register_reverse_v2 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xe92 ) : undefined reference to ` tflite : : ops : : builtin : : register_add_n ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xeac ) : undefined reference to ` tflite : : ops : : builtin : : register_gather_nd ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xecc ) : undefined reference to ` tflite : : ops : : builtin : : register_where ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xeec ) : undefined reference to ` tflite : : ops : : builtin : : register_elu ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf06 ) : undefined reference to ` tflite : : ops : : builtin : : register_reverse_sequence ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf20 ) : undefined reference to ` tflite : : ops : : builtin : : register_matrix_diag ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf3a ) : undefined reference to ` tflite : : ops : : builtin : : register_quantize ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf5a ) : undefined reference to ` tflite : : ops : : builtin : : register_matrix_set_diag ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf74 ) : undefined reference to ` tflite : : ops : : builtin : : register_if ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xf8e ) : undefined reference to ` tflite : : ops : : builtin : : register_while ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xfa8 ) : undefined reference to ` tflite : : ops : : builtin : : register_non_max_suppression_v4 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xfc2 ) : undefined reference to ` tflite : : ops : : builtin : : register_non_max_suppression_v5 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xfdc ) : undefined reference to ` tflite : : ops : : builtin : : register_scatter_nd ( ) ' / usr / bin / ld : register . cc <sad> . text + 0 xff6 ) : undefined reference to ` tflite : : ops : : builtin : : register_densify ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 1 0 ) : undefined reference to ` tflite : : ops : : builtin : : register_segment_sum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 2 a ) : undefined reference to ` tflite : : ops : : builtin : : register_batch_matmul ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 4 a ) : undefined reference to ` tflite : : ops : : builtin : : register_cumsum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 6 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_broadcast_to ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 8 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_call_once ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 9 e ) : undefined reference to ` tflite : : ops : : builtin : : register_rfft2d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 b8 ) : undefined reference to ` tflite : : ops : : builtin : : register_conv_3d ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 d2 ) : undefined reference to ` tflite : : ops : : builtin : : register_imag ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 0 ec ) : undefined reference to ` tflite : : ops : : builtin : : register_real ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 0 6 ) : undefined reference to ` tflite : : ops : : builtin : : register_complex_abs ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 2 0 ) : undefined reference to ` tflite : : ops : : builtin : : register_broadcast_args ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 3 a ) : undefined reference to ` tflite : : ops : : builtin : : register_hashtable ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 5 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_hashtable_find ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 6 e ) : undefined reference to ` tflite : : ops : : builtin : : register_hashtable_import ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 8 8 ) : undefined reference to ` tflite : : ops : : builtin : : register_hashtable_size ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 a2 ) : undefined reference to ` tflite : : ops : : builtin : : register_conv_3d_transpose ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 bc ) : undefined reference to ` tflite : : ops : : builtin : : register_var_handle ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 d6 ) : undefined reference to ` tflite : : ops : : builtin : : register_read_variable ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 1 f0 ) : undefined reference to ` tflite : : ops : : builtin : : register_assign_variable ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 0 a ) : undefined reference to ` tflite : : ops : : builtin : : register_multinomial ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 2 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_random_standard_normal ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 3 e ) : undefined reference to ` tflite : : ops : : builtin : : register_bucketize ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 5 8 ) : undefined reference to ` tflite : : ops : : builtin : : register_random_uniform ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 7 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_gelu ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 9 2 ) : undefined reference to ` tflite : : ops : : builtin : : register_dynamic_update_slice ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 ac ) : undefined reference to ` tflite : : ops : : builtin : : register_unsorted_segment_prod ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 c6 ) : undefined reference to ` tflite : : ops : : builtin : : register_unsorted_segment_max ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 e0 ) : undefined reference to ` tflite : : ops : : builtin : : register_unsorted_segment_min ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 2 fa ) : undefined reference to ` tflite : : ops : : builtin : : register_unsorted_segment_sum ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 1 4 ) : undefined reference to ` tflite : : ops : : builtin : : register_atan2 ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 2 e ) : undefined reference to ` tflite : : ops : : builtin : : register_sign ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 4 e ) : undefined reference to ` tflite : : ops : : custom : : register_numeric_verify ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 6 a ) : undefined reference to ` tflite : : ops : : custom : : register_mfcc ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 8 6 ) : undefined reference to ` tflite : : ops : : custom : : register_audio_spectrogram ( ) ' / usr / bin / ld : register . cc <sad> . text + 0x 1 3 a2 ) : undefined reference to ` tflite : : ops : : custom : : register_detection_postprocess ( ) ' / usr / bin / ld : tensorflow - lite / libtensorflow - lite . a ( xnnpack_delegate . cc . o ) : in function ` tflitexnnpackdelegatecreatewiththreadpool ' : xnnpack_delegate . cc <sad> . text + 0 xc74f ) : undefined reference to ` tflite : : cpubackendcontext : : getfromcontext ( tflitecontext <wink> ' / usr / bin / ld : xnnpack_delegate . cc <sad> . text + 0 xc757 ) : undefined reference to ` tflite : : cpubackendcontext : : get_xnnpack_threadpool ( ) ' collect2 : error : ld returned <number> exit status gmake [ <number> <sad> * * * [ cmakefiles / minimal . dir / build . make : <number> : minimal ] error <number> gmake [ <number> <sad> * * * [ cmakefiles / makefile <time> <number> : cmakefiles / minimal . dir / all ] error <number> gmake : * * * [ makefile : <number> error <number> , , ,",2
tensorflow/tensorflow,"on mtk platforms , tflite calls the gpu with clgetplatformids return - <number> * * system information * * - android device information ( use ` adb shell getprop ro . build . fingerprint ` if possible ) : redmi / rembrandt / rembrandt : <number> / tp1a . <number> / <date> : user / release - keys - tensorflow lite in play services sdk version ( found in ` build . gradle ` <sad> <number> - google play services version ( ` settings ` > ` apps ` > ` google play services ` > ` app details ` <sad> * * standalone code to reproduce the issue * * on mtk platforms code : if ( interpreter - > modifygraphwithdelegate ( delegate ) = ktfliteok ) { loge ( "" delegate init failed ! "" ); exit ( - <number> ); } get returned - <number> or clgetplatformids returned - <number> * * any other info / logs * * this mtk platform has a gpu device .",2
tensorflow/tensorflow,"how to create ` tf . keras . layers . input ` without batch_size dimension ? <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution centos7 # # # mobile device _no response_ # # # python version <date> # # # bazel version <number> # # # gcc / compiler version <number> # # # cuda / cudnn version <number> # # # gpu model and memory a30 # # # current behaviour ? one input of my model has nothing to do with batch_size , for example , its shape is [ <number> , <number> ] , how to avoid automatically adding <number> dimension when creating ` tf . keras . layers . input ` ? if i manually slice it , the slice operator will be introduced , resulting in a decrease in inference performance . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf x = tf . keras . layers . input ( shape =( <number> ) ) # x . shape : ( none , <number> , <number> ) x = x[ <number> , :, <happy> # x . shape <number> ) ` ` `",2
tensorflow/tensorflow,"graph execution error i am facing an issue that says "" graph execution error "" tried to solve it but could not .",2
tensorflow/tensorflow,how to invoke . h5 model instead of . tflite model for pose classification ? <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell a bug happened ! ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i need to implement pose_classifier . h5 model instead of pose_classifier . tflite . ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"pass additional parameters to <user> . custom_gradient # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # current behaviour ? currently we can define a custom gradient using ` <user> . custom_gradient ` following <url> what i am looking for is to be able to pass to this function additional information ( in my case , the loss of my network ) for example ( it ' s stupid , but it ' s to give an idea ) to take the incoming gradient and multiply it by loss that has generated that gradient",2
tensorflow/tensorflow,"can not convert custom model to tflite with model config and weights file being separate hi , i am trying to convert a custom model to tflite with these steps : ` ` ` <hashtag> creates </hashtag> a custom model that i have previously defined ( resnet50 as a backbone ( pretrained weights ) + transformer encoder + mlp head ) model = create_model ( classes = <number> ) <hashtag> the </hashtag> best weights file obtained after my custom model training model . load_weights ( model_weights_path ) converter = tf . lite . tfliteconverter . from_keras_model ( model ) converter . optimizations = [ tf . lite . optimize . default ] converter . target_spec . supported_ops = [ tf . lite . opsset . tflite_builtins , tf . lite . opsset . select_tf_ops ] tfmodel = converter . convert ( ) with tf . io . gfile . gfile ( tf_lite_path + ' tflite_model ' + "" . tflite "" , ' wb ' ) as f ` ` ` i do not get any output on this , no errors as well . the question is - is it possible to convert my custom model with creating the model ( without saving it ) and then loading the weights the way i have done ? is there any other way how to make my model smaller after training ?",2
tensorflow/tensorflow,error in importing tensor flow <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution windows # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i have installed tensorflow using below command . ! pip install tensorflow ! pip install keras then i tried to import and throwing error attributeerror : module ' numpy ' has no attribute ' typedict ' current numpy version numpy - <number> . <number> numpy - <number> ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i have installed tensorflow using below command . ! pip install tensorflow ! pip install keras then i tried to import and throwing error attributeerror ' numpy ' has no attribute ' typedict ' current numpy version numpy - <number> . <number> numpy - <number> ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"missing fp16 ops when converting from tf to tflite * * system information * * - os platform and distribution ( e . g . , linux ubuntu <number> <sad> ubuntu <number> - tensorflow installed from ( source or binary ) : pip install - tensorflow version ( or github sha if from source ) : <number> * * provide the text output from tflite_convert * * ` ` ` [ full_error . txt ] ( <url> error : failed while converting : ' main ' : some ops are not supported by the native tflite runtime , you can enable tf kernels fallback using tf select . see instructions : <url> tf select ops cast , concatv2 , conv2d , depthwiseconv2dnative , elu , gatherv2 , max , minimum , mul , pad , realdiv , relu , sqrt , sum , transpose ` ` ` * * standalone code to reproduce the issue * * converter = tf . lite . tfliteconverter . from_saved_model ( tf_path ) tflite_model = converter . convert ( )",2
tensorflow/tensorflow,tensorflow data validation - visualize_statistics ( ) - not working on databricks <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i am unable to display the output of tfdv . visualize_statistics ( ) output on databricks where as the same is working on local system . is it because that facets are not supported on databricks ? please provide a solution on how i can use modules of tensorflow data validation library on databricks . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell tfdv . visualize_statistics ( ) <hashtag> link </hashtag> <url> ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"model save issues with custom optimizer - dpkerassgdoptimizer <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell problem a keras sequential model was compiled using dpkerassgd optimizer and saved . but when the saved model is loaded it throws the below error "" unknown optimizer please ensure you are using a ` keras . utils . custom_object_scope ` and that this object is included in the scope . see <url> for details "" expected behaviour since the model was already compiled using the optimizer it should run perfectly fine when fitted with the data . whereas the same works when the optimizer is replaced . please provide a solution on how to save the model with this custom optimizer . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell <hashtag> building </hashtag> the model model = tf . keras . sequential ( [ tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dense ( <number> , activation = ' sigmoid ' ) ] ) <hashtag> compiling </hashtag> the model model . compile ( loss = tf . keras . losses . categoricalcrossentropy ( from_logits = true , reduction = tf . losses . reduction . none ) , optimizer = tensorflow_privacy . dpkerassgdoptimizer ( l2_norm_clip = <number> , noise_multiplier = <number> , learning_rate = <number> ) , metrics =[ tf . keras . metrics . binaryaccuracy ( name = ' accuracy ' ) , tf . keras . metrics . precision ( name = ' precision ' ) , tf . keras . metrics . recall ( name = ' recall ' ) ] ) <hashtag> saving </hashtag> the model model . save ( ' my_model ' ) <hashtag> loading </hashtag> the model keras . models . load_model ( ' mymodel ' ) <hashtag> throws </hashtag> the error ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"how to get label of detection after ( <user> / tfjs - tflite ) predict returns result from a custom tflite model in reactjs web app <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source binary # # # tensorflow version tfjs @ ^ <number> . <number> # # # custom code no # # # os platform and distribution windows <number> # # # mobile device windows laptop # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i am using import * as tflite from ' <user> / tfjs - tflite ' ; and have successfully loaded a custom tflite model : tflitemodel = await tflite . loadtflitemodel ( "" model / mymodel . tflite "" ); as well as gotten a prediction var outputtensor = await tflitemodel . predict ( img ) ; the code works well when i only have <number> label to detect initially . now i need to detect multiple labels . i created a new custom model and tested that it is able to detect multiple labels in an android project , eg . i can get labels ' l1 ' , and ' l2 ' can i detect multi labels using the code above after i load in the new model ? if not , what changes do i have to make to detect multi labels and know which label i am detecting ( eg . obtain the name of the label ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell see above . not a bug issue . ` ` ` # # # relevant log output ` ` ` shell instance of result , outputtensor ' s value outputtensor : { "" statefulpartitionedcall : <number> "" : { kept : false , isdisposedinternal : false , shape : [ <number> , <number> , ] , dtype : "" float32 "" , size : <number> , strides : [ <number> , ] , dataid : { id : <number> , } , id : <number> , ranktype : "" <number> "" , } , "" statefulpartitionedcall : <number> "" : { kept : false , isdisposedinternal : false , shape : [ <number> , <number> , <number> , ] , dtype : "" float32 "" , size : <number> , strides : [ <number> , <number> , ] , dataid : { id : <number> , } , id : <number> , ranktype : "" <number> "" , } , "" statefulpartitionedcall : <number> "" : { kept : false , isdisposedinternal : false , shape : [ <number> , ] , dtype : "" float32 "" , size : <number> , strides : [ ] , dataid : { id : <number> , } , id : <number> , ranktype : "" <number> "" , } , "" statefulpartitionedcall : <number> "" : { kept : false , isdisposedinternal : false , shape : [ <number> , <number> , ] , dtype : "" float32 "" , size : <number> , strides : [ <number> , ] , dataid : { id : <number> , } , id : <number> , ranktype } , } ` ` ` </details>",2
tensorflow/tensorflow,"error while evaluating tflite model with bazel run_eval hi , i managed to train a ssd_mobilenet_v1 using model_main_tf2 . py . after that i managed to export with export_tflite_graph_tf2 . py changing the max_detections paramenter to <number> , and then i successfully exported a tflite model . i tried then to run some inference with the new tflite model and it worked as expected . i decided then to try to evaluate the tflite model using this [ guide ] ( <url> but when i run this command : ` bazel run - c opt - - / / tensorflow / lite / tools / evaluation / tasks / coco_object_detection : run_eval - - model_file <annoyed> home / gab / pycharmprojects / tensorflow_prova2 / new_model_quant_f16q . tflite - - ground_truth_images_path <annoyed> home / gab / pycharmprojects / tensorflow_prova2 / images - - model_output_labels <annoyed> home / gab / pycharmprojects / tensorflow_prova2 / label_map . txt - - output_file_path <annoyed> home / gab / pycharmprojects / tensorflow_prova2 / coco_output . txt - - debug_mode = true ` output shows odd results like this : object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right : <number> object [ <number> ] score : <number> class - id : <number> bounding box : normalized top : <number> normalized bottom : <number> normalized left : <number> normalized right this is for the first image of the folder i used , one thing i noted is that the first score of the first object is always <number> that is the parameter i changed when i exported the trained model with export_tflite_graph_tf2 . py is there a way to fix this so i can evaluate my tflite model ? thanks",2
tensorflow/tensorflow,"serialized_pb please go to stack overflow for help and support : <url> if you open a github issue , here is our policy : <number> . it must be a bug , a feature request , or a significant problem with the documentation ( for small docs fixes please send a pr instead ) . <number> . the form below must be filled out . <number> . it should not be a tensorboard issue . those go [ here ] ( <url> * * here ' s why we have that policy * * : tensorflow developers respond to issues . we want to focus on work that benefits the whole community , e . g . , fixing bugs and adding features . support only helps individuals . github also notifies thousands of people when issues are filed . we want them to see you communicating an interesting problem , rather than being redirected to stack overflow . - - - - - - - - - - - - - - - - - - - - - - - - # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : - * * python version * * : - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : - * * gpu model and memory * * : - * * exact command to reproduce * * : you can collect some of this information using our environment capture script : <url> you can obtain the tensorflow version with python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` ` ` # # # describe the problem describe the problem clearly here . be sure to convey here why it ' s a bug in tensorflow or a feature request . # # # source code / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . try to provide a reproducible test case that is the bare minimum necessary to generate the problem .",2
tensorflow/tensorflow,"how to design tf . keras callback to save model predictions for each batch and each epoch <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i want to create a tf . keras callback to save model predictions for each batch and each epoch during the training using the training data sets to a numpy array i have tried the following callback , however it gives error like attributeerror : ' predictioncallback ' object has no attribute ' x_train ' ` ` ` # # # standalone code to reproduce the issue ` ` ` shell class predictioncallback ( tf . keras . callbacks . callback ) : def on_epoch_end ( self , epoch , logs ={}) : y_pred = self . model . predict ( self . x_train ) print ( ' prediction : { } at epoch : { } ' . format ( y_pred , epoch ) ) pd . dataframe ( y_pred ) . assign ( epoch = epoch ) . to_csv ( ' { } _ { } . csv ' . format ( filename , epoch ) ) cnn_model . fit ( x_train , y_train , validation_data =[ x_valid , y_valid ] , epochs = epochs , batch_size = batch_size , callbacks =[ model_checkpoint , reduce_lr , csv_logger , early_stopping , predictioncallback ( ) ] , verbose = <number> ) ` ` ` # # # relevant log output ` ` ` shell attributeerror object has no attribute ' x_train ' i also tried tensorflow - create keras callback to save model predictions and targets for each batch during training - stack overflow but not get success yet . hope experts will help me . thanks . ` ` ` </details>",2
tensorflow/tensorflow,cannot import name ' build_info ' from ' tensorflow . python . platform ' <details> <summary> click to expand </summary> # # # issue type support # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> 2 2 h2 # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version no # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory mx150 2 gb vram # # # current behaviour ? ` ` ` shell cannot import name ' build_info ' from ' tensorflow . python . platform ' ` ` ` # # # standalone code to reproduce the issue ` ` ` shell cannot import name ' build_info ' from ' tensorflow . python . platform ' import tensorflow as tf from object_detection . utils import config_util from object_detection . protos import pipeline_pb2 from google . protobuf import text_format ` ` ` # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"i want to reduce the app size with tensorflowliteselecttfops # # # <number> . system information - ios - tflite model - cocoapods pod ' tensorflowliteobjc ' , ' = <number> . <number> ' pod ' tensorflowliteselecttfops ' , ' <number> . <number> - nightly . <number> ' # # # <number> . code and question with parsing linkmap file , i found the tensorflowliteselecttfops take <number> mb , which is too bigger . when converting the tflite model , my model just use three tensorflow ops : flextensorlistreserve , flextensorlistsetitem , flextensorliststack . who can tell me some ways to reduce the size of app with tensorflowliteselecttfops",2
tensorflow/tensorflow,a bug in tensorflow <details> < su # # # standalone code to reproduce the issue # # # relevant log output _no response_ </details>,2
tensorflow/tensorflow,"8 2 2 2 c1cfc866126111f23bd <phone> cebf2c1 . tar . gz <number> <details> <summary> click to expand </summary> # # # issue type support # # # source binary # # # tensorflow version <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell <url> this file can download at <number> . <number> , today [ <number> . <number> ] is <number> ` ` ` # # # standalone code to reproduce the issue ` ` ` shell <url> this file can download at <number> . <number> , today [ <number> . <number> ] is <number> ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"error not find a version that satisfies the requirement tensorflow = = <number> . <number> i use google colab and was wondering if anyone can solve this problem , if you want to fix my notebook i will leave a link down below , thanks if you solve it <url>",2
tensorflow/tensorflow,"error : ' tf . conv2d ' op is neither a custom op nor a flex op # # # <number> . system information - os platform and distribution ( e . g . , linux ubuntu <number> <sad> linux ubuntu <number> - tensorflow installation ( pip package or built from source ) : pip package - tensorflow library ( version , if pip package or github sha , if built from source ) : v2 . <number> # # # <number> . code code for conversion ` ` ` converter = tf . lite . tfliteconverter . from_saved_model ( f ' savedmodel / decoder ' ) tflite_model = converter . convert ( ) # save the model with open ( f ' { name } . tflite ' , ' wb ' ) as f : f . write ( tflite_model ) ` ` ` code for the model ` ` ` latent = keras . layers . input ( ( n_h , n_w , <number> ) ) decoder = decoder ( ) decoder = keras . models . model ( latent , decoder ( latent ) ) ` ` ` ` ` ` class decoder ( keras . sequential ) : def __init__ ( self ) : super ( ) . __init__ ( [ keras . layers . lambda ( lambda <kiss> <number> / <number> * x) , paddedconv2d ( <number> , <number> ) , paddedconv2d ( <number> , <number> , padding = <number> ) , resnetblock ( <number> , <number> ) , attentionblock ( <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , keras . layers . upsampling2d ( size =( <number> , <number> ) ) , paddedconv2d ( <number> , <number> , padding = <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , keras . layers . upsampling2d ( size =( <number> , <number> ) ) , paddedconv2d ( <number> , <number> , padding = <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , keras . layers . upsampling2d ( size =( <number> , <number> ) ) , paddedconv2d ( <number> , <number> , padding = <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , resnetblock ( <number> , <number> ) , tfa . layers . groupnormalization ( epsilon = 1 e - <number> ) , keras . layers . activation ( "" swish "" ) , paddedconv2d ( <number> , <number> , padding = <number> ) , ] ) ` ` ` # # # <number> . failure after conversion conversion fails # # # <number> . ( optional ) any other info / logs [ error . log ] ( <url> ` ` ` some ops are not supported by the native tflite runtime , you can enable tf kernels fallback using tf select . see instructions : <url> tf select ops : conv2d details : tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <1x1x512x512xf32> ) - > ( tensor < ? x ? x ? x512xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x128x128xf32> ) - > ( tensor < ? x ? x ? x128xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x128x3xf32> ) - > ( tensor < ? x ? x ? x3 xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x256x128xf32> ) - > ( tensor < ? x ? x ? x128xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x256x256xf32> ) - > ( tensor < ? x ? x ? x256xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x512x256xf32> ) - > ( tensor < ? x ? x ? x256xf32 > ) : { data_format = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } tf . conv2d ( tensor < ? x ? x ? x ? xf32 > , tensor <3x3x512x512xf32> ) - > ( tensor < ? x ? x ? x512xf32 > ) = "" nhwc "" , device = "" "" , dilations = [ <number> , <number> , <number> , <number> ] , explicit_paddings = [ ] , padding = "" valid "" , strides = [ <number> , <number> , <number> , <number> ] , use_cudnn_on_gpu = true } ` ` ` according to the error message , i suspect that it can not recognize the input shape . but as you can see on the above code , input is specified for the functional api for ` decoder ` model . ( fyi , the inference code is called with ` predict_on_batch ` method . i found out other model with ` predict_on_batch ` is converted successfully , but that model does not contain ` conv2d ` block inside . can using ` predict_on_batch ` together with ` conv2d ` be a problem ? ) * * i am sure ` conv2d ` is on the allowlist for tflite operators . any suggestions for this problem ? thank you . * *",2
tensorflow/tensorflow,"__init__ ( ) got an unexpected keyword argument ' reduction ' <details> <summary> click to expand </summary> # # # issue type support # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell compile a model with ` metrics =[ tf . keras . losses . x <elongated> ] ` does not raise error . the training process seems also good . however , if we save the model and try to reload it , it does not work . now i know we could not use ` tf . keras . losses ` in ` metrics ` , we must use some functions in ` tf . keras . metrics ` or some customized metrics . but the error message "" __init__ ( ) got an unexpected keyword argument ' reduction ' "" gives no information . why not raise an error during the training or even during the compiling ? that would be much more friendly for new comers . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import numpy as np from tensorflow . keras . layers import dense , dropout import tensorflow as tf input_length = <number> latent_dim = <number> output_length = <number> model = tf . keras . sequential ( [ dense ( latent_dim , activation = ' relu ' , input_shape =( input_length , ) ) , dropout ( rate = <number> ) , dense ( units = latent_dim , activation = ' relu ' ) , dense ( units = output_length ) , ] ) model . compile ( optimizer = tf . keras . optimizers . adam ( learning_rate = 1 e - <number> ) , loss = tf . keras . losses . meansquarederror ( ) , metrics =[ tf . keras . losses . meanabsoluteerror ( ) ] ) x_train = np . ones ( shape =( <number> , <number> ) ) y_train = np . ones ( shape =( <number> , <number> ) ) history = model . fit ( x_train , y_train , epochs = <number> , batch_size = <number> ) model . save ( ' saved_model / my_model ' ) pretrained = tf . keras . models . load_model ( ' saved_model / my_model ' ) ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" d <annoyed> vscode / m2 - smc / debug . py "" , line <number> , in <module> pretrained = tf . keras . models . load_model ( ' saved_model / my_model ' ) file "" c :\\ users \ \ yunhao \ \ anaconda3 \ \ lib \ \ site - packages \ \ keras \ \ utils \ \ traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" c :\\ users \ \ yunhao \ \ anaconda3 \ \ lib \ \ site - packages \ \ keras \ \ engine \ \ base_layer . py "" , line <number> , in from_config return cls ( * * config ) typeerror got an unexpected keyword argument ' reduction ' ` ` ` </details>",2
tensorflow/tensorflow,"tf . custom_gradient with multiple input and output # # # system information - * * have i written custom code ( as opposed to using a stock example script provided in tensorflow ) * * : - * * os platform and distribution ( e . g . , linux ubuntu <number> )* * : ubuntu <number> - * * mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device * * : - * * tensorflow installed from ( source or binary ) * * : - * * tensorflow version ( use command below ) * * : <number> . <number> - * * python version * * : <number> - * * bazel version ( if compiling from source ) * * : - * * gcc / compiler version ( if compiling from source ) * * : - * * cuda / cudnn version * * : - * * gpu model and memory * * : - * * exact command to reproduce * * : # # # describe the problem i have a function with <number> inputs (x 1 , x2 , x3 , x4 ) and <number> outputs ( y1 , y2 ) using tensorflow . i would like to specify the gradients , since i perform some non - autodiff operations inside the function . i need to specify the derivatives of the outputs with respect to the inputs . we can see these derivatives as a jacobian of size ( <number> ) . regarding this , i have <number> derivatives : dy1_dx1 , dy1_dx2 , dy1_dx3 , dy1_dx4 , dy2_dx1 , dy2_dx2 , dy2_dx3 and dy2_dx4 . however , the grad function used in this tf . custom . gradient needs to have the same length as the inputs , this is <number> . so , i do not know how tensorflow handles with the introduction of the <number> derivatives using just <number> elements . i tried to include them as lists , but it gives the error . here is a general code to reproduce the error : ` ` ` import tensorflow as tf <user> . custom_gradient def bar ( x1 , x2 , x3 , x4 ) : def grad ( dy1 , dy2 ) : dy1_dx1 = x2 * * <number> * x3 * * <number> * x4 * * <number> # <number> dy1_dx2 = x1 * <number> *x 2 * x3 * * <number> * x4 * * <number> # <number> dy1_dx3 = x1 * x2 * * <number> * <number> x3 <emphasis> * <number> * x4 * * <number> # <number> dy1_dx4 = x1 * x2 * * <number> * x3 * * <number> * <number> x4 <emphasis> * <number> # <number> dy2_dx1 = x2 * * <number> + x3 * * <number> + x4 * * <number> # <number> dy2_dx2 = x1 + <number> *x 2 + x3 * * <number> + x4 * * <number> # <number> dy2_dx3 = x1 + x2 * * <number> + <number> x3 <emphasis> * <number> + x4 * * <number> # <number> dy2_dx4 = x1 + x2 * * <number> + x3 * * <number> + <number> x4 <emphasis> * <number> # <number> return [ dy1_dx1 , dy2_dx1 ] , [ dy1_dx2 , dy2_dx2 ] , [ dy1_dx3 , dy2_dx3 ] , [ dy1_dx4 , dy2_dx4 ] y1 = x1 * x2 * * <number> * x3 * * <number> * x4 * * <number> y2 = x1 + x2 * * <number> + x3 * * <number> + x4 * * <number> return [ y1 , y2 ] , grad x1 = tf . constant ( <number> , dtype = tf . float32 ) x2 = tf . constant ( <number> , dtype = tf . float32 ) x3 = tf . constant ( <number> , dtype = tf . float32 ) x4 = tf . constant ( <number> , dtype = tf . float32 ) with tf . gradienttape ( persistent = true ) as tape : tape . watch ( x1 ) tape . watch ( x2 ) tape . watch ( x3 ) tape . watch ( x4 ) z = bar ( x1 , x2 , x3 , x4 ) print ( tape . gradient ( z , x1 ) ) #[ dy1_dx1 , dy2_dx1 ] print ( tape . gradient ( z , x2 ) ) #[ dy1_dx2 , dy2_dx2 ] print ( tape . gradient ( z , x3 ) ) #[ dy1_dx3 , dy2_dx3 ] print ( tape . gradient ( z , x4 ) ) #[ dy1_dx4 , dy2_dx4 ] ` ` ` the error says function expected to return <number> gradients , but returned <number> instead "" . i expect someway to specify the correspondent <number> derivatives . thank you in advance",2
tensorflow/tensorflow,"strange c macro generated by tensorflow . lite . python . util . convert_bytes_to_c_source ( ) # # # <number> . system information although i believe the system setup does not matter in this issue , here goes : windows <number> and ubuntu <number> python <number> , python <number> , python <number> tensorflow lite <number> and <number> ( pip package ) # # # <number> . code ` tensorflow . lite . python . util . convert_bytes_to_c_source ( ) ` generates the following strange c macro : ` ` ` / / we need to keep the data array aligned on some architectures . <hashtag> if def </hashtag> __has_attribute <hashtag> define </hashtag> have_attribute ( x ) __has_attribute ( x ) <hashtag> else </hashtag> <hashtag> define </hashtag> have_attribute ( x ) <number> <hashtag> end if </hashtag> <hashtag> if </hashtag> have_attribute ( aligned ) || ( defined ( __gnuc__ ) & & defined ( __clang__ ) ) <hashtag> define </hashtag> data_align_attribute __attribute__ ( ( aligned ( <number> ) ) ) <hashtag> else </hashtag> <hashtag> define </hashtag> data_align_attribute <hashtag> end if </hashtag> ` ` ` the strange part is & & ! defined ( __clang__ ) ) ` if the code is compiled with the gnu c compiler , then of course it is not compiled with the clang compiler . the ` & & ! defined ( __clang__ ) ` seems redundant . and if it ' s not redundant , can someone explain it to me ?",2
tensorflow/tensorflow,"how to reduce power consumption for gpu delegate on android <details> <summary> click to expand </summary> # # # issue type performance # # # source source # # # tensorflow version <number> or <number> # # # custom code yes # # # os platform and distribution android # # # mobile device android # # # python version <number> # # # bazel version no # # # gcc / compiler version no # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell here it is not a bug . i want to reduce power consumption for gpu delegate on android , because i find the execution of tflite model using gpu occupys many power （ ma ） . i use opencl backend . do you have some optimization method in order to reduce power of model . thank you very much ` ` ` # # # standalone code to reproduce the issue ` ` ` shell my configuration is following , tflitegpudelegateoptionsv2 gpu_options = tflitegpudelegateoptionsv2default ( ); gpu_options . inference_priority1 = tflite_gpu_inference_priority_min_memory_usage ; gpu_options . inference_priority2 = tflite_gpu_inference_priority_min_latency ; gpu_options . inference_priority3 = tflite_gpu_inference_priority_max_precision ; gpu_options . experimental_flags |= tflite_gpu_experimental_flags_enable_quant ; gpu_options . inference_preference = tflite_gpu_inference_preference_fast_single_answer ; ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"how to release memory when i want to change model with tf . saved_model . load already ? <details> <summary> click to expand </summary> # # # issue type support # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i use tf . saved_model . load to load a trained model . i want to change another model when it done . but the memory is still high , how to release the memory ? i have test tf . keras . backend . clear_session ( ) and gc . collect ( ) , all of these did not work . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell model = tf . saved_model . load ( model_dir ) tf . keras . backend . clear_session ( ) gc . collect ( ) ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,how to calculate the model ' s flops when i use tensorflow ? <details> <summary> click to expand </summary> # # # issue type support # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i do not find a good way to calculate the model ' s flops by tensorflow . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i do not find a good way to calculate the model ' s flops by tensorflow . ` ` ` # # # relevant log output ` ` ` shell i do not find a good way to calculate the model ' s flops by tensorflow . ` ` ` </details>,2
tensorflow/tensorflow,"deploy yolo5 model into tensorflow lite object detection android hello tensorflow team i checked your real time object detection app which works good with the initial models ( mobilenet v1 , efficientnet lite0 , efficientnet lite1 , efficientnet lite2 ) . <url> but i receive errors when i try to add other model for example yolo5s model trained on coco dataset which converted from * pt <emphasis> * format to * tflite <emphasis> * with export . py script <url> below you can find converted models for yolo5s ( fp16 and int8 options ) , i already checked they are working fine with detect . py . <url> <url> is it possible to deploy / integrate yolo5 model into your real time object detection app ? if yes , can you please check if you can deploy into app one of those two models ( fp16 or int8 ) i shared with you above . in case if it works for you , can you share your experience what did you exactly modify in initial tensorflow lite object detection app scripts . thank you in advance !",2
tensorflow/tensorflow,"which encoding can i use to parse tensorflow_stats . pb ? # # # issue type others # # # source binary # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? i get a tensorflow_stats . pb file by tf . profiler . code like this : ` ` ` tf . profiler . experimental . start ( "" . / logs_dir "" ) loss = dnn_model . train_step ( batch_data ) tf . profiler . experimental . stop ( ) ` ` ` i want to read tensorflow_stats . pb in my python code , but i do not know which encoding this file use . the file content is "" ä °  p     host   updatemetricstate ""  updatemetricstate (  1 ňňmb  x  <user>  x  <user>  x  <user>  x  <user> × ýťá .  ň ? i × ýťá .  ň ?    unknown r     host   updatemetricstate ""  updatemetricstate_1 (  <number>  ź zb  <user>  ź zb  <user>  ź zb  <user>  ź zb  <user> [ ťżóě ? iľą ´ k  aŕ ?    unknown b     host   idle ""  idle1áęąeś × l <user> × l <user>     ľśş ? i  ô  űů  ă ?    unknown r     host   updatemetricstate ""  updatemetricstate_2 (  <number>   ůî ÷ ób <user>   ůî ÷ ób <user>   ůî ÷ ób <user>   ůî ÷ ób <user> × ; ö  ą ? iďî  ă  éĺ ?    unknown p     host   asyncpushgradient ""  asyncpushgradient (  1 î | ? <number> ^ bv <user> | ? <number> ^ bv <user> | ? <number> ^ bv <user> | ? <number> ^ bv <user>  ć ^ eă  ¤ ? iyýe ÷ p  ç ?    unknown d     host   emboutputtransfer ""  label (  <number> × łp =  i <user> × łp =  i <user> × łp =  i <user> × łp =  i <user>  â ­ ¤  ? i [ ve  đç ?    unknown \ \     host   printv2 ""  printv2 (  <number>  +   ůîf <user>  +   ůîf <user>  +   ůîf <user>  +   ůîf <user>  ­ ß  ? i [ ůţb  yč ?    unknown       host   globalnormgrad "" bgradient_tape / simple_dnn_model / global_normalization / globalnormgrad (  <number> ´ čvž ? b <user> ´ čvž ? b <user> ´ čvž ? b <user> ´ čvž ? b <user> ~ [ ęć  ? irńşľěé ?    unknown b    host  ` ` `",2
tensorflow/tensorflow,"how to convert model with multiple input ? # # # <number> . system information - os platform and distribution ( e . g . , window <number> <sad> - tensorflow installation ( pip package ) : - tensorflow library ( tensorflow <number> . <number> <sad> # # # <number> . code provide code to help us reproduce your issues using one of the following options : i am use code as below , what should i change to make convertion success ? ` ` ` import numpy as np import tensorflow as tf from tensorflow import keras # how to write "" representative_dataset_gen "" function ? def representative_dataset_gen ( <sad> for _ in range ( <number> <sad> data1 = np . random . rand ( <number> , <number> , <number> , <number> ) . astype ( np . float32 ) data2 = np . random . rand ( <number> , <number> , <number> , <number> ) . astype ( np . float32 ) yield [ data1 , data2 ] # build model kinput1 = keras . input ( shape =( <number> , <number> , <number> ) , batch_size = <number> , name = "" input_1 "" ) kinput2 = keras . input ( shape =( <number> , <number> , <number> ) , batch_size = <number> , name = "" input_2 "" ) conv1 = keras . layers . conv2d ( <number> , <number> , <number> ) ( kinput1 ) conv2 = keras . layers . conv2d ( <number> , <number> , <number> ) ( kinput2 ) out = conv1 + conv2 keras_model = keras . model ( inputs =[ kinput1 , kinput2 ] , outputs = out ) keras_model . trainable = false keras_model . summary ( ) # convert converter = tf . lite . tfliteconverter . from_keras_model ( keras_model ) converter . target_spec . supported_ops = [ tf . lite . opsset . tflite_builtins_int8 ] converter . inference_input_type = tf . uint8 # or tf . int8 converter . inference_output_type = tf . uint8 # or tf . int8 converter . representative_dataset = representative_dataset_gen tflite_model = converter . convert ( ) with open ( "" . / model . tflite "" , "" wb "" ) as fp : fp . write ( tflite_model ) ` ` ` # # # <number> . failure after conversion if the conversion is successful , but the generated model is wrong , then state what is wrong : - model produces wrong results and / or has lesser accuracy . - model produces correct results , but it is slower than expected . # # # <number> . ( optional ) rnn conversion support if converting tf rnn to tflite fused rnn ops , please prefix [ rnn ] in the title . # # # <number> . ( optional ) any other info / logs include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . error info : ` ` ` valueerror inference_input_type and inference_output_type must be tf . float32 . ` ` `",2
tensorflow/tensorflow,"tf - lite model with a conv2dtranspose layer is fail to run on mobile gpu . # # # <number> . system information - os platform and distribution ( e . g . , linux ubuntu <number> <sad> - conversion env : centos7 - test env : galaxy s10 ` ( samsung / beyond1lteks / beyond <time> / qp1a . <number> / g973nksu4cte9 : user / release - keys ) ` - tensorflow installation ( pip package or built from source ) : pip install - tensorflow library ( version , if pip package or github sha , if built from source ) : <number> . <number> # # # <number> . code provide code to help us reproduce your issues using one of the following options : ` ` ` def get_model ( <sad> inputs = tf . keras . input ( shape =( <number> , <number> , <number> ) ) x = keras . layers . conv2d ( <number> , <number> , activation = "" relu "" , name = "" conv1 "" ) ( inputs ) x = keras . layers . conv2d ( <number> , <number> , activation = "" relu "" , name = "" conv2 "" )(x ) x = keras . layers . conv2dtranspose ( <number> , <number> , strides = <number> , activation = "" relu "" , name = "" deconv1 "" )(x ) x = keras . layers . conv2dtranspose ( <number> , <number> , strides = <number> , activation = "" relu "" , name = "" deconv2 "" )(x ) outputs = x model = keras . model ( inputs = inputs , outputs = outputs , name = "" custom "" ) x = tf . ones ( ( <number> , <number> , <number> , <number> ) ) y = model ( x ) print ( x . shape ) print ( y . shape ) return model def convert_model ( saved_model_dir , tflite_save_dir ) : converter = tf . lite . tfliteconverter . from_saved_model ( saved_model_dir ) converter . optimizations = [ tf . lite . optimize . default ] converter . target_spec . supported_types = [ tf . float32 ] tflite_model = converter . convert ( ) with open ( tflite_save_dir , "" wb "" ) as f : f . write ( tflite_model ) if __name__ = = "" __main__ "" = get_model ( ) current_path = os . path . dirname ( os . path . realpath ( __file__ ) ) save_dir = os . path . join ( current_path , "" custom / <number> / "" ) tf . saved_model . save ( model , save_dir ) tflite_save_dir = os . path . join ( current_path , "" my_model . tflite "" ) convert_model ( save_dir , tflite_save_dir ) test_tflite ( tflite_save_dir ) ` ` ` # # # <number> . failure after conversion tf lite conversion and run on the mobile phone with cpu is ok , but when running with gpu , an error occurs . ( i tested the model in adb shell ) [ image ] ( <url>",2
tensorflow/tensorflow,"add support for 3 rd part object storage ( s3 ) in docker image <details> <summary> click to expand </summary> # # # issue type support # # # source binary # # # tensorflow version <number> + # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell when using tensorflow docker image to run tensorboard ( kubeflow uses this method ) , out of the box the image only supports gcs - filesystem and to support for 3 rd party object storage ( eg . s3 ) due to the dependency on ` tensorflow - io ` . the ask is to make tensorflow - io as required pip dependency while docker build happens that way lot of customizations could be avoided and the support is out of the box . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell when using the below sample spec ( simplified to make the reproducibility easy ) : spec : affinity : { } containers : - args : - aws_region = us - east - <number> s3_endpoint = https :// some_s3_endpoint aws_access_key_id = some_aws_key aws_secret_access_key = some_aws_secret / usr / local / bin / tensorboard - - logdir =s 3:/ logs_path - - bind_all command : - / bin / sh - - c image : tensorflow / tensorflow : <number> . <number> imagepullpolicy : ifnotpresent name : tensorboard ports : - containerport : <number> protocol : tcp resources : { } terminationmessagepath : / dev / termination - log terminationmessagepolicy : file workingdir : / dnspolicy : clusterfirst imagepullsecrets : - name : regcred restartpolicy : always schedulername : default - scheduler securitycontext : { } terminationgraceperiodseconds : <number> ` ` ` # # # relevant log output ` ` ` shell when the above spec is deployed we will run into the following error ` error : unsupported filename scheme ' s3 ' ( supported schemes ' file ' , ' ' , ' gs ' ] ) . for additional filesystem support , consider instal │ │ ling tensorflow i / o ( <url> via ` pip install tensorflow - io ` . ` ` ` ` </details>",2
tensorflow/tensorflow,"attributeerror : ' tensorflow . python . framework . ops . eagertensor ' object has no attribute ' _unique_id ' <details> <summary> click to expand </summary> # # # issue type bug # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution debian64 ( unix ) # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hello to everyone i need to training an actor network for a reinforcement learning algorithm but in time i am trying to predict continuous actions i have a problem with the function : self . actor . optimizer . apply_gradients ( zip ( grads2 , std ) ) also this one gives me the same error : self . actor . optimizer . apply_gradients ( zip ( [ grads1 , grads2 ] , [ self . actor . model . trainable_variables , std ] ) ) please note that with the following function all goes well : self . actor . optimizer . minimize ( loss_actions , var_list = [ self . actor . model . trainable_variables , std ] , tape = tape_actions ) the problem seems to be caused by the external variable std ( that it is not internal to the model ) , did i make some mistakes ? is there a way to clip_by_norm the std variable ' s gradients ? best regards samir ` ` ` # # # standalone code to reproduce the issue ` ` ` shell one = tf . ones ( self . settings . action_size , dtype = float ) std = tf . variable ( one , dtype = float , name = ' std ' ) sample = self . replay . sample ( ) with tf . gradienttape ( persistent = false ) as tape_actions : tape_actions . reset ( ) actions_local = self . actor . model ( sample [ <number> ] ) tfp_dist = tfp . distributions . normal ( loc = actions_local , scale = std ) new_probs = tfp_dist . log_prob ( sample [ <number> ] ) new_probs = tf . math . reduce_sum ( new_probs , axis = <number> ) new_probs = tf . math . exp ( new_probs ) ratio = new_probs / sample [ <number> ] # >= <number> ratio = tf . math . reduce_min ( tf . convert_to_tensor ( [ ratio * sample [ <number> ] , \ \ tf . clip_by_value ( ratio , clip_value_min = <number> - <number> , clip_value_max = <number> + <number> ) * sample [ <number> ] ] ) , axis = <number> ) loss_actions = - tf . math . reduce_mean ( ratio ) grads = tape_actions . gradient ( loss_actions , [ self . actor . model . trainable_variables , std ] ) grads1 = [ tf . clip_by_norm ( t = w , clip_norm = <number> ) for w in grads [ <number> ] ] grads2 = [ tf . clip_by_norm ( t = w , clip_norm = <number> ) for w in grads [ <number> ] ] self . actor . optimizer . apply_gradients ( zip ( grads1 , self . actor . model . trainable_variables ) ) self . actor . optimizer . apply_gradients ( zip ( grads2 , std ) ) ` ` ` # # # relevant log output ` ` ` shell <date> / usr / lib / python3 / dist - packages / apport / report . py : <number> : deprecationwarning : the imp module is deprecated in favour of importlib and slated for removal in python <number> ; see the module ' s documentation for alternative uses import fnmatch , glob , traceback , errno , sys , atexit , locale , imp , stat traceback ( most recent call last ) : file "" / home / kaumi / git / deepl_rl2 / <number> . ml_agents / <number> . p2_continuous - control / reacher_exam_code / reacher . py "" , line <number> , in <module> future . cross_entropy_loss ( ) file "" / home / kaumi / git / deepl_rl2 / <number> . ml_agents / <number> . p2_continuous - control / reacher_exam_code / agent . py "" , line <number> , in cross_entropy_loss self . play_ep ( ie , self . settings . envs . reset ( train_mode = true ) [ self . settings . brain_name ] ) file "" / home / kaumi / git / deepl_rl2 / <number> . ml_agents / <number> . p2_continuous - control / reacher_exam_code / agent . py "" , line <number> , in play_ep print ( '' ) ; self . training_for_elite ( real_steps_to_train ) ; print ( '' ) file "" / home / kaumi / git / deepl_rl2 / <number> . ml_agents / <number> . p2_continuous - control / reacher_exam_code / agent . py "" , line <number> , in training_for_elite self . training ( self . future_rewards_elite , real_steps_to_train ) file "" / home / kaumi / git / deepl_rl2 / <number> . ml_agents / <number> . p2_continuous - control / reacher_exam_code / agent . py "" , line <number> , in training self . actor . optimizer . apply_gradients ( zip ( grads2 , std ) ) file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / keras / optimizers / optimizer_v2 / optimizer_v2 . py "" , line <number> , in apply_gradients self . _create_all_weights ( var_list ) file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / keras / optimizers / optimizer_v2 / optimizer_v2 . py "" , line <number> , in _create_all_weights self . _create_slots ( var_list ) file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / keras / optimizers / optimizer_v2 / adam . py "" , line <number> , in _create_slots self . add_slot ( var , "" m "" ) file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / keras / optimizers / optimizer_v2 / optimizer_v2 . py "" , line <number> , in add_slot var_key = _var_key ( var ) file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / keras / optimizers / optimizer_v2 / optimizer_v2 . py "" , line <number> , in _var_key return var . _unique_id file "" / home / kaumi / . local / lib / python3 . <number> / site - packages / tensorflow / python / framework / ops . py "" , line <number> , in __getattr__ self . __getattribute__ ( name ) attributeerror object has no attribute ' _unique_id ' ` ` ` </details>",2
tensorflow/tensorflow,"how much fps in ios ( swift ) ? <details> <summary> click to expand </summary> # # # issue type support # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution macos # # # mobile device iphone x # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell can i increase number of frames per second ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell how much fps in ios ( swift ) ? ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"tensorflow lite quantization debugger issue # # # <number> . system information - os platform and distribution ( e . g . , linux ubuntu <number> ) : ubuntu <number> . <number> - tensorflow library ( version , if pip package or github sha , if built from source ) : tensorflow - gpu = = <number> . <number> # # # <number> . code def dense_block ( inputs , filters ) : y = dense ( units = filters , use_bias = false ) ( inputs ) y = batchnormalization ( ) ( y ) return y def model ( <sad> inputs = input ( shape = ( <number> , <number> , <number> ) ) . . . . . . . x = dense_block ( x , <number> ) - > ( input_size = <number> , <number> , output_size = <number> , <number> ) return model ( inputs , x) # # # <number> . conversion is successful , but the predicted value of int8 tflite has a large error value with the . pb weights file if the conversion is successful , but the generated model is wrong , then state what is wrong : - model produces wrong results and accuracy drop <number> ~ <percent> . # # # <number> . ( optional ) any other info / logs question1 weights . pb file is successfully quantized into an int8 tflite model . when doing tf . lite . experimental . quantizationdebugger , the rmse / scale value of the last layer ( conv2d ) is <number> , which is far more than <number> , but the rmse / scale values ​ ​ in other layers are all it is around <number> , and if i change the output of the last layer to more nodes , the value of rmse / scale of the last layer will be closer to <number> . does anyone know what could be causing this to happen ? thanks",2
tensorflow/tensorflow,"from keras . models import load_model raises no module named tensorflow . compat error <details> <summary> click to expand </summary> # # # issue type support # # # source binary # # # tensorflow version <number> # # # custom code no # # # os platform and distribution windoes <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell when doing keras . models import load_model , an error saying no module named tensorflow . compat appears ` ` ` # # # standalone code to reproduce the issue ` ` ` shell just open python <number> and type keras . models import load_model ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" c <annoyed> users / noah ryu / coding / ai / teachable machine / guesser3 . py "" , line <number> , in <module> from keras . models import load_model # tensorflow is needed for keras to work file "" c :\\ users \ \ noah ryu \ \ appdata \ \ local \ \ programs \ \ python \ \ python37 \ \ lib \ \ site - packages \ \ keras \ \ __init__ . py "" , line <number> , in <module> from keras import models file "" c :\\ users \ \ noah ryu \ \ appdata \ \ local \ \ programs \ \ python \ \ python37 \ \ lib \ \ site - packages \ \ keras \ \ models \ \ __init__ . py "" , line <number> , in <module> from keras . engine . functional import functional file "" c :\\ users \ \ noah ryu \ \ appdata \ \ local \ \ programs \ \ python \ \ python37 \ \ lib \ \ site - packages \ \ keras \ \ engine \ \ functional . py "" , line <number> , in <module> import tensorflow . compat . v2 as tf modulenotfounderror module named ' tensorflow . compat ' ` ` ` </details>",2
tensorflow/tensorflow,there is no operator to calculate the matrix ' s inverse using tflite tf . raw_ops . matrixinverse and tf . linalg . svd is not supported in tflite batchmatrixinverse is not available in graphdef version <number> . hence ， how to calculate the matrix ' s inverse using tflite ? i need some op to calculate the matrix ' s inverse best wishes,2
tensorflow/tensorflow,"difference between core / grappler / optimizers / graph_optimizer . h and core / common_runtime / graph_optimizer . h <details> <summary> click to expand </summary> # # # issue type support # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell this is not a bug report , just a question . now i am learning the source code of tensorflow and i can not understand the difference between these two graph_optimizer . h ( core / grappler / optimizers / graph_optimizer . h and core / common_runtime / graph_optimizer . h ) . can anyone explain this for me ? thanks . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell none ` ` ` # # # relevant log output _no response_ </details>",2
tensorflow/tensorflow,"old versions of tflite native benchmark binaries hi , is it possible to download old versions of tflite native benchmark binaries ? i can find [ here ] ( <url> only the nightly version .",2
tensorflow/tensorflow,running tensorflow on sharc processors ? are there any information on the possibility to compile tensorflow ( lite c ) targeting sharc processor from analog devices ?,2
tensorflow/tensorflow,implement llama <number> using tensorflow # # # issue type feature request # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? - # # # standalone code to reproduce the issue ` ` ` shell - ` ` ` # # # relevant log output ` ` ` shell - ` ` `,1
tensorflow/tensorflow,"will there a mlp model in the future version ? # # # issue type feature request # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? when building deep learning models like multi - layer perceptrons ( mlps ) , code reusability and conciseness are crucial factors . currently , using ` tf . keras . sequential ` in tensorflow allows for convenient creation of sequential models . however , manually adding common operations such as batch normalization or dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance . therefore , proposing the addition of a feature in tensorflow to directly create mlps with batch normalization and dropout is highly beneficial . here are several reasons why this feature would be advantageous for tensorflow users : <number> . * * simplified code * * : users will not need to manually add batch normalization and dropout operations to each layer , resulting in cleaner , more readable , and maintainable code . <number> . * * reduced error rate * * : manual copy - pasting of code is error - prone , especially as model complexity increases . automating the integration of batch normalization and dropout operations can reduce issues arising from code errors . <number> . * * increased productivity * * : developers can build and iterate on models more swiftly , focusing on design and parameter tuning rather than rewriting the same code segments for every new model . <number> . * * education and learning * * : for newcomers to tensorflow , this feature can provide a quicker onboarding process , lowering the learning curve and enabling them to grasp and apply deep learning concepts faster . certainly , here ' s the additional information : i also believe that pytorch has implemented mlp functionality quite effectively . an example of this can be found in the following url mlp ] ( <url> pytorch ' s approach to creating mlps provides a good reference for how tensorflow could potentially integrate similar features . # # # standalone code to reproduce the issue origin ` ` ` python model = tf . keras . sequential ( [ tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , ] ) ` ` ` with mlp model ` ` ` python model = tf . keras . mlp ( hidden_channels =[ <number> , <number> , <number> , <number> ] , norm_layer = tf . keras . layers . batchnormalization , activation_layer = tf . keras . layers . relu , dropout = <number> , ) ` ` ` # # # relevant log output _no response_",1
tensorflow/tensorflow,"pybind11_proto from python to c + + <user> thanks for the explanation . i have been exploring how to update the [ ` import_graph_def ( ) ` ] ( <url> code - path to use pybind11_protobuf and i could use your help with the following to how pybind11_protobuf allows us to pass protos directly from c + + to python , is there a way to pass a ` graphdef ` proto from python to c + + without performing serialization ? this would be needed to invoke the [ tf_graphimportgraphdefwithresults ] ( <url> from pywrap session in c + + . _originally posted by <user> in <url>",1
tensorflow/tensorflow,[ feature ] the heaviside step function as a activation function some of the implementations like single layer perceptron needs discrete outputs like <number> or <number> . adding this could make the model building ease .,1
tensorflow/tensorflow,"float8 support for array ops # # # issue type feature request # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution macos - <number> . <number> - arm64 - arm - 6 4 bit # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? please add fp8 datatype support for array ops ( like reshape , transpose , gatherv2 , expanddims , squeeze , concatv2 , split , pack , unpack , and stridedslice ) . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf from tensorflow . python . framework import dtypes a = tf . constant ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = dtypes . float16 ) print ( a ) a_fp8 = tf . cast ( a , dtypes . float8_e4m3fn ) print ( a_fp8 ) b = a_fp8 [ <number> : <number> ] # tensorflow . python . framework . errors_impl . notfounderror b = tf . transpose ( a_fp8 , [ <number> , <number> ] ) # tensorflow . python . framework . errors_impl . notfounderror ` ` ` # # # relevant log output _no response_",1
tensorflow/tensorflow,issue still tittle # # # issue type documentation feature request # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? what you see what i am see # # # standalone code to reproduce the issue ` ` ` shell us see you ` ` ` # # # relevant log output ` ` ` shell productive projects ` ` `,1
tensorflow/tensorflow,"the tf keras models load_model ( ) used for loading ml model is not able to load model <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? hi , i am trying to load my deep learning model using * * tensorflow keras models load_model ()* * . when i run it for first time it got loaded but from next time * * it is not loading * * . like it is in this function for more than <number> min . i store my model in * * . h5 * * format . model size is approx <number> mb . at the time of saving deep learning , i use model . save ( ) i am using a machine with <number> gb ram . i am using multiprocessing with no of worker <number> . sometime it worked and sometime is got stucked . # # # standalone code to reproduce the issue ` ` ` shell tensorflow . keras . models . load_model . ( ) ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"tf . keras . metrics . precision treats label as binary ? <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # have you reproduced the bug with tf nightly ? yes # # # source binary # # # tensorflow version <number> . <number> - dev20230611 # # # custom code yes # # # os platform and distribution linux # # # mobile device na # # # python version <number> . <number> # # # bazel version na # # # gcc / compiler version na # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? tf . keras . metrics . precision is returning precision assuming the label is binary ? it looks so to me . see : ` ` ` in [ <number> <sad> m = tf . keras . metrics . precision ( ) . <repeated> : m . update_state ( [ <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> ] ) . <repeated> : m . result ( ) . numpy ( ) out [ <number> <sad> <number> in [ <number> <sad> import tensorflow as tf in [ <number> <sad> m = tf . keras . metrics . precision ( ) . <repeated> : m . update_state ( [ <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> ] ) . <repeated> : m . result ( ) . numpy ( ) out [ <number> <sad> <number> in [ <number> <sad> tf . __version__ out [ <number> <sad> ' <number> . <number> - dev20230611 ' in [ <number> <sad> m = tf . keras . metrics . precision ( ) . <repeated> : m . update_state ( [ <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> ] ) . <repeated> : m . result ( ) . numpy ( ) out [ <number> ] ` ` ` above should not be <number> if labels are treated as non binary . it appears to me that 0 s are treated as <number> while non zeros are treated as <number> . but nowhere in the doc mentions this behavior . i can not find categorical precision or similar either . please update doc to explain this behavior . # # # standalone code to reproduce the issue ` ` ` shell see above . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"could not interpret loss function "" ndcg lambda weight v2 "" a erro happened could not interpret loss function ndcg lambda weight v2 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - model . compile ( optimizer = optimizer , loss = tfr . keras . losses . ndcglambdaweightv2 ( topn = <number> ) , metrics =[ tfr . keras . metrics . ndcgmetric ( topn = <number> ) , tfr . keras . metrics . opametric ( ) ] ) model . fit ( train_dataset , epochs = <number> ) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - some error happened in "" mode . fit ( ) "" valueerror : could not interpret loss function identifier object at 0x 7 fa0e84ee6e0 >",1
tensorflow/tensorflow,"gpu delegate dynamic tensor input shape ( feature request ) <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution android # # # mobile device android # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? hello , i am writing you to ask , are there plans to implement dynamic input shape in gpu delegate in near future ? right now dynamic input shape working fine on cpu , but on gpu delegate i am getting the issue ` java . lang . illegalargumentexception : internal error : error applying delegate : ` this is very important feature , if this feature already exist and it is possible somehow to rung with dynamic shape will be good to have some information in documentation . # # # standalone code to reproduce the issue ` ` ` shell <user> ( ioexception : : class ) private fun initinterpreter ( context : context ) : interpreter { val tfliteoptions = interpreter . options ( ) if ( delegate ! = null ) { tfliteoptions . addelegate <elongated> ( delegate ) tfliteoptions . numthreads = <number> } else { tfliteoptions . numthreads = <number> } val interpreter = interpreter ( loadmodelfile ( context ) , tfliteoptions ) interpreter . resizeinput ( <number> , intarrayof ( <number> , modelheight , modelwidth , <number> ) , true ) interpreter . allocatetensors ( ) return interpreter } ` ` ` # # # relevant log output ` ` ` shell java . lang . illegalargumentexception : internal error : error applying delegate method ) org . tensorflow . lite . nativeinterpreterwrapper . init ( nativeinterpreterwrapper . java : <number> ) org . tensorflow . lite . nativeinterpreterwrapper . <init> ( nativeinterpreterwrapper . java : <number> ) org . tensorflow . lite . nativeinterpreterwrapperexperimental . <init> ( nativeinterpreterwrapperexperimental . java : <number> ) org . tensorflow . lite . interpreter . <init> ( interpreter . java : <number> ) ` ` ` </details>",1
tensorflow/tensorflow,add docs reference to latest numpy version for ` tf . experimental . numpy ` functions <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? as the tf version requirements for running the latest version we need the latest version of numpy and all ` tf . experimental . numpy ` functions point to the ` numpy ` ` v1 . <number> ` <url> i am thinking if we can update the referencing docs link . thanks # # # standalone code to reproduce the issue ` ` ` shell check this <url> ` ` ` # # # relevant log output _no response_ </details>,1
tensorflow/tensorflow,"network - level control of precision ( fp32 in training , fp16 in inference ) <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source binary # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? a very common use case is that we train and export the model in fp32 , then use fp16 mode in inference . but it seems xla does not support this ? it would be good if xla can support [ network - level control of precision ] ( <url> like tensorrt # # # standalone code to reproduce the issue ` ` ` shell n / a ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"how to write custom xla op for tensorflow <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # have you reproduced the bug with tf nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution linux # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? hi , i have custom tensorflow ops implemented in c + + [ using this guide ] ( <url> which are invoked during training . they work fine as long as xla is not used , however i am unable to get them working ( and even a dummy custom xla op ) . the test op i have written is mentioned below and compiled by following the guide linked above . the compilation succeeds but i get following runtime error when the module with custom op is imported : ` ` ` non - ok - status : lookup_status status : not_found : op type not registered ' xlatestop ' in binary running on <hostname> . make sure the op and kernel are registered in the binary running in this process . ` ` ` i am not able to find any more info how to resolve this issue . # # # standalone code to reproduce the issue ` ` ` shell c + + xla op implementation : <hashtag> include </hashtag> "" tensorflow / compiler / tf2xla / shape_util . h "" <hashtag> include </hashtag> "" tensorflow / compiler / tf2xla / xla_compiler . h "" <hashtag> include </hashtag> "" tensorflow / compiler / tf2xla / xla_op_kernel . h "" <hashtag> include </hashtag> "" tensorflow / compiler / tf2xla / xla_op_registry . h "" <hashtag> include </hashtag> "" tensorflow / compiler / xla / client / xla_builder . h "" <hashtag> include </hashtag> "" tensorflow / compiler / xla / service / custom_call_target_registry . h "" <hashtag> include </hashtag> "" tensorflow / compiler / xla / service / hlo . pb . h "" <hashtag> include </hashtag> "" tensorflow / compiler / xla / shape . h "" <hashtag> include </hashtag> "" tensorflow / compiler / xla / xla_data . pb . h "" <hashtag> include </hashtag> "" tensorflow / core / framework / op_kernel . h "" <hashtag> include </hashtag> "" tensorflow / core / lib / hash / hash . h "" <hashtag> include </hashtag> "" tensorflow / core / platform / human_readable_json . h "" void test_op ( cudastream_t stream , void * * buffers , const char * opaque , size_t opaque_len ) { std : : cout < < "" executing test_op ( ) \ \ n "" ; } class xlatestop : public xlaopkernel { public : explicit xlatestop ( opkernelconstruction * ctx ) { } void compile ( xlaopkernelcontext * ctx ) override { : : xla : : xlabuilder * const builder = ctx - > builder ( ); : : xla : : xlaop input = ctx - > input ( <number> ); / / use output aliasing to reuse input buffer for output std : : vector < std : : pair < : : xla : : shapeindex , std : : pair < int64 , : : xla : : shapeindex > > > output_operand_aliasing = { <happy> : xla : : shapeindex { } , { <number> , : : xla : : shapeindex { } } } }; : : xla : : shape output_shape = builder - > getshape ( input ) . value ( ); : : xla : : xlaop output_op = builder - > reporterrororreturn ( : : xla : : customcall ( builder , "" test_op "" , { input } , output_shape , / * opaque =*/ "" "" , / * has_side_effect =*/ false , output_operand_aliasing , / * literal =*/ nullptr ) ); ctx - > setoutput ( <number> , output_op ) ; } }; register_xla_op ( name ( "" xlatestop "" ) , xlatestop ) ; xla_register_custom_call_target ( test_op , "" cuda "" ); ` ` ` ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"need add perl to supported api languages this page lists all the languages with support for the tensorflow api : <url> thanks to an official grant from the perl foundation , we have recently released a perl api <url> how do we go about getting perl added to the list ?",1
tensorflow/tensorflow,"make time series_from_array ( ) more intuitive to use <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> . <number> ( tensorflow - macos ) # # # custom code yes # # # os platform and distribution macos <number> . <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? timeseriesgenerator ( ) is deprecated , and tensorflow docs encourage the use of time series_from_array ( ) instead . however , this is not intuitive to use , requiring far more boilerplate code to achieve the same effect . in addition , the results are not as expected . i spent hours debugging my code to realise time series_from_array ( ) is not behaving as expected . using the code below , i would expect there to be <number> different inputs and outputs , however , there are only <number> . running the same code with timeseriesgenerator ( ) , without the <happy> - <number> ] and [ <number> <happy> indexing , produces the expected <number> inputs and outputs . # # # standalone code to reproduce the issue ` ` ` shell x = np . zeros ( ( <number> , <number> ) ) y = np . array ( [ <number> , <number> , <number> , <number> , <number> , <number> , <number> , <number> , <number> , <number> ] ) test = keras . utils . timeseries_dataset_from_array ( x [ : - <number> ] , y [ <number> <happy> , sequence_length = <number> , batch_size = <number> ) list ( test . as_numpy_iterator ( ) ) ` ` ` # # # relevant log output ` ` ` shell [ ( array ( [ [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] ] ) , array ( [ <number> , <number> , <number> ] ) ) ] ` ` ` # # # code for timeseriesgenerator ( ) ( expected output ) ` ` ` python test = keras . preprocessing . sequence . timeseriesgenerator ( x , y , length = <number> ) test [ <number> ] ` ` ` # # # expected output ` ` ` shell ( array ( [ [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] , [ [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] , [ <number> . , <number> . , <number> . ] ] ] ) , array ( [ <number> , <number> , <number> , <number> , <number> , <number> ] ) ) ` ` ` </details>",1
tensorflow/tensorflow,"how to build with xnnpack <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device qualcomm # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i want to build without xnnpack , or with xnnpack - qs8 diabled . how to achieve that ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i tried with "" - - define = tflite_with_xnnpack_qu8 = false - - define = tflite_with_xnnpack_qs8 = false "" , and "" - - define tflite_with_xnnpack = false "" . but it seems that they did not work . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"freeze zero weights during training <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell is there any way to freeze zero weights during model retraining ? ( this is different than freezing a layer which is already available in tf ) this is relevant to model pruning . for example , i want to retrain only non - zero weights of a pruned model . is it possible to use tf . indexedslices efficiently for this ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i think we can achieve this if there ' s any feature in tf to calculate grads only for non - zero weights . i could not find any method to achieve this <user> . function def train_step ( inputs , targets ) : with tf . gradienttape ( ) as tape = model ( inputs , training = true ) loss_value = loss_fn ( targets , predictions ) grads = tape . gradient ( loss_value , model . trainable_weights ) optimizer . apply_gradients ( zip ( grads , model . trainable_weights ) ) return loss_value ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"cast int32 to bfloat16 does not run on a100 gpu <details> <summary> click to expand </summary> # # # issue type feature request # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tensorflow version <number> . <number> - dev20230215 # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> / <number> # # # gpu model and memory single a100 8 0 g # # # current behaviour ? ` ` ` shell when using tf . cast to cast tf . int32 tensor to tf . bfloat16 tensor , op run on gpu . when i convert int32 - > bfloat16 , it run on cpu . when i convert int32 - > float32 - > bfloat16 , it run on gpu . is it expected ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf from tensorflow . keras import mixed_precision tf . debugging . set_log_device_placement ( true ) policy = mixed_precision . policy ( ' mixed_bfloat16 ' ) print ( policy . name ) mixed_precision . set_global_policy ( policy ) class toy_layer ( tf . keras . layers . layer ) : def build ( self , input_shape ) : self . kernel = self . add_weight ( ' kernel ' , ( input_shape [ - <number> ] , <number> ) ) def call ( self , inputs ) : out = tf . linalg . matmul ( inputs , self . kernel ) out2 = tf . ones ( ( <number> , <number> ) , dtype = tf . int32 ) <hashtag> out 2 </hashtag> = tf . cast ( out2 , tf . float32 , name = "" cast_out2_1 "" ) out2 = tf . cast ( out2 , out . dtype , name = "" cast_out2_2 "" ) out3 = out * out2 return out3 layer = toy_layer ( ) y = layer ( tf . ones ( ( <number> , <number> ) ) ) ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt mixed_bfloat16 <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia a100 8 0 gb pcie , pci bus id : <number> <time> . <number> , compute capability : <number> input : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] input : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> _eagerconst : ( _eagerconst ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] _eagerconst : ( _eagerconst ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> input : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] input : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> _eagerconst : ( _eagerconst ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] _eagerconst : ( _eagerconst ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> dims : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] dims : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> fill : ( fill ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] fill : ( fill ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op fill in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> seed : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] seed : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> statelessrandomgetkeycounter : ( statelessrandomgetkeycounter ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] statelessrandomgetkeycounter : ( statelessrandomgetkeycounter ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> key_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] key_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> counter_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] counter_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op statelessrandomgetkeycounter in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> shape : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] shape : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> key : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] key : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> counter : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] counter : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> alg : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] alg : ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> statelessrandomuniformv2 : ( statelessrandomuniformv2 ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] statelessrandomuniformv2 : ( statelessrandomuniformv2 ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op statelessrandomuniformv2 in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> sub : ( sub ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] sub : ( sub ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op sub in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> mul : ( mul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] mul : ( mul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op mul in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> addv2 : ( addv2 ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] addv2 : ( addv2 ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op addv2 in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> resource_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] resource_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> varhandleop : ( varhandleop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] varhandleop : ( varhandleop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op varhandleop in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> resource : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] resource : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> assignvariableop : ( assignvariableop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] assignvariableop : ( assignvariableop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op assignvariableop in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> cast : ( cast ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] cast : ( cast ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> y_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op cast in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> resource : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] resource : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> readvariableop : ( readvariableop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] readvariableop : ( readvariableop ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> value_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] value_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op readvariableop in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op cast in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> a : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] a : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> b : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] b : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> matmul : ( matmul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] matmul : ( matmul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> product_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] product_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op matmul in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op _eagerconst in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> dims : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] dims : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] value : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> fill : ( fill ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] fill : ( fill ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] output_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op fill in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <kiss> ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _devicearg ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> cast : ( cast ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] cast : ( cast ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> y_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op cast in device / job : localhost / replica : <number> / task : <number> / device : cpu : <number> <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] <kiss> ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] y : ( _arg ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> mul : ( mul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] mul : ( mul ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / placer . cc : <number> ] z_retval : ( _retval ) : / job : localhost / replica : <number> / task : <number> / device : gpu : <number> <number> - <number> - <number> <time> . <number> tensorflow / core / common_runtime / eager / execute . cc : <number> ] executing op mul in device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> ` ` ` </details>",1
tensorflow/tensorflow,"feature request : more descriptive get_weights ( ) method # # # issue type feature request # # # tensorflow version tf <number> . <number> # # # os platform and distribution windows <number> # # # python version <number> . <number> hello currently , i can get the weights of each layer of a keras model by calling tensorflow ' s ` get_weights ( ) ` method . additionally , this method returns the bias of the layer if the ` use_bias ` term is true . there is no mention of the order in which weights and biases are returned in tensorflow or keras documentation . when the method returns two or more numpy arrays , how can we determine what these layers represent ? it would be awesome if the ` get_weights ( ) ` method could also indicate whether a particular numpy array is a weights array , bias array , etc . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf def get_model ( <sad> mnist_model = tf . keras . sequential ( [ tf . keras . layers . flatten ( input_shape =( <number> , <number> ) ) , tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) ] ) return mnist_model def get_model_state_dict ( model ) : for index , layer in enumerate ( model . layers ) state_dict = ( get_model_state_dict ( get_model ( ) ) ) ` ` `",1
tensorflow/tensorflow,"proto file missing in nightly wheels # # # have you reproduced the bug with tf nightly ? yes # # # tensorflow version tf <number> # # # current behaviour ? i work on the tensorflow - directml plugin , and we use the . proto files included in the tf wheel to generate pb . cc / pb . h files needed by the plugin . one of the files needed is the tensorflow / tsl / profiler / protobuf / xplane . proto file , which has not been included in the nightly wheels so far . would it be possible to make sure that it ' s included for tf <number> ?",1
tensorflow/tensorflow,"how to change specific body line colour eg . hip - knee - ankle line in swift <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution macos # # # mobile device iphone x # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell unable to change specific body line colour eg . hip - knee - ankle line in swift ` ` ` # # # standalone code to reproduce the issue ` ` ` shell how to change specific body line colour eg . hip - knee - ankle line in swift ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"keras . models . clone_model not working <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell to clone a model using keras . models . clone_model to keep ema weights ` ` ` # # # standalone code to reproduce the issue ` ` ` shell when cloning a model that inherits from ( tf . keras . models . model ) , can not use keras . models . clone_model . ` ` ` # # # relevant log output ` ` ` shell valueerror traceback ( most recent call last ) input in [ <number> ] , in < cell line : <number> > ( ) - - - - > <number> gan = gan ( generator = generator , discriminator = discriminator ) <number> # define losses <number> generator_loss , discriminator_loss = get_loss ( ' nsl ' ) input in [ <number> ] , in adatgan . __init__ ( self , generator , discriminator ) <number> super ( adatgan , self ) . __init__ ( ) <number> self . generator = generator - - - - > <number> self . ema_generator = tf . keras . models . clone_model ( self . generator ) <number> self . discriminator = discriminator <number> self . noise_dim = noise_dim file ~ / anaconda3 / envs / tf28 / lib / python3 . <number> / site - packages / keras / models / cloning . py : <number> , in clone_model ( model , input_tensors , clone_function ) <number> return _clone_sequential_model ( <number> model , input_tensors = input_tensors , layer_fn = clone_function <number> ) <number> else : - - > <number> return _clone_functional_model ( <number> model , input_tensors = input_tensors , layer_fn = clone_function <number> ) file ~ / anaconda3 / envs / tf28 / lib / python3 . <number> / site - packages / keras / models / cloning . py : <number> , in _clone_functional_model ( model , input_tensors , layer_fn ) <number> raise valueerror ( <number> "" expected ` model ` argument "" <number> "" to be a functional ` model ` instance , "" <number> f "" got a ` sequential ` instance instead : { model } "" <number> ) <number> if not model . _is_graph_network : - - > <number> raise valueerror ( <number> "" expected ` model ` argument "" <number> "" to be a functional ` model ` instance , "" <number> f "" but got a subclassed model instead : { model } "" <number> ) <number> new_input_layers = { } # cache for created layers . <number> if input_tensors is not none : <number> # make sure that all input tensors come from a keras layer . valueerror : expected ` model ` argument to be a functional ` model ` instance , but got a subclassed model instead object at 0x 7 efff88591c0 > ` ` ` </details>",1
tensorflow/tensorflow,how to train with mobilenet models using model_spec . get ( ) ? <repeated> hello . i am using this tutorial to train my dataset for the object detection and efficientdet_lite ( <number> - <number> ) models are working fine . <url> i found a documentation for * * model_spec . get * * and it seems you can train only with * efficientdet_lite <emphasis> * models . <url> is there any way to train the model with * * ssd - mobilenet - v2 * * or * * ssd - mobilenet - v1 * * models and what i should write inside of * * model_spec . get ( ' . <repeated> ' )* * ? <repeated> look forward to hearing from you,1
tensorflow/tensorflow,"request : some way to initialise a dynamically - sized state variable in tflite * * system information * * - os platform and distribution ( e . g . , linux ubuntu <number> <sad> * all <emphasis> * - tensorflow installed from ( source or binary ) : * binary <emphasis> * - tensorflow version ( or github sha if from source ) : * * <number> . <number> * * * * provide the text output from tflite_convert * * ` ` ` tensorflow . python . ops . op_selector . unliftableerror : unable to lift tensor < tf . tensor ' zeros : <number> ' shape =( none , none ) dtype = float32 > because it depends transitively on placeholder < tf . operation ' arr ' type = placeholder > via at least one path , e . g . : zeros ( fill ) < - shape ( shape ) < - arr ( placeholder ) ` ` ` * * standalone code to reproduce the issue * * i want to define a stateful graph where the shape of the state variable is defined when model is loaded , not when it ' s saved . as a simple example - lets say i just want to compute a temporal difference - ie . a graph that returns the difference between the input in two consecutive calls . the following should pass : ` ` ` func = load_tflite_model_func ( tflite_model_file_path ) runtime_shape = <number> , <number> rng = np . random . randomstate ( <number> ) ims = [ rng . randn ( * runtime_shape ) . astype ( np . float32 ) for _ in range ( <number> ) ] assert np . allclose ( func ( ims [ <number> ] ) , ims [ <number> ] ) assert np . allclose ( func ( ims [ <number> ] ) , ims [ <number> ] - ims [ <number> ] ) assert np . allclose ( func ( ims [ <number> ] ) , ims [ <number> ] - ims [ <number> ] ) ` ` ` however as far as i know there is not way i can save a model , or define some function ` load_tflite_model_func ` for loading it , that would make this work for any ` runtime_shape ` , because variables in the graph can only be saved with a pre - determined size . a full stand - alone notebook demonstrating the issue is here * * request is for tflite to be able to initialise state - variables , where the shape of the initialised variable can depend on the input - shape * * see also <url>",1
tensorflow/tensorflow,"support for keepdims and padding in tf . boolean_mask or tf . ragged . boolean_mask <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? given two example tensors ` input ` and ` mask ` : ` ` ` > > > input = tf . random . normal ( [ <number> , <number> ] ) > > > input < tf . tensor : shape =( <number> , <number> , <number> ) , dtype = float32 , numpy = array ( [ [ [ <number> , - <number> , <number> , - <number> , <number> ] , [ <number> , <number> , - <number> , <number> , <number> ] , [ <number> , - <number> , - <number> , - <number> , - <number> ] ] , [ [ - <number> , - <number> , <number> , <number> , <number> ] , [ - <number> , <number> , - <number> , <number> , - <number> ] , [ <number> , <number> , - <number> , <number> , <number> ] ] ] , dtype = float32 ) > > > > mask = tf . constant ( [ [ <number> , <number> ] , [ <number> , <number> ] ] ) > > > mask < tf . tensor : shape =( <number> , <number> ) , dtype = int32 , numpy = array ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = int32 ) > ` ` ` i need to mask out ` input ` according to ` mask ` where values are <number> . however , since the number of masked out elements for each example in the batch ` input ` might be different , to keep the output a valid tensor , the output should be : ` ` ` > > > masked_input < tf . tensor : shape =( <number> , <number> , <number> ) , dtype = float32 , numpy = array ( [ [ [ <number> , <number> , - <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> , <number> ] ] , [ [ - <number> , - <number> , <number> , <number> , <number> ] , [ <number> , <number> , - <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> , <number> ] ] ] , dtype = float32 ) > ` ` ` i . e . in the output , the masked input keeps only elements where ` mask ` is <number> , and , with zero - padding at the end to ensure that the output is a valid tensor . i have searched around and tried using : <number> . ` tf . gather ` , however , still can not figure out how to proceed . <number> . ` tf . boolean_mask ` , however , it does not support masking but just drops the first ( zeroth ) dimension , as shown below : ` ` ` > > > tf . boolean_mask ( input , mask ) < tf . tensor : shape =( <number> , <number> ) , dtype = float32 , numpy = array ( [ [ <number> , <number> , - <number> , <number> , <number> ] , [ - <number> , - <number> , <number> , <number> , <number> ] , [ <number> , <number> , - <number> , <number> , <number> ] ] , dtype = float32 ) > ` ` ` <number> . ` tf . ragged . boolean_mask ` , this is by far the closest one to what i want , it keeps the dimension , however , still does not support masking , so the result is a ragged tensor . <repeated> similar issues are mentioned in github : <url> in short tensor = [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] mask = np . array ( [ [ true , false , true ] , [ false , false , true ] , [ true , true , true ] ] ) boolean_mask ( tensor , mask , keepdims = false ) # [ <number> , <number> , <number> , <number> , <number> , <number> ] boolean_mask ( tensor , mask , keepdims = true , pad_val = <number> ) # [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] ` ` ` # # # standalone code to reproduce the issue ` ` ` as mentioned above ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"tensorflow lite image classification - add a new model hello . i tested your tensorflow lite image classification android app and it works perfect with these models ( mobilenet v1 , efficientnet lite0 , efficientnet lite1 , efficientnet lite2 ) which are downloading automatically by the download . gradle file . <url> then i wanted to add my own classification model which has different labels compared to models mentioned above . i placed my model in * assets <emphasis> * folder app \ \ src \ \ main \ \ assets and then did some modification in * * imageclassifierhelper . kt * * file . val modelname = when ( currentmodel ) { model_mymodel - > "" mymodel . tflite "" model_mobilenetv1 - > "" mobilenetv1 . tflite "" else - > "" mymodel . tflite "" } companion object { const val delegate_cpu = <number> const val delegate_gpu = <number> const val delegate_nnapi = <number> const val model_mymodel = <number> const val model_mobilenetv1 = <number> private const val tag = "" imageclassifierhelper "" } while i build and launch the app , i received the following error ' app ' on xiaomi . <repeated> install successfully finished in <number> s <number> ms . $ adb shell am start - n "" org . tensorflow . lite . examples . imageclassification / org . tensorflow . lite . examples . imageclassification . mainactivity "" - a android . intent . action . main - c android . intent . category . launcher connected to process <number> on device ' xiaomi - . <repeated> ' . * * can you please let me know how to fix the issue and do i need to do other modification in scripts to make it work with my model . look forward to hearing from you and thank you in advance",1
tensorflow/tensorflow,"expecting the same names when converting to tflite model hello , i have created very simple model , and i want to export it to tflite model , to benchmark it on android device , and run simple inferences . this is code to create model : ` ` ` python import tensorflow as tf import numpy as np from tensorflow import keras from tensorflow . keras import layers input0 = keras . input ( shape =( <number> , ) , name = "" input0 "" ) input1 = keras . input ( shape =( <number> , ) , name = "" input1 "" ) input2 = keras . input ( shape =( <number> , ) , name = "" input2 "" ) layer0 = layers . dense ( <number> , activation = "" relu "" , name = "" layer0 "" ) ( input0 ) layer1 = layers . dense ( <number> , activation = "" relu "" , name = "" layer1 "" ) ( input1 ) layer2 = layers . dense ( <number> , activation = "" relu "" , name = "" layer2 "" ) ( input2 ) x0 = layers . concatenate ( [ layer0 , layer1 , layer2 ] ) x1 = layers . concatenate ( [ layer0 , layer1 , layer2 ] ) output0 = layers . dense ( <number> , name = "" output0 "" )(x0 ) output1 = layers . dense ( <number> , name = "" output1 "" )(x 1 ) model = keras . model ( inputs =[ input0 , input1 , input2 ] , outputs =[ output0 , output1 ] , ) model . compile ( optimizer = ' sgd ' , loss = ' mean_squared_error ' ) converter = tf . lite . tfliteconverter . from_keras_model ( model ) converter . optimizations = [ tf . lite . optimize . default ] tflite_model = converter . convert ( ) with open ( ' model . tflite ' , ' wb ' ) as f : f . write ( tflite_model ) ` ` ` environment : google colab i expect , that tflite model would have ` input0 ` , ` input1 ` , ` input2 ` input names , and ` output0 ` , ` output1 ` output names . but . <repeated> real names are ` serving_default_input1 : <number> ` , ` serving_default_input2 : <number> ` , ` statefulpartitionedcall : <number> ` , ` statefulpartitionedcall : <number> `",1
tensorflow/tensorflow,"tensorflow lite ios interpreter production logs <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution ios # # # mobile device ios # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell the log outputs of the interpreter cannot be disabled for production use , but we would need that . see : <url> would be nice if you could introduce another option for the interpreter where you could set log outputs enabled / disabled or even with loglevels . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell interpreter ( modelpath : path , options ` ` ` # # # relevant log output ` ` ` shell initialized tensorflow lite runtime . ` ` ` </details>",1
tensorflow/tensorflow,"feature request ( tensorflow lite ) : implement mechanism to load flex ops in c for android # # # system information - * * have i written custom code * * : no - * * os platform and distribution * * : macos monterey <number> - * * tensorflow installed from * * : source - * * tensorflow version * * : <number> . <number> - * * python version * * : <number> . <number> - * * bazel version * * : <number> . <number> - * * gcc / compiler version * * : apple clang <number> . <number> ( clang - <number> . <number> ) - * * cuda / cudnn version * * : not using cuda - * * gpu model and memory * * : not using gpu acceleration - * * exact command to reproduce * * request , no # # # describe the problem this is a feature request . in tensorflow lite <number> and higher , if a model contains flex ops you must go through java to load the flex library ( see [ nativeinterpreterwrapper . java ] ( <url> and [ flexdelegate . java ] ( <url> we develop a cross - platform app where most of the ml inference logic runs in a separate c library . we test that library separately from the rest of the app using gtest , without going through the android runtime . for our purposes , it would be much more convenient if we could load the flex library directly in c . as a tentative implementation , we could move the logic in ` nativeinterpreterwrapper . java ` to c , so that we could automatically load flex ops when creating models from the c api with ` tflitemodelcreate ` . the existing java classes could be kept as shallow wrappers to the underlying c implementation . java and c entry points could be toggled at build time through a flag . is this something that you would consider implementing , and , if not , would you accept prs that go in that direction ?",1
tensorflow/tensorflow,"` tf . identity ` docs missing instructions , gpu to cpu # # # issue type documentation feature request # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? the [ docs ] ( <url> do not describe how ` . cpu ( ) ` should be accomplished . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf tf . constant ( <number> . ) . cpu ( ) ` ` ` # # # relevant log output ` ` ` shell _eagertensorbase . cpu ( from tensorflow . python . framework . ops ) is deprecated and will be removed in a future version . instructions for updating tf . identity instead . ` ` `",1
tensorflow/tensorflow,"document behavior of tf . keras . layers . bidirectional with return_state = true <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # source source # # # tensorflow version <number> # # # custom code no </details> it is unclear from the documentation how a ` tf . keras . layers . bidirectional ` layer will behave if ` return_state ` is true ; ie , in what order the hidden / cell states will be returned , and what , if any , interaction there is with ` merge_mode ` . per the source code , any states of forward layer are returned after the output ( s ) , followed by any states of the backward layer . this should be documented . ` ` ` if self . return_state : states = y [ <number> <happy> + y_rev [ <number> <happy> y = y [ <number> ] y_rev = y_rev [ <number> ] # . <repeated> if self . return_state : if self . merge_mode is none output + states return [ output ] + states return output ` ` `",1
tensorflow/tensorflow,"how to enable gspmd ? <details> <summary> click to expand </summary> # # # issue type support # # # source binary # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution linux # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell we are trying to utilize gspmd on a server with <number> gpus , we find the ut - test cases as follows : <url> to dump the irs , we enable the environment variables as : tf_dump_graph_prefix <annoyed> tmp / generated \ \ tf_xla_flags = "" - - tf_xla_clustering_debug - - tf_xla_auto_jit = <number> "" \ \ xla_flags = "" - - xla_dump_hlo_as_text - - xla_dump_to <annoyed> tmp / generated "" \ \ my / tensorflow / program "" . but , only <number> irs are saved : mark_for_compilation . pbtxt , mark_for_compilation_annotated . pbtxt , before_mark_for_compilation . pbtxt , before_increase_dynamism_for_auto_jit_pass . pbtxt . none of them is related to spmd pass . it seems that our current run does not enable gspmd functionality at all . is there any tutorial or instructions for us to follow to enable gspmd on multiple gpus ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell from tensorflow . compiler . xla . experimental . xla_sharding import xla_sharding from tensorflow . python . ops import array_ops from tensorflow . python . ops import math_ops from tensorflow . python . framework import test_util from tensorflow . python . framework import dtypes from tensorflow . python . framework import ops import numpy as np from tensorflow . python . eager import def_function class xlashardingtest ( test_util . tensorflowtestcase ) : def test_dot_split ( self ) : <user> . function def split_helper ( tensor ) : device_mesh = np . array ( [ [ <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> ] ] ) split_tensor = xla_sharding . mesh_split ( tensor , device_mesh , [ <number> , <number> ] ) self . assertisinstance ( split_tensor , ops . tensor ) split_sharding = xla_sharding . get_tensor_sharding ( split_tensor ) split_shape = xla_sharding . get_sharding_tile_shape ( split_sharding ) expected_shape = [ <number> , <number> ] self . assertequal ( expected_shape , split_shape ) y_tensor = array_ops . ones ( [ <number> , <number> ] , dtype = dtypes . float32 ) y_split = xla_sharding . mesh_split ( y_tensor , device_mesh , [ <number> , <number> ] ) result = math_ops . matmul ( split_tensor , y_split ) device_mesh = np . array ( [ [ <number> , <number> ] , [ <number> , <number> ] , [ <number> , <number> ] , [ <number> , <number> ] ] ) result = xla_sharding . mesh_split ( result , device_mesh , [ <number> , <number> ] ) result = math_ops . sqrt ( result ) result = xla_sharding . mesh_split ( result , device_mesh , [ <number> , <number> ] ) return result in_tensor = <number> * np . sqrt ( <number> ) * array_ops . ones ( [ <number> , <number> ] , dtype = dtypes . float32 ) result = split_helper ( array_ops . ones ( [ <number> , <number> ] , dtype = dtypes . float32 ) ) self . assertallequal ( in_tensor , result ) if __name__ = = "" __main__ "" = xlashardingtest ( ) xlasharding . test_dot_split ( ) ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"support for c + + builder <number> <details> <summary> click to expand </summary> # # # issue type others # # # source binary # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution windows <number> x64 # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell is there anyway to support c + + builder <number> ? current static libs are in a format not supported by c + + builder . static libs are only in coff format so they cant be linked with in c + + builder , they need to be in omf format for c + + builder to link the static libs . the only way i can solve this is to do a loadlibrary in code and get proc address of each function ` ` ` # # # standalone code to reproduce the issue ` ` ` shell no able to link static libs only in msvc not c + + builder . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"leakyrelu in tensorflow lite with the hexagon delegate not supported when using a tflite model ( <number> - bits quantized via tensorflow lite conversion framework ) that includes the activation function "" leakyrelu "" , the hexagon delegate from tensorflow framework cannot perform the dnn inference on the whole graph , but rather it falls back to the cpu / xnnpack delegate . this is due to the fact that ' leakyrelu ' operation is not supported by the hexagon delegate ( confirmed in tensorflow doc : <url> when using relu activation function ( and relu6 as well ) , we can see below that the tf hexagon delegate can process the whole dnn graph , unfortunately , the qualitative results i get are much worse , hence the need of having ' leaky relu ' implemented in the hexagon delegate . we can easily reproduce this behavior by using tensorflow benchmark tool ( see below ) could we consider to implement leaky relu in tensorflow lite dsp delegate ? * * system information * * - os platform and distribution ) : android <number> , ndk r21e - tensorflow installed from ( source or binary ) : from source using the release tag ' <number> . <number> ' - tensorflow version ( or github sha if from source ) : <number> . <number> * * provide the text output from tflite_convert * * ` ` ` warning : absl : found untraced functions such as _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op , _jit_compiled_convolution_op while saving ( showing <number> of <number> ) . these functions will not be directly callable after loading . c :\\ users \ \ eelfahsi \ \ miniconda3 \ \ envs \ \ ml \ \ lib \ \ site - packages \ \ tensorflow \ \ lite \ \ python \ \ convert . py : <number> : userwarning : statistics for quantized inputs were expected , but not specified ; continuing anyway . warnings . warn ( "" statistics for quantized inputs were expected , but not "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / mlir / lite / python / tf_tfl_flatbuffer_helpers . cc : <number> ] ignored output_format . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / mlir / lite / python / tf_tfl_flatbuffer_helpers . cc : <number> ] ignored drop_control_dependency . <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / reader . cc : <number> ] reading savedmodel from : c :\\ users \ \ eelfahsi \ \ appdata \ \ local \ \ temp \ \ tmp731h7tgk <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / reader . cc : <number> ] reading meta graph with tags { serve } <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / reader . cc : <number> ] reading savedmodel debug info ( if present ) from : c :\\ users \ \ eelfahsi \ \ appdata \ \ local \ \ temp \ \ tmp731h7tgk <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / mlir / mlir_graph_optimization_pass . cc : <number> ] mlir v1 optimization pass is not enabled <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / loader . cc : <number> ] restoring savedmodel bundle . <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / loader . cc : <number> ] running initialization op on savedmodel bundle at path : c :\\ users \ \ eelfahsi \ \ appdata \ \ local \ \ temp \ \ tmp731h7tgk <number> - <number> - <number> <time> . <number> : i tensorflow / cc / saved_model / loader . cc : <number> ] savedmodel load for tags { serve }; status : success : ok . took <number> microseconds . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / mlir / tensorflow / utils / dump_mlir_util . cc : <number> ] disabling mlir crash reproducer , set env var ` mlir_crash_reproducer_directory ` to enable . found <number> files belonging to <number> classes . fully_quantize : <number> , inference_type : <number> , input_inference_type : <number> , output_inference_type : <number> process finished with exit code <number> ` ` ` * * standalone code to reproduce the issue * * here is a colab link with the code used to : - generate the model that include leaky relu op . - quantize and save the model in int8 ops . colab code link : <url> * * link to the quantized model : * * [ my_quant_model_leaky_relu . zip ] ( <url> the tool below from tensorflow allow to run any model , when running the model above , it fails because ' leakyrelu ' is not supported . * * link to tensorflow benchmark * * : <url> tflite benchmark command line : ` ` ` . / tf_benchmark - - graph <annoyed> data / local / tmp / my_quant_model_leaky_relu . tflite - - require_full_delegation = true - - use_hexagon = true - - hexagon_lib_path <annoyed> data / local / tmp / ` ` ` when performing the inference on dsp using tensorflow lite benchmark tool for android , we get the following error message : ` ` ` . / tf_benchmark - - use_hexagon = true - - graph = . / my_quant_model_leaky_relu . tflite - - require_full_delegation = true - - hexagon_lib_path <annoyed> data / local / tmp / < starting log parameter values verbosely : [ <number> ] graph : [ . / my_quant_model_leaky_relu . tflite ] require full delegation : [ <number> ] use hexagon : [ <number> ] hexagon lib path : [ / data / local / tmp / ] loaded model . / my_quant_model_leaky_relu . tflite info : initialized tensorflow lite runtime . loaded libcdsprpc . so hexagon delegate created . info : tflitehexagondelegate delegate : <number> nodes delegated out of <number> nodes with <number> partitions . verbose : replacing <number> node ( s ) with delegate ( tflitehexagondelegate ) node , yielding <number> partitions . disallowed cpu fallback detected . benchmarking failed . ` ` ` if i disable the usage of the dsp , then it works properly tflite benchmark command line : ` ` ` . / tf_benchmark - - graph <annoyed> data / local / tmp / my_quant_model_leaky_relu . tflite ` ` ` logs ` ` ` . / tf_benchmark - - graph = . / my_quant_model_leaky_relu . tflite starting ! log parameter values verbosely : [ <number> ] graph : [ . / my_quant_model_leaky_relu . tflite ] loaded model . / my_quant_model_leaky_relu . tflite info : initialized tensorflow lite runtime . the input model file size ( mb ) : <number> initialized session in <number> . 5 7 ms . running benchmark for at least <number> iterations and at least <number> seconds but terminate if exceeding <number> seconds . count = <number> first = <number> curr = <number> min = <number> max = <number> avg = <number> std = <number> running benchmark for at least <number> iterations and at least <number> seconds but terminate if exceeding <number> seconds . count = <number> first = <number> curr = <number> min = <number> max = <number> avg = <number> std = <number> inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> , inference ( avg ) : <number> note : as the benchmark tool itself affects memory footprint , the following is only approximate to the actual memory footprint of the model at runtime . take the information at your discretion . memory footprint delta from the start of the tool ( mb ) : init = <number> overall = <number> . <repeated> ` ` ` if i replace leakyrelu by relu , then it works properly here is a colab link with the code used to : - generate the model that include relu op . - quantize and save the model in int8 ops . colab code link : <url> * * link to the quantized model : * * [ my_quant_model_relu . zip ] ( <url> tflite benchmark command line : ` ` ` . / tf_benchmark - - use_hexagon = true - - graph = . / my_quant_model_relu . tflite - - require_full_delegation = true - - hexagon_lib_path <annoyed> data / local / tmp / ` ` ` logs ` ` ` . / tf_benchmark - - use_hexagon = true - - graph = . / my_quant_model_relu . tflite - - require_full_delegation = true - - hexagon_lib_path <annoyed> data / local / tmp / < starting ! log parameter values verbosely : [ <number> ] graph : [ . / my_quant_model_relu . tflite ] require full delegation : [ <number> ] use hexagon : [ <number> ] hexagon lib path : [ / data / local / tmp / ] loaded model . / my_quant_model_relu . tflite info : initialized tensorflow lite runtime . loaded libcdsprpc . so hexagon delegate created . info : tflitehexagondelegate delegate : <number> nodes delegated out of <number> nodes with <number> partitions . verbose : replacing <number> node ( s ) with delegate ( tflitehexagondelegate ) node , yielding <number> partitions . explicitly applied hexagon delegate , and the model graph will be completely executed by the delegate . the input model file size ( mb ) : <number> initialized session in <number> . 2 2 ms . running benchmark for at least <number> iterations and at least <number> seconds but terminate if exceeding <number> seconds . count = <number> first = <number> curr = <number> min = <number> max = <number> avg = <number> std = <number> running benchmark for at least <number> iterations and at least <number> seconds but terminate if exceeding <number> seconds . count = <number> first = <number> curr = <number> min = <number> max = <number> avg = <number> std = <number> inference timings in us : init : <number> , first inference : <number> , warmup ( avg ) : <number> , inference ( avg ) : <number> note : as the benchmark tool itself affects memory footprint , the following is only approximate to the actual memory footprint of the model at runtime . take the information at your discretion . memory footprint delta from the start of the tool ( mb ) overall = <number> ` ` `",1
tensorflow/tensorflow,"making tensorflow . tile similar to numpy . tile and torch . tile <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell running this code : ` tf . tile ( tf . ones ( ( <number> , ) ) , ( <number> ) ) ` will currently give an error for tensorflow since the dimension length of the input and the multiples is different . numpy and pytorch handle this by modifying the dimension of the multiples or the input argument . i believe it would be nice if tensorflow did this too . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf tf . tile ( tf . ones ( ( <number> , ) ) , ( <number> ) ) ` ` ` # # # relevant log output ` ` ` shell tensors = pywrap_tfe . tfe_py_execute ( ctx . _handle , device_name , op_name , tensorflow . python . framework . errors_impl . invalidargumenterror multiples argument to be a vector of length <number> but got length <number> [ op : tile ] ` ` ` </details>",1
tensorflow/tensorflow,"supporting int32 value type in textfileinitializer . <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution linux # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell in [ textfileinitializer ] ( <url> we have a validation to check whether the value_dtype is of type tf . int64 , wondering why do we have such constraints ? the context is that , we are trying to initialize a vocabulary lookup table in the tensorflow model and would like to save memory for the hashtable , both the key and value type can be of int32 type , but seems that the value type is restricted to int64 . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell initializer = tf . lookup . textfileinitializer ( vocab_file , key_dtype = tf . int64 , key_index = <number> , value_dtype = tf . int32 , value_index = <number> , delimiter = "" "" ) ` ` ` # # # relevant log output ` ` ` shell [ <number> ] <stderr> : vocab_table = tf . lookup . staticvocabularytable ( initializer = initializer , num_oov_buckets = <number> ) [ <number> ] <stderr> : file "" / home / coder / ads - ai - offline / build / scin - azkaban / environments / development - venv / lib / python3 . <number> / site - packages / tensorflow / python / ops / lookup_ops . py "" , line <number> , in __init__ [ <number> ] <stderr> : ( dtypes . int64 , initializer . value_dtype ) ) [ <number> ] <stderr> : typeerror : invalid value dtype , expected < dtype : ' int64 ' > but got < dtype ` ` ` </details>",1
tensorflow/tensorflow,"how to build tensorflow from source with clang ? <details> <summary> click to expand </summary> # # # issue type build / install # # # source source # # # tensorflow version v2 . <number> # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version clang <number> # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i am trying to build tensorflow with clang and add options for generating coverage information ( <url> however , it fails with clang build . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell bazel build - - verbose_failures / / tensorflow / tools / pip_package : build_pip_package - - repo_env = cc = clang - - copt = - fprofile - instr - generate - - copt = - fcoverage - mapping removing ` - - copt = - fprofile - instr - generate - - copt = - fcoverage - mapping ` also fails . ` ` ` # # # relevant log output ` ` ` shell "" "" "" error : / home / jiawei / . cache / bazel / _bazel_jiawei / b619a772dd31287f179cb3c11ac7f523 / external / llvm - project / mlir / build . bazel : <number> <time> : compiling mlir / lib / support / indentedostream . cpp failed : undeclared inclusion ( s ) in rule ' <user> - project / / mlir : support ' : this rule is missing dependency declarations for the following files included by ' mlir / lib / support / indentedostream . cpp ' : ' bazel - out / k8 - opt / bin / external / llvm - project / llvm / config . cppmap ' ' bazel - out / k8 - opt / bin / external / llvm - project / llvm / demangle . cppmap ' ' bazel - out / k8 - opt / bin / external / llvm_terminfo / terminfo . cppmap ' ' bazel - out / k8 - opt / bin / external / llvm_zlib / zlib . cppmap ' target / / tensorflow / tools / pip_package : build_pip_package failed to build info : elapsed time : <number> . 4 5 3 s , critical path : <number> . 3 3 s info : <number> processes : <number> internal , <number> local . failed did not complete successfull "" "" "" ` ` ` </details>",1
tensorflow/tensorflow,adan optimizer <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell adan seems to be a superior optimizer compared to adam and its variants . can we introduce this in tf / keras optimizers ? please find a few references below : adan paper : <url> pytorch implementation : - <url> - <url> blog article <url> ` ` ` # # # standalone code to reproduce the issue ` ` ` shell na ` ` ` # # # relevant log output _no response_ </details>,1
tensorflow/tensorflow,"gpu detection src location <details> <summary> click to expand </summary> # # # issue type others # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory tesla v100 # # # current behaviour ? ` ` ` shell hello , i am a student researcher and i am only posting this as i got no response on the tensorflow google group . ( please direct me to next place to the answer ) . i am trying to develop a feature where if tensorflow does not detect a local gpu , i direct the binary to my custom application ( a gpu virtualization framework ) for it to talk to a remote gpu execute the calls . i want to know , in the large codebase of tensorflow where should i start looking for this ? i need source file where it decides "" no gpu so use cpu "" or something similar . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell na ` ` ` # # # relevant log output ` ` ` shell na ` ` ` </details>",1
tensorflow/tensorflow,"tf . image . combined_non_max_suppression crash with segmentation fault <details> <summary> click to expand </summary> # # # issue type bug # # # source binary # # # tensorflow version <number> . <number> - dev20220916 # # # custom code no # # # os platform and distribution ubuntu <number> . <number> lts (x 8 6 _64 ) # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version n / a # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell tf . image . combined_non_max_suppression crash with segmentation fault when ` max_total_size ` is given a large value ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import numpy as np import tensorflow as tf tf . image . combined_non_max_suppression ( max_output_size_per_class = <number> , max_total_size = <phone> , scores = np . ones ( ( <number> , <number> ) ) , boxes = np . ones ( ( <number> , <number> ) ) ) ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' libcuda . so . <number> ' ; dlerror : libcuda . so . <number> : cannot open shared object file : no such file or directory <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed call to cuinit : unknown error ( <number> ) <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_diagnostics . cc : <number> ] no nvidia gpu device is present : / dev / nvidia0 does not exist <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 avx512f fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : w tensorflow / core / kernels / image / non_max_suppression_op . cc : <number> ] detected a large value for ` max_total_size ` . this may cause oom error . ( max_total_size segmentation fault ( core dumped ) ` ` ` </details>",1
tensorflow/tensorflow,"include "" name "" parameter in tf . keras . models . clone_model ( ) function to give a new name to the clone . <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell when using the ` tf . keras . models . clone_model ( previous_model ) ` function , it assigns the same name as "" previous_model "" to the created clone . i want to assign a new name to the created clone . this can probably be done by passing a name parameter to the ` clone_model ( ) ` function . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell ` pre_m1 = tf . keras . sequential ( [ sentence_encoder_layer , layers . dense ( <number> , activation = "" relu "" ) , layers . dense ( <number> , activation = "" sigmoid "" ) ] , name = "" pre_trained_m1 "" ) pre_m2 = tf . keras . models . clone_model ( pre_m1 ) # in the summary , the name of pre_m2 is printed as "" pre_trained_m1 "" i . e . same as pre_m1 . print ( pre_m2 . summary ( ) ) ` ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,feature : balanced accuracy <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version tf <number> . <number> - rc3 # # # custom code no # # # os platform and distribution debian <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behaviour ? i suggest adding [ balanced accuracy ] ( <url> as additional [ keras metrics ] ( <url> it is useful for training with an imbalanced validation dataset . ( note : i see various suggestions on stackoverflow [ <number> ] ( <url> # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf metrics = [ tf . keras . metrics . balancedaccuracy ( ) ] ` ` ` # # # relevant log output _no response_ </details>,1
tensorflow/tensorflow,"tf . image . rot90 should add a note for the case k < <number> <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell the documentation only describes how the code runs when k > <number> , and dont mention k < <number> . the code shows that when k < <number> , the image will be rotated clockwise . i think a note should be added to explain what happens when k < <number> ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf results ={} try : arg_0 = tf . saturate_cast ( tf . random . uniform ( [ <number> , <number> , <number> ] , minval = - <number> , maxval = <number> , dtype = tf . int64 ) , dtype = tf . int32 ) k = - <number> results [ "" res "" ] = tf . image . rot90 ( arg_0 , k = k , ) except exception as e = "" error : "" + str ( e ) print ( results ) ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"` tf . pad ` fails when executing in ` tf . autodiff . forwardaccumulator ` <details> <summary> click to expand </summary> # # # issue type bug # # # source binary # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell ` tf . pad ` fails when executing in ` tf . autodiff . forwardaccumulator ` and throws ` typeerror ` . however , if we run ` tf . pad ` outside of ` forwardaccumulator ` with the same input , it will pass . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf input_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , minval = <number> , maxval = <number> , dtype = tf . float32 ) paddings = tf . random . uniform ( [ <number> , <number> ] , minval = <number> , maxval = <number> , dtype = tf . int64 ) mode = "" constant "" constant_values = <number> result = tf . pad ( input_tensor , paddings , mode = mode , constant_values = constant_values , ) print ( result . shape ) # pass tangent = tf . reshape ( tf . one_hot ( <number> , tf . size ( input_tensor ) , dtype = input_tensor . dtype ) , shape = input_tensor . shape ) with tf . autodiff . forwardaccumulator ( input_tensor , tangent ) as acc : result = tf . pad ( input_tensor , paddings , mode = mode , constant_values = constant_values , ) # fail ` ` ` # # # relevant log output ` ` ` shell ( <number> , <number> , <number> , <number> ) typeerror ' y ' of ' sub ' op has type int64 that does not match type int32 of argument ' x'. ` ` ` </details>",1
tensorflow/tensorflow,"when will tensorflow support fp8 ? <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version tf2 . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell pytorch added fp8 support , will tf add fp8 support sometime for both e4m3 and e5m2 ? ` ` ` # # # standalone code to reproduce the issue ` ` ` shell no source code , just a feature request question ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"please publish official wheels for apple silicon <details> <summary> click to expand </summary> # # # issue type build / install # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution macos <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ </details> # # # current behaviour ? users of macs with apple silicon ( m1 , m2 , etc . ) cannot ` pip install tensorflow ` because wheels for the architecture are not published on pypi . the current state of this ( <date> ) is that these wheels are published under a different package name , [ ` tensorflow - macos ` ] ( <url> which is released separately by apple . this makes it more complicated to install software that uses tensorflow on macos ; e . g . , a ` requirements . txt ` file might have to look something like ` ` ` tensorflow ; sys . platform ! = ' darwin ' or platform . machine ! = ' arm64 ' tensorflow - macos ; sys . platform = = ' darwin ' and platform . machine = = ' arm64 ' ` ` ` apple ' s repository , [ ` tensorflow_macos ` ] ( <url> is archived which makes it unclear where users should get support . is it here ? moreover , at the time of writing , ` tensorflow - macos ` is version <number> . <number> , which is one bugfix release ahead of the latest official ` tensorflow ` , so comparing version numbers across the two is difficult . and it leaves many other unanswered questions : are these being tested in ci ? how soon after an upstream release will ` tensorflow - macos ` get an update ? finally , both ` tensorflow ` and ` tensorflow - macos ` support intel - based macs , which is another potential point of confusion — which is preferred ? cc <user> <user> # # # standalone code to reproduce the issue ` ` ` shell ( . venv ) % arch arm64 ( . venv ) % python3 - m pip install tensorflow error : could not find a version that satisfies the requirement tensorflow ( from versions : none ) error matching distribution found for tensorflow ` ` `",1
tensorflow/tensorflow,"how to fix the "" undefined reference "" problem for the tf lite ? a suggestion . when [ cmakelists . txt ] ( <url> is used to build a shared library of tf lite , it is impossible to link with it because of the "" undefined reference "" problem . those "" undefined reference "" objects reside on the ` tensorflow ` main branch , but the following macro ` populate_tf_source_vars ` has never populated a single line of main tensorflow code for the tf lite build , yet the main code is referenced by it . no wonder we see the "" undefine reference "" . i did try to use ` populate_tf_source_vars ` to bring those missing objects to tf lite . unfortunately , however , the references will quickly escalate to reference a lot of main tensorflow stuff . that defeats the very motivation of having a "" lite "" version of tensorflow , does not it ? can you please make tf lite stop referencing those objects because they do not seem to be important ? if you do think they are important , can you make a light version of them just for tf lite ? <url> * * _the problem : _ * * ` ` ` / usr / bin / ld : / usr / local / lib / libtensorflow - lite . so : undefined reference to ` tensorflow : : profiler : : internal : : g_trace_level ' / usr / bin / ld : / usr / local / lib / libtensorflow - lite . so : undefined reference to ` tensorflow : : profiler : : tracemerecorder : : record ( tensorflow : : profiler : : tracemerecorder : : event & & ) ' / usr / bin / ld : / usr / local / lib / libtensorflow - lite . so : undefined reference to ` tensorflow : : profiler : : getcurrenttimenanos ( ) ' / usr / bin / ld : / usr / local / lib / libtensorflow - lite . so reference to ` tensorflow : : profiler : : scopedmemorydebugannotation : : threadmemorydebugannotation ( ) ' ` ` `",1
tensorflow/tensorflow,"is possible to measure height of person using tensorflow pose estimation <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i want to performe vertical jump test & want to measure distance between floor to ankle . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i want to performe vertical jump test & want to measure distance between floor to ankle . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"rsqrt does not support for tf . lite . opsset . experimental_tflite_builtins_activations_int16_weights_int8 * * system information * * - os platform and distribution ( e . g . , linux ubuntu <number> <sad> - tensorflow installed from ( source or binary ) : - tensorflow version ( or github sha if from source ) the text output from tflite_convert * * ` ` ` # copy and paste here ` ` ` * * standalone code to reproduce the issue * * provide a reproducible test case that is the bare minimum necessary to generate the problem . if possible , please share a link to colab / jupyter / any notebook . also , please include a link to a graphdef or the model if possible . * * any other info / logs * * include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached .",1
tensorflow/tensorflow,are there on - device training with tensorflow lite in arduino ides ? <repeated> <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version tf2 . <number> # # # custom code no # # # os platform and distribution windows # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell is there a way to perform on - device training with tensorflow lite in arduino ide ? i see an option for java and c + + with android but can not find the c + + doc . <repeated> seems like c ain ' t supported too . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell no code ` ` ` # # # relevant log output _no response_ </details>,1
tensorflow/tensorflow,"[ feature request ] gelu activation with the hexagon delegate * * system information * * - os platform and distribution ( e . g . , linux ubuntu <number> <sad> ubuntu <number> - tensorflow installed from ( source or binary ) : binary - tensorflow version ( or github sha if from source ) : <number> . <number> i think i ' d be able to implement this myself , but wanted to see if there was any interest in including this upstream . most of this i am writing out to make sure my own understanding is correct . # # # the problem i ' d like to add support for the gelu op to the hexagon delegate . the motivation for this is mostly for use with [ distilbert ] ( <url> which uses this activation function in its feedforward network layers . ( also used by bert , gpt - <number> , roberta , etc . ) adding this as a supported op for the hexagon delegate would avoid creating a graph partition / transferring between dsp < - - > cpu each time the gelu activation function is used . # # # how i ' d implement this gelu in tf lite is implemented as a lookup table when there are integer inputs ( [ here ] ( <url> and [ here ] ( <url> this same approach could be used for the hexagon delegate , as it has int8 / uint8 data types and also supports lookup tables . i ' d plan to do this by adding a new op builder in the delegate , populating a lookup table for each node as is currently done for the cpu version of the op , and then using the [ gather_8 ] ( <url> nnlib library function to do the lookup . # # # possible workaround a workaround i thought of going to try removing the [ pattern matching ] ( <url> for approximate gelu in mlir , and then using the approximate version of gelu ( so that using tanh and not erf ) . this will probably be slower , but should let me keep execution on the dsp . since this will then be tanh , addition , multiplication ops instead of gelu they should all be runnable by the dsp .",1
tensorflow/tensorflow,"[ tf . io ] incompatible with aws s3 filepaths <details> <summary> click to expand </summary> # # # issue type bug # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution rhel # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? apparently , from the aws logs it seems the headers are obtained but the response body is empty . i am using ` tfds ` , however its accessing ( via ` etils ` ) the tf backend to create a stream for the s3 object . hence why its reproducible from a ` tf ` method . this was the full error from tfds , for those interested , to verify that it does indeed call ` tf . io ` as a backend to execute the request . ` ` ` traceback ( most recent call last ) : file "" scripts / kecam_tester . py "" , line <number> , in <module> description = ' preprocessed tfrecords of bdd100k dataset , long only , delayed by <number> frames for each tuple . ' , file "" / home / awesome / . local / lib / python3 . <number> / site - packages / tensorflow_datasets / core / folder_dataset / write_metadata_utils . py "" , line <number> , in write_metadata f for f in data_dir . iterdir ( ) if naming . filenameinfo . is_valid ( f . name ) file "" / home / awesome / . local / lib / python3 . <number> / site - packages / tensorflow_datasets / core / folder_dataset / write_metadata_utils . py "" , line <number> , in <listcomp> f for f in data_dir . iterdir ( ) if naming . filenameinfo . is_valid ( f . name ) file "" / home / awesome / . local / lib / python3 . <number> / site - packages / etils / epath / gpath . py "" , line <number> , in iterdir for f in self . _backend . listdir ( self . _path_str ) : file "" / home / awesome / . local / lib / python3 . <number> / site - packages / etils / epath / backend . py "" , line <number> , in listdir return self . gfile . listdir ( path ) file "" / home / awesome / . local / lib / python3 . <number> / site - packages / tensorflow / python / lib / io / file_io . py "" , line <number> , in list_directory_v2 message = "" could not find directory { } "" . format ( path ) ) tensorflow . python . framework . errors_impl . notfounderror : could not find directory s3 :// s - laion / ssd - videos / new_tfrecs ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf # insert any s3 filepath tf . io . gfile . listdir ( ' s3 :// . <repeated> ' ) ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> file "" / home / awesome / . local / lib / python3 . <number> / site - packages / tensorflow / python / lib / io / file_io . py "" , line <number> , in list_directory_v2 message = "" could not find directory { } "" . format ( path ) ) tensorflow . python . framework . errors_impl . notfounderror not find directory s3 :// . <repeated> ` ` ` </details>",1
tensorflow/tensorflow,"cmake has no key to disable the build of python wrapper when building tf lite from source using xcode or android studio <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version tf2 . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> or macos # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell cmake has no key to disable the build of python wrapper when building tf lite from source using xcode or android studio . current cmake in tensorflow / tensorflow / lite with the a non optional python wrapper , which triggers errors when using tensorflow as a submodule in an xcode or android studio project # python interpreter wrapper . add_library ( _pywrap_tensorflow_interpreter_wrapper shared exclude_from_all ${ tflite_source_dir } / python / interpreter_wrapper / interpreter_wrapper . cc ${ tflite_source_dir } / python / interpreter_wrapper / interpreter_wrapper_pybind11 . cc ${ tflite_source_dir } / python / interpreter_wrapper / numpy . cc ${ tflite_source_dir } / python / interpreter_wrapper / python_error_reporter . cc ${ tflite_source_dir } / python / interpreter_wrapper / python_utils . cc ) # to remove "" lib "" prefix . set_target_properties ( _pywrap_tensorflow_interpreter_wrapper properties prefix "" "" ) target_include_directories ( _pywrap_tensorflow_interpreter_wrapper public ${ tflite_include_dirs } ) target_link_libraries ( _pywrap_tensorflow_interpreter_wrapper tensorflow - lite ${ cmake_dl_libs } ) target_compile_options ( _pywrap_tensorflow_interpreter_wrapper public ${ tflite_target_public_options } private ${ tflite_target_private_options } ) i would expect a cmake key to enable or disable the python wrapper e . g . if ( tflite_enable_pywrap ) . <repeated> endif ( ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell cmake has no key to disable the build of python wrapper when building tf lite from source using xcode or android studio . ` ` ` # # # relevant log output ` ` ` shell cannot find python . h ` ` ` </details>",1
tensorflow/tensorflow,"custom op written in c api errors when requesting gpu_device stream <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # source binary # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution linux # # # mobile device <number> # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hi there ! i was trying to write a custom op using the tensorflow c api and in the cuda version i was facing errors while requesting for the gpu_device stream . my questions are there any way to convert sp_stream ( returned by tf_getstream ( context , status ) ) to cudastream ? <number> . what is the equivalent of eigen_device <gpudevice> ( ) . stream ( ) ( c + + ) for the c api ? <number> . where is the implementation of tf_opkernelcontext . thanks , yoga ` ` ` # # # standalone code to reproduce the issue ` ` ` shell / / c + + code cudastream_t thestream = context - > eigen_device <gpudevice> ( ) . stream ( ); / / equivalent c code cudastream_t thestream = tf_getstream ( context , status ) ; ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"reproducible sparse - dense matrix multiplication <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version v2 . <number> - <number> - gd8ce9f9c301 <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell there is currently no deterministic sparse - dense matrix multiplication implementation on the gpu . i would like to have one . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf tf . keras . utils . set_random_seed ( <number> ) tf . config . experimental . enable_op_determinism ( ) m = tf . sparsetensor ( indices =[[ <number> , <number> ] , [ <number> , <number> ] ] , values = np . array ( [ <number> , <number> ] , dtype = np . float32 ) , dense_shape =( <number> , <number> ) , ) v = tf . constant ( [ [ <number> ] , [ <number> ] ] , dtype = tf . float32 ) tf . sparse . sparse_dense_matmul ( m , v ) ` ` ` # # # relevant log output ` ` ` shell unimplementederror deterministic gpu implementation of sparsetensordensematmulop is not currently available . [ op : sparsetensordensematmul ] ` ` ` </details>",1
tensorflow/tensorflow,"tensorflow gpu delegate limit resources ( feature request ) hello , sometimes when i am running the tflite model on gpu delegate it use almost all gpu resources while execution and this behaviour freeze ui because of lack resources . if it possible , will be good to have some initial option with percentage of gpu use . os android for example - gpudelegate . options ( ) . gpuusepercentage = <number> this feature will provide more control over resources .",1
tensorflow/tensorflow,"is it possible to sync tensorflow stream_executor ' s cuda stream with cuda stream outside ? in tensorflow , streams including cuda streams are handled by stream_executer . now i make some modification to tensorflow and create an independent cuda stream outside stream_executer . i was wondering is it possible to sync the independent cuda stream with the streams that wrap a cuda stream in stream_executer ? i really appreciate if someone has advices .",1
tensorflow/tensorflow,"custom op written in c api compilation issue <details> <summary> click to expand </summary> # # # issue type documentation feature request # # # source binary # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution linux # # # mobile device <number> # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell hi there ! is there any example where custom op written using c api was compiled successfully and executed after tf_load_library ( ) ? i have referred the entire kernels . h , ops . h and c_api . h files of the official github tf repo but i am not able to figure it out . could someone give the command to execute a c api custom op file along with the code ? thanks , yoga ` ` ` # # # standalone code to reproduce the issue ` ` ` shell <hashtag> include </hashtag> "" tensorflow / c / kernels . h "" <hashtag> include </hashtag> "" tensorflow / c / ops . h "" <hashtag> include </hashtag> "" tensorflow / c / tf_tensor . h "" <hashtag> include </hashtag> "" tensorflow / core / framework / common_shape_fns . h "" <hashtag> include </hashtag> "" tensorflow / core / framework / op . h "" <hashtag> include </hashtag> "" tensorflow / core / framework / registration / registration . h "" <hashtag> include </hashtag> "" tensorflow / core / framework / shape_inference . h "" <hashtag> include </hashtag> "" tensorflow / core / platform / macros . h "" / / shape inference void basic_shape_inference_fn ( tf_shapeinferencecontext * ctx , tf_status * status ) { tf_shapehandle * handle = tf_newshapehandle ( ); tf_shapeinferencecontextgetinput ( ctx , <number> , handle , status ) ; tf_shapeinferencecontextsetoutput ( ctx , <number> , handle , status ) ; tf_deleteshapehandle ( handle ) ; assert ( tf_getcode ( status ) ! = tf_ok ) ; } typedef struct basic { tf_tensor * input ; tf_tensor * output ; } basic ; static void * basic_create ( tf_opkernelconstruction * context ) { basic * k = ( basic <wink> calloc ( <number> , sizeof ( basic ) ); tf_status * status = tf_newstatus ( ); /* initialize the fields of k as needed */ assert ( tf_getcode ( status ) ! = tf_ok ) ; tf_deletestatus ( status ) ; return ( void <wink> k ; } static void basic_compute ( void * k , tf_opkernelcontext * ctx ) { /* compute the result */ tf_tensor * input_tensor = null ; tf_status * status = tf_newstatus ( ); tf_getinput ( ctx , <number> , & input_tensor , status ) ; tf_setoutput ( ctx , <number> , input_tensor , status ) ; assert ( tf_getcode ( status ) ! = tf_ok ) ; tf_deletestatus ( status ) ; } static void basic_delete ( void * kernel ) { delete static_cast < basic *>( kernel ) ; } / / op and kernel registration void initplugin ( ) { tf_status * status = tf_newstatus ( ); tf_opdefinitionbuilder * op_builder = tf_newopdefinitionbuilder ( "" basic "" ); tf_opdefinitionbuilderaddinput ( op_builder , "" to_zero : int32 "" ); tf_opdefinitionbuilderaddoutput ( op_builder , "" zeroed tf_opdefinitionbuildersetshapeinferencefunction ( op_builder , & basic_shape_inference_fn ) ; tf_registeropdefinition ( op_builder , status ) ; assert ( tf_getcode ( status ) ! = tf_ok ) ; tf_deletestatus ( status ) ; tf_status * status1 = tf_newstatus ( ); auto * builder = tf_newkernelbuilder ( "" basic "" , tensorflow : : device_cpu , & basic_create , & basic_compute , & basic_delete ) ; tf_registerkernelbuilder ( "" basic "" , builder , status1 ) ; assert ( tf_ok ! = tf_getcode ( status1 ) ); tf_deletestatus ( status1 ) ; } / / tf_attribute_unused static bool isbasickernelregistered = [ ] ( ) { / / if ( should_register_op_kernel ( "" basic "" ) ) { / / registerbasickernel ( ); / / } / / return true ; / / }(); ` ` ` # # # relevant log output ` ` ` shell module ' 0 1 2 ff3e36e3c24aefc4a3a7b68a03fedd1e7a7e1 ' has no attribute ' basic ' ` ` ` </details>",1
tensorflow/tensorflow,"add tf . complex to tflite ops . support padding and slicing of complex tensors <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell it would be great if we could get support in tflite for the ` tf . complex ` op as well as support for slicing and padding complex numbers . this is very important for audio use cases as it often involves running stfts and working with complex numbers . here ' s the error i currently get when i try to convert my model : some ops are not supported by the native tflite runtime , you can enable tf kernels fallback using tf select . see instructions : <url> tf select ops : complex , pad , stridedslice details : tf . complex ( tensor <2x128x2048x2xf32> , tensor <2x128x2048x2xf32> ) - > ( tensor < 2 x128x2048x2xcomplex <f32> > ) : { device = "" "" } tf . pad ( tensor < 2 x128x2048x2xcomplex <f32> > , tensor <4x2xi32> ) - > ( tensor < 2 x128x2049x2xcomplex <f32> > ) : { device = "" "" } tf . stridedslice ( tensor < 2 x128x2049x2xcomplex <f32> > , tensor <4xi32> , tensor <4xi32> , tensor <4xi32> ) - > ( tensor < 2 x128x2048x2xcomplex <f32> > ) : { begin_mask = <number> : i64 , ellipsis_mask = <number> : i64 , end_mask = <number> : i64 , new_axis_mask = <number> : i64 , shrink_axis_mask = <number> ` ` ` ` ` ` # # # standalone code to reproduce the issue ` ` ` shell na ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,f1 score metric # # # issue type <kbd> feature request </kbd> - - - # # # tensorflow version <number> . <number> - - - # # # current behaviour ? no f1 score metric . it ' s a holistic measure for classification . <url> <url> - - - # # # standalone code to reproduce the issue <url>,1
tensorflow/tensorflow,"is it possible to change colour of specific keypoint in ios <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution macos # # # mobile device iphone x # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i want to change colour of specific body landmarks . is it possible to achieve in ios using swift ` ` ` # # # standalone code to reproduce the issue ` ` ` shell multicolor on overlay image ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"building tflite compiling out pthread <details> <summary> click to expand </summary> # # # issue type build / install # # # source source # # # tensorflow version tf2 . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell i am trying to build tflite example "" minimal "" for a device which does not support posix thread . i know one can disabled multi - threading by setting num_threads = <number> in the interpreter , but what i am trying to get is disabling pthread fully during compilation . is there a way to achieve this ? thanks ` ` ` # # # standalone code to reproduce the issue ` ` ` shell you can run the <url> test . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"how to display custom label with angles in ios ? <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell how to display degree of angle on every keypoint . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell below code for to create degree of angle i used func angle ( firstlandmark : cgpoint , midlandmark : cgpoint , lastlandmark : cgpoint ) - > cgfloat { let radians = atan2 ( lastlandmark . y - midlandmark . y , lastlandmark . x - midlandmark . x) - atan2 ( firstlandmark . y - midlandmark . y , firstlandmark . x - midlandmark . x) var degrees = radians * <number> / . pi / / var degrees = radians * . pi / <number> / / / . pi degrees = abs ( degrees ) / / angle should never be negative if degrees > <number> { degrees = <number> - degrees / / always get the acute representation of the angle } let roundedvalue1 = round ( degrees ) return roundedvalue1 } ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"how to calculate <number> degree standing position of body from camera in swift ( pose estimation ) <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell how to calculate <number> degree standing position of body from camera in swift . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell how to calculate <number> degree standing position of body from camera in swift using the body keypoints . ( pose estimation ) ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"is it possible to calculate body degree angle using pose estimation <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version pod ' tensorflowliteswift ' , ' ~ > <number> . <number> - nightly ' , : subspecs => [ ' coreml ' , ' metal ' ] # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell how to calculate degree of angles using body keypoints ? & pose angle . ie . <number> degree , <number> degree . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell i want to detect human body pose angle is it possible ? ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"restore non max suppression ( nms ) support for tensorflow lite gpu delegate . <details> <summary> click to expand </summary> # # # issue type feature request # # # source binary # # # tensorflow version v2 . <number> - rc2 - <number> - g8a20d54a3c1 <number> . <number> # # # custom code yes # # # os platform and distribution google colab # # # mobile device _no response_ # # # python version <date> ( google colab ) # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell tl ; dr : please restore support for nonmaxsuppression for tensorflow lite gpu delegate . it was supported before ( tf <number> ) and is not supported now . background : back in tensorflow version <number> one could take and run object detection model ( e . g . from tensorflow object detection api ) and run it fully on the tflite with gpu via delegate . right now it is impossible - operations that were previously supported ( like ` nonmaxsuppression ` ) are now unsupported . by dropping support to these operations user ' s are forced - locked to use older tensorflow version . please bring back support for ` nonmaxsuppression ` as it was previously supported . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell # colab link : # <url> # let us verify support with tflite ' s authoring tool import tensorflow as tf <user> . function ( input_signature = [ tf . tensorspec ( shape =( <number> , <number> , <number> ) , dtype = tf . float32 ) , # boxes tf . tensorspec ( shape =( <number> , <number> , <number> ) , dtype = tf . float32 ) # scores ] ) def nms ( boxes , scores ) : return tf . image . non_max_suppression_padded ( boxes , scores , max_output_size = <number> , pad_to_max_output_size = true , ) # verify this works : boxes = tf . ones ( shape =( <number> , <number> , <number> ) ) scores = tf . ones ( shape =( <number> , <number> , <number> ) ) selected_indices_padded , num_valid = nms ( boxes , scores ) # verify crashes tflite gpu delegate ( authoring tool ) target_spec = tf . lite . targetspec ( ) target_spec . experimental_supported_backends = [ "" gpu "" ] func_to_test = tf . lite . experimental . authoring . compatible ( nms , converter_target_spec = target_spec , raise_exception = true ) func_to_test ( boxes , scores ) ` ` ` # # # relevant log output ` ` ` shell # the log output is rather long and covers multiple sub ops . i am only pasting the lower part : during handling of the above exception , another exception occurred : compatibilityerror traceback ( most recent call last ) / usr / local / lib / python3 . <number> / dist - packages / tensorflow / lite / python / authoring / authoring . py in _decode_error ( self , err ) <number> <number> if self . _raise_exception and self . _log_messages : - - > <number> raise compatibilityerror ( f "" compatibilityexception at { repr ( self . _func ) } "" ) <number> <number> def _log ( self , message ) : compatibilityerror at < tensorflow . python . eager . def_function . function object at 0x 7 f70f5780590 > ` ` ` </details>",1
tensorflow/tensorflow,"[ tflite ] dynamic batch size with gpu delegate <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version <number> . <number> - dev20220427 # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device linux ubuntu <number> # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version cuda <number> # # # gpu model and memory nvidia geforce gtx <number> 4 bg # # # current behaviour ? considering these two assertions ( please tell me if they are wrong ) : * it is not possible to use dynamic - sized tensors with the gpu delegate . * nor is it possible to use a batch size greater than one with the gpu delegate . edit seems that when i saw the bug with batch size greater than one , the gpu delegate switched to opengl instead of opencl ( i do not know the reason ) , and now that it is working with opencl ( after rebooting the computer ) , the inference can run on the gpu with any batch size . thus it would still mean that this feature should be available with opengl . <repeated> however it would very convenient to be able to modify the batch size before creating the network on the gpu , or ( if requested by the user ) to recreate the network at any time with a different batch size . this would allow to convert and store a single model with dynamic - sized tensors , the size of which would actually be known ( i . e . , batch size would be set ) before making use of the network . then it would be possible to perform [ on - device training ] ( <url> on the gpu with a batch of inputs , once issue # <number> is solved . # # # standalone code to reproduce the issue try running [ this on - device training tutorial ] ( <url> with both flex and gpu delegates using the experimental c api . # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"build / release python <number> tflite - runtime wheels to pypi <details> <summary> click to expand </summary> # # # issue type build / install # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux & macos # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? [ tensorflow <number> . <number> was released with python <number> support ] ( <url> we would like to support python <number> in production , but in order to do this we need all of the following packages to support python <number> : - ` tensorflow ` - ` tensorflow - macos ` ( for arm macos ) - ` tflite - runtime ` ( for stripped down production on linux ) neither [ ` tensorflow - macos ` ] ( <url> or [ ` tflite - runtime ` ] ( <url> have <number> . <number> releases on pypi yet . # # # standalone code to reproduce the issue create a python <number> virtual environment on linux : ` ` ` bash ( venv ) $ python - m pip install tflite - runtime ` ` ` create a python <number> virtual environment on macos arm ( venv ) $ python - m pip install tensorflow - macos ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"3 d ssim hi , is it possible to add 3 d ssim ( batched ) into tensorflow ? we need this feature for 3 d inference . currently ` tf . image . ssim ` only accepts batched 2 d input . if not , is there any internal beta version of 3 d ssim for try - out ? thanks .",1
tensorflow/tensorflow,"tf . keras . optimizers . experimental . adamw only support constant weight_decay <details> <summary> click to expand </summary> # # # issue type feature request # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell tf . keras . optimizers . experimental . adamw only supports constant weight decay . but usually we want the weight_decay value to decay with learning rate schedule . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell the legacy tfa . optimizers . adamw supports callable weight_decay , which is much better . ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"how to resize ( downsample ) 5 d samples in tensorflow ? issue type : feature request source : binary tensorflow version : tf <number> custom code : yes os platform and distribution : windows <number> python version : <number> - - - # # note . reposting from [ here ] ( <url> it ' s closed in keras and recommended to post it on the tensorflow side because this functionality is not precisely available in tensorflow . it may need to write low - level ops in tf for best performance . # current behaviour ? currently , for * 5 d <emphasis> * data , ` ( batch_size , h , w , depth , channel ) ` , the [ ` tf . keras . backend . resize_volumes ` ] ( <url> or ` upsampling3d ` can be used to * * upsampling purpose * * . for example , i can do ` ` ` python a = tf . ones ( shape =( <number> , <number> , <number> , <number> , <number> ) ) tf . keras . backend . resize_volumes ( a , depth_factor = <number> , height_factor = <number> , width_factor = <number> , data_format = "" channels_last "" ) . shape tensorshape ( [ <number> , <number> , <number> , <number> , <number> ] ) ` ` ` these ` * _factor ` values ( above ) , should be an * integer <emphasis> * , and are coded here : <url> in that case , how can we * downsample <emphasis> * the input sample ? for example : ` ` ` python a = tf . ones ( shape =( <number> , <number> , <number> , <number> , <number> ) ) tf . keras . backend . resize_volumes ( a , depth_factor = <number> , height_factor = <number> , width_factor = <number> data_format = "" channels_last "" ) . shape typeerror : ' float ' object cannot be interpreted as an integer # expected tensorshape ( [ <number> , <number> , <number> , <number> , <number> ] ) ` ` ` [ here <url> another scenario where the factor needed to be fractional . ] # candidate solutions - [ scipy . ndimage . zoom ] ( <url> - [ in pytorch ] ( <url> ( did not test ) # # others - such downsampling feature needs to be implemented in low - level . - in the ` depth ` part of volumetric data , it might be hard to decide the appropriate strategy to drop the slices depending on the domain . for example , in medical data , if we drop the slice blindly , we might lose information . fyi , in ct / mri images , most of the information appears mainly in the * * middle range * * . - currently a workaround for * * medical data ( ct / mri ) * * , we are following : ` ` ` # ( input data : <number> , <number> , <number> , <number> , <number> ) # ( desired output <number> , <number> , <number> , <number> ) ` ` ` ` ` ` python a = tf . ones ( shape =( <number> , <number> , <number> , <number> , <number> ) ) a . shape # tensorshape ( [ <number> , <number> , <number> , <number> , <number> ] ) a2 = tf . reshape ( a , [ - <number> , <number> , <number> , <number> * <number> ] ) a2 . shape # tensorshape ( [ <number> , <number> , <number> , <number> ] ) a3 = tf . image . resize ( a2 , [ <number> , <number> ] ) a3 . shape # tensorshape ( [ <number> , <number> , <number> , <number> ] ) a4 = tf . reshape ( a3 , [ - <number> , <number> , <number> , <number> , <number> ] ) a4 . shape # tensorshape ( [ <number> , <number> , <number> , <number> , <number> ] ) # here , picking some middle slices - # - assuming that by this we may get relevant slices . # how convenient is this ? # may not general a5 = a4 [ . <repeated> , <time> , <happy> a5 . shape tensorshape ( [ <number> , <number> , <number> , <number> , <number> ] ) ` ` `",1
tensorflow/tensorflow,"tflite tensor name converted by <number> . <number> is much longer than <number> . <number> # # # system information - * * os platform and distribution * * : linux ubuntu <number> - * * mobile device * * : no - * * tensorflow installed from ( source or binary ) * * : binary , pip install tensorflow = = <number> . <number> - * * tensorflow version * * : <number> . <number> - * * python version * * : <date> - * * bazel version ( if compiling from source ) * * : none - * * gcc / compiler version ( if compiling from source ) * * : none - * * cuda / cudnn version * * : none - * * gpu model and memory * * : none - * * exact command to reproduce * * : none # # # describe the problem using tensorflow <number> . <number> convert . pb model to tflite as follows : ` ` ` python import tensorflow as tf graph_def_file = ' / path / to / mobilenet_v1 . pb ' input_arrays = [ ' input ' ] output_arrays = [ ' mobilenetv1 / predictions / reshape_1 ' ] input_shapes = { ' input ' : [ <number> , <number> , <number> , <number> ] } convert = tf . lite . tfliteconverter . from_frozen_graph ( graph_def_file , input_arrays , output_arrays , input_shapes ) tflite_model = convert . convert ( ) with open ( ' mobilenet_v1_tf1 . <number> . tflite ' , ' wb ' ) as f : f . write ( tflite_model ) ` ` ` after update tensorflow to <number> . <number> and modify scripts as follows : ` ` ` python import tensorflow as tf graph_def_file = ' / path / to / mobilenet_v1 . pb ' input_arrays = [ ' input ' ] output_arrays = [ ' mobilenetv1 / predictions / reshape_1 ' ] input_shapes = { ' input ' : [ <number> , <number> , <number> , <number> ] } convert = tf . compat . v1 . lite . tfliteconverter . from_frozen_graph ( graph_def_file , input_arrays , output_arrays , input_shapes ) tflite_model = convert . convert ( ) with open ( ' mobilenet_v1_tf2 . <number> . tflite ' , ' wb ' ) as f : f . write ( tflite_model ) ` ` ` i found tensor name in ` mobilenet_v1_tf2 . <number> . tflite ` is much longer than ` mobilenet_v1_tf1 . <number> . tflite ` . such a long name caused a lot of trouble when comparing the tflite model with the original model . however , i also noticed that there will be a new ` attributes ` in tfliteconverter . ` ` ` python convert = tf . compat . v1 . lite . tfliteconverter . from_frozen_graph ( graph_def_file , input_arrays , output_arrays , input_shapes ) convert . experimental_new_convert = false tflite_model = convert . convert ( ) ` ` ` but the log as follows warning me that old converter is deprecated warning : absl : please consider switching to the new converter by setting experimental_new_converter = true . the old converter ( toco ) is deprecated . ` ` ` is is possible to add an attribute in the new version , so that the model conversion name can remain unchanged , and the tensor of folder const can be removed .",1
tensorflow/tensorflow,"replacing add layer with linear combination of learnable weights <details> <summary> click to expand </summary> # # # issue type feature request the add ( ) layer can be expressed as a linear combination of inputs where each coefficient value is <number> . i was trying to make a custom layer that performs the same as add , except uses trainable coefficients as we assume a coefficient of <number> is optimal . unfortunately , i was unable to do so on my own , specifically the gradients . i do not think this would be too hard to implement as all the code is kind of already there , just need to calculate gradients and then update . # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? ` ` ` shell the add ( ) layer can be expressed as a linear combination of inputs where each coefficient value is <number> . i was trying to make a custom layer that performs the same as add , except uses trainable coefficients as we assume a coefficient of <number> is optimal . unfortunately , i was unable to do so on my own , specifically the gradients . i do not think this would be too hard to implement as all the code is kind of already there , just need to calculate gradients and then update . ` ` ` # # # standlone code to reproduce the issue ` ` ` shell x ` ` ` # # # relevant log output _no response_ </details>",1
tensorflow/tensorflow,"request for feature gather and matmul <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : <number> . <number> - are you willing to contribute it ( yes / no ) : yes * * describe the feature and the current behavior / state . * * this function can realize that the corresponding weight is taken from the matrix b according to the index given by the matrix c and then multiplied by the matrix a . a : [ batch , h ] b : [ num , h , d ] c : [ batch , ] out : [ batch , d ] batch > > num ` ` ` python for i , j in enumerate ( c ) = matmul ( a [ i ] , b [ j ] ) ` ` ` currently this function can be implemented by tf . gather then tf . matmul . however , it will waste a lot of gpu memory due to the need to define an intermediate variable w . ` ` ` python w = tf . gather ( b , c ) out = tf . matmul ( a , w ) ` ` ` it can also be implemented by one hot matmul . however the time complexity of the calculation will be very high . ` ` ` python tmp = tf . one_hot ( c , depth = num ) out = tf . enisum ( ' bh , bn , nhd - > bd ' , a , tmp , b ) ` ` ` * * will this change the current api ? how ? * * no * * who will benefit with this feature ? * * this function can be applied to the moe model to achieve the assignment of experts strictly according to the probability of the router . avoids the limitations of the expert capacity currently required . * * any other info . * * no",1
tensorflow/tensorflow,"rfe tensorflow - aarch64 = = <number> . <number> build ? * * system information * * tensorflow version ( you are using ) : <number> . <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * brainchip akida akd1000 snn neuromorphic metatf sdk support <number> . <number> on x86_64 . they claim support for aarch64 , but when creating a virtualenv it fails on aarch64 due to lacking tensorflow - aarc64 = = <number> . <number> build . * * will this change the current api ? how ? * * na * * who will benefit with this feature ? * * customer of brainchip akida who run on arm64 platforms . * * any other info . * * <url>",1
tensorflow/tensorflow,"tf . image . extract_patches default value for areas outside the input running tf . image . extract_patches by the "" same "" padding leads to zeros for areas outside the input . there is no way to change the default value and it is best to have this as another parameter for this function . this background label should be always zero ; otherwise , this function mess up the labels .",1
tensorflow/tensorflow,"` pfor ` / ` tf . vectorize_map ` : improve feedback / hints messages when it ` fallback_to_while_loop ` * * system information * * - tensorflow version ( you are using ) : - are you willing to contribute it ( yes / no ) if i can talk with a codeowner of this namepace * * describe the feature and the current behavior / state . * * as we are adding [ a root cause message ] ( <url> on the cause we are going internally to rely on ` fallback_to_while_loop ` it would be nice to have a more clear feedback string to the user on what kind of action is required ( e . g . refactoring his function , open a new ticket with a code gist on tf github , etc . <repeated> ) . see more at <url> / cc <user> * * will this change the current api ? how ? * * no * * who will benefit with this feature ? * * developers that partially fail to fully ` tf . vectorize_map ` their functions * * any other info . * *",1
tensorflow/tensorflow,list of tf ops that tf . vectorized_map supports where can i get this information ? which tf operation is a vectorized operation ? and which one is not ?,1
tensorflow/tensorflow,"generalise ` xla : : map ` to functions over arbitrary shapes * * system information * * - tensorflow version ( you are using ) : <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * currently ` xla : : map ` only allow functions from shape [ ] to [ ] . i ' d like to use functions with arbitrary shapes . for example , apply a function with shapes [ <number> , <number> ] - > [ <number> ] to a tensor [ <number> , <number> , <number> , <number> ] to get a [ <number> , <number> , <number> ] . i have not yet thought about how it would work when mapping multiple input tensors at once . * * will this change the current api ? how ? * * it would extend ` xla : : map ` , either internally , or as an overload . it ' s possible additional information would need to be passed to a more general implementation of map , in which an overload may be preferable to maintain backwards compatibility . * * who will benefit with this feature ? * * anyone using xla who ' d like to apply a function over sections of a tensor . it would effectively be an alternative to ( some portion of ) broadcasting , allowing people to use functions that do not allow leading dimensions to tensors with leading dimensions . it is particularly useful for me as i am working with dependent types and adding leading dimensions is non - trivial ( and verbose ) in type signatures . * * any other info . * * it is already possible to do this by indexing into the tensor and iteratively applying the function to the contents , then concatenating the results , but i expect this is significantly slower than could be achieved within xla . i have considered using ` xla : : while ` for this but i still expect the slicing and concatenation would still come with a significant performance cost .",1
tensorflow/tensorflow,"please publish tensorflowliteobjc <number> <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : android <number> ios <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * * * will this change the current api ? how ? * * * * who will benefit with this feature ? * * * * any other info . * *",1
tensorflow/tensorflow,"capture_tpu_profile on cloud tpu vm hello it seems that capture_tpu_profile only works for legacy tpu devices , but not for cloud tpu vm . i was wondering if and when we can expect to have support for cloud tpu vm . at the moment , its is impossible to monitor jax / flax tpu workloads , as tensorboard instrumentation does not exhibit the right metrics ( eg % utilization of tpu matrix units ) . other related thread for legacy tpu devices thanks",1
tensorflow/tensorflow,"xla tf . bincount support * * system information * * - tensorflow version ( you are using ) : master - are you willing to contribute it ( yes / no ) : only if we have a clear path and a reviewer on how to contribute this * * describe the feature and the current behavior / state . * * ` tf . bincount ` is not supported by xla * * will this change the current api ? how ? * * no * * who will benefit with this feature ? * * speedup functions / loops that rely on ` tf . bincount ` * * any other info . * * how to reproduce it : ` ` ` python import tensorflow as tf <user> . function ( jit_compile = true ) def compiled_bincount ( values ) : return tf . math . bincount ( values ) values = tf . constant ( [ <number> , <number> , <number> , <number> ] ) print ( compiled_bincount ( values ) ) #[ <number> <number> <number> <number> <number> <number> ] ` ` ` ` ` ` python invalidargumenterror : detected unsupported operations when trying to compile graph __inference_compiled_bincount_296 [ _xlamustcompile = true , config_proto = <number> , executor_type = <number> ] on xla_cpu_jit : bincount ( no registered ' bincount ' opkernel for xla_cpu_jit devices compatible with node { { node bincount / bincount } } ) { { node bincount / bincount } } the op is created at : ` ` ` without this we had problems to compile intermediate ms coco recall function in keras - cv : <url> extra : please also note that the cpu / gpu tf2xla supported ops tables are probably outdated ( <number> ) / cc <user> - eph <user>",1
tensorflow/tensorflow,"gradient_function / gradient_tapes with device annotations * * system information * * - tensorflow version ( you are using ) : <number> - are you willing to contribute it ( yes / no ) : yes * * describe the feature and the current behavior / state . * * feature request : adding ` colocate_gradients_with_ops ` option to ` tf . gradients ` in <number> . x to ` tf . gradienttape ` in <number> . x . this feature request / issue was 1 st mentioned in <url> almost two years ago by <user> . allow me to quickly review the op in which the current behavior ( still hold in tf <number> ) is described . > currently gradienttape . gradient ( ) is executed on the device of the scope it is called in . have a look at the following code : > ` ` ` with tf . gradienttape ( ) as tape : with tf . device ( ' / gpu : <number> ' <sad> x = f1 ( input ) with tf . device ( ' / gpu : <number> ' <sad> x = f2 ( x ) with tf . device ( ' / gpu : <number> ' ) = tape . gradient ( x , f_vars ) ` ` ` > here all gradient calculations will be carried out by gpu : <number> and all variables needed for the gradient calculation will also be allocated on gpu : <number> . this is a problem if these temporary variables are too large to fit into the vram of gpu : <number> . > > please provide a way to execute the backward functions on the device of the corresponding forward function and allocate temporary variables for gradient calculation there . this allows to split a large model and distribute it among as many gpus as necessary . > also , <user> provided a pr <url> to achieved such feature and it get merged at the beginning , but shortly get rollbacked due to certain performance issues . i opened another pr <url> to fit the current master ( tf2 . <number> ) and build and test it . it looks to me the gradient can now be split correctly according to the device annotation . * * will this change the current api ? how ? * * it will allow tf . gradienttape to do gradient in the device of the corresponding forward function . hence , it would be possible to split the training of large models into as many possible gpus as necessary * * who will benefit with this feature ? * * anyone who wants to train large models that do not fit into the vram of a single gpu . * * any other info . * * it looks to me that the original post has gone silence . <repeated> so i raise the issue again here to draw more attentions . it is a very import feature for peoples working in large image segmentation tasks . some times the input tensor is so large and the model can not even be fitted into a single a100 card . moreover , i noticed that in the discussion flow in the original post that there are some other work around like split into different tapes . i tried but with no luck . all gradient calculation still be allocated on gpu : <number> , and moreover , splitting tape would be very different for model like u - net which has a lot of skip - connections . last but not the least , i know for a fact that there is the mash - tensorflow which propose to do such job , but a native tensorflow support would be also very useful for people work on large models .",1
tensorflow/tensorflow,"runtimeerror : exporting / importing meta graphs is not supported when eager execution is enabled . no graph exists when eager execution is enabled . <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] successfully opened dynamic library libcudart . so . <number> traceback ( most recent call last ) : file "" 0 1 _genquadproposals . py "" , line <number> , in <module> cnnmodel . restorecnnsess ( ) file "" / home / dms / supplementarymaterials / codeanddata / code / quadproposals / cnnquaddetector . py "" , line <number> , in restorecnnsess saver = tf . train . import_meta_graph ( self . cfg . cornerdet_sess + ' . meta ' , import_scope = "" cornerdet "" ) file "" / home / dms / anaconda3 / envs / supplementarymaterials / lib / python3 . <number> / site - packages / tensorflow / python / training / saver . py "" , line <number> , in import_meta_graph return _import_meta_graph_with_return_elements ( meta_graph_or_file , file "" / home / dms / anaconda3 / envs / supplementarymaterials / lib / python3 . <number> / site - packages / tensorflow / python / training / saver . py "" , line <number> , in _import_meta_graph_with_return_elements raise runtimeerror ( "" exporting / importing meta graphs is not supported when "" runtimeerror : exporting / importing meta graphs is not supported when eager execution is enabled . no graph exists when eager execution is enabled . * * system information * * - tensorflow version ( you are using ) : tensorboard <number> . <number> tensorboard - data - server <number> . <number> tensorboard - plugin - wit <number> . <number> tensorflow <number> . <number> tensorflow - estimator <number> . <number> tensorflow - hub <number> . <number> tensorflow - io - gcs - filesystem <number> . <number> tensorflowjs <number> . <number> cuda <number> cudnn <number> python <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * * * will this change the current api ? how ? * * * * who will benefit with this feature ? * * * * any other info . * *",1
tensorflow/tensorflow,"option to avoid caching with bijectors . <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : <number> . <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * the current behaviour for ` tensorflow_probability . bijectors . bijector ` is to cache the input to be used if the inverse function is called . i would very much like a keyword being able to turn this caching off to ensure the inverse function of the bijector is always called . whilst setting the property of ` _is_injective ` can accomplish this , you lose other features . * * will this change the current api ? how ? * * yes , a keyword in the definition of the bijector class . * * who will benefit with this feature ? * * anyone who wishes to have bijectors where the inverse function is always called rather than the input that ' s been cached . * * any other info . * *",1
tensorflow/tensorflow,"gradient for tf . sparse . reduce_max <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * describe the feature and the current behavior / state . * * tf . sparse . reduce_max do not define grad . so it cannot be used in training step . * * will this change the current api ? how ? * * will not change current api . just add grad in tf . sparse . reduce_max * * who will benefit with this feature ? * * people using sparse . reduce_max in training . if they have huge sparse matrix and cannot turn to dense array . * * any other info . * * i am working with pointnet - like model . and it will take a huge sparse matrix . if i turn this sparse matrix to dense , it will take ~ 6 0 g memory . so i need do global maxpooling . using tf sparse reduce_max . unfortunately , it cannot be used in training right now . thanks",1
tensorflow/tensorflow,"iterating over ` tf . tensor ` is not allowed <em> please make sure that this is a bug . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : bug_template </em> * * system information * * - have i written custom code ( as opposed to using a stock example script provided in tensorflow ) : yes - os platform and distribution ( e . g . , linux ubuntu <number> <sad> - mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on mobile device : - tensorflow installed from ( source or binary ) : - tensorflow version ( use command below ) : <number> - python version : <number> - bazel version ( if compiling from source ) : - gcc / compiler version ( if compiling from source ) : - cuda / cudnn version : - gpu model and memory : you can collect some of this information using our environment capture [ script ] ( <url> you can also obtain the tensorflow version with : <number> . tf <number> : ` python - c "" import tensorflow as tf ; print ( tf . git_version , tf . version ) "" ` <number> . tf <number> : ` python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` * * describe the current behavior * * i have a dictionary that i build by accessing only the channel dimensions of an output layer of a convolutional neural network ( it has a shape ( <number> , <number> ) ) therefore keys of this dictionary are tuples of tensor shape ( <number> , ) . i want to map these keys to the input of the next layer using the tf . map_fn ( ) . however , i am incapable of doing it because the keys of my dictionary are of type tensor and i cannot iterate over them . looking for some help . thank you . * * describe the expected behavior * * * *[ contributing ] ( <url> - do you want to contribute a pr ? ( yes / no ) : - briefly describe your candidate solution ( if contributing ) code to reproduce the issue * * provide a reproducible test case that is the bare minimum necessary to generate the problem . if possible , please share a link to colab / jupyter / any notebook . * * other info / logs * * include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached .",1
tensorflow/tensorflow,"count number of leaves in the ensemble gradient boostedtree this is an issue related to the performance of [ tensorflow . boostedtree ] ( <url> * * system information * * - tensorflow version ( you are using ) : <number> . <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * is there any method that returns the number of entire leaves in the built ensemble ? if the feature already exists , would you please introduce it a bit more here , if not could we develop it ? * * will this change the current api ? how ? * * no * * who will benefit with this feature ? * * users of the tensorflow",1
tensorflow/tensorflow,"normalization should be done using moving averages ? # # url ( s ) with the issue : <url> # # description of issue ( what needs changing ) : hello this is a small feature request . in the time series tutorial at the normalization section in the 3 rd paragraph there is a remark saying that this normalization should be done using moving averages . "" i am very curious on how a moving average would be used here . should a "" simple moving average "" for the whole dataset be computed and then on that new "" dataset - ma "" just use the normalization technique that is described in the tutorial ? the feature i am requesting is something like a remark on how this would be done or maybe a pointer to different tutorial if one exists or some further references / research users could look into .",1
tensorflow/tensorflow,"using scipy fsolve or other solvers ( e . g . gekko ) in combination with backpropagation models i was wondering if there was any progress or existing methodologies to use existing solvers such as gekko to find / update certain values in your network . e . g : when using pinn networks on a electrical circuit , one might use a variable resistance dependent on one of your network input parameters . when this dependency is given by an explicit equation it would be nice to , given the inputs , be able to use existing solvers as scipy ' s fsolve to get this value and use it further on in the network . from my experience in order to be able to use solvers , tensors should be converted to numpy elements and used in combination with tf . numpy_function , this way i can use solvers as such , however it breaks the chain of gradients and tf can no longer find gradients for certain values and thus no longer train the network . * * system information * * - tensorflow <number> . <number> ( but using some tf1 functionalities from tf . compat . v1 : - no possibility to share existing code , but see below for small ( hypothetical ) case example . # # # start of code from scipy . optimize import fsolve class ppinlayer ( tf . keras . layers . layer ) def __init__ ( self ) : super ( ppinlayer , self ) . __init__ ( * * kwargs ) def call ( self , input , nn * args ) : nnoutput = nn ( input ) # this is a nn already initialized somewhere else with trainable parameters output = self . f ( nnoutput , input ) return output # the output is then used later on to compare with labeled data as loss function for training def f ( self , nnoutput , input ) r0 = input [ <number> ] r1 = input [ <number> ] v0 = input [ <number> ] def r2_eq ( r2 , r0 , r1 ) np . exp ( r2 / r1 ) * r1 / r2 + r0 * * <number> r2 = fsolve ( r2_eq , <number> , args =( r0 , r1 ) ) return ( v0 / r0 * r2 ) thanks in advance cedric",1
tensorflow/tensorflow,"[ tflite ] optional debug tools can only support log in desktop computer hi tensorflowers , the ` tflite : : printinterpreterstate ( interpreter . get ( ) ) ` in [ minimal . cc ] ( <url> can only be used on computer system with desktop . because it use ` printf ` style to output info in [ optional_debug_tools . cc ] ( <url> not the ` tflite_log ` in [ minimal_logging ] ( <url> the better way supporting more systems . besides , why not use something like ` std : : stringstream ` to format info , that seems more general ?",1
tensorflow/tensorflow,"i have made wrapper of tensor <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : - are you willing to contribute it ( yes / no ) github release * * describe the feature and the current behavior / state . * * graph * * will this change the current api ? how ? * * nope * * who will benefit with this feature ? * * everyone * * any other info . * * i have started wrapper of tensorflow using cppflow . i need some help with graph . it is unclear how it works in c api . i am thinking of wrapping c + + api . it works for all swig languages . the question is does c / api suppot the graph enough to use it or should i wrap the c + + api ?",1
tensorflow/tensorflow,"tf . data . dataset gather element <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : tensorflow <number> - are you willing to contribute it ( yes / no ) * * describe the feature and the current behavior / state . * * i want to do something like tf . gather on tf . data . dataset . for example , dataset = tf . data . dataset . from_tensor_slices ( [ ' a ' , ' b ' , ' c ' , ' d ' , ' e ' , ' f ' , ' g ' , ' h ' , ' i ' ] ) choice_dataset = tf . data . dataset . range ( <number> ) . repeat ( <number> ) # define a dataset containing [ <number> , <number> , <number> , <number> , <number> , <number> ] i want it to output a dataset having [ ' a ' , ' b ' , ' c ' , ' a ' , ' b ' , ' c ' ] this is somewhat similar to choose_from_datasets , but that method choose from multiple datasets not element in the dataset * * will this change the current api ? how ? * * add new method to tf . data . dataset * * who will benefit with this feature ? * * tensorflow dataset user * * any other info . * * i am not sure if there is a workaround . please correct me if i am wrong .",1
tensorflow/tensorflow,request for randomillumination layer in keras something like this already is possible with ` imagedatagenerator ` but with corners in it ( only applies brightness scale ) . what is needed is a full keras layer that randomize illumination differences spatially across the image ( or even a single scaling factor to the whole image ) . i think more ideas can be added to something like this if it was implemented properly .,1
tensorflow/tensorflow,"release tensorflow - macos wheel compatible with python <number> and x86 currently in version <number> . <number> of ` tensorflow - macos ` , there are only <number> wheels released in pip : <number> for arm64 supporting python <number> - <number> and <number> for x86 only supporting python <number> . it would be great to have also a wheel for python <number> for the x86 architecture . the same thing also applies for ` tensorflow - metal ` . * * system information * * - tensorflow version ( you are using ) * * describe the feature and the current behavior / state . * * see above * * will this change the current api ? how ? * * no * * who will benefit with this feature ? * * every intel mac user with python <number> * * any other info . * *",1
tensorflow/tensorflow,"tf . debugging . assert_type throws error when checking type of raggedtensor <em> please make sure that this is a bug . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : bug_template </em> * * system information * * - have i written custom code ( as opposed to using a stock example script provided in tensorflow ) : * yes <emphasis> * - os platform and distribution ( e . g . , linux ubuntu <number> <sad> * * ubuntu <number> . <number> lts * * - mobile device ( e . g . iphone <number> , pixel <number> , samsung galaxy ) if the issue happens on a mobile device : * * n / a * * - tensorflow installed from ( source or binary ) : * binary <emphasis> * - tensorflow version ( use command below ) : * * v2 . <number> - rc1 - <number> - gc256c071bb2 <number> . <number> * * - python version : * * <number> . <number> * * - bazel version ( if compiling from source ) : * * n / a * * - gcc / compiler version ( if compiling from source ) : * * n / a * * - cuda / cudnn version : * * n / a * * - gpu model and memory : * * n / a * * you can collect some of this information using our environment capture [ script ] ( <url> you can also obtain the tensorflow version with : <number> . tf <number> : ` python - c "" import tensorflow as tf ; print ( tf . git_version , tf . version ) "" ` <number> . tf <number> : ` python - c "" import tensorflow as tf ; print ( tf . version . git_version , tf . version . version ) "" ` * * describe the current behavior * * when calling ` tf . debugging . assert_type ` on a ` raggedtensor ` , a ` valueerror : typeerror : object of type ' raggedtensor ' has no len ( ) ` is thrown . * * describe the expected behavior * * the assertion check should pass if the ragged tensor is the correct type or fail otherwise . * *[ contributing ] ( <url> - do you want to contribute a pr ? ( yes / no ) : * no <emphasis> * - briefly describe your candidate solution ( if contributing ) : * * n / a * * * * standalone code to reproduce the issue * * provide a reproducible test case that is the bare minimum necessary to generate the problem . if possible , please share a link to colab / jupyter / any notebook . ` ` ` python import tensorflow as tf a = tf . ragged . constant ( [ [ <number> , <number> ] , [ <number> ] ] ) tf . debugging . assert_type ( a , tf_type = tf . int32 ) # should pass > > > valueerror : typeerror of type ' raggedtensor ' has no len ( ) ` ` ` * * other info / logs * * include any logs or source code that would be helpful to diagnose the problem . if including tracebacks , please include the full traceback . large logs and files should be attached . * workaround <emphasis> * ` ` ` import tensorflow as tf a = tf . ragged . constant ( [ [ <number> , <number> ] , [ <number> ] ] ) assert a . dtype = = tf . int32 ` ` `",1
tensorflow/tensorflow,"support partial parameter warm - start from pretrained checkpoints in tensorflow v2 ( non - estimator mode ) <em> please make sure that this is a feature request . as per our [ github policy ] ( <url> we only address code / doc bugs , performance issues , feature requests and build / installation issues on github . tag : feature_template </em> * * system information * * - tensorflow version ( you are using ) : tensorflow <number> . x - are you willing to contribute it ( yes / no ) : no * * describe the feature and the current behavior / state . * * warm - starting from an existing checkpoint is an important feature for all kinds of model training that ' s well supported under tf . estimator framework . the latter unfortunately seems relegated to second class status in tensorflow <number> . x , since it ' s not eager ( at least not natively ? ) . the closest way to achieve warm - start in tf2 . x seems to be via tf . train . checkpoint , a more directly approach than tf . estimator . warm_start_utils , that is , if implemented well . so far however i see that it supports loading all the parameters in a checkpoint altogether , but not loading only some of the parameters . * * will this change the current api ? how ? * * possibly . maybe provide a kwarg in checkpoint initializer called parameter_list . * * who will benefit with this feature ? * * anyone who wants to reuse a pretrained model but not all of its parameters . * * any other info . * * there is a lot of emphasis on eagerness , efficiency , in tensorflow v2 , but in my opinion not sufficient focus on flexibility and directness of usage so far . warm - starting is one example . update colleague found an unofficial implementation of flexible warm - start from a checkpoint in the tf2 version of bert [ here ] ( <url> i would recommend adding some official examples along similar lines .",1
tensorflow/tensorflow,"new dtype : bcomplex32 * * system information * * - tensorflow version ( you are using ) : <number> - are you willing to contribute it ( yes / no ) yes * * describe the feature and the current behavior / state . * * add bcomplex32 dtype . currently only a real ( like in math , real field ) version is supported ( bfloat16 ) * * will this change the current api ? how ? * * each function that supports the dtype paramter , should also support the new bcomplex32 * * who will benefit with this feature ? * * anyone who uses the complex plane and wants a speedup for further calculations on supported hardware such as gpus with compatibility level of <number> and above , for example while using fft . * * any other info . * *",1
tensorflow/tensorflow,"can i repeat a <number> - d tensor in segment form ? according to the doc , tf has : tf . segment_xxx and tf . repeat . but can i repeat a <number> - d tensor in segment form ? # # # # example : segment = [ <number> <number> <number> <number> <number> ] value = [ <number> <number> <number> <number> <number> ] # # # # [ <number> ] if repeat_cnt = [ <number> <number> <number> <number> <number> ] with tf . repeat , i can easily repeat value to : [ <number> <number> <number> <number> <number> <number> <number> <number> ] # # # # [ <number> ] but i want a segment repeat , which means : segment - <number> repeat <number> times , segment - <number> repeat <number> time . that is to say , if repeat_cnt_new = [ <number> <number> ] , and i want to repeat value to : [ <number> <number> <number> <number> <number> <number> <number> <number> ] . # # # # i mean , repeat blocks in a vector <number> <number> ] * <number> + [ <number> <number> ] * <number> . are there any tf func or api can help ? thanks",1
tensorflow/tensorflow,"lstm expansion * * system information * * - tensorflow version ( you are using ) : <number> - are you willing to contribute it ( yes / no ) : maybe * * describe the feature and the current behavior / state . * * simple lstms have limitations . it cannot extract some features , i . e . some mathematical operands , conditional logic , etc . i do not think the lstm advancements cover this . this paper kind of shows what logic needs to be part of lstm out of the box can we improve the lstm to include ability to figure out some basic logic like mathematical operands and conditional logic ? * * will this change the current api ? how ? * * should not . * * who will benefit with this feature ? * * everyone . * * any other info . * * lstm is brokened at the moment . pretty rudimentary .",1
tensorflow/tensorflow,"keyerror : ' min ' # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution <number> . <number> - microsoft - standard - wsl2 # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behavior ? traceback ( most recent call last ) : file "" o2k . py "" , line <number> , in <module> k_model = onnx_to_keras ( onnx_model , [ ' input . <number> ' ] , name_policy = ' renumerate ' , verbose = true ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / converter . py "" , line <number> , in onnx_to_keras available_converters [ node_type ] ( file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / operation_layers . py "" , line <number> , in convert_clip if params [ ' min ' ] = = <number> : keyerror : ' min ' # # # standalone code to reproduce the issue ` ` ` shell from onnx2keras import onnx_to_keras import keras import onnx import sys # sys . path . append ( "" / root / mr "" ) onnx_model = onnx . load ( ' ssd_bmv1_torch . onnx ' ) onnx_inputs = onnx_model . graph . input print ( "" = = = = = = = = = = = = = = = = = = = = = = = = = =="") print ( onnx_inputs ) # onnx_model = onnx . load ( ' vgg11 . onnx ' ) k_model = onnx_to_keras ( onnx_model , [ ' input . <number> ' ] , name_policy = ' renumerate ' , verbose = true ) keras . models . save_model ( k_model , ' ssd_bmv1_torch . h5 ' , overwrite = true , save_format = "" h5 "" ) onnx file can be downloaded at <url> ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" o2k . py "" , line <number> , in <module> k_model = onnx_to_keras ( onnx_model , [ ' input . <number> ' ] , name_policy = ' renumerate ' , verbose = true ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / converter . py "" , line <number> , in onnx_to_keras available_converters [ node_type ] ( file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / operation_layers . py "" , line <number> , in convert_clip if params [ ' min ' ] = = <number> : keyerror ` ` `",0
tensorflow/tensorflow,"attributeerror : number of inputs is not equal <number> for unsqueeze layer # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution <number> . <number> - microsoft - standard - wsl2 # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behavior ? traceback ( most recent call last ) : file "" o2k . py "" , line <number> , in <module> k_model = onnx_to_keras ( onnx_model , [ ' onnx : : unsqueeze_0 ' ] , name_policy = ' renumerate ' , verbose = true ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / converter . py "" , line <number> , in onnx_to_keras available_converters [ node_type ] ( file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / reshape_layers . py "" , line <number> , in convert_unsqueeze raise attributeerror ( ' number of inputs is not equal <number> for unsqueeze layer ' ) attributeerror : number of inputs is not equal <number> for unsqueeze layer # # # standalone code to reproduce the issue ` ` ` shell please run the below codes to reproduce : from onnx2keras import onnx_to_keras import keras import onnx import sys # sys . path . append ( "" / root / mr "" ) onnx_model = onnx . load ( ' textcnn_torch . onnx ' ) onnx_inputs = onnx_model . graph . input print ( "" = = = = = = = = = = = = = = = = = = = = = = = = = =="") print ( onnx_inputs ) # onnx_model = onnx . load ( ' vgg11 . onnx ' ) k_model = onnx_to_keras ( onnx_model , [ ' onnx : : unsqueeze_0 ' ] , name_policy = ' renumerate ' , verbose = true ) keras . models . save_model ( k_model , ' textcnn_torch . h5 ' , overwrite = true , save_format = "" h5 "" ) ` ` ` onnx file can be downloaded at <url> ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" o2k . py "" , line <number> , in <module> k_model = onnx_to_keras ( onnx_model , [ ' onnx : : unsqueeze_0 ' ] , name_policy = ' renumerate ' , verbose = true ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / converter . py "" , line <number> , in onnx_to_keras available_converters [ node_type ] ( file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / reshape_layers . py "" , line <number> , in convert_unsqueeze raise attributeerror ( ' number of inputs is not equal <number> for unsqueeze layer ' ) attributeerror of inputs is not equal <number> for unsqueeze layer ` ` `",0
tensorflow/tensorflow,"valueerror : exception encountered when calling layer "" <number> "" ( type lambda ) . # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution <number> . <number> - microsoft - standard - wsl2 # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behavior ? < html xmlns : v = "" urn : schemas - microsoft - com : vml "" xmlns <surprise> = "" urn : schemas - microsoft - com : office : office "" xmlns <kiss> = "" urn : schemas - microsoft - com : office : excel "" xmlns = "" <url> <head> < meta name = progid content = excel . sheet > < meta name = generator content = "" microsoft excel <number> "" > < link id = main - file rel = main - file href = "" file :/// c <annoyed> users / pengg / appdata / local / temp / msohtmlclip1 / <number> / clip . htm "" > < link rel = file - list href = "" file :/// c <annoyed> users / pengg / appdata / local / temp / msohtmlclip1 / <number> / clip_filelist . xml "" > <style> < - - table { mso - displayed - decimal - separator :""\\ . "" ; mso - displayed - thousand - separator :""\\ , "" ;} <user> { margin : . 7 5 in . 7 in . 7 5 in . 7 in ; mso - header - margin : . 3 in ; mso - footer - margin : . 3 in ;} . font5 { color : windowtext ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ;} tr { mso - height - source : auto ; mso - ruby - visibility : none ;} col { mso - width - source : auto ; mso - ruby - visibility : none ;} br { mso - data - placement : same - cell ;} td { padding - top : 1 px ; padding - right : 1 px ; padding - left : 1 px ; mso - ignore : padding ; color : black ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ; mso - number - format : general ; text - align : general ; vertical - align : middle ; border : none ; mso - background - source : auto ; mso - pattern : auto ; mso - protection : locked visible ; white - space : nowrap ; mso - rotate : <number> ;} . xl65 { text - align : center ;} . xl66 { text - align : center ; white - space : normal ;} ruby { ruby - align : left ;} rt { color : windowtext ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ; mso - char - type : none ; display : none ;} - - > </style> </head> < body link = "" <hashtag> 0 5 6 3 c1 </hashtag> "" vlink = "" <hashtag> 9 5 4 f72 </hashtag> "" > valueerror : exception encountered when calling layer "" <number> "" ( type lambda ) . dimensions must be equal , but are <number> and <number> for ' { { node <number> / add } } = addv2 [ t = dt_float ] ( placeholder , placeholder_1 ) ' with input shapes : [ ? , <number> , <number> ] , [ ? , <number> , <number> ] . call arguments received by layer "" <number> "" ( type lambda ) : • inputs =[ ' tf . tensor ( shape =( none , <number> , <number> , <number> ) , dtype = float32 ) ' , ' tf . tensor ( shape =( none , <number> , <number> , <number> ) , dtype = float32 ) ' ] • mask = none • training = none - - </body> </html> # # # standalone code to reproduce the issue ` ` ` shell just run the following code to reproduce : from onnx2keras import onnx_to_keras import keras import onnx import sys # sys . path . append ( "" / root / mr "" ) onnx_model = onnx . load ( ' yolov3_darknet53 . onnx ' ) onnx_inputs = onnx_model . graph . input print ( "" = = = = = = = = = = = = = = = = = = = = = = = = = =="") print ( onnx_inputs ) # onnx_model = onnx . load ( ' vgg11 . onnx ' ) k_model = onnx_to_keras ( onnx_model , ['x ' ] , name_policy = ' renumerate ' , verbose = true ) keras . models . save_model ( k_model , ' ssd_resnet50fpn_torch . h5 ' , overwrite = true , save_format = "" h5 "" ) ` ` ` the onnx file can be downloaded at <url> ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" o2k . py "" , line <number> , in <module> k_model = onnx_to_keras ( onnx_model , ['x ' ] , name_policy = ' renumerate ' , verbose = true ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / converter . py "" , line <number> , in onnx_to_keras available_converters [ node_type ] ( file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / elementwise_layers . py "" , line <number> , in convert_elementwise_add layers [ node_name ] = lambda_layer ( [ input_0 , input_1 ] ) file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / keras / src / utils / traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" / root / miniconda3 / envs / onnx / lib / python3 . <number> / site - packages / onnx2keras / elementwise_layers . py "" , line <number> , in target_layer layer = tf . add ( valueerror : exception encountered when calling layer "" layer_12 "" ( type lambda ) . dimensions must be equal , but are <number> and <number> for ' { { node layer_12 / add } } = addv2 [ t = dt_float ] ( placeholder , placeholder_1 ) ' with input shapes : [ ? , <number> , <number> ] , [ ? , <number> , <number> ] . call arguments received by layer "" layer_12 "" ( type lambda ) inputs =[ ' tf . tensor ( shape =( none , <number> , <number> , <number> ) , dtype = float32 ) ' , ' tf . tensor ( shape =( none , <number> , <number> , <number> ) , dtype = float32 ) ' ] • mask = none • training = none ` ` `",0
tensorflow/tensorflow,"training vanilla transformer on tpu gives internalerror # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> . <number> ( google colab ) # # # mobile device colab # # # python version <date> # # # bazel version colab # # # gcc / compiler version colab # # # cuda / cudnn version colab # # # gpu model and memory colab # # # current behavior ? training transformer model on tpu gives internal error i have some idea on the error that there ' s a incompatible tensor ops thats causing the problem but i can not pinpoint it . i had already done a bigger model which is using pretrained embeddings and it went off without a hitch i tried to replicate the same but with different tfds dataset if this is already solved please direct me to the relevant links # # # standalone code to reproduce the issue ` ` ` [ this is the notebook ] ( <url> [ this notebook worked fine ] ( <url> thankyou in advance . i will respond asap ` ` ` # # # relevant log output ` ` ` shell - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - internalerror traceback ( most recent call last ) [ < ipython - input - <number> - 0 1 3 fa12d9e3a > ] ( https :// localhost : <number> /# ) in < cell line : <number> > ( ) - - - - > <number> model . fit ( <number> train_ds , <number> validation_data = valid_ds , <number> epochs = epochs , <number> steps_per_epoch = train_steps , <number> frames [ / usr / local / lib / python3 . <number> / dist - packages / keras / src / utils / traceback_utils . py ] ( https :// localhost : <number> /# ) in error_handler ( * args , * * kwargs ) <number> # to get the full stack trace , call : <number> # ` tf . debugging . disable_traceback_filtering ( ) ` - - - > <number> raise e . with_traceback ( filtered_tb ) from none <number> finally : <number> del filtered_tb [ / usr / local / lib / python3 . <number> / dist - packages / tensorflow / core / function / capture / capture_container . py ] ( https :// localhost : <number> /# ) in capture_by_value ( self , graph , tensor , name ) <number> graph_const = self . by_val_internal . get ( id ( tensor ) ) <number> if graph_const is none : - - > <number> graph_const = tensor . _capture_as_const ( name ) # pylint : disable = protected - access <number> if graph_const is none : <number> # some eager tensors , e . g . parallel tensors , are not convertible to internalerror : failed to connect to all addresses ; last error : unknown : ipv <time> <number> . <number> . <time> <number> : failed to connect to remote host : connection refused additional grpc error information from remote target / job : localhost / replica : <number> / task : <number> / device : cpu : <number> : : unknown : failed to connect to all addresses ; last error : unknown : ipv <time> <number> . <number> . <time> <number> : failed to connect to remote host refused { created_time : "" <number> - <number> - 1 9 t <time> . <number> + <time> "" , grpc_status : <number> } executing non - communication op <multideviceiteratorinit> originally returned unavailableerror , and was replaced by internalerror to avoid invoking tf network error handling logic . ` ` `",0
tensorflow/tensorflow,"body function of ` while_loop ` cannot access the external variable after compilation # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> - dev20230914 # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? the body function of ` while_loop ` cannot access the external variable after compilation . it will raise the error ` unboundlocalerror : local variable ' x ' referenced before assignment ` however , if i run the model without the ` <user> . function ( jit_compile = true ) ` , the model can be executed without any error . # # # standalone code to reproduce the issue ` ` ` shell class model ( tf . keras . model ) : def __init__ ( self ) : super ( model , self ) . __init__ ( ) <user> . function ( jit_compile = true ) # comment this line , it will succeed def call ( self , x) : def cond ( i , _ ) : return i < <number> def body ( i , y ) : y = tf . math . add ( y , <number> ) x = tf . math . multiply ( x , <number> ) return [ tf . math . subtract ( i , <number> ) , y + x] i = tf . constant ( <number> ) y = tf . constant ( <number> ) _ , final_y = tf . while_loop ( cond , body , [ i , y ] , shape_invariants =[ i . shape , y . shape ] ) return final_y m = model ( ) input_shape = [ <number> ] x = tf . constant ( [ <number> . , <number> . ] , shape = input_shape ) y = m ( x ) ` ` ` # # # relevant log output ` ` ` shell unboundlocalerror : exception encountered when calling layer ' model_28 ' ( type model ) . in user code : file "" < ipython - input - <number> - 1 eb50a9c2c75 > "" , line <number> , in body * x = tf . math . multiply ( x , <number> ) unboundlocalerror : local variable ' x ' referenced before assignment call arguments received by layer ' model_28 ' ( type model ) x = tf . tensor ( shape =( <number> , <number> ) , dtype = float32 ) ` ` `",0
tensorflow/tensorflow,"could not find matching concrete function to call loaded from the savedmodel # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution win10 # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? how to match data of signature ? # # # standalone code to reproduce the issue ` ` ` shell . <repeated> model . save ( filepath = save_model_dir , save_format = ' tf ' , signatures = none ) local_model = tf . keras . models . load_model ( filepath = save_model_dir ) y_local_pred = local_model . predict ( x_test ) y_model_pred = model . predict ( x_test ) print ( ' y_local_pred = = y_model_pred :', numpy . allclose ( y_local_pred , y_model_pred ) ) user_inputs = [ tf . tensorspec . from_tensor ( tf . convert_to_tensor ( user_inputs [ <number> ] ) , name = ' inputs / <number> ' ) , tf . tensorspec . from_tensor ( tf . convert_to_tensor ( user_inputs [ <number> ] ) , name = ' inputs / <number> ' ) , tf . tensorspec . from_tensor ( tf . convert_to_tensor ( user_inputs [ <number> ] ) , name = ' inputs / <number> ' ) , ] user_outputs = local_model . user_fn ( user_inputs ) ` ` ` # # # relevant log output ` ` ` shell could not find matching concrete function to call loaded from the savedmodel . got : positional arguments ( <number> total ) : * [ < tf . tensor ' inputs / <number> : <number> ' shape =( <number> , <number> ) dtype = float32 > , < tf . tensor ' inputs / <number> : <number> ' shape =( <number> , <number> ) dtype = int32 > , < tf . tensor ' inputs / <number> : <number> ' shape =( <number> , <number> , <number> ) dtype = int32 > ] keyword arguments : { } expected these arguments to match one of the following <number> option ( s ) : option <number> : positional arguments ( <number> total ) : * ( tensorspec ( shape =( none , <number> ) , dtype = tf . float32 , name = ' inputs / <number> ' ) , tensorspec ( shape =( none , <number> ) , dtype = tf . int32 , name = ' inputs / <number> ' ) , tensorspec ( shape =( none , <number> , <number> ) , dtype = tf . int32 , name = ' inputs / <number> ' ) ) keyword arguments ` ` `",0
tensorflow/tensorflow,"valueerror : unable to create dataset ( name already exists ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? epoch <number> / <number> <number> / <number> [== = = = = = = = = = = = = = = = = = = = = = = = = = = ==] - eta : 0 s - loss : <number> - accuracy : <number> - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - valueerror traceback ( most recent call last ) < ipython - input - <number> - 8 6 5 e41f45523 > in < cell line : <number> > ( ) - - - - > <number> transformer . fit ( train_ds , epochs = epochs , validation_data = val_ds , batch_size = batch_size , callbacks =[ early_stop , checkpoint_call , plot_losses ] ) <number> frames / usr / local / lib / python3 . <number> / dist - packages / h5py / _hl / dataset . py in make_new_dset ( parent , shape , dtype , data , name , chunks , compression , shuffle , fletcher32 , maxshape , compression_opts , fillvalue , scaleoffset , track_times , external , track_order , dcpl , dapl , efile_prefix , virtual_prefix , allow_unknown_filter , rdcc_nslots , rdcc_nbytes , rdcc_w0 ) <number> sid = h5s . create_simple ( shape , maxshape ) <number> - - > <number> dset_id = h5d . create ( parent . id , name , tid , sid , dcpl = dcpl , dapl = dapl ) <number> <number> if ( data is not none ) and ( not isinstance ( data , empty ) <sad> h5py / _objects . pyx in h5py . _objects . with_phil . wrapper ( ) h5py / _objects . pyx in h5py . _objects . with_phil . wrapper ( ) h5py / h5d . pyx in h5py . h5d . create ( ) valueerror : unable to create dataset ( name already exists ) i want to save each epoch as a checkpoint two weeks back back without any any error each checkpoint will save as a checkpoint but suddenly now getting error # # # standalone code to reproduce the issue ` ` ` shell import matplotlib . pyplot as plt from tensorflow . keras . callbacks import callback import os checkpoint_dir = ' / model / checkpoints_m_1 ' if not os . path . exists ( checkpoint_dir ) from tensorflow . keras . callbacks import earlystopping early_stop = earlystopping ( monitor = ' val_loss ' , patience = <number> ) # set up the model checkpoint callback checkpoint_call = modelcheckpoint ( filepath = checkpoint_dir + "" / checkpoint_ { epoch } . hdf5 "" , monitor = ' val_loss ' , save_best_only = true , save_weights_only = false , mode = ' min ' , save_freq = ' epoch ' ) transformer . fit ( train_ds , epochs = epochs , validation_data = val_ds , batch_size = batch_size , callbacks =[ early_stop , checkpoint_call ) ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"bazel test failed with some ctest # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution centos7 . <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? after compiling tensorflow based on the source code , bazel test is executed for unit testing , some test items are passed , but there are many failures , and the reasons for the error are as follows so i want to know if i am executing the statement incorrectly , how should i set it up ? bazel test - c opt - - config = cuda - - test_sharding_strategy = disabled / / tensorflow / core / kernels / . <repeated> output information ${ pager <annoyed> usr / bin / less } "" <money> "" || exit <number> executing tests from / / tensorflow / core / kernels / image : resize_ops_test_gpu [ image ] ( <url> ! [ image ] ( <url> # # # standalone code to reproduce the issue ` ` ` shell bazel test - c opt - - config = cuda - - test_sharding_strategy = disabled / / tensorflow / core / kernels / . <repeated> ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"unable to open file ( truncated file : eof = <number> , sblock - > base_addr = <number> , stored_eof = <number> ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version v2 . <number> - rc2 - <number> - g1cb1a030a62 <number> . <number> # # # custom code no # # # os platform and distribution mac os <number> . <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - oserror traceback ( most recent call last ) cell in [ <number> ] , line <number> <number> # create the base model from the pre - trained model mobilenet v2 <number> img_shape = img_size + ( <number> , ) - - - - > <number> base_model = tf . keras . applications . mobilenet_v2 . mobilenetv2 ( input_shape = img_shape , <number> include_top = false , <number> weights = ' imagenet ' ) file [ ~ / . pyenv / versions / <number> . <number> / lib / python3 . <number> / site - packages / keras / src / applications / mobilenet_v2 . py : <number> ] , in mobilenetv2 ( input_shape , alpha , include_top , weights , input_tensor , pooling , classes , classifier_activation , * * kwargs ) <number> weight_path = base_weight_path + model_name <number> weights_path = data_utils . get_file ( <number> model_name , weight_path , cache_subdir = "" models "" <number> ) - - > <number> model . load_weights ( weights_path ) <number> elif weights is not none : <number> model . load_weights ( weights ) file [ ~ / . pyenv / versions / <number> . <number> / lib / python3 . <number> / site - packages / keras / src / utils / traceback_utils . py : <number> ] , in filter_traceback . <repeated> error_handler ( * args , * * kwargs ) <number> filtered_tb = _process_traceback_frames ( e . __traceback__ ) <number> # to get the full stack trace , call : <number> # ` tf . debugging . disable_traceback_filtering ( ) ` - - - > <number> raise e . with_traceback ( filtered_tb ) from none <number> finally : <number> del filtered_tb . <repeated> file h5py / _objects . pyx : <number> , in h5py . _objects . with_phil . wrapper ( ) file h5py / h5f . pyx : <number> , in h5py . h5f . open ( ) oserror : unable to open file ( truncated file = <number> , sblock - > base_addr = <number> , stored_eof = <number> ) # # # standalone code to reproduce the issue ` ` ` shell <url> # create the base model from the pre - trained model mobilenet v2 img_shape = img_size + ( <number> , ) base_model = tf . keras . applications . mobilenetv2 ( input_shape = img_shape , include_top = false , weights = ' imagenet ' ) ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"xla compiled tf . where with known output shape error ( set_shape , tf . concat / tf . stack ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> # # # custom code no # # # os platform and distribution google colab # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? output of ` tf . where ` , when used inside ` tf . function ` with ` jit_compile = true ` , can sometimes be used correctly ( as with sum ) , and sometimes raises shape mismatch error ( as with concatenation ) . this error is present even if output shape is set manually with ` set_shape ` . the code below runs without ` jit_compile ` or with sum instead of ` tf . concat ` , and only fails if concatenating inside a compiled function . note : ` autoclustering ` solves the issue on the toy example , but _not_ on the codebase i am working on . # # # standalone code to reproduce the issue colab : <url> ` ` ` python import tensorflow as tf def fun ( x , y ) : x = tf . where ( x = = <number> ) print ( f ' shape before ( unknown ) : {x . shape } ' ) x . set_shape ( shape =[ y . shape [ <number> ] , <number> ] ) print ( f ' shape after ( known ) : {x . shape } ' ) return tf . concat ( [x , y ] , axis = <number> ) # concatentation fails # return x + y # sum would succeed x = tf . constant ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , ] , dtype = tf . int32 ) y = tf . expand_dims ( tf . range ( x . shape [ <number> ] * <number> , dtype = tf . int64 ) , axis = - <number> ) fun ( x , y ) tf . function ( fun ) (x , y ) tf . function ( fun , jit_compile = true ) (x , y ) # fails as described above ` ` ` # # # relevant log output ` ` ` shell shape before ( unknown ) : ( none , <number> ) shape after ( known ) : ( <number> , <number> ) invalidargumenterror : cannot concatenate arrays that differ in dimensions other than the one being concatenated . dimension <number> in both shapes must be equal vs s64 [ <number> ] . ` ` `",0
tensorflow/tensorflow,"will there a mlp model in the future version ? # # # issue type feature request # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? when building deep learning models like multi - layer perceptrons ( mlps ) , code reusability and conciseness are crucial factors . currently , using ` tf . keras . sequential ` in tensorflow allows for convenient creation of sequential models . however , manually adding common operations such as batch normalization or dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance . therefore , proposing the addition of a feature in tensorflow to directly create mlps with batch normalization and dropout is highly beneficial . here are several reasons why this feature would be advantageous for tensorflow users : <number> . * * simplified code * * : users will not need to manually add batch normalization and dropout operations to each layer , resulting in cleaner , more readable , and maintainable code . <number> . * * reduced error rate * * : manual copy - pasting of code is error - prone , especially as model complexity increases . automating the integration of batch normalization and dropout operations can reduce issues arising from code errors . <number> . * * increased productivity * * : developers can build and iterate on models more swiftly , focusing on design and parameter tuning rather than rewriting the same code segments for every new model . <number> . * * education and learning * * : for newcomers to tensorflow , this feature can provide a quicker onboarding process , lowering the learning curve and enabling them to grasp and apply deep learning concepts faster . certainly , here ' s the additional information : i also believe that pytorch has implemented mlp functionality quite effectively . an example of this can be found in the following url mlp ] ( <url> pytorch ' s approach to creating mlps provides a good reference for how tensorflow could potentially integrate similar features . # # # standalone code to reproduce the issue ` ` ` shell origin model = tf . keras . sequential ( [ tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , tf . keras . layers . batchnormalization ( ) , tf . keras . layers . relu ( ) , tf . keras . layers . dropout ( <number> ) , tf . keras . layers . dense ( <number> ) , ] ) with mlp model ` ` ` python model = tf . keras . mlp ( hidden_channels =[ <number> , <number> , <number> , <number> ] , norm_layer = tf . keras . layers . batchnormalization , activation_layer = tf . keras . layers . relu , dropout = <number> , ) ` ` ` ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"extensions eglquerydevicesext , eglquerydeviceattribext and eglgetplatformdisplayext not available # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution unbuntu <number> # # # mobile device _no response_ # # # python version python <number> # # # bazel version _no response_ # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version <number> # # # gpu model and memory gtx <number> # # # current behavior ? hi , i got this error when i run dirt model with my tensorflow - gpu <number> . <number> . ` <number> - <number> - <number> <time> . <number> : f / home / engineer1 / dirt / csrc / gl_common . h : <number> ] extensions eglquerydevicesext , eglquerydeviceattribext and eglgetplatformdisplayext not available ` # # # standalone code to reproduce the issue ` ` ` shell import numpy as np import tensorflow as tf import dirt canvas_width , canvas_height = <number> , <number> centre_x , centre_y = <number> , <number> square_size = <number> def get_non_dirt_pixels ( <sad> xs , ys = tf . meshgrid ( tf . range ( canvas_width ) , tf . range ( canvas_height ) ) xs = tf . cast ( xs , tf . float32 ) + <number> ys = tf . cast ( ys , tf . float32 ) + <number> x_in_range = tf . less_equal ( tf . abs ( xs - centre_x ) , square_size / <number> ) y_in_range = tf . less_equal ( tf . abs ( ys - centre_y ) , square_size / <number> ) return tf . cast ( tf . logical_and ( x_in_range , y_in_range ) , tf . float32 ) def get_dirt_pixels ( <sad> # build square in screen space square_vertices = tf . constant ( [ [ <number> , <number> ] , [ <number> , <number> ] , [ <number> , <number> ] , [ <number> , <number> ] ] , dtype = tf . float32 ) * square_size - square_size / <number> . square_vertices + = [ centre_x , centre_y ] # transform to homogeneous coordinates in clip space square_vertices = square_vertices * <number> . / [ canvas_width , canvas_height ] - <number> . square_vertices = tf . concat ( [ square_vertices , tf . zeros ( [ <number> , <number> ] ) , tf . ones ( [ <number> , <number> ] ) ] , axis = <number> ) return dirt . rasterise ( vertices = square_vertices , faces =[[ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , vertex_colors = tf . ones ( [ <number> , <number> ] ) , background = tf . zeros ( [ canvas_height , canvas_width , <number> ] ) , height = canvas_height , width = canvas_width , channels = <number> )[: , :, <number> ] def main ( <sad> if ' . ' in tf . __version__ and int ( tf . __version__ . split ( ' . ' ) [ <number> ] ) < <number> : session = tf . session ( ) with session . as_default ( <sad> non_dirt_pixels = get_non_dirt_pixels ( ) . eval ( ) dirt_pixels = get_dirt_pixels ( ) . eval ( ) else : non_dirt_pixels = get_non_dirt_pixels ( ) . numpy ( ) dirt_pixels = get_dirt_pixels ( ) . numpy ( ) if np . all ( non_dirt_pixels = = dirt_pixels ) : print ( ' successful : all pixels agree ' ) else : print ( ' failed : { } pixels disagree ' . format ( np . sum ( non_dirt_pixels = dirt_pixels ) ) ) if __name__ = = ' __main__ ' : main ( ) ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] your cpu supports instructions that this tensorflow binary was not compiled to use : sse4 . <number> sse4 . <number> avx avx2 fma <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / profile_utils / cpu_utils . cc : <number> ] cpu frequency : <phone> hz <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] xla service 0x 2 4 d6bf0 executing computations on platform host . devices : <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> <undefined> , <undefined> <number> - <number> - <number> <time> . <number> : e tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] could not open file to read numa node : / sys / bus / pci / devices / <number> <time> . <number> / numa_node your kernel may have been built without numa support . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] xla service 0x 2 3 3 ae60 executing computations on platform cuda . devices : <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia geforce gtx <number> 6 gb , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] found device <number> with properties : name : nvidia geforce gtx <number> 6 gb major : <number> minor : <number> memoryclockrate ( ghz ) : <number> pcibusid : <number> <time> . <number> totalmemory : <number> . 0 0 gib freememory : <number> . 0 9 gib <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] adding visible gpu devices : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] device interconnect streamexecutor with strength <number> edge matrix : <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] <number> : n <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] could not identify numa node of platform gpu id <number> , defaulting to <number> . your kernel may not have been built with numa support . <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created tensorflow device ( / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory ) - > physical gpu ( device : <number> , name : nvidia geforce gtx <number> 6 gb , pci bus id : <number> <time> . <number> , compute capability : <number> ) <number> - <number> - <number> <time> . <number> / home / engineer1 / dirt / csrc / gl_common . h : <number> ] extensions eglquerydevicesext , eglquerydeviceattribext and eglgetplatformdisplayext not available aborted ` ` `",0
tensorflow/tensorflow,"batch matmul imprecision # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> / <number> . <number> - dev20230706 # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version ` ` ` $ mamba list | grep cu cuda - nvcc <date> <number> nvidia cudatoolkit <number> . <number> h4ba93d1_12 conda - forge nvidia - cublas - cu11 <number> . <number> pypi_0 pypi nvidia - cublas - cu117 <number> . <number> pypi_0 pypi nvidia - cuda - nvrtc - cu11 <number> . <number> pypi_0 pypi nvidia - cuda - nvrtc - cu117 <date> pypi_0 pypi nvidia - cudnn - cu11 <number> . <number> pypi_0 pypi ` ` ` # # # gpu model and memory nvidia geforce rtx <number> - <number> gb driver version : <number> . <number> # # # current behavior ? i am trying to do a batch matrix multiply ( i . e . i have got a bunch of m x n matrices , all in one tensor , and i want to do a matrix multiply of each of them with some other matrix ) . however , i am getting slightly different results than i get from numpy ( and previous tensorflow versions , this script passed for me in <number> ) . one interesting thing is that the value <number> ( the second dimension of ` x ` ) is significant . if i reduce this to <number> or below , it passes . also , if i change the second dimension of ` c ` from <number> to <number> , it also passes . # # # standalone code to reproduce the issue ` ` ` python import numpy as np import tensorflow as tf print ( f "" file : { tf . __file__ } "" ) print ( f "" git version : { tf . version . git_version } "" ) print ( f "" version : { tf . __version__ } "" ) rng = np . random . randomstate ( <number> ) x = rng . uniform ( - <number> , <number> , size =( <number> , <number> , <number> ) ) . astype ( np . float32 ) c = rng . uniform ( - <number> , <number> , size =( <number> , <number> ) ) . astype ( np . float32 ) y = x @ c x2 = np . tile ( x , ( <number> , <number> , <number> ) ) y2 = x2 @ c for y2i in y2 : np . testing . assert_allclose ( y2i , y . squeeze ( <number> ) ) tols = dict ( atol = 1 e - <number> , rtol = 1 e - <number> ) z = tf . matmul ( x , c ) . numpy ( ) # z = tf . einsum ( "" . <repeated> tq , . <repeated> qr - > . <repeated> tr "" , x , c ) np . testing . assert_allclose ( z , y , * * tols ) z2 = tf . matmul ( x2 , c ) . numpy ( ) # z2 = tf . einsum ( "" . <repeated> tq , . <repeated> qr - > . <repeated> tr "" , x2 , c ) np . testing . assert_allclose ( z2 , y2 , * * tols ) ` ` ` # # # relevant log output ` ` ` shell file : / home / ehunsber / mambaforge / envs / tf213 / lib / python3 . <number> / site - packages / tensorflow / __init__ . py git version : v1 . <number> - <number> - gfa4d29bfef8 version : <number> . <number> - dev20230706 traceback ( most recent call last ) : file "" test_batch . py "" , line <number> , in <module> np . testing . assert_allclose ( z2 , y2 ) file "" / home / ehunsber / mambaforge / envs / tf213 / lib / python3 . <number> / site - packages / numpy / testing / _private / utils . py "" , line <number> , in assert_allclose assert_array_compare ( compare , actual , desired , err_msg = str ( err_msg ) , file "" / home / ehunsber / mambaforge / envs / tf213 / lib / python3 . <number> / contextlib . py "" , line <number> , in inner return func ( * args , * * kwds ) file "" / home / ehunsber / mambaforge / envs / tf213 / lib / python3 . <number> / site - packages / numpy / testing / _private / utils . py "" , line <number> , in assert_array_compare raise assertionerror ( msg ) assertionerror : not equal to tolerance rtol = 1 e - <number> , atol = <number> mismatched elements : <number> / <number> ( <percent> ) max absolute difference : <number> max relative difference : <number> <kiss> array ( [ [ [ - <number> , <number> ] , [ - <number> , - <number> ] , [ <number> , - <number> ] , . <repeated> y <number> ] , [ - <number> , - <number> ] , [ <number> , - <number> ] , . <repeated> ` ` `",0
tensorflow/tensorflow,"tf quantizer breaks pip package # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version git head # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device n / a # # # python version <date> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version n / a # # # gpu model and memory n / a # # # current behavior ? the commit <url> added pywrap_quantize_model . so to the pip package but the build fails to set the rpath for _pywrap_tensorflow_internal . so in the new . so leading to a failure in auditwheel when attempting to ' repair ' to make it manylinux2014 compatible . # # # standalone code to reproduce the issue ` ` ` shell $ python3 - m auditwheel repair - - plat manylinux2014_aarch64 . / tensorflow - pkg / tensorflow_aarch64 - <number> . <number> - cp311 - cp311 - linux_aarch64 . whl - - wheel - dir . / whl / ` ` ` # # # relevant log output ` ` ` shell $ python3 - m auditwheel repair - - plat manylinux2014_aarch64 . / tensorflow - pkg / tensorflow_aarch64 - <number> . <number> - cp311 - cp311 - linux_aarch64 . whl - - wheel - dir . / whl / info : auditwheel . main_repair : repairing tensorflow_aarch64 - <number> . <number> - cp311 - cp311 - linux_aarch64 . whl traceback ( most recent call last ) : file "" < frozen runpy > "" , line <number> , in _run_module_as_main file "" < frozen runpy > "" , line <number> , in _run_code file "" / usr / local / lib / python3 . <number> / dist - packages / auditwheel / __main__ . py "" , line <number> , in <module> sys . exit ( main ( ) ) ^^^ ^^^ file "" / usr / local / lib / python3 . <number> / dist - packages / auditwheel / main . py "" , line <number> , in main rval = args . func ( args , p ) ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ file "" / usr / local / lib / python3 . <number> / dist - packages / auditwheel / main_repair . py "" , line <number> , in execute out_wheel = repair_wheel ( ^^^ ^^^ ^^^ ^^^ ^ file "" / usr / local / lib / python3 . <number> / dist - packages / auditwheel / repair . py "" , line <number> , in repair_wheel raise valueerror ( valueerror repair wheel , because required library "" _pywrap_tensorflow_internal . so "" could not be located also $ ldd venv_bad / lib / python3 . <number> / site - packages / tensorflow / compiler / mlir / quantization / tensorflow / python / pywrap_quantize_model . so linux - vdso . so . <number> ( 0x0 0 0 0 ffffb8af9000 ) libtensorflow_framework . so . <number> => / workspace / venv_bad / lib / python3 . <number> / site - packages / tensorflow / compiler / mlir / quantization / tensorflow / python / . <repeated> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / libtensorflow_framework . so . <number> ( 0x0 0 0 0 ffffb6800000 ) _pywrap_tensorflow_internal . so => not found libdl . so . <number> => / lib / aarch64 - linux - gnu / libdl . so . <number> ( 0x0 0 0 0 ffffb8923000 ) libpthread . so . <number> => / lib / aarch64 - linux - gnu / libpthread . so . <number> ( 0x0 0 0 0 ffffb88f2000 ) libm . so . <number> => / lib / aarch64 - linux - gnu / libm . so . <number> ( 0x0 0 0 0 ffffb8847000 ) libstdc + + . so . <number> => / lib / aarch64 - linux - gnu / libstdc + + . so . <number> ( 0x0 0 0 0 ffffb661b000 ) libgcc_s . so . <number> => / lib / aarch64 - linux - gnu / libgcc_s . so . <number> ( 0x0 0 0 0 ffffb8823000 ) libc . so . <number> => / lib / aarch64 - linux - gnu / libc . so . <number> ( 0x0 0 0 0 ffffb64a8000 ) / lib / ld - linux - aarch64 . so . <number> ( 0x0 0 0 0 ffffb8ac9000 ) librt . so . <number> => / lib / aarch64 - linux - gnu / librt . so . <number> ( 0x0 0 0 0 ffffb8809000 ) compare with $ ldd venv_bad / lib / python3 . <number> / site - packages / tensorflow / python / saved_model / pywrap_saved_model . so linux - vdso . so . <number> ( 0x0 0 0 0 ffffa40a2000 ) libtensorflow_framework . so . <number> => / workspace / venv_bad / lib / python3 . <number> / site - packages / tensorflow / python / saved_model / . <repeated> / . <repeated> / libtensorflow_framework . so . <number> ( 0x0 0 0 0 ffffa1e00000 ) _pywrap_tensorflow_internal . so => / workspace / venv_bad / lib / python3 . <number> / site - packages / tensorflow / python / saved_model / . <repeated> / _pywrap_tensorflow_internal . so ( 0x0 0 0 0 ffffa1c0a000 ) libdl . so . <number> => / lib / aarch64 - linux - gnu / libdl . so . <number> ( 0x0 0 0 0 ffffa3ee5000 ) libm . so . <number> => / lib / aarch64 - linux - gnu / libm . so . <number> ( 0x0 0 0 0 ffffa3e3a000 ) libpthread . so . <number> => / lib / aarch64 - linux - gnu / libpthread . so . <number> ( 0x0 0 0 0 ffffa3e09000 ) libstdc + + . so . <number> => / lib / aarch64 - linux - gnu / libstdc + + . so . <number> ( 0x0 0 0 0 ffffa1a25000 ) libgcc_s . so . <number> => / lib / aarch64 - linux - gnu / libgcc_s . so . <number> ( 0x0 0 0 0 ffffa3de5000 ) libc . so . <number> => / lib / aarch64 - linux - gnu / libc . so . <number> ( 0x0 0 0 0 ffffa18b2000 ) / lib / ld - linux - aarch64 . so . <number> ( 0x0 0 0 0 ffffa4072000 ) librt . so . <number> => / lib / aarch64 - linux - gnu / librt . so . <number> ( 0x0 0 0 0 ffffa3dcd000 ) libtensorflow_cc . so . <number> => / workspace / venv_bad / lib / python3 . <number> / site - packages / tensorflow / python / saved_model / . <repeated> / . <repeated> / libtensorflow_cc . so . <number> ( 0x0 0 0 0 ffff8be00000 ) libml_dtypes . so . so => / workspace / venv_bad / lib / python3 . <number> / site - packages / tensorflow / python / saved_model / . <repeated> / . <repeated> / tsl / python / lib / core / libml_dtypes . so . so ( 0x0 0 0 0 ffffa3d99000 ) libomp . so . <number> => / usr / lib / llvm - <number> / lib / libomp . so . <number> ( 0x0 0 0 0 ffff8bcc0000 ) ` ` `",0
tensorflow/tensorflow,"file "" / usr / local / lib / python3 . <number> / dist - packages / tensorflow / lite / python / interpreter . py "" , line <number> , in invoke self . _interpreter . invoke ( ) runtimeerror : tensorflow / lite / kernels / concatenation . cc : <number> t - > dims - > data [ d ] = t0 - > dims - > data [ d ] ( <number> ! = <number> ) node number <number> ( concatenation ) failed to prepare . # # # <number> . system information ! [ image ] ( <url> - os platform and distribution ( linux ubuntu <number> <sad> - tensorflow installation ( <number> . <number> + nv22 . <number> <sad> # # # <number> . code ` import numpy as np import tensorflow as tf def main ( <sad> # load the tflite model and allocate tensors . interpreter = tf . lite . interpreter ( model_path = "" model . tflite "" ) interpreter . allocate_tensors ( ) <hashtag> return </hashtag> # get input and output tensors . input_details = interpreter . get_input_details ( ) output_details = interpreter . get_output_details ( ) # test the model on random input data . input_shape = input_details [ <number> ] [ ' shape ' ] input_data = np . array ( np . random . random_sample ( input_shape ) , dtype = np . float32 ) <hashtag> interpreter </hashtag> . set_tensor ( input_details [ <number> ] [ ' index ' ] , input_data ) signatures = interpreter . get_signature_list ( ) print ( signatures ) interpreter . invoke ( ) return # the function ` get_tensor ( ) ` returns a copy of the tensor data . # use ` tensor ( ) ` in order to get a pointer to the tensor . output_data = interpreter . get_tensor ( output_details [ <number> ] [ ' index ' ] ) print ( output_data ) if __name__ = = ' __main__ ' : main ( ) ` * * how to get the model : * * i used a library here to get a unet with efficientnetb0 as encoder <url> so you can try to use my script to get the model backbone = ' efficientnetb0 ' batch_size = <number> classes = [ "" background "" , "" target "" , "" others "" ] lr = <number> epochs = <number> preprocess_input = sm . get_preprocessing ( backbone ) # define network parameters n_classes = <number> if len ( classes ) = = <number> else ( len ( classes ) + <number> ) # case for binary and multiclass segmentation activation = ' sigmoid ' if n_classes = = <number> else ' softmax ' <hashtag> create </hashtag> model model = sm . unet ( backbone , classes = n_classes , activation = activation ) `",0
tensorflow/tensorflow,"check failed when running tensorflow . python . ops . gen_nn_ops . max_pool_grad_with_argmax # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? specific input combination is caused check failure . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . ops import gen_nn_ops try : try : with tf . device ( ' / cpu ' <sad> arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_2_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , minval = - <number> , maxval = <number> , dtype = tf . int64 ) arg_2 = tf . identity ( arg_2_tensor ) ksize_0 = <number> ksize_1 = <number> ksize_2 = <number> ksize_3 = <number> ksize = [ ksize_0 , ksize_1 , ksize_2 , ksize_3 , ] strides_0 = <number> strides_1 = <number> strides_2 = <number> strides_3 = <number> strides = [ strides_0 , strides_1 , strides_2 , strides_3 , ] padding = "" valid "" include_batch_in_index = false out = gen_nn_ops . max_pool_grad_with_argmax ( arg_0 , arg_1 , arg_2 , ksize = ksize , strides = strides , padding = padding , include_batch_in_index = include_batch_in_index , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> arg_0 = tf . identity ( arg_0_tensor ) arg_0 = tf . cast ( arg_0 , tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_1 = tf . cast ( arg_1 , tf . float32 ) arg_2 = tf . identity ( arg_2_tensor ) arg_2 = tf . cast ( arg_2 , tf . int64 ) ksize = [ ksize_0 , ksize_1 , ksize_2 , ksize_3 , ] strides = [ strides_0 , strides_1 , strides_2 , strides_3 , ] gen_nn_ops . max_pool_grad_with_argmax ( arg_0 , arg_1 , arg_2 , ksize = ksize , strides = strides , padding = padding , include_batch_in_index = include_batch_in_index , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : f tensorflow / core / kernels / maxpooling_op . cc : <number> ] check failed : grad_out_index >= output_start & & grad_out_index < output_end invalid output gradient index <number> , <number> aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"cannot build tensorflow from source # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version latest # # # custom code yes # # # os platform and distribution debian <number> # # # mobile device _no response_ # # # python version python <number> # # # bazel version latest # # # gcc / compiler version clang <number> # # # cuda / cudnn version dont have # # # gpu model and memory dont have # # # current behavior ? i tried several times , still same error . searched on google found nothing # # # standalone code to reproduce the issue ` ` ` shell i follow the guide from tensorflow . com but still faced this error . please help ` ` ` # # # relevant log output ` ` ` shell ( myenv ) root <user> : ~ / tensorflow # bazel build - j <number> - - local_ram_resources = <number> - - config = opt - - verbose_failures / / tensorflow / tools / pip_package : build_pip_package info : options provided by the client : inherited ' common ' options : - - isatty = <number> - - terminal_columns = <number> info : reading rc options for ' build ' from / root / tensorflow / . bazelrc : inherited ' common ' options : - - experimental_repo_remote_exec info : reading rc options for ' build ' from / root / tensorflow / . bazelrc : ' build ' options : - - define framework_shared_object = true - - define tsl_protobuf_header_only = true - - define = use_fast_cpp_protos = true - - define = allow_oversize_protos = true - - spawn_strategy = standalone - c opt - - announce_rc - - define = grpc_no_ares = true - - noincompatible_remove_legacy_whole_archive - - features = - force_no_whole_archive - - enable_platform_specific_config - - define = with_xla_support = true - - config = short_logs - - config = v2 - - define = no_aws_support = true - - define = no_hdfs_support = true - - experimental_cc_shared_library - - experimental_link_static_libraries_once = false - - incompatible_enforce_config_setting_visibility info : reading rc options for ' build ' from / root / tensorflow / . tf_configure . bazelrc : ' build ' options : - - action_env python_bin_path <annoyed> root / anaconda3 / envs / myenv / bin / python3 - - action_env python_lib_path <annoyed> root / anaconda3 / envs / myenv / lib / python3 . <number> / site - packages - - python_path <annoyed> root / anaconda3 / envs / myenv / bin / python3 - - action_env clang_compiler_path <annoyed> usr / lib / llvm - <number> / bin / clang - - repo_env = cc <annoyed> usr / lib / llvm - <number> / bin / clang - - repo_env = bazel_compiler <annoyed> usr / lib / llvm - <number> / bin / clang - - copt = - wno - gnu - offsetof - extensions info : found applicable config definition build : short_logs in file / root / tensorflow / . bazelrc : - - output_filter = dont_match_anything info : found applicable config definition build : v2 in file / root / tensorflow / . bazelrc : - - define = tf_api_version = <number> - - action_env = tf2_behavior = <number> info : found applicable config definition build : opt in file / root / tensorflow / . tf_configure . bazelrc : - - copt = - wno - sign - compare - - host_copt = - wno - sign - compare info : found applicable config definition build : linux in file / root / tensorflow / . bazelrc : - - host_copt = - w - - copt = - wno - all - - copt = - wno - extra - - copt = - wno - deprecated - - copt = - wno - deprecated - declarations - - copt = - wno - ignored - attributes - - copt = - wno - array - bounds - - copt = - wunused - result - - copt = - werror = unused - result - - copt = - wswitch - - copt = - werror = switch - - copt = - wno - error = unused - but - set - variable - - define = prefix <annoyed> usr - - define = libdir =$( prefix ) / lib - - define = includedir =$( prefix ) / include - - define = protobuf_include_path =$( prefix ) / include - - cxxopt = - std =c + + <number> - - host_cxxopt = - std =c + + <number> - - config = dynamic_kernels - - experimental_guard_against_concurrent_changes info : found applicable config definition build : dynamic_kernels in file / root / tensorflow / . bazelrc : - - define = dynamic_loaded_kernels = true - - copt = - dautoload_dynamic_kernels info : analyzed target / / tensorflow / tools / pip_package : build_pip_package ( <number> packages loaded , <number> targets configured ) . info : found <number> target . <repeated> info : deleting stale sandbox base / root / . cache / bazel / _bazel_root / efb88f6336d9c4a18216fb94287b8d97 / sandbox error : / root / tensorflow / tensorflow / compiler / mlir / tensorflow / build : <number> <time> : compiling tensorflow / compiler / mlir / tensorflow / ir / tf_ops . cc failed : ( killed ) : clang failed : error executing command ( from target / / tensorflow / compiler / mlir / tensorflow : tensorflow_ops ) ( cd / root / . cache / bazel / _bazel_root / efb88f6336d9c4a18216fb94287b8d97 / execroot / org_tensorflow & & \ \ exec env - \ \ clang_compiler_path <annoyed> usr / lib / llvm - <number> / bin / clang \ \ path <annoyed> root / . cache / bazelisk / downloads / bazelbuild / bazel - <number> . <number> - linux - x86_64 / bin <annoyed> root / anaconda3 / envs / myenv / bin <annoyed> root / anaconda3 / condabin <annoyed> usr / local / sbin <annoyed> usr / local / bin <annoyed> usr / sbin <annoyed> usr / bin <annoyed> sbin <annoyed> bin \ \ pwd <annoyed> proc / self / cwd \ \ python_bin_path <annoyed> root / anaconda3 / envs / myenv / bin / python3 \ \ python_lib_path <annoyed> root / anaconda3 / envs / myenv / lib / python3 . <number> / site - packages \ \ tf2_behavior = <number> \ \ / usr / lib / llvm - <number> / bin / clang - u_fortify_source - fstack - protector - wall - wthread - safety - wself - assign - wunused - but - set - parameter - wno - free - nonheap - object - fcolor - diagnostics - fno - omit - frame - pointer - g0 - o2 ' - d_fortify_source = <number> ' - dndebug - ffunction - sections - fdata - sections ' - std =c + + 0x ' - md - mf bazel - out / k8 - opt / bin / tensorflow / compiler / mlir / tensorflow / _objs / tensorflow_ops / tf_ops . pic . d ' - frandom - seed = bazel - out / k8 - opt / bin / tensorflow / compiler / mlir / tensorflow / _objs / tensorflow_ops / tf_ops . pic . o ' - fpic ' - dllvm_on_unix = <number> ' ' - dhave_backtrace = <number> ' ' - dbacktrace_header =< execinfo . h > ' ' - dltdl_shlib_ext ="". so "" ' ' - dllvm_plugin_ext ="". so "" ' ' - dllvm_enable_threads = <number> ' ' - dhave_deregister_frame = <number> ' ' - dhave_libpthread = <number> ' ' - dhave_pthread_getname_np = <number> ' ' - dhave_pthread_h = <number> ' ' - dhave_pthread_setname_np = <number> ' ' - dhave_register_frame = <number> ' ' - dhave_setenv_r = <number> ' ' - dhave_strerror_r = <number> ' ' - dhave_sysexits_h = <number> ' ' - dhave_unistd_h = <number> ' - d_gnu_source ' - dhave_link_h = <number> ' ' - dhave_mallinfo = <number> ' ' - dhave_sbrk = <number> ' ' - dhave_struct_stat_st_mtim_tv_nsec = <number> ' ' - dllvm_native_arch =""x 8 6 "" ' ' - dllvm_native_asmparser = llvminitializex86asmparser ' ' - dllvm_native_asmprinter = llvminitializex86asmprinter ' ' - dllvm_native_disassembler = llvminitializex86disassembler ' ' - dllvm_native_target = llvminitializex86target ' ' - dllvm_native_targetinfo = llvminitializex86targetinfo ' ' - dllvm_native_targetmc = llvminitializex86targetmc ' ' - dllvm_native_targetmca = llvminitializex86targetmca ' ' - dllvm_host_triple =""x 8 6 _64 - unknown - linux - gnu "" ' ' - dllvm_default_target_triple =""x 8 6 _64 - unknown - linux - gnu "" ' ' - dllvm_version_major = <number> ' ' - dllvm_version_minor = <number> ' ' - dllvm_version_patch = <number> ' ' - dllvm_version_string = "" <number> . 0 git "" ' - d__stdc_limit_macros - d__stdc_constant_macros - d__stdc_format_macros ' - dblake3_use_neon = <number> ' - dblake3_no_avx2 - dblake3_no_avx512 - dblake3_no_sse2 - dblake3_no_sse41 - deigen_mpl2_only ' - deigen_max_align_bytes = <number> ' - dhave_sys_uio_h - dtf_use_snappy ' - dbazel_current_repository = "" "" ' - iquote . - iquote bazel - out / k8 - opt / bin - iquote external / com_google_absl - iquote bazel - out / k8 - opt / bin / external / com_google_absl - iquote external / llvm - project - iquote bazel - out / k8 - opt / bin / external / llvm - project - iquote external / nsync - iquote bazel - out / k8 - opt / bin / external / nsync - iquote external / com_google_protobuf - iquote bazel - out / k8 - opt / bin / external / com_google_protobuf - iquote external / gif - iquote bazel - out / k8 - opt / bin / external / gif - iquote external / libjpeg_turbo - iquote bazel - out / k8 - opt / bin / external / libjpeg_turbo - iquote external / com_googlesource_code_re2 - iquote bazel - out / k8 - opt / bin / external / com_googlesource_code_re2 - iquote external / farmhash_archive - iquote bazel - out / k8 - opt / bin / external / farmhash_archive - iquote external / fft2d - iquote bazel - out / k8 - opt / bin / external / fft2d - iquote external / highwayhash - iquote bazel - out / k8 - opt / bin / external / highwayhash - iquote external / zlib - iquote bazel - out / k8 - opt / bin / external / zlib - iquote external / eigen_archive - iquote bazel - out / k8 - opt / bin / external / eigen_archive - iquote external / ml_dtypes - iquote bazel - out / k8 - opt / bin / external / ml_dtypes - iquote external / double_conversion - iquote bazel - out / k8 - opt / bin / external / double_conversion - iquote external / snappy - iquote bazel - out / k8 - opt / bin / external / snappy - ibazel - out / k8 - opt / bin / external / llvm - project / mlir / _virtual_includes / arithcanonicalizationincgen - ibazel - out / k8 - opt / bin / external / llvm - project / mlir / _virtual_includes / asmparsertokenkinds - isystem external / llvm - project / llvm / include - isystem bazel - out / k8 - opt / bin / external / llvm - project / llvm / include - isystem external / llvm - project / mlir / include - isystem bazel - out / k8 - opt / bin / external / llvm - project / mlir / include - isystem external / nsync / public - isystem bazel - out / k8 - opt / bin / external / nsync / public - isystem external / com_google_protobuf / src - isystem bazel - out / k8 - opt / bin / external / com_google_protobuf / src - isystem external / gif - isystem bazel - out / k8 - opt / bin / external / gif - isystem external / farmhash_archive / src - isystem bazel - out / k8 - opt / bin / external / farmhash_archive / src - isystem external / zlib - isystem bazel - out / k8 - opt / bin / external / zlib - isystem third_party / eigen3 / mkl_include - isystem bazel - out / k8 - opt / bin / third_party / eigen3 / mkl_include - isystem external / eigen_archive - isystem bazel - out / k8 - opt / bin / external / eigen_archive - isystem external / ml_dtypes - isystem bazel - out / k8 - opt / bin / external / ml_dtypes - isystem external / ml_dtypes / ml_dtypes - isystem bazel - out / k8 - opt / bin / external / ml_dtypes / ml_dtypes - wno - all - wno - extra - wno - deprecated - wno - deprecated - declarations - wno - ignored - attributes - wno - array - bounds - wunused - result ' - werror = unused - result ' - wswitch ' - werror = switch ' ' - wno - error = unused - but - set - variable ' - dautoload_dynamic_kernels - wno - gnu - offsetof - extensions - wno - sign - compare ' - std =c + + <number> ' - no - canonical - prefixes - wno - builtin - macro - redefined ' - d__date__ = "" redacted "" ' ' - d__timestamp__ = "" redacted "" ' ' - d__time__ = "" redacted "" ' - c tensorflow / compiler / mlir / tensorflow / ir / tf_ops . cc - o bazel - out / k8 - opt / bin / tensorflow / compiler / mlir / tensorflow / _objs / tensorflow_ops / tf_ops . pic . o ) # configuration : 4 3 3 2 b06bceb8e99a0d8ed4f75fa26218a779c66acf683ffad0936df1e9f625df # execution platform : <user> / / : platform in file included from tensorflow / compiler / mlir / tensorflow / ir / tf_ops . cc : <number> : in file included from . / tensorflow / compiler / mlir / tensorflow / ir / tf_ops . h : <number> : in file included from . / tensorflow / compiler / mlir / tensorflow / ir / tf_attributes . h : <number> : in file included from . / tensorflow / core / ir / types / dialect . h : <number> : bazel - out / k8 - opt / bin / tensorflow / core / ir / types / dialect . h . inc : <number> <time> : warning : ' parsetype ' overrides a member function but is not marked ' override ' [ - winconsistent - missing - override ] : : mlir : : type parsetype ( : : mlir : : dialectasmparser & parser ) const ; ^ external / llvm - project / mlir / include / mlir / ir / dialect . h : <number> <time> : note : overridden virtual function is here virtual type parsetype ( dialectasmparser & parser ) const ; ^ in file included from tensorflow / compiler / mlir / tensorflow / ir / tf_ops . cc : <number> : in file included from . / tensorflow / compiler / mlir / tensorflow / ir / tf_ops . h : <number> : in file included from . / tensorflow / compiler / mlir / tensorflow / ir / tf_attributes . h : <number> : in file included from . / tensorflow / core / ir / types / dialect . h : <number> : bazel - out / k8 - opt / bin / tensorflow / core / ir / types / dialect . h . inc : <number> <time> : warning : ' printtype ' overrides a member function but is not marked ' override ' [ - winconsistent - missing - override ] void printtype ( : : mlir : : type type , : : mlir : : dialectasmprinter & printer ) const ; ^ external / llvm - project / mlir / include / mlir / ir / dialect . h : <number> <time> : note : overridden virtual function is here virtual void printtype ( type , dialectasmprinter & ) const { ^ target / / tensorflow / tools / pip_package : build_pip_package failed to build info : elapsed time : <number> . 0 1 1 s , critical path : <number> . 1 6 s info : <number> processes : <number> internal , <number> local . failed did not complete successfully ` ` `",0
tensorflow/tensorflow,"libtensorflowlite_jni . so ( offset 0x 2 f3000 ) : signal <number> ( sigsegv ) , code <number> ( segv_accerr ) , fault addr 0 xa59fdbc0 # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version org . tensorflow : tensorflow - lite : <number> . <number> - nightly org . tensorflow : tensorflow - lite - gpu : <number> . <number> org . tensorflow : tensorflow - lite - support : <number> . <number> # # # custom code yes # # # os platform and distribution andorid <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? our process com . vt . tv . aipq use tensorflow - lite cause process crash : * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * build fingerprint : ' hisense / songshan - ffm / songshan : <number> / rtt2 . <number> / <number> . <number> : user / release - keys ' revision : ' <number> ' abi : ' arm ' timestamp : <number> - <number> - <number> <time> + <number> pid : <number> , tid : <number> , name : tfservice - t > > > com . vt . tv . aipq < < < uid : <number> signal <number> ( sigsegv ) , code <number> ( segv_accerr ) , fault addr 0 xa59fdbc0 r0 <number> r1 <number> r2 <number> r3 a59fdbc0 r4 3 1 a05640 r5 0 0 0 0 0 2 f0 r6 3 1 a05740 r7 a59fe4c0 r8 3 1 a056c0 r9 a5674220 r10 a59fe1c0 r11 3 1 a055c0 ip a59fdec0 sp 8 4 db73e8 lr 8 3 2 ded97 pc 8 3 2 f08ac backtrace pc 0 0 0 2 5 8 ac / system_ext / app / hiaipq / hiaipq . apklibtensorflowlite_jni . so ( offset 0x 2 f3000 ) # # # standalone code to reproduce the issue ` ` ` shell the google server background statistics process crash information , unable to clarify the steps to reproduce the problem . ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"overflow bug when running tf . image . resize # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large elements in the input list # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : arg_0_tensor = tf . constant ( - <number> , shape =[ <number> , <number> , <number> , <number> ] , dtype = tf . float64 , ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_0 = <phone> arg_1_1 = <number> arg_1 = [ arg_1_0 , arg_1_1 , ] out = tf . image . resize ( arg_0 , arg_1 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__resizebilinear_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <phone> , result : - <number> [ op : resizebilinear ] name ` ` `",0
tensorflow/tensorflow,"overflow when running tf . compat . v1 . linalg . diag # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large elements in the input list # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : diagonal_0_0_0_0 = <number> diagonal_0_0_0_1 = <number> diagonal_0_0_0 = [ diagonal_0_0_0_0 , diagonal_0_0_0_1 , ] diagonal_0_0_1_0 = <number> diagonal_0_0_1_1 = <number> diagonal_0_0_1 = [ diagonal_0_0_1_0 , diagonal_0_0_1_1 , ] diagonal_0_0 = [ diagonal_0_0_0 , diagonal_0_0_1 , ] diagonal_0_1_0_0 = <number> diagonal_0_1_0_1 = <number> diagonal_0_1_0 = [ diagonal_0_1_0_0 , diagonal_0_1_0_1 , ] diagonal_0_1_1_0 = <number> diagonal_0_1_1_1 = <number> diagonal_0_1_1 = [ diagonal_0_1_1_0 , diagonal_0_1_1_1 , ] diagonal_0_1 = [ diagonal_0_1_0 , diagonal_0_1_1 , ] diagonal_0 = [ diagonal_0_0 , diagonal_0_1 , ] diagonal_1_0_0_0 = <number> diagonal_1_0_0_1 = <number> diagonal_1_0_0 = [ diagonal_1_0_0_0 , diagonal_1_0_0_1 , ] diagonal_1_0_1_0 = <number> diagonal_1_0_1_1 = <number> diagonal_1_0_1 = [ diagonal_1_0_1_0 , diagonal_1_0_1_1 , ] diagonal_1_0 = [ diagonal_1_0_0 , diagonal_1_0_1 , ] diagonal_1_1_0_0 = <number> diagonal_1_1_0_1 = <number> diagonal_1_1_0 = [ diagonal_1_1_0_0 , diagonal_1_1_0_1 , ] diagonal_1_1_1_0 = <number> diagonal_1_1_1_1 = <number> diagonal_1_1_1 = [ diagonal_1_1_1_0 , diagonal_1_1_1_1 , ] diagonal_1_1 = [ diagonal_1_1_0 , diagonal_1_1_1 , ] diagonal_1 = [ diagonal_1_0 , diagonal_1_1 , ] diagonal = [ diagonal_0 , diagonal_1 , ] name = "" diag_part "" k = <phone> padding_value = <number> align = "" right_left "" out = tf . compat . v1 . linalg . diag ( diagonal = diagonal , name = name , k = k , padding_value = padding_value , align = align , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__matrixdiagv3_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } encountered overflow when multiplying <number> with <phone> , result [ op : matrixdiagv3 ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow when running tf . compat . v1 . tile # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large elements in the input list # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : input_tensor = tf . random . uniform ( [ <number> , <number> , <number> ] , dtype = tf . float32 ) input = tf . identity ( input_tensor ) multiples_0 = <number> multiples_1 = <number> multiples_2 = <number> multiples = [ multiples_0 , multiples_1 , multiples_2 , ] name_tensor = tf . random . uniform ( [ ] , dtype = tf . int32 , maxval = <number> ) name = tf . identity ( name_tensor ) name = tf . variable ( name ) out = tf . compat . v1 . tile ( input = input , multiples = multiples , name = name , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__tile_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result [ [ { { node tile } } ] ] [ op : tile ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow bug when running tf . raw_ops . resizenearestneighbor # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large list element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : images_tensor = tf . constant ( - <number> , shape =[ <number> , <number> , <number> , <number> ] , dtype = tf . float16 , ) images = tf . identity ( images_tensor ) size_0 = <number> size_1 = <phone> size = [ size_0 , size_1 , ] align_corners = false half_pixel_centers = false name = none out = tf . raw_ops . resizenearestneighbor ( images = images , size = size , align_corners = align_corners , half_pixel_centers = half_pixel_centers , name = name , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__resizenearestneighbor_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <phone> , result [ op : resizenearestneighbor ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow bug when running tf . keras . layers . zeropadding2d # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large list element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : padding_0_0 = <number> padding_0_1 = false padding_0 = [ padding_0_0 , padding_0_1 , ] padding_1_0 = <number> padding_1_1 = <number> padding_1 = [ padding_1_0 , padding_1_1 , ] padding = [ padding_0 , padding_1 , ] arg_class = tf . keras . layers . zeropadding2d ( padding = padding , ) arg_input_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_input_0 = tf . identity ( arg_input_0_tensor ) arg_input = [ arg_input_0 , ] out = arg_class ( * arg_input ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell [ ' { { function_node __wrapped__pad_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result ' \ \ t [ [ { { node pad } } ] ] [ op : pad ] \ \ n ' , ' \ \ n ' , "" call arguments received by layer ' zero_padding2d ' ( type zeropadding2d ) :\\ n "" , ' • inputs = tf . tensor ( shape =( <number> , <number> , <number> , <number> ) , dtype = float32 ) \ \ n ' ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow bug when running tf . linalg . diag on colab # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large integer list element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : diagonal_0_0_0_0 = <number> diagonal_0_0_0_1 = <number> diagonal_0_0_0 = [ diagonal_0_0_0_0 , diagonal_0_0_0_1 , ] diagonal_0_0_1_0 = <number> diagonal_0_0_1_1 = <number> diagonal_0_0_1 = [ diagonal_0_0_1_0 , diagonal_0_0_1_1 , ] diagonal_0_0 = [ diagonal_0_0_0 , diagonal_0_0_1 , ] diagonal_0_1_0_0 = <number> diagonal_0_1_0_1 = <number> diagonal_0_1_0 = [ diagonal_0_1_0_0 , diagonal_0_1_0_1 , ] diagonal_0_1_1_0 = <number> diagonal_0_1_1_1 = <number> diagonal_0_1_1 = [ diagonal_0_1_1_0 , diagonal_0_1_1_1 , ] diagonal_0_1 = [ diagonal_0_1_0 , diagonal_0_1_1 , ] diagonal_0 = [ diagonal_0_0 , diagonal_0_1 , ] diagonal_1_0_0_0 = <number> diagonal_1_0_0_1 = <number> diagonal_1_0_0 = [ diagonal_1_0_0_0 , diagonal_1_0_0_1 , ] diagonal_1_0_1_0 = <number> diagonal_1_0_1_1 = <number> diagonal_1_0_1 = [ diagonal_1_0_1_0 , diagonal_1_0_1_1 , ] diagonal_1_0 = [ diagonal_1_0_0 , diagonal_1_0_1 , ] diagonal_1_1_0_0 = <number> diagonal_1_1_0_1 = <number> diagonal_1_1_0 = [ diagonal_1_1_0_0 , diagonal_1_1_0_1 , ] diagonal_1_1_1_0 = <number> diagonal_1_1_1_1 = <number> diagonal_1_1_1 = [ diagonal_1_1_1_0 , diagonal_1_1_1_1 , ] diagonal_1_1 = [ diagonal_1_1_0 , diagonal_1_1_1 , ] diagonal_1 = [ diagonal_1_0 , diagonal_1_1 , ] diagonal = [ diagonal_0 , diagonal_1 , ] name = "" none "" k = - <number> padding_value = <number> align = "" right_left "" out = tf . linalg . diag ( diagonal = diagonal , name = name , k = k , padding_value = padding_value , align = align , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__matrixdiagv3_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } encountered overflow when multiplying <number> with <phone> , result [ op : matrixdiagv3 ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow bug when running tf . clip_by_value on colab # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large tensor # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> , <number> , <number> ] , dtype = tf . float64 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1 = <number> arg_2 = false out = tf . clip_by_value ( arg_0 , arg_1 , arg_2 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 9 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 9 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 9 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 9 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 7 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 5 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 8 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 8 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 9 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 9 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 1 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernel started : 9 2 d4365c - be07 - <number> - a024 - 4 0 9 4 c7317470 , name : python3 "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" got events for closed stream < zmq . eventloop . zmqstream . zmqstream object at 0x 7 9 d03bf475b0 > "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 7 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 2 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 2 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 0 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at tile_ops . cc : <number> : invalid_argument : encountered overflow when multiplying <number> with <number> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" task exception was never retrieved "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" future : < task finished name = ' task - <number> ' coro =< websocketprotocol13 . write_message . <locals> . wrapper ( ) done , defined at / usr / local / lib / python3 . <number> / dist - packages / tornado / websocket . py : <number> > exception = websocketclosederror ( ) > "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" traceback ( most recent call last ) :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" file \ \ "" / usr / local / lib / python3 . <number> / dist - packages / tornado / websocket . py \ \ "" , line <number> , in wrapper "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" await fut "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" tornado . iostream . streamclosederror : stream is closed "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" during handling of the above exception , another exception occurred :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" traceback ( most recent call last ) :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" file \ \ "" / usr / lib / python3 . <number> / asyncio / tasks . py \ \ "" , line <number> , in __step "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" result = coro . send ( none ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" file \ \ "" / usr / local / lib / python3 . <number> / dist - packages / tornado / websocket . py \ \ "" , line <number> , in wrapper "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" raise websocketclosederror ( ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" tornado . websocket . websocketclosederror "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" got events for closed stream < zmq . eventloop . zmqstream . zmqstream object at 0x 7 9 d03bffc0d0 > "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 3 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <number> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 3 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 9 2 d4365c - be07 - <number> - a024 - 4 0 9 4 c7317470 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 0 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 2 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 2 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 6 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <number> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 9 2 d4365c - be07 - <number> - a024 - 4 0 9 4 c7317470 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 4 z "" , "" v "" : <number> } ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow when running tf . raw_ops . padv2 # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to the large list of element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : input_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) input = tf . identity ( input_tensor ) paddings_0_0 = <number> paddings_0_1 = <number> paddings_0 = [ paddings_0_0 , paddings_0_1 , ] paddings_1_0 = <number> paddings_1_1 = <number> paddings_1 = [ paddings_1_0 , paddings_1_1 , ] paddings_2_0 = <number> paddings_2_1 = <number> paddings_2 = [ paddings_2_0 , paddings_2_1 , ] paddings_3_0 = <number> paddings_3_1 = <number> paddings_3 = [ paddings_3_0 , paddings_3_1 , ] paddings = [ paddings_0 , paddings_1 , paddings_2 , paddings_3 , ] constant_values = <number> name = none out = tf . raw_ops . padv2 ( input = input , paddings = paddings , constant_values = constant_values , name = name , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__padv2_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result [ [ { { node padv2 } } ] ] [ op : padv2 ] ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow when running tf . image . pad_to_bounding_box on colab # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large list element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : arg_0_0_0_0 = <number> arg_0_0_0_1 = <number> arg_0_0_0_2 = <number> arg_0_0_0 = [ arg_0_0_0_0 , arg_0_0_0_1 , arg_0_0_0_2 , ] arg_0_0_1_0 = <number> arg_0_0_1_1 = <number> arg_0_0_1_2 = <number> arg_0_0_1 = [ arg_0_0_1_0 , arg_0_0_1_1 , arg_0_0_1_2 , ] arg_0_0 = [ arg_0_0_0 , arg_0_0_1 , ] arg_0_1_0_0 = <number> arg_0_1_0_1 = <number> arg_0_1_0_2 = <number> arg_0_1_0 = [ arg_0_1_0_0 , arg_0_1_0_1 , arg_0_1_0_2 , ] arg_0_1_1_0 = <number> arg_0_1_1_1 = <number> arg_0_1_1_2 = <number> arg_0_1_1 = [ arg_0_1_1_0 , arg_0_1_1_1 , arg_0_1_1_2 , ] arg_0_1 = [ arg_0_1_0 , arg_0_1_1 , ] arg_0 = [ arg_0_0 , arg_0_1 , ] arg_1 = <number> arg_2 = <number> arg_3 = <number> arg_4 = <number> out = tf . image . pad_to_bounding_box ( arg_0 , arg_1 , arg_2 , arg_3 , arg_4 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__pad_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result : - <number> [ [ { { node pad } } ] ] [ op : pad ] name ` ` `",0
tensorflow/tensorflow,"overflow when running tf . keras . layers . zeropadding2d # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large list elements # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : padding_0_0 = <number> padding_0_1 = false padding_0 = [ padding_0_0 , padding_0_1 , ] padding_1_0 = <number> padding_1_1 = <number> padding_1 = [ padding_1_0 , padding_1_1 , ] padding = [ padding_0 , padding_1 , ] arg_class = tf . keras . layers . zeropadding2d ( padding = padding , ) arg_input_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_input_0 = tf . identity ( arg_input_0_tensor ) arg_input = [ arg_input_0 , ] out = arg_class ( * arg_input ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error : exception encountered when calling layer ' zero_padding2d ' ( type zeropadding2d ) . { { function_node __wrapped__pad_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result : - <number> [ [ { { node pad } } ] ] [ op : pad ] call arguments received by layer ' zero_padding2d ' ( type zeropadding2d ) inputs = tf . tensor ( shape =( <number> , <number> , <number> , <number> ) , dtype = float32 ) { } ` ` ` ` ` `",0
tensorflow/tensorflow,"overflow bug when running tf . pad on colab # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large input tensor # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np try : try : with tf . device ( ' / cpu ' <sad> arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . random . uniform ( [ <number> , <number> ] , dtype = tf . int32 , maxval = <number> ) arg_1 = tf . identity ( arg_1_tensor ) out = tf . pad ( arg_0 , arg_1 , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> arg_0 = tf . identity ( arg_0_tensor ) arg_0 = tf . cast ( arg_0 , tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_1 = tf . cast ( arg_1 , tf . int32 ) tf . pad ( arg_0 , arg_1 , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__pad_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } encountered overflow when multiplying <number> with <number> , result : - <number> [ op : pad ] name : error :{{ function_node __wrapped__pad_device_ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> } } encountered overflow when multiplying <number> with <number> , result [ op : pad ] name ` ` ` ` ` `",0
tensorflow/tensorflow,"cannot type "" i accept "" to extract from hexagon_nn_skel # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version hexagon_nn_skel_v1 . <number> . <number> # # # custom code no # # # os platform and distribution android # # # mobile device android # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? same as here <url> running the . / tflite_hexagon_nn_skel_v1 . <number> . <number> . run instantly prints : ` ` ` license . extraction aborted . . / extract . sh [ <number> <sad> read : - p : no coprocess aborting extraction . <repeated> done ` ` ` without giving time to write "" i accept "" . neither does - - accept option work . # # # standalone code to reproduce the issue ` ` ` shell adb push tflite_hexagon_nn_skel_v1 . <number> . <number> . run / data / local / tmp adb shell cd / data / local / tmp chmod + x tflite_hexagon_nn_skel_v1 . <number> . <number> . run . / tflite_hexagon_nn_skel_v1 . <number> . <number> . run [ - - accept ] ` ` ` # # # relevant log output ` ` ` shell . <repeated> limited or its designated affiliate . licensee shall be solely responsible to obtain such separate license from apical limited . the provision or license of a pkla product kit to licensee does not convey any license or other right under any patents of qualcomm incorporated or snaptrack , inc . type "" i accept "" if you agree to the terms of the license : you did not accept the license . extraction aborted . . / extract . sh [ <number> <sad> read : - p coprocess aborting extraction . <repeated> done hnnth <annoyed> data / local / tmp $ ` ` `",0
tensorflow/tensorflow,"crash when running tf . compat . v1 . keras . layers . maxpool2d on colab # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? due to large elements in input lists # # # standalone code to reproduce the issue ` ` ` shell results = dict ( ) import tensorflow as tf import os import numpy as np try : pool_size_0 = 1 e + <number> pool_size_1 = <number> pool_size = [ pool_size_0 , pool_size_1 , ] strides_0 = <number> strides_1 = <number> strides = [ strides_0 , strides_1 , ] padding = "" same "" data_format = none arg_class = tf . compat . v1 . keras . layers . maxpool2d ( pool_size = pool_size , strides = strides , padding = padding , data_format = data_format , ) arg_input_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_input_0 = tf . identity ( arg_input_0_tensor ) arg_input = [ arg_input_0 , ] out = arg_class ( * arg_input ) except exception as e : print ( "" error : "" + str ( e ) ) print ( results ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 2 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 4 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 4 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 1 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 1 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernel started : 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 , name : python3 "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 3 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 7 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor_shape . cc : <number> ] check failed : size >= <number> ( <number> vs . - <phone> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 4 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 6 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 6 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor_shape . cc : <number> ] check failed : size >= <number> ( <number> vs . - <phone> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 4 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 4 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 4 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 5 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 2 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 2 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 9 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernel restarted : 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 9 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 9 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <number> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 6 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <phone> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 4 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 6 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <phone> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 5 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 5 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 9 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 9 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 9 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <phone> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 4 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 2 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 2 3 z "" , "" v "" : <number> } ` ` ` ` ` `",0
tensorflow/tensorflow,"crash when running tf . keras . layers . maxpooling2d # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution pretty_name = "" ubuntu <number> . <number> lts "" name = "" ubuntu "" version_id = "" <number> "" version = "" <number> . <number> lts ( jammy jellyfish ) "" version_codename = jammy id = ubuntu id_like = debian home_url = "" <url> support_url = "" <url> bug_report_url = "" <url> privacy_policy_url = "" <url> ubuntu_codename = jammy # # # mobile device _no response_ # # # python version <date> ( main , <date> , <time> ) # # # bazel version _no response_ # # # gcc / compiler version [ gcc <number> . <number> ] # # # cuda / cudnn version nvcc : nvidia ( r ) cuda compiler driver copyright ( c ) <number> - <number> nvidia corporation built on wed_sep_21_ <time> _pdt_2022 cuda compilation tools , release <number> , v11 . <number> build cuda_11 . <number> . r11 . <number> / compiler . 3 1 8 3 3 9 0 5 _0 # # # gpu model and memory t4 # # # current behavior ? due to the large list of elements # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np try : arg_0_0 = 1 e + <number> arg_0_1 = true arg_0 = [ arg_0_0 , arg_0_1 , ] strides_0 = <number> strides_1 = <number> strides = [ strides_0 , strides_1 , ] arg_class = tf . keras . layers . maxpooling2d ( arg_0 , strides = strides , ) arg_input_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_input_0 = tf . identity ( arg_input_0_tensor ) arg_input = [ arg_input_0 , ] out = arg_class ( * arg_input ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 4 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] searching [ ' / root / . jupyter ' , ' / root / . local / etc / jupyter ' , ' / usr / etc / jupyter ' , ' / usr / local / etc / jupyter ' , ' / etc / jupyter ' ] for config files "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / etc / jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / usr / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . local / etc / jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] looking for jupyter_notebook_config in / root / . jupyter "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" :""[ d <time> . <number> notebookapp ] loaded config file : / root / . jupyter / jupyter_notebook_config . py "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 6 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 2 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 4 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 4 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . d / panel - client - jupyter . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / usr / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 0 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . local / etc / jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 1 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" \ \ t / root / . jupyter / jupyter_notebook_config . json "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 1 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" writing notebook server cookie secret to / root / . local / share / jupyter / runtime / notebook_cookie_secret "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" authentication of / metrics is off , since other authentication is disabled . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" google . colab serverextension initialized . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 0 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" serving notebooks from local directory : / "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" jupyter notebook <number> . <number> is running at :"", "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" use control - c to stop this server and shut down all kernels ( twice to skip confirmation ) . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 4 3 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernel started : 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 , name : python3 "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 3 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 3 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 7 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor_shape . cc : <number> ] check failed : size >= <number> ( <number> vs . - <phone> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 8 6 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 4 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 6 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 6 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 0 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor_shape . cc : <number> ] check failed : size >= <number> ( <number> vs . - <phone> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 0 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 7 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 4 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 4 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 1 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 4 9 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 5 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 1 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 2 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 2 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 2 5 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 0 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter : restarting kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 8 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 0 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 7 9 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernel restarted : 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 9 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 3 9 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 5 0 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_bfc_allocator . cc : <number> ] overriding orig_value setting because the tf_force_gpu_allow_growth environment variable is set . original config value was <number> . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : tesla t4 , pci bus id : <number> <time> . <number> , compute capability : <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 3 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <number> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 6 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 0 5 6 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <phone> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 5 4 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / cpu_allocator_impl . cc : <number> ] allocation of <phone> exceeds <percent> of free system memory . "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 2 6 8 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at matrix_diag_op . cc : <number> : invalid_argument : encountered overflow when multiplying <phone> with <phone> , result : - <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 1 7 2 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 5 3 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed : cudnnsetpoolingnddescriptor ( handle_ . get ( ) , ( pooling_descriptor . mode ( ) = = dnn : : poolingmode : : kmaximum ? cudnn_max_pooling_mode : cudnn_pooling_average_count_exclude_padding ) , propagate_nans ? cudnn_propagate_nan : cudnn_not_propagate_nan , nd , shape . data ( ) , padding . data ( ) , strides . data ( ) ) = = cudnn_status_success ( <number> vs . <number> ) "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 9 5 4 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" kernelrestarter kernel ( <number> / <number> ) , keep random ports "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } { "" pid "" : <number> , "" type "" : "" jupyter "" , "" level "" : <number> , "" msg "" : "" warning : root : kernel 5 b91d383 - 2 a5d - 4 d17 - 8 f8d - 5 f59277ecb11 restarted "" , "" time "" : "" <number> - <number> - 1 7 t <time> . 6 1 7 z "" , "" v "" : <number> } ` ` ` ` ` `",0
tensorflow/tensorflow,"integer overflow when running tf . experimental . numpy . identity # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution pretty_name = "" ubuntu <number> . <number> lts "" name = "" ubuntu "" version_id = "" <number> "" version = "" <number> . <number> lts ( jammy jellyfish ) "" version_codename = jammy id = ubuntu id_like = debian home_url = "" <url> support_url = "" <url> bug_report_url = "" <url> privacy_policy_url = "" <url> ubuntu_codename = jammy # # # mobile device _no response_ # # # python version <date> ( main , <date> , <time> ) # # # bazel version _no response_ # # # gcc / compiler version [ gcc <number> . <number> ] # # # cuda / cudnn version [ nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> ] ( nvcc : nvidia ( r ) cuda compiler driver copyright ( c ) <number> - <number> nvidia corporation built on wed_sep_21_ <time> _pdt_2022 cuda compilation tools , release <number> , v11 . <number> build cuda_11 . <number> . r11 . <number> / compiler . 3 1 8 3 3 9 0 5 _0 ) # # # gpu model and memory t4 # # # current behavior ? due to large integer variable # # # standalone code to reproduce the issue ` ` ` shell results = dict ( ) import tensorflow as tf import numpy as np try : try : with tf . device ( ' / cpu ' <sad> n_tensor = <phone> n = tf . identity ( n_tensor ) dtype = tf . uint16 out = tf . experimental . numpy . identity ( n = n , dtype = dtype , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> n = tf . identity ( n_tensor ) n = tf . cast ( n , tf . complex64 ) dtype = tf . uint16 tf . experimental . numpy . identity ( n = n , dtype = dtype , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) print ( results ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell error :{{ function_node __wrapped__matrixdiagv3_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } encountered overflow when multiplying <phone> with <phone> , result : - <number> [ op : matrixdiagv3 ] name : diag / usr / local / lib / python3 . <number> / dist - packages / tensorflow / python / framework / ops . py : <number> : complexwarning : casting complex values to real discards the imaginary part return int ( self . _numpy ( ) ) error :{{ function_node __wrapped__matrixdiagv3_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } encountered overflow when multiplying <phone> with <phone> , result : - <number> [ op : matrixdiagv3 ] name { } ` ` ` ` ` `",0
tensorflow/tensorflow,"could not find a version that satisfies the requirement tensorflow - compression ~ = <number> . <number> ( from versions : none ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version fail # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i try to pip intsall tensorflow - federated it fail , so i build it on source , i pip install - - requirement "" requirements . txt "" then happened this error . can i know how to solve this problem ? # # # standalone code to reproduce the issue ` ` ` shell - ` ` ` # # # relevant log output ` ` ` shell error : could not find a version that satisfies the requirement tensorflow - compression ~ = <number> . <number> ( from versions : none ) error matching distribution found for tensorflow - compression ~ = <number> . <number> ` ` `",0
tensorflow/tensorflow,"train simple audio recognition - tinyml # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? * * issue report * * - error in training audio recognition model * description <emphasis> * : i am trying to train a simple audio recognition model as described in the book "" tinyml . "" i am using google colab to train the model . however , i encountered errors during both the installation of dependencies and the training process . * * error during install dependencies * * : when attempting to install dependencies using the command pip uninstall - y tensorflow tensorflow_estimator tensorboard ! pip install - q tf - estimator - nightly = = <number> . <number> . dev <phone> tf - nightly - gpu = = <number> . <number> . dev20190729 i encountered the following error : error : could not find a version that satisfies the requirement tf - nightly - gpu = = <number> . <number> . dev20190729 ( from versions : <number> . <number> ) error : no matching distribution found for tf - nightly - gpu = = <number> . <number> . dev20190729 error during training - modulenotfounderror : * * error to begin training * * : upon running the training script with tensorflow in the "" begin training "" section , i received the following error : traceback ( most recent call last ) : file "" / content / tensorflow / tensorflow / examples / speech_commands / train . py "" , line <number> , in <module> import input_data file "" / content / tensorflow / tensorflow / examples / speech_commands / input_data . py "" , line <number> , in <module> from tensorflow . contrib . framework . python . ops import audio_ops as contrib_audio modulenotfounderror : no module named ' tensorflow . contrib ' * observations <emphasis> * : i suspect that these errors are occurring because the code provided in the book is intended for tensorflow <number> , while i am using tensorflow <number> . <number> . since tensorflow <number> is no longer supported . not sure how i can resolve this . please help me to fix this issue . # # # standalone code to reproduce the issue ` ` ` shell <url> ` ` ` # # # relevant log output ` ` ` shell error : could not find a version that satisfies the requirement tf - nightly - gpu = = <number> . <number> . dev20190729 ( from versions : <number> . <number> ) error : no matching distribution found for tf - nightly - gpu = = <number> . <number> . dev20190729 traceback ( most recent call last ) : file "" / content / tensorflow / tensorflow / examples / speech_commands / train . py "" , line <number> , in <module> import input_data file "" / content / tensorflow / tensorflow / examples / speech_commands / input_data . py "" , line <number> , in <module> from tensorflow . contrib . framework . python . ops import audio_ops as contrib_audio modulenotfounderror module named ' tensorflow . contrib ' ` ` `",0
tensorflow/tensorflow,"crash when running tensorflow . python . ops . nn_ops . fractional_max_pool # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to feeding large integer value # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import nn_ops try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float64 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_0 = <number> arg_1_1 = - <number> arg_1_2 = <number> arg_1_3 = <number> arg_1 = [ arg_1_0 , arg_1_1 , arg_1_2 , arg_1_3 , ] seed = <number> seed2 = <number> deterministic = true out = nn_ops . fractional_max_pool ( arg_0 , arg_1 , seed = seed , seed2 = seed2 , deterministic = deterministic , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> warning : tensorflow : from / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / tensorflow / python / util / dispatch . py : <number> : fractional_max_pool ( from tensorflow . python . ops . nn_ops ) is deprecated and will be removed in a future version . instructions for updating and ` deterministic ` args are deprecated . use fractional_max_pool_v2 . segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"crash when running tensorflow . python . eager . context . check_alive # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? probably due to an invalid string argument . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . eager import context try : try : with tf . device ( ' / cpu ' <sad> arg_0 = "" / job : remote_device / replica : <number> / task : <number> "" out = context . check_alive ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> context . check_alive ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"abort when running tensorflow . python . ops . gen_ctc_ops . ctc_loss # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to large input tensor # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import gen_ctc_ops try : arg_0_tensor = tf . constant ( - <number> , shape =[ <number> , <number> , <number> ] , dtype = tf . float32 , ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . constant ( <number> , shape =[ <number> , <number> ] , dtype = tf . int64 , ) arg_1 = tf . identity ( arg_1_tensor ) arg_2_tensor = tf . random . uniform ( [ <number> ] , minval = - <number> , maxval = <number> , dtype = tf . int32 ) arg_2 = tf . identity ( arg_2_tensor ) arg_3_tensor = tf . random . uniform ( [ <number> ] , minval = - <number> , maxval = <number> , dtype = tf . int32 ) arg_3 = tf . identity ( arg_3_tensor ) preprocess_collapse_repeated = false ctc_merge_repeated = true ignore_longer_outputs_than_inputs = false out = gen_ctc_ops . ctc_loss ( arg_0 , arg_1 , arg_2 , arg_3 , preprocess_collapse_repeated = preprocess_collapse_repeated , ctc_merge_repeated = ctc_merge_repeated , ignore_longer_outputs_than_inputs = ignore_longer_outputs_than_inputs , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> <number> - <number> - <number> <time> . <number> : f tensorflow / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed handle_ . get ( ) , convolution_descriptor . group_count ( ) ) = = cudnn_status_success ( <number> vs . <number> ) aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"abort when running tensorflow . python . ops . nn_ops . conv3d_transpose # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to feeding invalid list element # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import nn_ops try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_2_0 = <number> arg_2_1 = <number> arg_2_2 = <number> arg_2_3 = <number> arg_2_4 = false arg_2 = [ arg_2_0 , arg_2_1 , arg_2_2 , arg_2_3 , arg_2_4 , ] strides_0 = <number> strides_1 = <number> strides_2 = <number> strides_3 = <number> strides_4 = <number> strides = [ strides_0 , strides_1 , strides_2 , strides_3 , strides_4 , ] padding = "" valid "" out = nn_ops . conv3d_transpose ( arg_0 , arg_1 , arg_2 , strides = strides , padding = padding , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> <number> - <number> - <number> <time> . <number> : f tensorflow / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed handle_ . get ( ) , convolution_descriptor . group_count ( ) ) = = cudnn_status_success ( <number> vs . <number> ) aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"internal assertion failure when running tensorflow . python . eager . remote . connect_to_remote_host # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to feeding nan input # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . eager import remote try : try : with tf . device ( ' / cpu ' <sad> arg_0 = "" nan "" out = remote . connect_to_remote_host ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> remote . connect_to_remote_host ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : e tensorflow / core / distributed_runtime / rpc / grpc_server_lib . cc : <number> ] invalid_argument : could not interpret "" nan "" as a host - port pair . e0814 <time> . <number> <number> completion_queue . cc : <number> ] assertion failed = = <number> aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"crash when running tensorflow . python . ops . gen_array_ops . lower_bound # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to mismatch between input tensors # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . ops import gen_array_ops try : try : with tf . device ( ' / cpu ' <sad> arg_0_tensor = tf . random . uniform ( [ <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . random . uniform ( [ <number> ] , dtype = tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_2 = tf . int32 arg_3 = false out = gen_array_ops . lower_bound ( arg_0 , arg_1 , arg_2 , arg_3 , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> arg_0 = tf . identity ( arg_0_tensor ) arg_0 = tf . cast ( arg_0 , tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_1 = tf . cast ( arg_1 , tf . float32 ) arg_2 = tf . int32 gen_array_ops . lower_bound ( arg_0 , arg_1 , arg_2 , arg_3 , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> free ( ) pointer aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"crash when running tensorflow . python . ops . gen_list_ops . tensor_list_reserve # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to the large integer value # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import gen_list_ops try : element_shape_tensor = tf . random . uniform ( [ <number> ] , minval = - <number> , maxval = <number> , dtype = tf . int32 ) element_shape = tf . identity ( element_shape_tensor ) num_elements = <number> element_dtype = tf . float64 out = gen_list_ops . tensor_list_reserve ( element_shape = element_shape , num_elements = num_elements , element_dtype = element_dtype , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"abort when running tensorflow . python . ops . math_ops . sobol_sample # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to empty input value # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import math_ops try : arg_0 = <number> arg_1 = <number> arg_2 = [ ( ) ] dtype = none out = math_ops . sobol_sample ( arg_0 , arg_1 , arg_2 , dtype = dtype , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor . cc : <number> ] check failed = = numelements ( ) ( <number> vs . <number> ) must have a one element tensor aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"crash when running tensorflow . python . eager . context . add_function # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to feeding none value # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . eager import context try : arg_0 = none out = context . add_function ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"segmentation fault when running tensorflow . python . ops . nn_ops . fractional_max_pool # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to large input tensor # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import nn_ops try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float64 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_0 = <number> arg_1_1 = <number> arg_1_2 = <number> arg_1_3 = <number> arg_1 = [ arg_1_0 , arg_1_1 , arg_1_2 , arg_1_3 , ] seed = <number> seed2 = <number> deterministic = true out = nn_ops . fractional_max_pool ( arg_0 , arg_1 , seed = seed , seed2 = seed2 , deterministic = deterministic , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> warning : tensorflow : from / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / tensorflow / python / util / dispatch . py : <number> : fractional_max_pool ( from tensorflow . python . ops . nn_ops ) is deprecated and will be removed in a future version . instructions for updating and ` deterministic ` args are deprecated . use fractional_max_pool_v2 . segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"unicodedecodeerror when loading model from a path with chinese characters . # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> 2 2 h2 # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? current behavior : when i attempt to use tf . keras . models . load_model to load a saved model from a folder path that contains chinese characters , tensorflow throws a unicodedecodeerror . this suggests that tensorflow may not be parsing chinese characters correctly in the folder path . expected behavior : i expect tf . keras . models . load_model to be able to load the model correctly from any folder path , regardless of whether it contains chinese characters or not . if there are limitations on the folder name , it should be clearly documented or provide a more descriptive error message . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf # create a simple model model = tf . keras . models . sequential ( [ tf . keras . layers . dense ( <number> , input_shape =( <number> , ) ) ] ) model . compile ( optimizer = ' adam ' , loss = ' mse ' ) model . save ( ' 模型 / ' ) # ' 模型 ' is chinese for ' model ' # now , try to load the saved model loaded_model = tf . keras . models . load_model ( ' 模型 / ' ) ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : . <repeated> unicodedecodeerror : ' utf - <number> ' codec can not decode byte 0 xa1 in position <number> start byte ` ` `",0
tensorflow/tensorflow,"segmentation fault when running tensorflow . python . framework . importer . _gatherreturnelements # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to feeding none value # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . framework import importer try : arg_0_0 = "" a "" arg_0 = [ arg_0_0 , ] arg_1 = none arg_2 = none out = importer . _gatherreturnelements ( arg_0 , arg_1 , arg_2 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"abort when running # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to a negative large integer . the behavior is bizarre . it would be best if you ran multiple times to see the abort . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . ops import gen_math_ops try : try : with tf . device ( ' / cpu ' <sad> splits_tensor = tf . constant ( - <number> , shape =[ <number> , <number> , <number> ] , dtype = tf . float16 , ) splits = tf . identity ( splits_tensor ) values_tensor = tf . saturate_cast ( tf . constant ( - <number> , shape =[ <number> ] , dtype = tf . int64 , ) , dtype = tf . uint64 ) values = tf . identity ( values_tensor ) weights_tensor = tf . saturate_cast ( tf . constant ( - <phone> , shape =[ <number> , <number> ] , dtype = tf . int64 , ) , dtype = tf . int16 ) weights = tf . identity ( weights_tensor ) size = - <phone> out = gen_math_ops . ragged_bincount ( splits = splits , values = values , weights = weights , size = size , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> splits = tf . identity ( splits_tensor ) splits = tf . cast ( splits , tf . float16 ) values = tf . identity ( values_tensor ) values = tf . cast ( values , tf . uint64 ) weights = tf . identity ( weights_tensor ) weights = tf . cast ( weights , tf . int16 ) gen_math_ops . ragged_bincount ( splits = splits , values = values , weights = weights , size = size , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> error : can not convert negative int to unsigned <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 8 1 m ( <number> bytes ) from device : cuda_error_out_of_memory : out of memory <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 5 3 m ( <number> bytes ) from device : cuda_error_out_of_memory : out of memory <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 7 8 m ( <number> bytes ) from device : cuda_error_out_of_memory : out of memory <number> - <number> - <number> <time> . <number> : f . / tensorflow / python / eager / pywrap_tensor_conversion . h : <number> ] check failed aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"abort when running tensorflow . python . ops . gen_array_ops . depth_to_space # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? due to very large integer argument # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import gen_array_ops try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1 = <phone> arg_2 = "" nhwc "" out = gen_array_ops . depth_to_space ( arg_0 , arg_1 , arg_2 , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 8 8 m ( <number> bytes ) from device : cuda_error_out_of_memory : out of memory <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 4 9 m ( <number> bytes ) from device : cuda_error_out_of_memory : out of memory <number> - <number> - <number> <time> . <number> : f tensorflow / core / framework / tensor_shape . cc : <number> ] non - ok - status : initdims ( dim_sizes ) status : invalid_argument a non - negative size , got - <number> aborted ` ` ` ` ` `",0
tensorflow/tensorflow,"segmentation fault when running tensorflow . python . ops . list_ops . tensor_list_reserve # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? when num_elements is very large # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import list_ops try : element_shape_0 = <number> element_shape = [ element_shape_0 , ] num_elements = <number> element_dtype = tf . float32 out = list_ops . tensor_list_reserve ( element_shape = element_shape , num_elements = num_elements , element_dtype = element_dtype , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_driver . cc : <number> ] failed to allocate <number> . 4 4 m ( <number> bytes ) from device : cuda_error_out_of_memory of memory segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"check failure when running tensorflow . python . ops . nn_ops . conv3d_transpose_v2 # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? ` ` ` the following input combination causes check failure ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import os import numpy as np from tensorflow . python . ops import nn_ops try : arg_0_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_0 = tf . identity ( arg_0_tensor ) arg_1_tensor = tf . random . uniform ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = tf . float32 ) arg_1 = tf . identity ( arg_1_tensor ) arg_2_0 = <number> arg_2_1 = <number> arg_2_2 = <number> arg_2_3 = <number> arg_2_4 = false arg_2 = [ arg_2_0 , arg_2_1 , arg_2_2 , arg_2_3 , arg_2_4 , ] arg_3_0 = <number> arg_3_1 = <number> arg_3_2 = <number> arg_3_3 = <number> arg_3_4 = <number> arg_3 = [ arg_3_0 , arg_3_1 , arg_3_2 , arg_3_3 , arg_3_4 , ] padding = "" valid "" data_format = "" ndhwc "" dilations = none out = nn_ops . conv3d_transpose_v2 ( arg_0 , arg_1 , arg_2 , arg_3 , padding = padding , data_format = data_format , dilations = dilations , ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' libnvinfer . so . <number> ' ; dlerror : libnvinfer . so . <number> : cannot open shared object file : no such file or directory ; ld_library_path : / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / <annoyed> home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / nvidia / cudnn / lib : <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' libnvinfer_plugin . so . <number> ' ; dlerror : libnvinfer_plugin . so . <number> : cannot open shared object file : no such file or directory ; ld_library_path : / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / <annoyed> home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / nvidia / cudnn / lib : <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' libnvinfer . so . <number> ' ; dlerror : libnvinfer . so . <number> : cannot open shared object file : no such file or directory ; ld_library_path : / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / <annoyed> home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / nvidia / cudnn / lib : <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' libnvinfer_plugin . so . <number> ' ; dlerror : libnvinfer_plugin . so . <number> : cannot open shared object file : no such file or directory ; ld_library_path : / home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / <annoyed> home / nimashiri / anaconda3 / envs / fuzzer_tf_2 . <number> / lib / python3 . <number> / site - packages / nvidia / cudnn / lib : <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : f tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] check failed handle_ . get ( ) , convolution_descriptor . group_count ( ) ) = = cudnn_status_success ( <number> vs . <number> ) ` ` ` ` ` `",0
tensorflow/tensorflow,"segmentation fault when running tensorflow . python . eager . context . check_alive # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version nvidia - cudnn - cu11 = = <number> . <number> , cudatoolkit = <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? probably due to invalid string argument . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf import numpy as np from tensorflow . python . eager import context try : try : with tf . device ( ' / cpu ' <sad> arg_0 = "" / job : remote_device / replica : <number> / task : <number> "" out = context . check_alive ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) try : with tf . device ( ' / gpu : <number> ' <sad> context . check_alive ( arg_0 , ) except exception as e : print ( "" error : "" + str ( e ) ) except exception as e : print ( "" error : "" + str ( e ) ) ` ` ` ` ` ` # # # relevant log output ` ` ` shell / cudnn / lib : <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : cannot dlopen some tensorrt libraries . if you would like to use nvidia gpu with tensorrt , please make sure the missing libraries mentioned above are installed properly . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized with oneapi deep neural network library ( onednn ) to use the following cpu instructions in performance - critical operations : avx2 fma to enable them in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia geforce gtx <number> ti , pci bus id : <number> <time> . <number> , compute capability segmentation fault ` ` ` ` ` `",0
tensorflow/tensorflow,"typeerror : unable to serialize <number> to json . unrecognized type < class ' tensorflow . python . framework . ops . eagertensor ' > . # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution windows <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version n / a # # # gcc / compiler version n / a # # # cuda / cudnn version not using gpu # # # gpu model and memory n / a # # # current behavior ? i have been receiving this message when trying to save a variety of tensorflow models since version <number> . i have reported it before . it appears to be produced by the lack of an ability by tensorfow to serialize the model representations it maintains in memory . typeerror : unable to serialize <number> to json . unrecognized type < class ' tensorflow . python . framework . ops . eagertensor ' > . see the debugger output below for details can i change anything in my models to dodge this logic ? is there another model saving function i can try ? # # # standalone code to reproduce the issue ` ` ` shell please email me for code . ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" d :\\ craig \ \ python \ \ projects \ \ craigspackages \ \ koopman_operator_autoencoder_unit_tests . py "" , line <number> , in <module> tf . keras . saving . save_model ( model = autoencoder , filepath = ' autoencoder_model ' , save_format = "" tf "" ) file "" c :\\ users \ \ craig \ \ appdata \ \ local \ \ programs \ \ python \ \ python311 \ \ lib \ \ site - packages \ \ keras \ \ src \ \ saving \ \ saving_api . py "" , line <number> , in save_model return legacy_sm_saving_lib . save_model ( ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^ file "" c :\\ users \ \ craig \ \ appdata \ \ local \ \ programs \ \ python \ \ python311 \ \ lib \ \ site - packages \ \ keras \ \ src \ \ utils \ \ traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" c :\\ users \ \ craig \ \ appdata \ \ local \ \ programs \ \ python \ \ python311 \ \ lib \ \ json \ \ encoder . py "" , line <number> , in encode chunks = self . iterencode ( o , _one_shot = true ) ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^ file "" c :\\ users \ \ craig \ \ appdata \ \ local \ \ programs \ \ python \ \ python311 \ \ lib \ \ json \ \ encoder . py "" , line <number> , in iterencode return _iterencode ( o , <number> ) ^^^ ^^^ ^^^ ^^^ ^^^ ^^ typeerror to serialize <number> to json . unrecognized type < class ' tensorflow . python . framework . ops . eagertensor ' > . ` ` `",0
tensorflow/tensorflow,"binaryfocalcrossentropy : alpha does not work # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? ` tf . keras . losses . binaryfocalcrossentropy ` computes the loss without using the ` alpha ` . ` ` ` import tensorflow as tf y_true_list = [ <number> , <number> , <number> , <number> ] logits_list = [ - <number> , <number> , <number> , - <number> ] gamma = <number> focal_func1 = tf . keras . losses . binaryfocalcrossentropy ( gamma = gamma , alpha = <number> , from_logits = true ) focal_loss1 = focal_func1 ( y_true_list , logits_list ) focal_func2 = tf . keras . losses . binaryfocalcrossentropy ( gamma = gamma , alpha = <number> , from_logits = true ) focal_loss2 = focal_func2 ( y_true_list , logits_list ) focal_func3 = tf . keras . losses . binaryfocalcrossentropy ( gamma = gamma , alpha = <number> , from_logits = true ) focal_loss3 = focal_func3 ( y_true_list , logits_list ) print ( focal_loss1 ) print ( focal_loss2 ) print ( focal_loss3 ) ` ` ` the results are tf . tensor ( <number> , shape =() , dtype = float32 ) tf . tensor ( <number> , shape =() , dtype = float32 ) tf . tensor ( <number> , shape =() , dtype = float32 ) ` ` ` ` focal_loss1 ` in the codes should be ` <number> ` # # # standalone code to reproduce the issue ` ` ` shell in above . ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"notimplementederror while converting a tensorflow model to coreml using coremltools # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i am facing a notimplementederror while trying to convert [ movinets ] ( <url> model to coreml . the model is saved in [ savedmodel ] ( <url> format and i am using a tensorflow version equal to * * <number> . <number> * * and a version of [ core ml tools ] ( <url> equal to * * <number> . <number> * * and a * * <number> . <number> * * python . i do not understand the origin of * statefulpartitionedcall <emphasis> * operation and its meaning , any idea what ' s could be going on ? # # # here are the steps to reproduce the error : <number> . download a savedmodel from the following [ link ] ( <url> <number> . convert the model using : ` ` ` shell import coremltools as ct coreml_model = ct . convert ( saved_dir , convert_to = "" mlprogram "" ) ` ` ` where saved_dir <emphasis> is the path to the downloaded model . # # # relevant log output ` ` ` shell notimplementederror : conversion for tf op ' statefulpartitionedcall ' not implemented . name : "" statefulpartitionedcall "" op : "" statefulpartitionedcall "" input : "" image "" input : "" unknown "" input : "" unknown_0 "" input : "" unknown_1 "" input : "" unknown_2 "" input : "" unknown_3 "" input : "" unknown_4 "" input : "" unknown_5 "" input : "" unknown_6 "" input : "" unknown_7 "" input : "" unknown_8 "" input : "" unknown_9 "" input : "" unknown_10 "" input : "" unknown_11 "" input : "" unknown_12 "" input : "" unknown_13 "" input : "" unknown_14 "" input : "" unknown_15 "" . <repeated> input : "" unknown_597 "" input : "" unknown_598 "" input : "" unknown_599 "" attr { key : "" tin "" value { list { type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float type : dt_float } } } attr { key : "" tout "" value { list { type : dt_float } } } attr { key : "" _xlamustcompile "" value { b : true } } attr { key : "" _collective_manager_ids "" value { list { } } } attr { key : "" _has_manual_control_dependencies "" value { b : true } } attr { key : "" _read_only_resource_inputs "" value { list { i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> i : <number> . <repeated> i : <number> i : <number> i : <number> i : <number> i : <number> } } } attr { key : "" config "" value { s: "" "" } } attr { key : "" config_proto "" value { s: "" \ \ n \ \ <number> \ \ n \ \ 0 0 3 cpu \ \ <number> \ \ <number> \ \ n \ \ <number> \ \ n \ \ 0 0 3 gpu \ \ <number> \ \ <number> \ \ <number> *\\ 0 0 1 0 j \ \ <number> \ \ <number> \ \ <number> \ \ <number> \ \ <number> "" } } attr { key : "" executor_type "" value { s: "" "" } } attr { key : "" f "" value { func { name } } } ` ` ` ` ` `",0
tensorflow/tensorflow,"tensorflow inference error # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux centos # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? my model can train normally , but there was an error when inference after the training was completed . the model structure code is as follows : conv_1 = conv2d ( <number> , ( <number> ) , ( <number> ) , name = ' mode0_conv_1 ' , padding = ' same ' ) ( input_1 ) bn_1 = batchnormalizetion ( name = ' mode0_bn_1 ' ) ( conv_1 ) out_1 = prelu ( shared_axes =[ <number> ] ) ( bn_1 ) out_2 = tf . reshape ( . <repeated> ) ( out_1 ) dp_1 = lstm ( . <repeated> ) ( out_2 ) dp_o1 = dense ( <number> , ) ( dp_1 ) dp_o2 = prelu ( shared_axes =[ <number> ] ) ( dp_o1 ) ls_o1 = lstm ( . <repeated> ) ( dp_o2 ) dp_o3 = dense ( <number> , ) ( ls_o1 ) the error is as follows : tensorflow . pyrhon . framework . errors_impl . invalidargumenterror : graph execution error node : ' model / model0_bn_1 / fusebatchnormv3 ' scale must have the same number of elements as the channels of x , got <number> and <number> [ [ { node model / model0_bn_1 / fusebatchnormv3 } ] ] [ op : __inference_predict_function_3355 ] may i ask what caused this and if it can be resolved ? # # # standalone code to reproduce the issue ` ` ` shell no ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"how to get detailed information about the issue # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version cuda <number> , cudnn <number> . <number> # # # gpu model and memory nvidia a10 # # # current behavior ? i am getting following error while compiling the model with xla : ` ` ` op_requires failed at xla_ops . cc : <number> : invalid_argument to access resource resource - <number> - at - 0x 5 5 5 5 5 dbbdfb0 located in device / job : localhost / replica : <number> / task : <number> / device : cpu : <number> from device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> ` ` ` this issue is documented as xla limitation [ here ] ( <url> which seems reasonable , but the error message does not specify where this issue is coming from . is there any way to get more details on which variable is creating this issue ? # # # standalone code to reproduce the issue ` ` ` shell na ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"tensorflow . python . framework . errors_impl . failedpreconditionerror : attempting to use uninitialized value num_blocks_2 / multihead_attention / conv1d_1 / kerne # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i have trained the [ carca ( context and attribute - aware sequential recommendation via cross - attention ) ] ( <url> on video games dataset . i saved the session after <number> epoch and try to restore the session since all the architecture was written using the concepts of the session . i saved the session using tf . train . saver ( ) . save ( ) method . ` ` ` session_saver = tf . train . saver ( save_relative_paths = true ) session_output_path = os . path . join ( args . output_dir , "" epochs_ "" + str ( epoch ) ) if not os . path . isdir ( session_output_path ) : os . makedirs ( session_output_path ) # make directory if not exists session_saver . save ( sess , session_output_path + "" / carca_model "" ) print ( f "" [ info ] : save the model after epochs : { epoch } "" ) ` ` ` then , i restored session using : ` ` ` saver = tf . train . import_meta_graph ( session_dir + "" carca_model . meta "" ) saver . restore ( sess , tf . train . latest_checkpoint ( session_dir ) ) ` ` ` i have tried to perform prediction on new dataset using restored session sess , but i have encountered attempting to use uninitialized value error . the full error is : ` ` ` traceback ( most recent call last ) : file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _do_call return fn ( * args ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _run_fn options , feed_dict , fetch_list , target_list , run_metadata ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _call_tf_sessionrun run_metadata ) tensorflow . python . framework . errors_impl . failedpreconditionerror : attempting to use uninitialized value num_blocks_2 / multihead_attention / conv1d_1 / kernel_1 [ [ { { node num_blocks_2 / multihead_attention / conv1d_1 / kernel_1 / read } } ] ] during handling of the above exception , another exception occurred : traceback ( most recent call last ) : file "" carca_train . py "" , line <number> , in <module> get_load_model_and_inference ( dataset , usernum , itemnum , args , itemfeatures , userfeatures , cxtdict ) file "" carca_train . py "" , line <number> , in get_load_model_and_inference predictions = - model . predict ( sess , np . ones ( args . maxlen ) * u , [ seq ] , item_idx , [ seqcxt ] , testitemscxt ) file "" carca_train . py "" , line <number> , in predict { self . test_user : u , self . input_seq : seq , self . test_item : item_idx , self . is_training : false , self . seq_cxt : seqcxt , self . test_item_cxt : testitemcxt } ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in run run_metadata_ptr ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _run feed_dict_tensor , options , run_metadata ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _do_run run_metadata ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / client / session . py "" , line <number> , in _do_call raise type ( e ) ( node_def , op , message ) tensorflow . python . framework . errors_impl . failedpreconditionerror : attempting to use uninitialized value num_blocks_2 / multihead_attention / conv1d_1 / kernel_1 [ [ node num_blocks_2 / multihead_attention / conv1d_1 / kernel_1 / read ( defined at carca_train . py : <number> ) ] ] original stack trace for ' num_blocks_2 / multihead_attention / conv1d_1 / kernel_1 / read ' : file "" carca_train . py "" , line <number> , in <module> get_load_model_and_inference ( dataset , usernum , itemnum , args , itemfeatures , userfeatures , cxtdict ) file "" carca_train . py "" , line <number> , in get_load_model_and_inference model = model ( usernum , itemnum , args , itemfeatures , userfeatures , cxt_size = cxt_size , use_res = true ) file "" carca_train . py "" , line <number> , in __init__ dropout_rate = args . dropout_rate , is_training = self . is_training ) file "" carca_train . py "" , line <number> , in feedforward outputs = tf . layers . conv1d ( * * params ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / util / deprecation . py "" , line <number> , in new_func return func ( * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / layers / convolutional . py "" , line <number> , in conv1d return layer . apply ( inputs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / keras / engine / base_layer . py "" , line <number> , in apply return self . __call__ ( inputs , * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / layers / base . py "" , line <number> , in __call__ outputs = super ( layer , self ) . __call__ ( inputs , * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / keras / engine / base_layer . py "" , line <number> , in __call__ self . _maybe_build ( inputs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / keras / engine / base_layer . py "" , line <number> , in _maybe_build self . build ( input_shapes ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / keras / layers / convolutional . py "" , line <number> , in build dtype = self . dtype ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / layers / base . py "" , line <number> , in add_weight * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / keras / engine / base_layer . py "" , line <number> , in add_weight aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / training / tracking / base . py "" , line <number> , in _add_variable_with_custom_getter * * kwargs_for_getter ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in get_variable aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in get_variable aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in get_variable aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in _true_getter aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in _get_single_variable aggregation = aggregation ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in __call__ return cls . _variable_v1_call ( * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in _variable_v1_call shape = shape ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in <lambda> previous_getter = lambda * * kwargs : default_variable_creator ( none , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variable_scope . py "" , line <number> , in default_variable_creator shape = shape ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in __call__ return super ( variablemetaclass , cls ) . __call__ ( * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in __init__ shape = shape ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / variables . py "" , line <number> , in _init_from_args self . _snapshot = array_ops . identity ( self . _variable , name = "" read "" ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / util / dispatch . py "" , line <number> , in wrapper return target ( * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / array_ops . py "" , line <number> , in identity ret = gen_array_ops . identity ( input , name = name ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / ops / gen_array_ops . py "" , line <number> , in identity "" identity "" , input = input , name = name ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / framework / op_def_library . py "" , line <number> , in _apply_op_helper op_def = op_def ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / util / deprecation . py "" , line <number> , in new_func return func ( * args , * * kwargs ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / framework / ops . py "" , line <number> , in create_op op_def = op_def ) file "" / home / zakipoint / miniconda3 / envs / sequential_recommendation_carca / lib / python3 . <number> / site - packages / tensorflow / python / framework / ops . py "" , line <number> , in __init__ self . _traceback = tf_stack . extract_stack ( ) i have also checked the variables in both the stored session and the session just after training . both outputs seem similar . i was stuck on this issue for a few days and also tested saving the model using tf . saved_model . builder . savedmodelbuilder ( ) and restoring the model using tf . saved_model . loader . load ( ) but not solved the issue . currently , i am using tensorflow <number> ` ` ` # # # standalone code to reproduce the issue ` ` ` shell session_saver = tf . train . saver ( save_relative_paths = true ) session_output_path = os . path . join ( args . output_dir , "" epochs_ "" + str ( epoch ) ) if not os . path . isdir ( session_output_path ) : os . makedirs ( session_output_path ) # make directory if not exists session_saver . save ( sess , session_output_path + "" / carca_model "" ) print ( f "" [ info ] : save the model after epochs saver = tf . train . import_meta_graph ( session_dir + "" carca_model . meta "" ) saver . restore ( sess , tf . train . latest_checkpoint ( session_dir ) ) ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"/ / tensorflow / compiler / mlir / lite / tests : optimize . mlir . test fails # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version git head # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device n / a # # # python version <date> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version n / a # # # gpu model and memory n / a # # # current behavior ? unit test fails since commit <url> # # # standalone code to reproduce the issue ` ` ` shell bazel test - - config = mkl_aarch64_threadpool - - copt = - flax - vector - conversions - - test_env = tf_enable_onednn_opts = <number> - - test_env = tf2_behavior = <number> - - define = tf_api_version = <number> - - jobs = <number> - - build_tests_only - - / / tensorflow / compiler / mlir / lite / tests : optimize . mlir . test ` ` ` # # # relevant log output ` ` ` shell = = = = = = = = = = = = = = = = = = = = test output for / / tensorflow / compiler / mlir / lite / tests : optimize . mlir . test : - - testing : <number> tests , <number> workers - - fail : mlir tests : : optimize . mlir ( <number> of <number> ) * * * * * * * * * * * * * * * * * * * * test ' mlir tests : : optimize . mlir ' failed * * * * * * * * * * * * * * * * * * * * script : - - : ' run : at line <number> ' ; / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / tf - opt / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - tfl - optimize | / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / llvm - project / llvm / filecheck / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir : ' run : at line <number> ' ; / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / tf - opt / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - tfl - optimize = ' enable - canonicalization = true ' | / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / llvm - project / llvm / filecheck - - check - prefix = fold / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir : ' run : at line <number> ' ; / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / tf - opt / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - tfl - legalize - tf - tfl - optimize | / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / llvm - project / llvm / filecheck - - check - prefix = fusing / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir : ' run : at line <number> ' ; / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / tf - opt / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - tfl - legalize - tf - tfl - optimize = ' disable - fuse - mul - and - fc = true ' | / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / llvm - project / llvm / filecheck - - check - prefix = nofusing / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - - exit code : <number> command output ( stderr ) : - - / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir : <number> <time> : error : check : expected string not found in input / / check : "" tfl . batch_matmul "" ( % arg0 , % arg1 ) ^ <stdin> : <number> <time> : note : scanning from here func . func <user> ( % arg0 : tensor <1x128x1024xf32> , % arg1 : tensor <1024x16xf32> ) - > tensor <1x128x16xf32> { ^ <stdin> : <number> : <number> : note : possible intended match here % <number> = "" tfl . batch_matmul "" ( % arg1 , % <number> ) { adj_x = true , adj_y = false , asymmetric_quantize_inputs = false } : ( tensor <1024x16xf32> , tensor <1024x128xf32> ) - > tensor <16x128xf32> ^ input file : <stdin> check file : / home / andrew / src / tf_test / tensorflow - git / bazel - ci_build - cache / . cache / bazel / _bazel_andrew / eab0d61a99b6696edb3d2aff87b585e8 / execroot / org_tensorflow / bazel - out / aarch64 - opt / bin / tensorflow / compiler / mlir / lite / tests / optimize . mlir . test . runfiles / org_tensorflow / tensorflow / compiler / mlir / lite / tests / optimize . mlir - dump - input = help explains the following input dump . input was : < < < < < < . . . <number> : % <number> = "" tfl . transpose "" (% <number> , % <number> ) : ( tensor <16x1x8x1280xf32> , tensor <4xi32> ) - > tensor <8x1x16x1280xf32> <number> : % <number> = "" tfl . gather_nd "" (% <number> , % <number> ) : ( tensor <8x1x16x1280xf32> , tensor <16x1xi32> ) - > tensor <16x1x16x1280xf32> <number> : % <number> = "" tfl . reshape "" (% <number> , % cst ) : ( tensor <16x1x16x1280xf32> , tensor <4xi32> ) - > tensor <1x16x16x1280xf32> <number> : return % <number> : tensor <1x16x16x1280xf32> <number> : } <number> : func . func <user> ( % arg0 : tensor <1x128x1024xf32> , % arg1 : tensor <1024x16xf32> ) - > tensor <1x128x16xf32> { check : <number> x ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ error : no match found <number> : % cst = arith . constant dense < [ <number> , <number> , <number> ] > : tensor <3xi32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % cst_0 = arith . constant dense < [ <number> , <number> , <number> ] > : tensor <3xi32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % cst_1 = arith . constant dense < [ <number> , <number> ] > : tensor <2xi32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % cst_2 = arith . constant dense < [ <number> , <number> , <number> ] > : tensor <3xi32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % <number> = "" tfl . transpose "" ( % arg0 , % cst_2 ) : ( tensor <1x128x1024xf32> , tensor <3xi32> ) - > tensor <1024x1x128xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % <number> = "" tfl . reshape "" (% <number> , % cst_1 ) : ( tensor <1024x1x128xf32> , tensor <2xi32> ) - > tensor <1024x128xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % <number> = "" tfl . batch_matmul "" ( % arg1 , % <number> ) { adj_x = true , adj_y = false , asymmetric_quantize_inputs = false } : ( tensor <1024x16xf32> , tensor <1024x128xf32> ) - > tensor <16x128xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ check : <number> ? possible intended match <number> : % <number> = "" tfl . reshape "" (% <number> , % cst_0 ) : ( tensor <16x128xf32> , tensor <3xi32> ) - > tensor <16x1x128xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : % <number> = "" tfl . transpose "" (% <number> , % cst ) : ( tensor <16x1x128xf32> , tensor <3xi32> ) - > tensor <1x128x16xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : return % <number> : tensor <1x128x16xf32> check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> : } check : <number> ~ ~ ~ <number> : func . func <user> ( % arg0 : tensor <16x1024xf32> , % arg1 : tensor <1024x128xf32> , % arg2 : none ) - > tensor <16x128xf32> { check : <number> ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ . . . > > > > > > - - * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * failed tests ( <number> <sad> mlir tests : : optimize . mlir testing time : <number> . 5 3 s failed = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ` ` `",0
tensorflow/tensorflow,"could not load dynamic library ' cudart64_110 . dll ' ; dlerror : cudart64_110 . dll not found # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> # # # mobile device n / a # # # python version <number> ( microsoft store ) # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version cuda : <number> # # # gpu model and memory rtx <number> ti 8 gb # # # current behavior ? i installed cuda <number> as recommended for tf <number> . <number> , here ' s the install : [ screenshot ] ( <url> at first , i thought it was a path issue , but after restarting my pc , i was able to access exe files in that folder : ! [ image ] ( <url> if the files are in path , why can not tensorflow find them ? many people say to use miniconda , so i did , but i got the same result . other resolved issues were resolved as the op ' s were using the wrong version of cuda , i checked on the website and i can confirm that my version is the required one . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cudart64_110 . dll ' ; dlerror : cudart64_110 . dll not found <number> - <number> - <number> <time> . <number> : i tensorflow / stream_executor / cuda / cudart_stub . cc : <number> ] ignore above cudart dlerror if you do not have a gpu set up on your machine . <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cudart64_110 . dll ' ; dlerror : cudart64_110 . dll not found <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cublas64_11 . dll ' ; dlerror : cublas64_11 . dll not found <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cublaslt64_11 . dll ' ; dlerror : cublaslt64_11 . dll not found <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cufft64_10 . dll ' ; dlerror : cufft64_10 . dll not found <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cusparse64_11 . dll ' ; dlerror : cusparse64_11 . dll not found <number> - <number> - <number> <time> . <number> : w tensorflow / stream_executor / platform / default / dso_loader . cc : <number> ] could not load dynamic library ' cudnn64_8 . dll ' ; dlerror not found ` ` `",0
tensorflow/tensorflow,"java - org . tensorflow . tensorflowexception : can not parse / <modelpath> / <somepathtofolder> / saved_model . pb as binary proto - jdk <number> # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution rhel <number> version , <number> . <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? ` / <modelpath> / <somepathtofolder> / ` has following files : <number> . assets / vocab . txt <number> . saved_model . pb <number> . variables / variables . data - <number> - of - <number> <number> . variables / variables . index i am using java binding code to run the inference . libraries i am using are ` ` ` <groupid> org . tensorflow </groupid> <artifactid> tensorflow </artifactid> <artifactid> <number> . <number> </artifactid> <groupid> org . tensorflow </groupid> <artifactid> libtensorflow </artifactid> <artifactid> <number> . <number> </artifactid> <groupid> org . tensorflow </groupid> <artifactid> proto </artifactid> <artifactid> <number> . <number> </artifactid> <groupid> org . tensorflow </groupid> <artifactid> libtensorflow_jni </artifactid> <artifactid> <number> . <number> </artifactid> ` ` ` tensorflow version - <number> . <number> jdk - oracle open jdk version <number> . <number> os - rhel <number> version , <number> . <number> * * describe the current behavior * * ` ` ` org . tensorflow . tensorflowexception : can not parse </modelpath> / <somepathtofolder> / saved_model . pb as binary proto at app / / org . tensorflow . savedmodelbundle . load ( native method ) at app / / org . tensorflow . savedmodelbundle . access <money> ( savedmodelbundle . java : <number> ) at app / / org . tensorflow . savedmodelbundle $ loader . load ( savedmodelbundle . java : <number> ) at app / / org . tensorflow . savedmodelbundle . load ( savedmodelbundle . java : <number> ) at app / / com . main . java . main . tensorflow . testclass . tfpredictor ( testclass . java : <number> ) ` ` ` * * describe the expected behavior * * in jdk <number> , the code runs while in jdk <number> its throws error . the model file is not corrupted & same model path file is able to run successfully in jdk <number> # # # standalone code to reproduce the issue ` ` ` shell code com . google . common . io . resources ; import org . junit . test ; import org . tensorflow . savedmodelbundle ; import org . tensorflow . session ; import java . io . ioexception ; import java . net . urisyntaxexception ; import java . nio . file . paths ; public class testclass { <user> public void tfpredictor ( ) throws ioexception , urisyntaxexception { savedmodelbundle b = savedmodelbundle . load ( "" / <modelpath> / <somepathtofolder> "" , "" serve "" ); session sess = b . session ( ); } } ` ` `",0
tensorflow/tensorflow,issue # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux ubuntu # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? xyz # # # standalone code to reproduce the issue ` ` ` shell xyz ` ` ` # # # relevant log output ` ` ` shell jkjnk ` ` `,0
tensorflow/tensorflow,"int8 tflite model allocate_tensors ( ) silently stop python process . # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> . <number> , <number> . <number> , ' <number> . <number> - dev20230706 ' # # # custom code yes # # # os platform and distribution windows <number> , windows <number> wsl with ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i have converted a resnet18 model from onnx to tflite . ( onnx > tf > tflite ) onnx to tf conversion is done by this [ repo ] ( <url> tflite is converted to int8 precision using post - training integer quantization [ link ] ( <url> netron can display the converted int8 model correctly . onnx model & tflite model [ link ] ( <url> tflite int8 model [ link ] ( <url> but when i try to do inference . calling the method allocate_tensors ( ) stop the python process without showing any error / warning . if the tflite model is converted with fp32 , this issue does not happen . i have no idea how do to fix this issue or is there any workaround ? thanks # # # standalone code to reproduce the issue ` ` ` shell # # tf to tflite conversion import tensorflow as tf import numpy as np saved_model_dir = ' resnet18 ' tflite_model_path = saved_model_dir + ' . tflite ' converter = tf . lite . tfliteconverter . from_saved_model ( saved_model_dir ) converter . optimizations = [ tf . lite . optimize . default ] def representative_dataset_gen ( <sad> for _ in range ( <number> <sad> data = np . random . rand ( <number> , <number> , <number> , <number> ) yield [ data . astype ( np . float32 ) ] converter . representative_dataset = representative_dataset_gen converter . target_spec . supported_ops = [ tf . lite . opsset . tflite_builtins_int8 ] converter . inference_input_type = tf . int8 converter . inference_output_type = tf . int8 tflite_model = converter . convert ( ) # save the model with open ( tflite_model_path , ' wb ' ) as f : f . write ( tflite_model ) # # inference time import tensorflow as tf interpreter = tf . lite . interpreter ( model_path = "" resnet18 . tflite "" ) print ( ' before ' ) interpreter . allocate_tensors ( ) print ( ' after ' ) # this line not displayed ` ` ` # # # relevant log output ` ` ` shell # inference time output ( tf <number> . <number> - dev20230706 ) warning : tensorflow : from c :\\ users \ \ ai \ \ miniconda3 \ \ envs \ \ tf \ \ lib \ \ site - packages \ \ tensorflow \ \ python \ \ ops \ \ distributions \ \ distribution . py : <number> : reparameterizationtype . __init__ ( from tensorflow . python . ops . distributions . distribution ) is deprecated and will be removed after <number> - <number> - <number> . instructions for updating : the tensorflow distributions library has moved to tensorflow probability ( <url> you should update all references to use ` tfp . distributions ` instead of ` tf . distributions ` . warning : tensorflow : from c :\\ users \ \ ai \ \ miniconda3 \ \ envs \ \ tf \ \ lib \ \ site - packages \ \ tensorflow \ \ python \ \ ops \ \ distributions \ \ bernoulli . py : <number> : registerkl . __init__ ( from tensorflow . python . ops . distributions . kullback_leibler ) is deprecated and will be removed after <number> - <number> - <number> . instructions for updating : the tensorflow distributions library has moved to tensorflow probability ( <url> you should update all references to use ` tfp . distributions ` instead of ` tf . distributions ` . before info tensorflow lite xnnpack delegate for cpu . ` ` `",0
tensorflow/tensorflow,"invalid work group size # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution ubuntu <number> # # # mobile device qualcomm sm6225 # # # python version <number> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version no # # # gpu model and memory qualcomm adreno ( tm ) qualcomm opencl c <number> adreno ( tm ) <number> # # # current behavior ? use tflite to invoke mobilnet - v2 or any model with op softmax on mobile device , will encounter an error : tflitegpudelegate invoke : failed to clenqueuendrangekernel - invalid work group size . more details inference_force_fp16 is false , no error occured . if work group size is smaller than <number> , no error occured . # # # standalone code to reproduce the issue ` ` ` shell same code as tflite invoke on android phone will reproduce this issue . ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"rng generators make python abort in python3 . <number> and tf2 . <number> , conda - forge # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution macos <number> . <number> ( c ) # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? unable to make random number generators . this happens with conda - forge ( python <number> . <number> + tensorflow . <number> . <number> ) . it works fine with pip ( python <number> . <number> + tensorflow <number> . <number> ) # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf g = tf . random . generator . from_seed ( <number> ) g . normal ( shape =[ <number> ] ) ` ` ` # # # relevant log output ` ` ` shell assertion failed = = nullptr || dynamic_cast <to> ( f ) = nullptr ) , function down_cast , file . / tensorflow / tsl / platform / default / casts . h , line <number> . [ <number> ] <number> abort python ` ` `",0
tensorflow/tensorflow,"cannot find explicitly assigned device for op # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code no # # # os platform and distribution ubuntu18 . <number> # # # mobile device _no response_ # # # python version _no response_ # # # bazel version <number> . <number> # # # gcc / compiler version gcc - <number> . <number> # # # cuda / cudnn version none # # # gpu model and memory none # # # current behavior ? model trained use gpu , but infer machine do not have gpu device , so will meet cannot find assigend device at model load stage . here is log ` ` ` e20230724 <time> . <number> <number> tf_model_core . cc : <number> ] failed to load model in / data / mfs6 / new_wifi_models / tf_ranking_model / cvr_model / cvr_dynamic_embedding / <number> - <date> / , cannot assign a device for operation lr_weight - parameter_mht_1of1_lookup_table_export_values / tfra > cuckoohashtableexport lr_weight - parameter_mht_1of1_lookup_table_export_values / tfra > cuckoohashtableexport } } was explicitly assigned to / device : gpu : <number> but available devices are [ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> ] . make sure the device specification refers to a valid device . the requested device appears to be a gpu , but cuda is not enabled . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell no code ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"lstm loss does not work tpu with bfloat16 # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution google colab + tpu # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? lstm on tpu works in float32 . gives error message in bfloat16 : value passed to parameter ' input ' has datatype bfloat16 not in list of allowed values : float16 , float32 , float64 # # # standalone code to reproduce the issue ` ` ` shell colab code in this gist : <url> run with tpu . ` ` ` # # # relevant log output ` ` ` shell typeerror traceback ( most recent call last ) < ipython - input - <number> - d2060b3c689a > in < cell line : <number> > ( ) <number> with strategy . scope ( <sad> - - - - > <number> model = build_model ( ) <number> <number> model . compile ( <number> loss = keras . losses . sparsecategoricalcrossentropy ( from_logits = true ) , <number> frames / usr / local / lib / python3 . <number> / dist - packages / tensorflow / python / framework / op_def_library . py in _satisfiestypeconstraint ( dtype , attr_def , param_name ) <number> allowed_values = "" , "" . join ( dtypes . as_dtype ( x ) . name for x in allowed_list ) <number> if dtype not in allowed_list : - - - > <number> raise typeerror ( <number> f "" value passed to parameter ' { param_name } ' has datatype "" <number> f "" { dtypes . as_dtype ( dtype ) . name } not in list of allowed values : "" typeerror : exception encountered when calling layer "" lstm_4 "" ( type lstm ) . value passed to parameter ' input ' has datatype bfloat16 not in list of allowed values : float16 , float32 , float64 call arguments received by layer "" lstm_4 "" ( type lstm ) inputs = tf . tensor ( shape =( none , none , <number> ) , dtype = bfloat16 ) • mask = none • training = none • initial_state = none ` ` `",0
tensorflow/tensorflow,"assertionerror : tried to export a function which references an ' untracked ' resource . tensorflow objects ( e . g . tf . variable ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? as you can see in the code below , the operation of multiplying * attn <emphasis> * by * temperature <emphasis> * at * mdta <emphasis> * is being performed . and the * temperature <emphasis> * is defined by tf . variable ( ) . the model using the attention module below runs normally until training ( model . fit ( ) ) , but an assertionerror occurs when saving the model . i would appreciate it if you could tell me how to make the * temperature <emphasis> * variable trackable . # # # standalone code to reproduce the issue ` ` ` shell # import tensorflow . compat . v2 as tf import tensorflow as tf import keras from keras import backend from keras . applications import imagenet_utils from keras . engine import training from keras . layers import versionawarelayers from keras . utils import data_utils from keras . utils import layer_utils from keras . layers import layer , activation , relu , concatenate , conv2d , add , dense , maxpool2d , avgpool2d , flatten , multiply , softmax from keras . layers import dropout , dense , globalaveragepooling2d , input , batchnormalization , depthwiseconv2d , zeropadding2d , layernormalization from tensorflow . keras import backend as k from keras . models import model <hashtag> import </hashtag> tensorflow . keras # 정상적으로 작동 ( temperature제외 ) # isort : off from tensorflow . python . platform import tf_logging as logging from tensorflow . python . util . tf_export import keras_export base_weight_path = ( "" <url> "" keras - applications / mobilenet / "" ) <user> ( "" keras . applications . mobilenet . mobilenet "" , "" keras . applications . mobilenet "" ) def mobilenet ( input_shape = none , alpha = <number> , depth_multiplier = <number> , dropout = 1 e - <number> , include_top = true , weights = "" imagenet "" , input_tensor = none , pooling = none , classes = <number> , classifier_activation = "" softmax "" , * * kwargs , <sad> # global layers # if "" layers "" in kwargs : # layers = kwargs . pop ( "" layers "" ) # else : # layers = versionawarelayers ( ) if kwargs : raise valueerror ( f "" unknown argument ( s ) : { ( kwargs , ) } "" ) if not ( weights in { "" imagenet "" , none } or tf . io . gfile . exists ( weights ) <sad> raise valueerror ( "" the ` weights ` argument should be either "" "" ` none ` ( random initialization ) , ` imagenet ` "" "" ( pre - training on imagenet ) , "" "" or the path to the weights file to be loaded . "" f "" received weights ={ weights } "" ) if weights = = "" imagenet "" and include_top and classes = <number> : raise valueerror ( ' if using ` weights ` as ` "" imagenet "" ` with ` include_top ` ' "" as true , ` classes ` should be <number> . "" f "" received classes ={ classes } "" ) # determine proper input shape and default size . if input_shape is none : default_size = <number> else : if backend . image_data_format ( ) = = "" channels_first "" : rows = input_shape [ <number> ] cols = input_shape [ <number> ] else : rows = input_shape [ <number> ] cols = input_shape [ <number> ] if rows = = cols and rows in [ <number> , <number> , <number> , <number> <sad> default_size = rows else : default_size = <number> input_shape = imagenet_utils . obtain_input_shape ( input_shape , default_size = default_size , min_size = <number> , data_format = backend . image_data_format ( ) , require_flatten = include_top , weights = weights , ) if backend . image_data_format ( ) = = "" channels_last "" : row_axis , col_axis = ( <number> , <number> ) else : row_axis , col_axis = ( <number> , <number> ) rows = input_shape [ row_axis ] cols = input_shape [ col_axis ] if weights = = "" imagenet "" : if depth_multiplier ! = <number> : raise valueerror ( "" if imagenet weights are being loaded , "" "" depth multiplier must be <number> . "" f "" received depth_multiplier ={ depth_multiplier } "" ) if alpha not in [ <number> , <number> , <number> , <number> <sad> raise valueerror ( "" if imagenet weights are being loaded , "" "" alpha can be one of "" "" ` <number> ` , ` <number> ` , ` <number> ` or ` <number> ` only . "" f "" received alpha ={ alpha } "" ) if rows ! = cols or rows not in [ <number> , <number> , <number> , <number> <sad> rows = <number> logging . warning ( "" ` input_shape ` is undefined or non - square , "" "" or ` rows ` is not in [ <number> , <number> , <number> , <number> ] . "" "" weights for input shape ( <number> , <number> ) will be "" "" loaded as the default . "" ) if input_tensor is none : img_input = input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ) : img_input = input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor num_heads = <number> expansion_factor = <number> x = _conv_block ( img_input , <number> , alpha , strides =( <number> , <number> ) ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , strides =( <number> , <number> ) , block_id = <number> ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , strides =( <number> , <number> ) , block_id = <number> ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) x = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , strides =( <number> , <number> ) , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , strides =( <number> , <number> ) , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) x = _depthwise_conv_block ( x , <number> , alpha , depth_multiplier , block_id = <number> ) <hashtag> x </hashtag> = _transformer_block ( x , num_heads , expansion_factor ) if include_top : x = layers . globalaveragepooling2d ( keepdims = true ) (x ) x = layers . dropout ( dropout , name = "" dropout "" )(x ) x = layers . conv2d ( classes , ( <number> , <number> ) , padding = "" same "" , name = "" conv_preds "" )(x ) x = layers . reshape ( ( classes , ) , name = "" reshape_2 "" )(x ) imagenet_utils . validate_activation ( classifier_activation , weights ) x = layers . activation ( activation = classifier_activation , name = "" predictions "" )(x ) else : if pooling = = "" avg "" : x = globalaveragepooling2d ( )(x ) elif pooling = = "" max "" : x = globalmaxpooling2d ( )(x ) # ensure that the model takes into account # any potential predecessors of ` input_tensor ` . if input_tensor is not none : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # create model . model = training . model ( inputs , x , name = "" mobilenet_ % <number> . 2 f_ %s "" % ( alpha , rows ) ) # load weights . if weights = = "" imagenet "" : if alpha = = <number> : alpha_text = "" 1 _0 "" elif alpha = = <number> : alpha_text = "" 7 _5 "" elif alpha = = <number> : alpha_text = "" 5 _0 "" else : alpha_text = "" 2 _5 "" if include_top : model_name = "" mobilenet_ %s _ % d_tf . h5 "" % ( alpha_text , rows ) weight_path = base_weight_path + model_name weights_path = data_utils . get_file ( model_name , weight_path , cache_subdir = "" models "" ) else : model_name = "" mobilenet_ %s _ % d_tf_no_top . h5 "" % ( alpha_text , rows ) weight_path = base_weight_path + model_name weights_path = data_utils . get_file ( model_name , weight_path , cache_subdir = "" models "" ) model . load_weights ( weights_path , by_name = true ) elif weights is not none : model . load_weights ( weights , by_name = true ) return model class mdta ( keras . layers . layer ) : ' ' ' * * important <emphasis> * * - the channels must be zero when divided by num_heads ' ' ' def __init__ ( self , num_heads ) : super ( mdta , self ) . __init__ ( ) self . num_heads = num_heads <hashtag> self </hashtag> . temperature = tf . variable ( [ [ [ [ <number> . ] ] for _ in range ( self . num_heads ) ] ] , shape =[ none , self . num_heads , <number> , <number> ] , trainable = true ) def build ( self , inputs ) : ' ' ' ( n , h , w , c ) - > ( n , h , w , c ) output of mdta feature should be added to input feature x ' ' ' b , h , w , c = inputs . shape # - - - - - - - - - - - - - - - - - - - - layers - - - - - - - - - - - - - - - - - - - - qkv = conv2d ( filters =c* <number> , kernel_size = <number> , use_bias = false ) qkv_conv = conv2d ( c * <number> , kernel_size = <number> , padding = ' same ' , groups =c* <number> , use_bias = false ) project_out = conv2d ( filters =c, kernel_size = <number> , use_bias = false ) temperature = tf . variable ( [ [ [ [ <number> . ] ] for _ in range ( self . num_heads ) ] ] , shape =[ none , self . num_heads , <number> , <number> ] , trainable = true ) # - - - - - - - - - - - - - - - - - - - - forward - - - - - - - - - - - - - - - - - - - - q , k , v = tf . split ( qkv_conv ( qkv ( inputs ) ) , num_or_size_splits = <number> , axis = - <number> ) # divide the # of channels into heads & learn separate attention map q = tf . reshape ( q , [ - <number> , self . num_heads , c / / self . num_heads , h * w ] ) # ( n , num_heads , c / num_heads , hw ) k = tf . reshape ( k , [ - <number> , self . num_heads , c / / self . num_heads , h * w ] ) v = tf . reshape ( v , [ - <number> , self . num_heads , c / / self . num_heads , h * w ] ) q , k = tf . nn . l2_normalize ( q , axis = - <number> ) , tf . nn . l2_normalize ( k , axis = - <number> ) # cxc attention map instead of hwxhw ( when num_heads = <number> ) attn = tf . matmul ( q , k , transpose_b = true ) attn = multiply ( [ attn , temperature ] ) attn = softmax ( axis = - <number> ) ( attn ) out = tf . matmul ( attn , v ) shape = [ tf . shape ( out ) [ k ] for k in range ( <number> ) ] # [ batch , num_heads , c / num_heads , h*w <censored> ] out = tf . reshape ( out , [ shape [ <number> ] , h , w , shape [ <number> ] * shape [ <number> ] ] ) out = project_out ( out ) # attn*v <censored> : ( n , num_heads , c / num_heads , hw ) return out def __call__ ( self , inputs ) : return self . build ( inputs ) class gdfn ( keras . layers . layer ) : def __init__ ( self ) : super ( gdfn , self ) . __init__ ( ) self . expansion_factor = <number> def build ( self , inputs ) : ' ' ' ( n , h , w , c ) - > ( n , h , w , c ) with expansion_factor = r output of gdfn feature should be added to input feature x ' ' ' b , h , w , c = inputs . shape hidden_channels = int ( c * self . expansion_factor ) # channel expansion # - - - - - - - - - - - - - - - - - - - - layers - - - - - - - - - - - - - - - - - - - - project_in = conv2d ( hidden_channels * <number> , kernel_size = <number> , use_bias = false ) conv = conv2d ( hidden_channels * <number> , kernel_size = <number> , padding = ' same ' , groups = hidden_channels * <number> , use_bias = false ) project_out = conv2d ( c , kernel_size = <number> , use_bias = false ) # - - - - - - - - - - - - - - - - - - - - forward - - - - - - - - - - - - - - - - - - - - x = project_in ( inputs ) # ( n , h , w , 2 cr ) x = conv ( x ) # ( n , h , w , 2 cr ) x1 , x2 = tf . split ( x , num_or_size_splits = <number> , axis = - <number> ) # ( n , h , w , cr ) , ( n , h , w , cr ) # gating : the element - wise product of <number> parallel paths of linear transformation layers out = relu ( )(x 1 ) * x2 # ( n , h , w , cr ) out = project_out ( out ) # ( n , h , w , cr ) return out def __call__ ( self , inputs ) : return self . build ( inputs ) def _transformer_block ( inputs , num_heads , expansion_factor ) : ' ' ' ( n , h , w , c ) - > ( n , h , w , c ) ' ' ' shape = [ tf . shape ( inputs ) [ k ] for k in range ( <number> ) ] b , h , w , c = inputs . shape [ <number> ] , inputs . shape [ <number> ] , inputs . shape [ <number> ] , inputs . shape [ <number> ] assert c % num_heads = = <number> norm1 = layernormalization ( ) # default : axis = - <number> attn = mdta ( num_heads ) norm2 = layernormalization ( ) ffn = gdfn ( ) # add mdta output feature inputs_norm1 = norm1 ( tf . reshape ( inputs , [ - <number> , h*w <censored> , c ] ) ) inputs_norm1 = tf . reshape ( inputs_norm1 , [ - <number> , h , w , c ] ) inputs = inputs + attn ( inputs_norm1 ) # add gdfn output feature inputs_norm2 = norm2 ( tf . reshape ( inputs , [ - <number> , h*w <censored> , c ] ) ) inputs_norm2 = tf . reshape ( inputs_norm2 , [ - <number> , h , w , c ] ) x = inputs + ffn ( inputs_norm2 ) return x def _conv_block ( inputs , filters , alpha , kernel =( <number> , <number> ) , strides =( <number> , <number> <sad> channel_axis = <number> if backend . image_data_format ( ) = = "" channels_first "" else - <number> filters = int ( filters * alpha ) x = conv2d ( filters , kernel , padding = "" same "" , use_bias = false , strides = strides , name = "" conv1 "" , ) ( inputs ) x = batchnormalization ( axis = channel_axis , name = "" conv1_bn "" )(x ) return relu ( <number> , name = "" conv1_relu "" )(x ) def _depthwise_conv_block ( inputs , pointwise_conv_filters , alpha , depth_multiplier = <number> , strides =( <number> , <number> ) , block_id = <number> , <sad> channel_axis = <number> if backend . image_data_format ( ) = = "" channels_first "" else - <number> pointwise_conv_filters = int ( pointwise_conv_filters * alpha ) if strides = = ( <number> , <number> <sad> x = inputs else : x = zeropadding2d ( ( ( <number> , <number> ) , ( <number> , <number> ) ) , name = "" conv_pad_ % d "" % block_id ) ( inputs ) x = depthwiseconv2d ( ( <number> , <number> ) , padding = "" same "" if strides = = ( <number> , <number> ) else "" valid "" , depth_multiplier = depth_multiplier , strides = strides , use_bias = false , name = "" conv_dw_ % d "" % block_id , )(x ) x = batchnormalization ( axis = channel_axis , name = "" conv_dw_ % d_bn "" % block_id )(x ) x = relu ( <number> , name = "" conv_dw_ % d_relu "" % block_id ) (x ) x = conv2d ( pointwise_conv_filters , ( <number> , <number> ) , padding = "" same "" , use_bias = false , strides =( <number> , <number> ) , name = "" conv_pw_ % d "" % block_id , )(x ) x = batchnormalization ( axis = channel_axis , name = "" conv_pw_ % d_bn "" % block_id )(x ) return relu ( <number> , name = "" conv_pw_ % d_relu "" % block_id ) (x ) def gen_mobilenetv1_mdta ( input_shape , dropout_rate , num_class ) : if input_shape ==( <number> , <number> , <number> <sad> weights = ' imagenet ' else : weights = none base_model = mobilenet ( weights = weights , include_top = false , input_tensor = input ( input_shape ) , input_shape = input_shape ) base_model . trainable = true x = base_model . output head_layer = tf . keras . sequential ( [ tf . keras . layers . globalaveragepooling2d ( name = ' simple_classifier_pooling ' ) , tf . keras . layers . dropout ( dropout_rate , name = ' simple_classifier_dropout ' ) , tf . keras . layers . dense ( <number> , activation = ' relu ' , name = ' simple_classifier_dense1 ' ) , tf . keras . layers . dense ( num_class , activation = ' softmax ' ) , ] ) predictions = head_layer ( x ) # this is the model we will train model = tf . keras . models . model ( inputs = base_model . input , outputs = predictions ) <hashtag> print </hashtag> ( model ) return model input_shape = ( <number> , <number> , <number> ) x = input ( input_shape ) model = gen_mobilenetv1_mdta ( input_shape , <number> , <number> ) out = model ( x ) save_path = ' d <annoyed> model_mdta . h5 ' model . compile ( optimizer = ' adam ' , loss = ' categorical_crossentropy ' , metrics =[ ' acc ' ] ) model . save ( save_path ) ` ` ` # # # relevant log output in the vscode - terminal ` ` ` shell assertionerror : tried to export a function which references an ' untracked ' resource . tensorflow objects ( e . g . tf . variable ) captured by functions must be ' tracked ' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly . see the information below : function name = b ' __inference_signature_wrapper_18418 ' captured tensor = < resourcehandle ( name = "" resource - <number> - at - 0x 5 5 f74d07bcf0 "" , device =""/ job : localhost / replica : <number> / task : <number> / device : cpu : <number> "" , container = "" anonymous "" , type = "" tensorflow : : var "" , dtype and shapes : "" [ dtype enum : <number> , shape : [ ? , <number> , <number> ] ] "" ) > trackable referencing this tensor = < tf . variable ' variable : <number> ' shape =( none , <number> , <number> , <number> ) dtype = float32 > internal tensor = tensor ( "" <number> : <number> "" , shape =() , dtype = resource ) during handling of the above exception , another exception occurred : ` ` ` in the jupyter - noetebook ` ` ` shell valueerror traceback ( most recent call last ) / tmp / ipykernel_828829 / <phone> . py in <module> <number> model . compile ( optimizer = ' adam ' , loss = ' categorical_crossentropy ' , metrics =[ ' acc ' ] ) - - - - > <number> model . save ( ' d <annoyed> model_mdta . h5 ' ) ~ / conda / envs / hrvi2 / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py in error_handler ( * args , * * kwargs ) <number> # to get the full stack trace , call : <number> # ` tf . debugging . disable_traceback_filtering ( ) ` - - - > <number> raise e . with_traceback ( filtered_tb ) from none <number> finally : <number> del filtered_tb ~ / conda / envs / hrvi2 / lib / python3 . <number> / json / encoder . py in encode ( self , o ) <number> # exceptions are not as detailed . the list call should be roughly <number> # equivalent to the pysequence_fast that ' ' . join ( ) would do . - - > <number> chunks = self . iterencode ( o , _one_shot = true ) <number> if not isinstance ( chunks , ( list , tuple ) <sad> <number> chunks = list ( chunks ) ~ / conda / envs / hrvi2 / lib / python3 . <number> / json / encoder . py in iterencode ( self , o , _one_shot ) <number> self . key_separator , self . item_separator , self . sort_keys , <number> self . skipkeys , _one_shot ) - - > <number> return _iterencode ( o , <number> ) <number> <number> def _make_iterencode ( markers , _default , _encoder , _indent , _floatstr , valueerror to serialize variablespec ( shape =( none , <number> , <number> , <number> ) , dtype = tf . float32 , trainable = true , alias_id = none ) to json , because the typespec class < class ' tensorflow . python . ops . resource_variable_ops . variablespec ' > has not been registered . ` ` `",0
tensorflow/tensorflow,"could not load dynamic library ' libcublaslt . so . <number> ' ; dlerror : libcublaslt . so . <number> : cannot open shared object file : no such file or directory # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version v2 . <number> - rc2 - <number> - g1cb1a030a62 <number> . <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> . <number> - <number> / <number> . <number> - <number> + cuda12 . <number> # # # gpu model and memory nvidia geforce gtx 9 6 0 m , 4 0 9 6 mib # # # current behavior ? running the mobilenet from keras included by tensorflow leads to the following error : ` ` ` could not load library libcublaslt . so . <number> . error : libcublaslt . so . <number> : cannot open shared object file : no such file or directory abgebrochen ( speicherabzug geschrieben ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell python - c ' from tensorflow . keras . applications . mobilenet import mobilenet ; import numpy as np ; m = mobilenet ( ); m . predict ( np . zeros ( ( <number> , <number> ) ) ) ' ` ` ` # # # relevant log output ` ` ` shell could not load library libcublaslt . so . <number> . error : libcublaslt . so . <number> : cannot open shared object file such file or directory abgebrochen ( speicherabzug geschrieben ) ` ` `",0
tensorflow/tensorflow,"tensorflow distributed training works for at most <number> gpus # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version cuda <number> , cudnn <number> . <number> # # # gpu model and memory _no response_ # # # current behavior ? i have <number> nvidia h100s on this system . below is the output of ` nvidia - smi ` . [ image ] ( <url> the following code works as intended if ` gpus ` includes at most <number> gpus . ! [ image ] ( <url> in the "" relevant log output "" section , i have the error log from running the code with <number> gpus in the ` gpus ` list . please let me know if you need any additional information ! # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf gpus = [ "" / gpu : <number> "" , "" / gpu : <number> "" , "" / gpu : <number> "" ] strategy = tf . distribute . mirroredstrategy ( gpus ) with strategy . scope ( <sad> model = tf . keras . sequential ( [ tf . keras . layers . dense ( <number> , input_shape =( <number> , ) ) ] ) model . compile ( loss = "" mse "" , optimizer = "" sgd "" ) dataset = tf . data . dataset . from_tensors ( ( [ <number> . ] , [ <number> . ] ) ) . repeat ( <number> ) . batch ( <number> ) model . fit ( dataset ) ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> : i tensorflow / core / util / port . cc : <number> ] onednn custom operations are on . you may see slightly different numerical results due to floating - point round - off errors from different computation orders . to turn them off , set the environment variable ` tf_enable_onednn_opts = <number> ` . <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . to enable the following instructions : avx2 avx512f avx512_vnni avx512_bf16 avx_vnni amx_tile amx_int8 amx_bf16 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] tensorflow was not built with cuda kernel binaries compatible with compute capability <number> . cuda kernels will be jit - compiled from ptx , which could take <number> minutes or longer . <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> <time> . <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> <happy> b : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : 4 c : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : 5 d : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : 9 b : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : bb : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : cb : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] created device / job : localhost / replica : <number> / task : <number> / device : gpu : <number> with <number> mb memory : - > device : <number> , name : nvidia h100 8 0 gb hbm3 , pci bus id : <number> : db : <number> , compute capability : <number> <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _1 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _1 } } ] ] <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _0 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _0 } } ] ] <number> - <number> - <number> <time> . <number> : w tensorflow / core / grappler / optimizers / data / auto_shard . cc : <number> ] auto sharding policy will apply data sharding policy as it failed to apply file sharding policy because of the following reason : found an unshardable source dataset : name : "" tensordataset / _2 "" op : "" tensordataset "" input : "" placeholder / _0 "" input : "" placeholder / _1 "" attr { key : "" toutput_types "" value { list { type : dt_float type : dt_float } } } attr { key : "" _cardinality "" value { i : <number> } } attr { key : "" metadata "" value { s: "" \ \ n \ \ 0 1 7 tensordataset : <number> "" } } attr { key : "" output_shapes "" value { list { shape { dim { size : <number> } } shape { dim { size : <number> } } } } } experimental_type { type_id : tft_product args { type_id : tft_dataset args { type_id : tft_product args { type_id : tft_tensor args { type_id : tft_float } } args { type_id : tft_tensor args { type_id : tft_float } } } } } <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _1 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _1 } } ] ] <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _1 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _1 } } ] ] <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _1 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _1 } } ] ] <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / device : cpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : invalid_argument : you must feed a value for placeholder tensor ' placeholder / _0 ' with dtype float and shape [ <number> ] [ [ { { node placeholder / _0 } } ] ] <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] xla service 0x 7 fe600008ff0 initialized for platform cuda ( this does not guarantee that xla will be used ) . devices : <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / service / service . cc : <number> ] streamexecutor device ( <number> <sad> nvidia h100 8 0 gb hbm3 , compute capability <number> <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] could not create cudnn handle : cudnn_status_not_initialized <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] possibly insufficient driver version : <number> . <number> <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] could not create cudnn handle : cudnn_status_not_initialized <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] possibly insufficient driver version : <number> . <number> <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] loaded cudnn version <number> <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / status_macros . cc : <number> ] internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr * * * begin stack trace * * * tsl : : currentstacktrace [ abi : cxx11 ] ( ) xla : : status_macros : : makeerrorstream : : impl : : getstatus ( ) xla : : gpu : : gpucompiler : : optimizehlomodule ( xla : : hlomodule * , stream_executor : : streamexecutor * , stream_executor : : devicememoryallocator * , xla : : gpu : : gputargetconfig const & , xla : : autotuneresults const <wink> xla : : gpu : : gpucompiler : : runhlopasses ( std : : unique_ptr < xla : : hlomodule , std : : default_delete < xla : : hlomodule > > , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & ) xla : : service : : buildexecutable ( xla : : hlomoduleproto const & , std : : unique_ptr < xla : : hlomoduleconfig , std : : default_delete < xla : : hlomoduleconfig > > , xla : : backend * , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & , bool ) xla : : localservice : : compileexecutables ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) xla : : localclient : : compile ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) tensorflow : : xladevicecompilerclient : : buildexecutable ( tensorflow : : xlacompiler : : options const & , tensorflow : : xlacompilationresult const & ) tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilestrict ( tensorflow : : devicecompilationclustersignature const & , tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : nameattrlist const & , tensorflow : : devicecompilationcache < xla : : localexecutable > : : value , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tsl : : mutex <wink> tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compileimpl ( tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , tensorflow : : nameattrlist const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : devicecompilemode , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tensorflow : : xlacompilationresult const * * , xla : : localexecutable * <wink> tensorflow : : xlalocallaunchbase : : computeasync ( tensorflow : : opkernelcontext * , std : : function < void ()>) tensorflow : : basegpudevice : : computeasync ( tensorflow : : asyncopkernel * , tensorflow : : opkernelcontext * , std : : function < void ()>) eigen : : threadpooltempl < tsl : : thread : : eigenenvironment > : : workerloop ( int ) std : : _function_handler < void ( ) , tsl : : thread : : eigenenvironment : : createthread ( std : : function < void ()>): : { lambda ( )# <number> }>: : _m_invoke ( std : : _any_data const & ) * * * end stack trace * * * <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at xla_ops . cc : <number> : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_0_1 / statefulpartitionedcall } } ] ] <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / status_macros . cc : <number> ] internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr * * * begin stack trace * * * tsl : : currentstacktrace [ abi : cxx11 ] ( ) xla : : status_macros : : makeerrorstream : : impl : : getstatus ( ) xla : : gpu : : gpucompiler : : optimizehlomodule ( xla : : hlomodule * , stream_executor : : streamexecutor * , stream_executor : : devicememoryallocator * , xla : : gpu : : gputargetconfig const & , xla : : autotuneresults const <wink> xla : : gpu : : gpucompiler : : runhlopasses ( std : : unique_ptr < xla : : hlomodule , std : : default_delete < xla : : hlomodule > > , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & ) xla : : service : : buildexecutable ( xla : : hlomoduleproto const & , std : : unique_ptr < xla : : hlomoduleconfig , std : : default_delete < xla : : hlomoduleconfig > > , xla : : backend * , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & , bool ) xla : : localservice : : compileexecutables ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) xla : : localclient : : compile ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) tensorflow : : xladevicecompilerclient : : buildexecutable ( tensorflow : : xlacompiler : : options const & , tensorflow : : xlacompilationresult const & ) tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilestrict ( tensorflow : : devicecompilationclustersignature const & , tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : nameattrlist const & , tensorflow : : devicecompilationcache < xla : : localexecutable > : : value , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tsl : : mutex <wink> tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compileimpl ( tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , tensorflow : : nameattrlist const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : devicecompilemode , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tensorflow : : xlacompilationresult const * * , xla : : localexecutable * <wink> tensorflow : : xlalocallaunchbase : : computeasync ( tensorflow : : opkernelcontext * , std : : function < void ()>) tensorflow : : basegpudevice : : computeasync ( tensorflow : : asyncopkernel * , tensorflow : : opkernelcontext * , std : : function < void ()>) eigen : : threadpooltempl < tsl : : thread : : eigenenvironment > : : workerloop ( int ) std : : _function_handler < void ( ) , tsl : : thread : : eigenenvironment : : createthread ( std : : function < void ()>): : { lambda ( )# <number> }>: : _m_invoke ( std : : _any_data const & ) * * * end stack trace * * * <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at xla_ops . cc : <number> : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_1_1 / statefulpartitionedcall } } ] ] <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] could not create cudnn handle : cudnn_status_not_initialized <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] possibly insufficient driver version : <number> . <number> <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / gpu / asm_compiler . cc : <number> ] falling back to the cuda driver for ptx compilation ; ptxas does not support cc <number> <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / xla / stream_executor / gpu / asm_compiler . cc : <number> ] used ptxas at ptxas <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] could not create cudnn handle : cudnn_status_not_initialized <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / stream_executor / cuda / cuda_dnn . cc : <number> ] possibly insufficient driver version : <number> . <number> <number> - <number> - <number> <time> . <number> : i . / tensorflow / compiler / jit / device_compiler . h : <number> ] compiled cluster using xla ! this line is logged at most once for the lifetime of the process . <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / status_macros . cc : <number> ] internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr * * * begin stack trace * * * tsl : : currentstacktrace [ abi : cxx11 ] ( ) xla : : status_macros : : makeerrorstream : : impl : : getstatus ( ) xla : : gpu : : gpucompiler : : optimizehlomodule ( xla : : hlomodule * , stream_executor : : streamexecutor * , stream_executor : : devicememoryallocator * , xla : : gpu : : gputargetconfig const & , xla : : autotuneresults const <wink> xla : : gpu : : gpucompiler : : runhlopasses ( std : : unique_ptr < xla : : hlomodule , std : : default_delete < xla : : hlomodule > > , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & ) xla : : service : : buildexecutable ( xla : : hlomoduleproto const & , std : : unique_ptr < xla : : hlomoduleconfig , std : : default_delete < xla : : hlomoduleconfig > > , xla : : backend * , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & , bool ) xla : : localservice : : compileexecutables ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) xla : : localclient : : compile ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) tensorflow : : xladevicecompilerclient : : buildexecutable ( tensorflow : : xlacompiler : : options const & , tensorflow : : xlacompilationresult const & ) tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilestrict ( tensorflow : : devicecompilationclustersignature const & , tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : nameattrlist const & , tensorflow : : devicecompilationcache < xla : : localexecutable > : : value , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tsl : : mutex <wink> tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compileimpl ( tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , tensorflow : : nameattrlist const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : devicecompilemode , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tensorflow : : xlacompilationresult const * * , xla : : localexecutable * <wink> tensorflow : : xlalocallaunchbase : : computeasync ( tensorflow : : opkernelcontext * , std : : function < void ()>) tensorflow : : basegpudevice : : computeasync ( tensorflow : : asyncopkernel * , tensorflow : : opkernelcontext * , std : : function < void ()>) eigen : : threadpooltempl < tsl : : thread : : eigenenvironment > : : workerloop ( int ) std : : _function_handler < void ( ) , tsl : : thread : : eigenenvironment : : createthread ( std : : function < void ()>): : { lambda ( )# <number> }>: : _m_invoke ( std : : _any_data const & ) * * * end stack trace * * * <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at xla_ops . cc : <number> : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr <number> - <number> - <number> <time> . <number> : e tensorflow / compiler / xla / status_macros . cc : <number> ] internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr * * * begin stack trace * * * tsl : : currentstacktrace [ abi : cxx11 ] ( ) xla : : status_macros : : makeerrorstream : : impl : : getstatus ( ) xla : : gpu : : gpucompiler : : optimizehlomodule ( xla : : hlomodule * , stream_executor : : streamexecutor * , stream_executor : : devicememoryallocator * , xla : : gpu : : gputargetconfig const & , xla : : autotuneresults const <wink> xla : : gpu : : gpucompiler : : runhlopasses ( std : : unique_ptr < xla : : hlomodule , std : : default_delete < xla : : hlomodule > > , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & ) xla : : service : : buildexecutable ( xla : : hlomoduleproto const & , std : : unique_ptr < xla : : hlomoduleconfig , std : : default_delete < xla : : hlomoduleconfig > > , xla : : backend * , stream_executor : : streamexecutor * , xla : : compiler : : compileoptions const & , bool ) xla : : localservice : : compileexecutables ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) xla : : localclient : : compile ( xla : : xlacomputation const & , absl : : lts_20220623 : : span < xla : : shape const * const > , xla : : executablebuildoptions const & ) tensorflow : : xladevicecompilerclient : : buildexecutable ( tensorflow : : xlacompiler : : options const & , tensorflow : : xlacompilationresult const & ) tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilestrict ( tensorflow : : devicecompilationclustersignature const & , tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : nameattrlist const & , tensorflow : : devicecompilationcache < xla : : localexecutable > : : value , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tsl : : mutex <wink> tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compileimpl ( tensorflow : : xlacompiler : : compileoptions const & , tensorflow : : xlacompiler : : options const & , tensorflow : : nameattrlist const & , std : : vector < tensorflow : : xlaargument , std : : allocator < tensorflow : : xlaargument > > const & , tensorflow : : devicecompiler < xla : : localexecutable , xla : : localclient > : : compilescope , tensorflow : : devicecompilemode , tensorflow : : opkernelcontext * , tensorflow : : devicecompilationprofiler * , tensorflow : : xlacompilationresult const * * , xla : : localexecutable * <wink> tensorflow : : xlalocallaunchbase : : computeasync ( tensorflow : : opkernelcontext * , std : : function < void ()>) tensorflow : : basegpudevice : : computeasync ( tensorflow : : asyncopkernel * , tensorflow : : opkernelcontext * , std : : function < void ()>) eigen : : threadpooltempl < tsl : : thread : : eigenenvironment > : : workerloop ( int ) std : : _function_handler < void ( ) , tsl : : thread : : eigenenvironment : : createthread ( std : : function < void ()>): : { lambda ( )# <number> }>: : _m_invoke ( std : : _any_data const & ) * * * end stack trace * * * <number> - <number> - <number> <time> . <number> : w tensorflow / core / framework / op_kernel . cc : <number> ] op_requires failed at xla_ops . cc : <number> : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr <number> - <number> - <number> <time> . <number> : i tensorflow / core / common_runtime / executor . cc : <number> ] [ / job : localhost / replica : <number> / task : <number> / device : gpu : <number> ] ( debug info ) executor start aborting ( this does not indicate an error and you can ignore this message ) : internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_0_1 / statefulpartitionedcall } } ] ] [ [ groupcrossdevicecontroledges_1 / identity_7 / _166 ] ] traceback ( most recent call last ) : file "" / home / user4 / project / src / test . py "" , line <number> , in <module> model . fit ( dataset ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / tensorflow / python / eager / execute . py "" , line <number> , in quick_execute tensors = pywrap_tfe . tfe_py_execute ( ctx . _handle , device_name , op_name , tensorflow . python . framework . errors_impl . internalerror : graph execution error : detected at node ' update_1_1 / statefulpartitionedcall ' defined at ( most recent call last ) : file "" / home / user4 / project / src / test . py "" , line <number> , in <module> model . fit ( dataset ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py "" , line <number> , in error_handler return fn ( * args , * * kwargs ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in fit tmp_logs = self . train_function ( iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in train_function return step_function ( self , iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in step_function outputs = model . distribute_strategy . run ( run_step , args =( data , ) ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in _distributed_apply_gradients_fn distribution . extended . update ( file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in apply_grad_to_update_var return self . _update_step_xla ( grad , var , id ( self . _var_key ( var ) ) ) node : ' update_1_1 / statefulpartitionedcall ' detected at node ' update_0_1 / statefulpartitionedcall ' defined at ( most recent call last ) : file "" / home / user4 / project / src / test . py "" , line <number> , in <module> model . fit ( dataset ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py "" , line <number> , in error_handler return fn ( * args , * * kwargs ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in fit tmp_logs = self . train_function ( iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in train_function return step_function ( self , iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in step_function outputs = model . distribute_strategy . run ( run_step , args =( data , ) ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in _distributed_apply_gradients_fn distribution . extended . update ( file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in apply_grad_to_update_var return self . _update_step_xla ( grad , var , id ( self . _var_key ( var ) ) ) node : ' update_0_1 / statefulpartitionedcall ' detected at node ' update_0_1 / statefulpartitionedcall ' defined at ( most recent call last ) : file "" / home / user4 / project / src / test . py "" , line <number> , in <module> model . fit ( dataset ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py "" , line <number> , in error_handler return fn ( * args , * * kwargs ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in fit tmp_logs = self . train_function ( iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in train_function return step_function ( self , iterator ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / engine / training . py "" , line <number> , in step_function outputs = model . distribute_strategy . run ( run_step , args =( data , ) ) file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in _distributed_apply_gradients_fn distribution . extended . update ( file "" / home / user4 / miniconda3 / envs / tf / lib / python3 . <number> / site - packages / keras / optimizers / optimizer . py "" , line <number> , in apply_grad_to_update_var return self . _update_step_xla ( grad , var , id ( self . _var_key ( var ) ) ) node : ' update_0_1 / statefulpartitionedcall ' <number> root error ( s ) found . ( <number> ) internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_1_1 / statefulpartitionedcall } } ] ] ( <number> ) internal : ret_check failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_0_1 / statefulpartitionedcall } } ] ] [ [ groupcrossdevicecontroledges_1 / identity_7 / _166 ] ] ( <number> ) internal failure ( tensorflow / compiler / xla / service / gpu / gpu_compiler . cc : <number> ) dnn ! = nullptr [ [ { { node update_0_1 / statefulpartitionedcall } } ] ] <number> successful operations . <number> derived errors ignored . [ op : __inference_train_function_1810 ] ` ` `",0
tensorflow/tensorflow,"the return value when layernormalization takes zero vectors as input # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution macos # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? # # # # output ` ` ` tf . tensor ( [ [ <number> . <number> . <number> . <number> . ] [ <number> . <number> . <number> . <number> . ] [ <number> . <number> . <number> . <number> . ] ] , shape =( <number> , <number> ) , dtype = float32 ) tf . tensor ( [ [ nan nan nan nan ] [ nan nan nan nan ] [ nan nan nan nan ] ] , shape =( <number> , <number> ) , dtype = float32 ) tf . tensor ( [ [ <number> . <number> . <number> . <number> . ] [ <number> . <number> . <number> . <number> . ] [ <number> . <number> . <number> . <number> . ] ] , shape =( <number> , <number> ) , dtype = float32 ) ` ` ` # # # # document epsilon is used to avoid division by zero errors , but when layernormalization takes a zero vector as input , it returns nan , and batchnormalization returns <number> （ p . s . epsilon is documented as a small floating - point number . in our experiments , we found that epsilon works with larger floating - point numbers . is this a bit vague ? ） # # # standalone code to reproduce the issue ` ` ` shell # # # # standalone code import tensorflow as tf data = tf . zeros ( shape =( <number> , <number> ) ) print ( data ) layer1 = tf . keras . layers . layernormalization ( axis = <number> , epsilon = <number> ) output1 = layer1 ( data ) print ( output1 ) layer2 = tf . keras . layers . batchnormalization ( axis = <number> , epsilon = <number> ) output2 = layer2 ( data ) print ( output2 ) ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"` tf . math . reduce_sum ` ' s ` name ` property does not change in ` model . compile ` # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf <number> # # # custom code no # # # os platform and distribution linux ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? ` ` ` . <repeated> out = tf . math . reduce_sum ( tf . cast ( position > <number> , tf . float32 ) , axis = - <number> , keepdims = true , name = ' final ' ) model = models . model ( inputs = input_ , outputs =[ position , out ] ) model . summary ( ) = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = . <repeated> tf . math . reduce_sum ( tfoplambda ( none , <number> ) <number> [ ' tf . cast [ <number> ] [ <number> ] ' ] ) = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ` ` ` # # # standalone code to reproduce the issue ` ` ` shell input_ = layers . input ( shape =( <number> ) ) position = layers . dense ( <number> , ' sigmoid ' , name = ' position ' ) ( input_ ) out = tf . math . reduce_sum ( tf . cast ( position > <number> , tf . float32 ) , axis = - <number> , keepdims = true , name = ' final ' ) model = models . model ( inputs = input_ , outputs =[ position , out ] ) model . summary ( ) ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"can not save compiled model as . tf # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution mac mini m1 # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory m1 gpu and 8 gb ram # # # current behavior ? can not save compile model as a . tf file . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf class customschedule ( tf . keras . optimizers . schedules . learningrateschedule ) : def __init__ ( self , d_model , f = <number> , warmup_steps = 4 _000 ) : super ( ) . __init__ ( ) self . d_model = d_model self . d_model = tf . cast ( self . d_model , tf . float32 ) self . warmup_steps = warmup_steps self . f = f def __call__ ( self , step ) : step = tf . cast ( step , dtype = tf . float32 ) arg1 = tf . math . rsqrt ( step ) arg2 = step * ( self . warmup_steps * * - <number> ) return self . f * tf . math . rsqrt ( self . d_model ) * tf . math . minimum ( arg1 , arg2 ) def get_config ( self ) : config = { "" d_model "" : self . d_model , "" warmup_steps "" : self . warmup_steps , "" f "" : self . f } return config model = tf . keras . sequential ( [ tf . keras . layers . input ( shape =( <number> , ) ) , tf . keras . layers . dense ( <number> ) ] ) learning_rate = customschedule ( <number> ) optimizer = tf . keras . optimizers . legacy . adam ( learning_rate , beta_1 = <number> , beta_2 = <number> , epsilon = 1 e - <number> ) model . compile ( optimizer = optimizer , loss = "" mse "" , metrics =[ ' accuracy ' ] ) model . save ( "" model . tf "" ) ` ` ` # # # relevant log output ` ` ` shell / opt / homebrew / caskroom / miniconda / base / envs / pythonproject / bin / python / users / albert / pycharmprojects / pythonproject / ai / test . py traceback ( most recent call last ) : file "" / users / albert / pycharmprojects / pythonproject / ai / test . py "" , line <number> , in <module> model . save ( "" model . tf "" ) file "" / opt / homebrew / caskroom / miniconda / base / envs / pythonproject / lib / python3 . <number> / site - packages / keras / src / utils / traceback_utils . py "" , line <number> , in error_handler raise e . with_traceback ( filtered_tb ) from none file "" / opt / homebrew / caskroom / miniconda / base / envs / pythonproject / lib / python3 . <number> / json / encoder . py "" , line <number> , in encode chunks = self . iterencode ( o , _one_shot = true ) file "" / opt / homebrew / caskroom / miniconda / base / envs / pythonproject / lib / python3 . <number> / json / encoder . py "" , line <number> , in iterencode return _iterencode ( o , <number> ) typeerror to serialize <number> to json . unrecognized type < class ' tensorflow . python . framework . ops . eagertensor ' > . ` ` `",0
tensorflow/tensorflow,segmentation fault # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tensorflow / tensorflow : latest - gpu - jupyter # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? python train . py - - x . <repeated> / . <repeated> / data / train2_x . npy - - y . <repeated> / . <repeated> / data / train2_y . npy # # # standalone code to reproduce the issue ` ` ` shell segmentation fault but i use tensorflow : <number> . <number> this problem does not occur ` ` ` # # # relevant log output _no response_,0
tensorflow/tensorflow,"the required input dimension and type changed after conversion of sac algorithm # # # <number> . system information - os platform and distribution : linux ubuntu <number> - tensorflow installation : pip package - tensorflow library : tensorflow <number> . <number> # # # <number> . code most of them i followed the sac minitaur tutorial here <url> but with some modification to fit with my environment , also with additional function to convert to tflite <url> # # # <number> . failure after conversion the observation spec reduced and changed from shape ( <number> , ) , float32 to shape [ <number> ] , int32 in this conversion > observation spec : boundedarrayspec ( shape =( <number> , ) , dtype = dtype ( ' float32 ' ) , name = ' observation ' , minimum = <number> , maximum = <number> ) reward spec : arrayspec ( shape =() , dtype = dtype ( ' float32 ' ) , name = ' reward ' ) action spec : boundedarrayspec ( shape =( <number> , ) , dtype = dtype ( ' float32 ' ) , name = ' action ' , minimum = - <number> , maximum = <number> ) time step : timestep ( { ' discount ' : array ( <number> . , dtype = float32 ) , ' observation ' : array ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = float32 ) , ' reward ' : array ( <number> . , dtype = float32 ) , ' step_type ' : array ( <number> , dtype = int32 ) } ) step = <number> : averagereturn = <number> , averageepisodelength = <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : loss = - <number> step = <number> : averagereturn = <number> , averageepisodelength = <number> step = <number> : loss = - <number> policy saved at / home / erde / minipads / sim / experiment / rl_ctler / tf_exp / temp_sac / policy # # # # # # # # # # # # # # # # # # # # convert to tflite input shape : { ' arg_0_discount ' : tensorspec ( shape =( none , ) , dtype = tf . float32 , name = ' <number> / discount ' ) , ' arg_0_observation ' : tensorspec ( shape =( none , <number> ) , dtype = tf . float32 , name = ' <number> / observation ' ) , ' arg_0_reward ' : tensorspec ( shape =( none , ) , dtype = tf . float32 , name = ' <number> / reward ' ) , ' arg_0_step_type ' : tensorspec ( shape =( none , ) , dtype = tf . int32 , name = ' <number> / step_type ' ) } info : created tensorflow lite xnnpack delegate for cpu . input details : [ { ' name ' : ' action_0_step_type : <number> ' , ' index ' : <number> , ' shape ' : array ( [ <number> ] , dtype = int32 ) , ' shape_signature ' : array ( [ - <number> ] , dtype = int32 ) , ' dtype ' : < class ' numpy . int32 ' > , ' quantization ' : ( <number> , <number> ) , ' quantization_parameters ' : { ' scales ' : array ( [ ] , dtype = float32 ) , ' zero_points ' : array ( [ ] , dtype = int32 ) , ' quantized_dimension ' : <number> } , ' sparsity_parameters ' : { } } , { ' name ' : ' action_0_discount : <number> ' , ' index ' : <number> , ' shape ' : array ( [ <number> ] , dtype = int32 ) , ' shape_signature ' : array ( [ - <number> ] , dtype = int32 ) , ' dtype ' : < class ' numpy . float32 ' > , ' quantization ' : ( <number> , <number> ) , ' quantization_parameters ' : { ' scales ' : array ( [ ] , dtype = float32 ) , ' zero_points ' : array ( [ ] , dtype = int32 ) , ' quantized_dimension ' : <number> } , ' sparsity_parameters ' : { } } , { ' name ' : ' action_0_observation : <number> ' , ' index ' : <number> , ' shape ' : array ( [ <number> , <number> ] , dtype = int32 ) , ' shape_signature ' : array ( [ - <number> , <number> ] , dtype = int32 ) , ' dtype ' : < class ' numpy . float32 ' > , ' quantization ' : ( <number> , <number> ) , ' quantization_parameters ' : { ' scales ' : array ( [ ] , dtype = float32 ) , ' zero_points ' : array ( [ ] , dtype = int32 ) , ' quantized_dimension ' : <number> } , ' sparsity_parameters ' : { } } , { ' name ' : ' action_0_reward : <number> ' , ' index ' : <number> , ' shape ' : array ( [ <number> ] , dtype = int32 ) , ' shape_signature ' : array ( [ - <number> ] , dtype = int32 ) , ' dtype ' : < class ' numpy . float32 ' > , ' quantization ' : ( <number> , <number> ) , ' quantization_parameters ' : { ' scales ' : array ( [ ] , dtype = float32 ) , ' zero_points ' : array ( [ ] , dtype = int32 ) , ' quantized_dimension ' : <number> } , ' sparsity_parameters ' : { } } ] output details : [ { ' name ' : ' statefulpartitionedcall : <number> ' , ' index ' : <number> , ' shape ' : array ( [ <number> , <number> ] , dtype = int32 ) , ' shape_signature ' : array ( [ - <number> , - <number> ] , dtype = int32 ) , ' dtype ' : < class ' numpy . float32 ' > , ' quantization ' : ( <number> , <number> ) , ' quantization_parameters ' : { ' scales ' : array ( [ ] , dtype = float32 ) , ' zero_points ' : array ( [ ] , dtype = int32 ) , ' quantized_dimension ' : <number> } , ' sparsity_parameters ' : { } } ] input data : [ <number> ] input shape : [ <number> ] output data - the converted model has less dimension input and different input type required",0
tensorflow/tensorflow,attributeerror : module ' tensorflow_datasets ' has no attribute ' load ' # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # os platform and distribution windows <number> # # # mobile device _no response_ # # # python version <date> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i cannot access [ ucf101 ] ( <url> and download . tensorflow <number> python <date> tensorflow_datasets <number> . <number> # # # standalone code to reproduce the issue ` ` ` shell import tensorflow_datasets ucf101 = tensorflow_datasets . video . ucf101 . ucf101 ( ) ` ` ` # # # relevant log output ` ` ` shell attributeerror ' tensorflow_datasets ' has no attribute ' load ' ` ` `,0
tensorflow/tensorflow,"tf . data train model worse than numpy data # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory _no response_ # # # current behavior ? * * <number> . used tf . data train model best val loss is <number> , and the code is : * * [ image ] ( <url> ` ` ` train_dataset1 = tf . data . dataset . from_tensor_slices ( ( np . array ( xtrain ) . reshape ( - <number> * <number> ) , np . array ( encoder_train ) , np . array ( decoder_train ) ) ) train_dataset2 = tf . data . dataset . from_tensor_slices ( ( np . array ( ytrain ) ) ) train_dataset = tf . data . dataset . zip ( ( train_dataset1 , train_dataset2 ) ) test_dataset1 = tf . data . dataset . from_tensor_slices ( ( np . array ( xtest ) . reshape ( - <number> * <number> ) , np . array ( encoder_test ) , np . array ( decoder_test ) ) ) test_dataset2 = tf . data . dataset . from_tensor_slices ( ( np . array ( ytest ) ) ) test_dataset = tf . data . dataset . zip ( ( test_dataset1 , test_dataset2 ) ) batch_size = <number> shuffle_buffer_size = <number> train_dataset = train_dataset . shuffle ( shuffle_buffer_size ) . batch ( batch_size ) test_dataset = test_dataset . batch ( batch_size ) model = model ( inputs = inputs , outputs = s2s ) model . compile ( loss = ' mse ' , optimizer = ' adam ' , metrics =[ ' mape ' ] ) early_stopping = earlystopping ( monitor = ' val_loss ' , patience = <number> , restore_best_weights = true ) model . fit ( train_dataset , epochs = <number> , <hashtag> batch size </hashtag> = <number> , validation_data = test_dataset , callbacks =[ early_stopping ] ) ` ` ` * * <number> . used numpy array train model best val loss is <number> , the code is : * * ! [ image ] ( <url> ` ` ` model = model ( inputs = inputs , outputs = s2s ) model . compile ( loss = ' mse ' , optimizer = ' adam ' , metrics =[ ' mape ' ] ) early_stopping = earlystopping ( monitor = ' val_loss ' , patience = <number> , restore_best_weights = true ) model . fit ( [ np . array ( xtrain ) . reshape ( - <number> * <number> ) , np . array ( encoder_train ) , np . array ( decoder_train ) ] , np . array ( ytrain ) , epochs = <number> , batch_size = <number> , validation_data = ( [ np . array ( xtest ) . reshape ( - <number> * <number> ) , np . array ( encoder_test ) , np . array ( decoder_test ) ] , np . array ( ytest ) ) , callbacks =[ early_stopping ] ) ` ` ` * * <number> . tf . data . dataset . from_generator val loss worse more : * * # # # standalone code to reproduce the issue ` ` ` shell tf . data bugs at version <number> ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"site / en / guide / create_op . md example code has memory leak # # # issue type documentation bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution linux centos <number> # # # mobile device linux centos <number> # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? in the demo code , tensor * * output_tensor <emphasis> * was allocated but the memory was not free ` <hashtag> include </hashtag> "" tensorflow / core / framework / op_kernel . h "" using namespace tensorflow ; class zerooutop : public opkernel { public : explicit zerooutop ( opkernelconstruction * context ) { } void compute ( opkernelcontext * context ) override { / / grab the input tensor const tensor & input_tensor = context - > input ( <number> ); auto input = input_tensor . flat <int32> (); / / create an output tensor tensor * output_tensor = null ; op_requires_ok ( context , context - > allocate_output ( <number> , input_tensor . shape ( ) , & output_tensor ) ); auto output_flat = output_tensor - > flat <int32> (); / / set all but the first element of the output tensor to <number> . const int n = input . size ( ); for ( int i = <number> ; i < n ; i + + ) { output_flat ( i ) = <number> ; } / / preserve the first input value if possible . if ( n > <number> ) output_flat ( <number> ) = input ( <number> ); } }; ` # # # standalone code to reproduce the issue ` ` ` shell repeatedly call the zerooutop . you will see the memory continue to increase ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"tf . image . adjust_gamma outputs incorrect error message for string input # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? when giving tf . image . adjust_gamma a string inputs , it outputs misleading error messages : ` ` ` valueerror to convert a numpy array to a tensor ( unsupported object type float ) . ` ` ` # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf t = tf . constant ( [ ] , dtype = tf . string ) i = tf . image . adjust_gamma ( t , gamma = <number> ) ` ` ` ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,"distributeddatasetinterface is not an attribute of input_lib - engine \ \ data_adapter . py # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> # # # custom code no # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i am new to machine learning and i was setting up this network with just one layer , with only one neuron inside it , just to see how it works . setting up went fine . the error occured when i asked it to predict . i am not sure what exactly happend , but apparently , a non existent attribute ( ` input_lib . distributeddatasetinterface ` ) was passed to ` isinstance ( ) ` in the file : . <repeated> \ \ site - packages \ \ tensorflow \ \ python \ \ keras \ \ engine \ \ data_adapter . py ( line <number> ) . i have replaced by the wrong one by the one suggested and worked as expected . # # # standalone code to reproduce the issue ` ` ` shell import numpy as np import tensorflow as tf from tensorflow . python . keras . layers import dense , input from tensorflow . python . keras import sequential from tensorflow . python . keras . activations import sigmoid import logging logging . getlogger ( "" tensorflow "" ) . setlevel ( logging . error ) tf . autograph . set_verbosity ( <number> ) x_train = np . array ( [ <number> . , <number> , <number> , <number> , <number> , <number> ] , dtype = np . float32 ) . reshape ( - <number> ) y_train = np . array ( [ <number> , <number> , <number> , <number> , <number> , <number> ] , dtype = np . float32 ) . reshape ( - <number> ) model = sequential ( [ input ( shape =( <number> , ) ) , dense ( <number> , activation = sigmoid , name = "" l1 "" ) ] ) model . summary ( ) # setting the weights and bias of the neuron logistic_layer = model . get_layer ( ' l1 ' ) set_w = np . array ( [ [ <number> ] ] ) set_b = np . array ( [ - <number> ] ) logistic_layer . set_weights ( [ set_w , set_b ] ) # performance test a1 = model . predict ( x_train [ <number> ] . reshape ( <number> ) ) # this line caused the error print ( a1 ) alog = sigmoid ( np . dot ( set_w , x_train [ <number> ] . reshape ( <number> ) ) + set_b ) print ( alog ) ` ` ` # # # relevant log output ` ` ` shell . <repeated> \ \ site - packages \ \ tensorflow \ \ python \ \ keras \ \ engine \ \ data_adapter . py "" , line <number> , in _is_distributed_dataset return isinstance ( ds , input_lib . distributeddatasetinterface ) attributeerror : module ' tensorflow . python . distribute . input_lib ' has no attribute ' distributeddatasetinterface ' . did you mean ` ` `",0
tensorflow/tensorflow,"my customized op gives incorrect outputs on gpus since ` tf - nightly <number> . <number> . dev20230413 ` # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? yes # # # source binary # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution fedora <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? i have a complex program based on tensorflow with several customized ops . these ops were created following <url> yesterday tf <number> . <number> was released , but after i upgraded to <number> . <number> , i found that one of my customized op gives incorrect results on gpus and still has the correct outputs on cpus . then i tested many ` tf - nightly ` versions and found that ` tf - nightly <number> . <number> . dev20230412 ` works but ` tf - nightly <number> . <number> . dev20230413 ` fails . so the situation is shown in the following table : | version | cpu | gpu | | - - - - - - - - | - - - - - - - - - | - - - - - - - - - - - | | tensorflow <number> . <number> | correct | correct | | tensorflow <number> . <number> | correct | incorrect | | tf - nightly <number> . <number> . dev20230412 | correct | correct | | tf - nightly <number> . <number> . dev20230413 | correct | incorrect | i ' d like to know what changed between <date> and 1 3 th related to the customized ops . this can be a breaking change to downstream applications or an internal bug . thanks here is a quick link for commits between <date> and 1 3 th # # # standalone code to reproduce the issue ` ` ` shell indeed , the reason is still unclear to me , so it is hard to create a minimal example . the code of our customized ops is <url> ` ` ` # # # relevant log output _no response_",0
tensorflow/tensorflow,python <number> . <number> is incompitable with tensorflow <number> . <number> # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution windows <number> educion # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? [ image ] ( <url> python <number> . <number> throws an exception that it cannot import ordereddict from typings . the fix was downgrading from tensorflow <number> . <number> to tensorflow <number> . <number> . - orderd dict is supported starting from python <number> . <number> : <url> - by default pip installs <number> . <number> for python <number> . <number> which is not supported - the doc states that <number> . <number> is supported : <url> # # # standalone code to reproduce the issue ` ` ` shell <number> ) install python <number> . <number> <number> ) pip install tensorflow = = <number> . <number> <number> ) inside python shell type tensorflow as tf <number> ) exception will rise ` ` ` # # # relevant log output _no response_,0
tensorflow/tensorflow,"tf . data dataset : warning when caching validation set . "" you should use ` dataset . take ( k ) . cache ( ) . repeat ( ) ` instead . "" # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution ubuntu <number> lts # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version cuda_11 . <number> . r11 . <number> / compiler . 2 9 9 2 0 1 3 0 _0 # # # gpu model and memory nvidia a100 - sxm4 - 8 0 gb # # # current behavior ? whenever i use the cache function on my tf . data validation dataset i get the warning below . when i use the cache only and without the validation set , no warning appears . # # # standalone code to reproduce the issue ` ` ` shell dataset = tf . data . dataset . from_generator ( pygen . generator , args =[ files , minmax ] , output_signature =( tf . tensorspec ( shape =s [ <number> ] , dtype = tf . float32 ) , tf . tensorspec ( shape =s [ <number> ] , dtype = tf . float32 ) ) ) val_dataset = tf . data . dataset . from_generator ( pygen . generator , args =[ val_files , minmax ] , output_signature =( tf . tensorspec ( shape =s [ <number> ] , dtype = tf . float32 ) , tf . tensorspec ( shape =s [ <number> ] , dtype = tf . float32 ) ) ) dataset = dataset . take ( len ( files ) ) . cache ( ) . batch ( args . bs ) . repeat ( args . epochs ) . prefetch ( tf . data . autotune ) val_dataset = val_dataset . take ( len ( val_files ) ) . cache ( filename = f ' { tempfile . gettempdir ( ) } / val ' ) . batch ( <number> ) . repeat ( args . epochs ) . prefetch ( tf . data . autotune ) strategy = tf . distribute . multiworkermirroredstrategy ( ) with strategy . scope ( <sad> m = unet28 . build ( s [ <number> ] ) m . fit ( dataset , validation_data = val_dataset , epochs = args . ep , steps_per_epoch = spe , validation_steps = vspe , callbacks =[ model_checkpoint_callback , save50 , model_csv_logger , model_tensorboard , model_earlystopping_30 ] , verbose = <number> ) ` ` ` # # # relevant log output ` ` ` shell <number> - <number> - <number> <time> . <number> tensorflow / core / kernels / data / cache_dataset_ops . cc : <number> ] the calling iterator did not fully read the dataset being cached . in order to avoid unexpected truncation of the dataset , the partially cached contents of the dataset will be discarded . this can happen if you have an input pipeline similar to ` dataset . cache ( ) . take ( k ) . repeat ( ) ` . you should use ` dataset . take ( k ) . cache ( ) . repeat ( ) ` instead . ` ` `",0
tensorflow/tensorflow,"not correct result from tf . split ( ) # # # issue type bug # # # have you reproduced the bug with tensorflow nightly ? no # # # source binary # # # tensorflow version tf <number> - <number> - <number> # # # custom code no # # # os platform and distribution both on mac and on ubuntu # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behavior ? tryng to split a four axes tensor , i got some unexpected zeros in the splitted tensors . this behaviour is not presented in the tf <number> . is an old bug ? # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf vec = tf . ones ( ( <number> , <number> ) ) splitted = tf . split ( vec , [ <number> ] , axis = <number> ) print ( splitted ) # <hashtag> zeros </hashtag> have occured in the tensor ` ` ` # # # relevant log output ` ` ` shell no ` ` `",0
tensorflow/tensorflow,"` <hashtag> include </hashtag> "" include / float8 . h "" / / from <user> ` is missing <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution docker / tensorflow / tensorflow : nightly - gpu # # # mobile device no # # # python version <number> # # # bazel version na # # # gcc / compiler version nr # # # cuda / cudnn version nr # # # gpu model and memory nr # # # current behaviour ? create a test . cc : ` ` ` <hashtag> include </hashtag> "" tensorflow / core / framework / op . h "" ` ` ` compile it with flags provided by tensorflow : get_compile_flags ( <sad> ` ` ` - i / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include - d_glibcxx_use_cxx11_abi = <number> - - std =c + + <number> - deigen_max_align_bytes = <number> ` ` ` get_link_flags ( <sad> ` ` ` - l / usr / local / lib / python3 . <number> / dist - packages / tensorflow - l : libtensorflow_framework . so . <number> ` ` ` ` ` ` g + + - wl , - r , ' $ origin / . <repeated> ' - wl , - rpath , ' $ origin ' - - std =c + + <number> - dndebug - shared test . cc - o / tmp / test . so - fpic - i / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include - d_glibcxx_use_cxx11_abi = <number> - - std =c + + <number> - deigen_max_align_bytes = <number> - i / usr / include - i / usr / local / cuda / include - l / usr / local / lib / python3 . <number> / dist - packages / tensorflow - l : libtensorflow_framework . so . <number> - o2 ` ` ` the result is : ` ` ` in file included from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / tsl / platform / types . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / tsl / framework / numeric_types . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / numeric_types . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / bfloat16 . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / types . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / op_def_builder . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / full_type_inference_util . h : <number> , from / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / core / framework / op . h : <number> , from / tmp / pip - req - build - 8 8 8 7 4 pcq / daliop . cc : <number> : / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / tensorflow / tsl / platform / float8 . h : <time> : fatal error : include / float8 . h such file or directory <number> | <hashtag> include </hashtag> "" include / float8 . h "" / / from <user> ` ` ` it is probably caused by [ this pr ] ( <url> either tensorflow / tsl / platform / float8 . h should have ` <hashtag> include </hashtag> "" external / ml_dtypes / include / float8 . h "" / / from <user> ` , or ` get_compile_flags ( ) ` should include ` - i / usr / local / lib / python3 . <number> / dist - packages / tensorflow / include / external / ml_dtypes ` # # # standalone code to reproduce the issue ` ` ` shell as above ` ` ` # # # relevant log output ` ` ` shell as above ` ` ` </details>",0
tensorflow/tensorflow,"model running on cpu when using nnapi even with nnapidelegate . options ( ) . usennapicpu = false <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version tensorflow - lite <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device google pixel <number> # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? i am running the openai whisper tiny model on android using the tensorflow lite nnapi delegate . it seems to be running on the cpu instead of the hardware accelerator on the google pixel <number> even though "" nnapidelegate . options ( ) . usennapicpu = false "" is set . # # # standalone code to reproduce the issue ` ` ` shell nnapidelegate . options ( ) . usennapicpu = false ` ` ` # # # relevant log output ` ` ` shell w access denied finding property "" ro . mediatek . platform "" w access denied finding property "" ro . chipname "" w access denied finding property "" ro . hardware . chipname "" i created tensorflow lite xnnpack delegate for cpu . ` ` ` </details>",0
tensorflow/tensorflow,"noclassdeffounderror : gpudelegatefactory $ options not found <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? false # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution android <number> # # # mobile device android <number> # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? the gpu is not being used correctly . # # # standalone code to reproduce the issue ` ` ` shell implementation ' org . tensorflow : tensorflow - lite : <number> . <number> ' implementation ' org . tensorflow : tensorflow - lite - gpu : <number> . <number> ' interpreter . options options = new interpreter . options ( ); options . addelegate <elongated> ( new gpudelegate ( )); file file = new file ( getfilesdir ( ) . getabsolutepath ( ) + "" / movenet_thunder . tflite "" ); interpreter interpreter = new interpreter ( file , options ) ; ` ` ` # # # relevant log output ` ` ` shell e / androidruntime : fatal exception : main process : com . litesnap . open . flow . diffusion , pid : <number> java . lang . noclassdeffounderror : failed resolution of : lorg / tensorflow / lite / gpu / gpudelegatefactory $ options ; at org . tensorflow . lite . gpu . gpudelegate . <init> ( gpudelegate . java : <number> ) at com . litesnap . open . flow . diffusion . mainactivity <money> . onclick ( mainactivity . java : <number> ) at android . view . view . performclick ( view . java : <number> ) at android . widget . textview . performclick ( textview . java : <number> ) at com . google . android . material . button . materialbutton . performclick ( materialbutton . java : <number> ) at android . view . view . performclickinternal ( view . java : <number> ) at android . view . view . access <money> ( view . java : <number> ) at android . view . view $ performclick . run ( view . java : <number> ) at android . os . handler . handlecallback ( handler . java : <number> ) at android . os . handler . dispatchmessage ( handler . java : <number> ) at android . os . looper . looponce ( looper . java : <number> ) at android . os . looper . loop ( looper . java : <number> ) at android . app . activitythread . main ( activitythread . java : <number> ) at java . lang . reflect . method . invoke ( native method ) at com . android . internal . os . runtimeinit $ methodandargscaller . run ( runtimeinit . java : <number> ) at com . android . internal . os . zygoteinit . main ( zygoteinit . java : <number> ) caused by : java . lang . classnotfoundexception : did not find class "" org . tensorflow . lite . gpu . gpudelegatefactory $ options "" on path file "" / data / app / ~ ~ omimw9ioqp1ajjtskenrxq ==/ com . litesnap . open . flow . diffusion - s0ywqvuhgu20kzbxckdqxw ==/ base . apk "" ] , nativelibrarydirectories =[/ data / app / ~ ~ omimw9ioqp1ajjtskenrxq ==/ com . litesnap . open . flow . diffusion - s0ywqvuhgu20kzbxckdqxw ==/ lib / arm64 , / data / app / ~ ~ omimw9ioqp1ajjtskenrxq ==/ com . litesnap . open . flow . diffusion - s0ywqvuhgu20kzbxckdqxw ==/ base . apk ! / lib / arm64 - v8a , / system / lib64 , / system / system_ext / lib64 ] ] at dalvik . system . basedexclassloader . findclass ( basedexclassloader . java : <number> ) at java . lang . classloader . loadclass ( classloader . java : <number> ) at java . lang . classloader . loadclass ( classloader . java : <number> ) at org . tensorflow . lite . gpu . gpudelegate . <init> ( gpudelegate . java : <number> ) at com . litesnap . open . flow . diffusion . mainactivity <money> . onclick ( mainactivity . java : <number> ) at android . view . view . performclick ( view . java : <number> ) at android . widget . textview . performclick ( textview . java : <number> ) at com . google . android . material . button . materialbutton . performclick ( materialbutton . java : <number> ) at android . view . view . performclickinternal ( view . java : <number> ) at android . view . view . access <money> ( view . java : <number> ) at android . view . view $ performclick . run ( view . java : <number> ) at android . os . handler . handlecallback ( handler . java : <number> ) at android . os . handler . dispatchmessage ( handler . java : <number> ) at android . os . looper . looponce ( looper . java : <number> ) at android . os . looper . loop ( looper . java : <number> ) at android . app . activitythread . main ( activitythread . java : <number> ) at java . lang . reflect . method . invoke ( native method ) at com . android . internal . os . runtimeinit $ methodandargscaller . run ( runtimeinit . java : <number> ) at com . android . internal . os . zygoteinit . main ( zygoteinit . java : <number> ) ` ` ` </details>",0
tensorflow/tensorflow,"tf2 . <number> breaks register_keras_serializable <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source pypi # # # tensorflow version tf2 . <number> . 0 rc2 # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? when building tf addons for tf2 . <number> we are noticing that our ability to register custom keras objects as serializable has been broken : ` ` ` <user> . keras . saving . register_keras_serializable ( ' my_package ' ) class mydense ( tf . keras . layers . dense ) : def __init__ ( self , units , * * kwargs ) : super ( ) . __init__ ( units , * * kwargs ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell here it is shown as working in tf2 . <number> : <url> here it breaks in tf2 . <number> : <url> ` ` ` # # # relevant log output ` ` ` shell valueerror : unknown layer please ensure you are using a ` keras . utils . custom_object_scope ` and that this object is included in the scope . see <url> for details . ` ` ` ` ` ` </details>",0
tensorflow/tensorflow,"error to run tf <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution windowsx <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? * when i try to run tensorflow ( import him ) , i get this error <kiss> ! [ image ] ( <url> # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf ` ` ` # # # relevant log output _no response_ </details>",0
tensorflow/tensorflow,"xla unit tests fail to build on gcc <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version git head # # # custom code no # # # os platform and distribution ubuntu <number> # # # mobile device n / a # # # python version <date> # # # bazel version <number> . <number> # # # gcc / compiler version <number> . <number> # # # cuda / cudnn version n / a # # # gpu model and memory n / a # # # current behaviour ? <number> unit tests fail to build and one fails with error . # # # standalone code to reproduce the issue ` ` ` shell ' bazel test - - config = = mkl_aarch64_threadpool ' ` ` ` # # # relevant log output ` ` ` shell error : / tf / tensorflow / tensorflow / compiler / xla / service / build : <number> <time> : compiling tensorflow / compiler / xla / service / hlo_parser_test . cc failed : ( exit <number> <sad> gcc failed : error executing command ( from target / / tensorflow / compiler / xla / service : hlo_parser_test ) ( cd / home / buildslave / . cache / bazel / _bazel_buildslave / fbac33eb30dbfb6b11b15a7ff5ac830d / execroot / org_tensorflow & & \ \ exec env - \ \ cachebuster = <number> \ \ path <annoyed> home / buildslave / . cache / bazelisk / downloads / bazelbuild / bazel - <number> . <number> - linux - arm64 / bin <annoyed> usr / local / sbin <annoyed> usr / local / bin <annoyed> usr / sbin <annoyed> usr / bin <annoyed> sbin <annoyed> bin \ \ pwd <annoyed> proc / self / cwd \ \ python_bin_path <annoyed> usr / local / bin / python3 \ \ tf2_behavior = <number> \ \ / dt10 / usr / bin / gcc - md - mf bazel - out / aarch64 - opt / bin / tensorflow / compiler / xla / service / _objs / hlo_parser_test / hlo_parser_test . d ' - frandom - seed = bazel - out / aarch64 - opt / bin / tensorflow / compiler / xla / service / _objs / hlo_parser_test / hlo_parser_test . o ' - deigen_mpl2_only ' - deigen_max_align_bytes = <number> ' - dhave_sys_uio_h - dtf_use_snappy - dbenchmark_static_define ' - dbazel_current_repository = "" "" ' - iquote . - iquote bazel - out / aarch64 - opt / bin - iquote external / eigen_archive - iquote bazel - out / aarch64 - opt / bin / external / eigen_archive - iquote external / com_google_absl - iquote bazel - out / aarch64 - opt / bin / external / com_google_absl - iquote external / nsync - iquote bazel - out / aarch64 - opt / bin / external / nsync - iquote external / double_conversion - iquote bazel - out / aarch64 - opt / bin / external / double_conversion - iquote external / com_google_protobuf - iquote bazel - out / aarch64 - opt / bin / external / com_google_protobuf - iquote external / snappy - iquote bazel - out / aarch64 - opt / bin / external / snappy - iquote external / com_googlesource_code_re2 - iquote bazel - out / aarch64 - opt / bin / external / com_googlesource_code_re2 - iquote external / farmhash_archive - iquote bazel - out / aarch64 - opt / bin / external / farmhash_archive - iquote external / com_google_googletest - iquote bazel - out / aarch64 - opt / bin / external / com_google_googletest - iquote external / com_google_benchmark - iquote bazel - out / aarch64 - opt / bin / external / com_google_benchmark - iquote external / zlib - iquote bazel - out / aarch64 - opt / bin / external / zlib - iquote external / bazel_tools - iquote bazel - out / aarch64 - opt / bin / external / bazel_tools - ibazel - out / aarch64 - opt / bin / external / com_google_benchmark / _virtual_includes / benchmark - isystem external / eigen_archive - isystem bazel - out / aarch64 - opt / bin / external / eigen_archive - isystem external / nsync / public - isystem bazel - out / aarch64 - opt / bin / external / nsync / public - isystem external / com_google_protobuf / src - isystem bazel - out / aarch64 - opt / bin / external / com_google_protobuf / src - isystem external / farmhash_archive / src - isystem bazel - out / aarch64 - opt / bin / external / farmhash_archive / src - isystem external / com_google_googletest / googlemock - isystem bazel - out / aarch64 - opt / bin / external / com_google_googletest / googlemock - isystem external / com_google_googletest / googlemock / include - isystem bazel - out / aarch64 - opt / bin / external / com_google_googletest / googlemock / include - isystem external / com_google_googletest / googletest - isystem bazel - out / aarch64 - opt / bin / external / com_google_googletest / googletest - isystem external / com_google_googletest / googletest / include - isystem bazel - out / aarch64 - opt / bin / external / com_google_googletest / googletest / include - isystem external / zlib - isystem bazel - out / aarch64 - opt / bin / external / zlib - wno - builtin - macro - redefined ' - d__date__ = "" redacted "" ' ' - d__timestamp__ = "" redacted "" ' ' - d__time__ = "" redacted "" ' - fpie - u_fortify_source ' - d_fortify_source = <number> ' - fstack - protector - wall - fno - omit - frame - pointer - no - canonical - prefixes - fno - canonical - system - headers - dndebug - g0 - o2 - ffunction - sections - fdata - sections - wno - all - wno - extra - wno - deprecated - wno - deprecated - declarations - wno - ignored - attributes - wno - array - bounds - wunused - result ' - werror = unused - result ' - wswitch ' - werror = switch ' ' - wno - error = unused - but - set - variable ' - dautoload_dynamic_kernels ' - mtune = generic ' ' - march = armv8 - a ' - o3 ' - std =c + + <number> ' ' - - sysroot <annoyed> dt10 ' - c tensorflow / compiler / xla / service / hlo_parser_test . cc - o bazel - out / aarch64 - opt / bin / tensorflow / compiler / xla / service / _objs / hlo_parser_test / hlo_parser_test . o ) # configuration : de691fac16e6bac2c61ad8c09da26892c92f72e8ffaa16698c2dd3959f4ebc3a # execution platform : <user> / / : platform tensorflow / compiler / xla / service / hlo_parser_test . cc : in member function ' virtual void xla : : { anonymous } : : hloparsertest_parsetrivialiotashardingpartialreplication_test : : testbody ( <sad> tensorflow / compiler / xla / service / hlo_parser_test . cc : <number> <time> : error : call of overloaded ' tileassignment ( < brace - enclosed initializer list > ) ' is ambiguous <number> | tileassignment tiling_last_dim_replicated ( { <number> , <number> }); | ^ in file included from . / tensorflow / compiler / xla / hlo / ir / hlo_sharding . h : <number> , from . / tensorflow / compiler / xla / hlo / ir / hlo_instruction . h : <number> , from . / tensorflow / compiler / xla / hlo / ir / hlo_computation . h : <number> , from . / tensorflow / compiler / xla / service / hlo_parser . h : <number> , from tensorflow / compiler / xla / service / hlo_parser_test . cc : <number> : . / tensorflow / compiler / xla / hlo / ir / tile_assignment . h : <number> <time> : note : candidate : ' xla : : tileassignment : : tileassignment ( absl : : lts_20230125 : : span < const long int > ) ' <number> | explicit tileassignment ( absl : : span < const int64_t > dims ) | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ . / tensorflow / compiler / xla / hlo / ir / tile_assignment . h : <number> <time> : note : candidate : ' xla : : tileassignment : : tileassignment ( xla : : iotatileassignment ) ' <number> | explicit tileassignment ( iotatileassignment iota ) { } and more ` ` ` </details>",0
tensorflow/tensorflow,"tf lite converter produces outputs in an incorrect order when multiple outputs are present # # # <number> . system information - os platform and distribution ( e . g . , linux ubuntu <number> <sad> linux ubuntu <number> - tensorflow installation ( pip package or built from source ) : pip - tensorflow library ( version , if pip package or github sha , if built from source ) : <number> . <number> - dev20230622 # # # <number> . code this is the minimized code to reproduce the issue : ` ` ` python import tensorflow as tf import numpy as np x1 = tf . constant ( [ <number> . ] , shape =[ <number> ] ) class model ( tf . keras . model ) : def __init__ ( self ) : super ( model , self ) . __init__ ( ) def call ( self , x1 ) : x2 = tf . eye ( <number> ) x3 = tf . eye ( <number> ) x4 = tf . eye ( <number> ) return [x 2 , x3 , x4 ] m = model ( ) expected_value = m ( x1 ) converter = tf . lite . tfliteconverter . from_keras_model ( m ) tflite_model = converter . convert ( ) def _evaluatetflitemodel ( tflite_model , input_data ) : interpreter = tf . lite . interpreter ( model_content = tflite_model ) interpreter . allocate_tensors ( ) input_details = interpreter . get_input_details ( ) output_details = interpreter . get_output_details ( ) for i in range ( len ( input_data ) <sad> interpreter . set_tensor ( input_details [ i ] [ ' index ' ] , input_data [ i ] ) interpreter . invoke ( ) output_data = [ interpreter . get_tensor ( output_details [ i ] [ ' index ' ] ) for i in range ( len ( output_details ) ) ] return output_data actual_value = _evaluatetflitemodel ( tflite_model , [x 1 ] ) <hashtag> outputs </hashtag> print ( f "" expected output_1 : { expected_value [ <number> ] . numpy ( ) } "" ) print ( f "" lite output_1 : { actual_value [ <number> ] } "" ) print ( "" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "" ) print ( f "" expected output_2 : { expected_value [ <number> ] . numpy ( ) } "" ) print ( f "" lite output_2 : { actual_value [ <number> ] } "" ) print ( "" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "" ) print ( f "" expected output_3 : { expected_value [ <number> ] . numpy ( ) } "" ) print ( f "" lite output_3 : { actual_value [ <number> ] } "" ) <hashtag> wrong </hashtag> order tf . lite . experimental . analyzer . analyze ( model_content = tflite_model ) <hashtag> output </hashtag> ir ` ` ` # # # <number> . failure after conversion output ( incorrect order ) : ` ` ` expected output_1 : [ [ <number> . ] ] lite output_1 : [ [ <number> . ] ] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - expected output_2 : [ [ <number> . <number> . ] [ <number> . <number> . ] ] lite output_2 : [ [ <number> . ] ] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - expected output_3 : [ [ <number> . ] ] lite output_3 : [ [ <number> . <number> . ] [ <number> . <number> . ] ] ` ` ` lite ir : ` ` ` subgraph # <number> main ( t # <number> ) - > [ t # <number> , t # <number> , t # <number> ] tensors of subgraph # <number> t # <number> ( serving_default_input_1 : <number> ) shape_signature <sad> - <number> , <number> ] , type : float32 t # <number> ( partitionedcall : <number> ) shape <sad> <number> , <number> ] , type : float32 ro <number> bytes , buffer : <number> , data <sad> <number> ] t # <number> ( partitionedcall : <number> ) shape <sad> <number> , <number> ] , type : float32 ro <number> bytes , buffer : <number> , data <sad> <number> , <number> , <number> , <number> ] ` ` ` tf lite converter produces wrong outputs in the lite ir , the ouput should be ` [ t # <number> , t # <number> , t # <number> ] ` instead of ` [ t # <number> , t # <number> , t # <number> ] ` .",0
tensorflow/tensorflow,"float8 ( both e4m3fn and e5m2 ) missing from numbertype # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution macos - <number> . <number> - arm64 - arm - 6 4 bit # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? fp8 datatypes are missing from ` knumbertypes ` in ` tensorflow / core / framework / types . h ` , and also missing from ` tf_call_float_types ( m ) ` in ` tensorflow / core / framework / register_types . h ` . this causes simple ops ( like slice , transpose , split , etc . ) to raise notfounderror . # # # standalone code to reproduce the issue ` ` ` python import tensorflow as tf from tensorflow . python . framework import dtypes a = tf . constant ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = dtypes . float16 ) print ( a ) a_fp8 = tf . cast ( a , dtypes . float8_e4m3fn ) print ( a_fp8 ) b = a_fp8 [ <number> : <number> ] # tensorflow . python . framework . errors_impl . notfounderror b = tf . transpose ( a_fp8 , [ <number> , <number> ] ) # tensorflow . python . framework . errors_impl . notfounderror ` ` ` # # # relevant log output ` ` ` tensorflow . python . framework . errors_impl . notfounderror : could not find device for node : { { node stridedslice } } = stridedslice [ index = dt_int32 , t = dt_float8_e4m3fn , begin_mask = <number> , ellipsis_mask = <number> , end_mask = <number> , new_axis_mask = <number> , shrink_axis_mask = <number> ] all kernels registered for op stridedslice : device = ' xla_cpu_jit ' ; index in [ dt_int32 , dt_int16 , dt_int64 ] ; t in [ dt_float , dt_double , dt_int32 , dt_uint8 , dt_int16 , <number> , dt_half , dt_uint32 , dt_uint64 , dt_float8_e5m2 , dt_float8_e4m3fn ] device = ' cpu ' ; t in [ dt_uint64 ] device = ' cpu ' ; t in [ dt_int64 ] device = ' cpu ' ; t in [ dt_uint32 ] device = ' cpu ' ; t in [ dt_uint16 ] device = ' cpu ' ; t in [ dt_int16 ] device = ' cpu ' ; t in [ dt_uint8 ] device = ' cpu ' ; t in [ dt_int8 ] device = ' cpu ' ; t in [ dt_int32 ] device = ' cpu ' ; t in [ dt_half ] device = ' cpu ' ; t in [ dt_bfloat16 ] device = ' cpu ' ; t in [ dt_float ] device = ' cpu ' ; t in [ dt_double ] device = ' cpu ' ; t in [ dt_complex64 ] device = ' cpu ' ; t in [ dt_complex128 ] device = ' cpu ' ; t in [ dt_bool ] device = ' cpu ' ; t in [ dt_string ] device = ' cpu ' ; t in [ dt_resource ] device = ' cpu ' ; t in [ dt_variant ] device = ' cpu ' ; t in [ dt_qint8 ] device = ' cpu ' ; t in [ dt_quint8 ] device = ' cpu ' ; t in [ dt_qint32 ] device = ' default ' ; t in [ dt_int32 ] [ op : stridedslice ] name : strided_slice / ` ` ` ` ` ` tensorflow . python . framework . errors_impl . notfounderror : could not find device for node : { { node transpose } } = transpose [ t = dt_float8_e4m3fn , tperm = dt_int32 ] all kernels registered for op transpose tperm in [ dt_int32 , dt_int64 ] ; t in [ dt_float , dt_double , dt_int32 , dt_uint8 , dt_int16 , <number> , dt_half , dt_uint32 , dt_uint64 , dt_float8_e5m2 , dt_float8_e4m3fn ] device = ' cpu ' ; t in [ dt_uint64 ] device = ' cpu ' ; t in [ dt_int64 ] device = ' cpu ' ; t in [ dt_uint32 ] device = ' cpu ' ; t in [ dt_uint16 ] device = ' cpu ' ; t in [ dt_int16 ] device = ' cpu ' ; t in [ dt_uint8 ] device = ' cpu ' ; t in [ dt_int8 ] device = ' cpu ' ; t in [ dt_int32 ] device = ' cpu ' ; t in [ dt_half ] device = ' cpu ' ; t in [ dt_bfloat16 ] device = ' cpu ' ; t in [ dt_float ] device = ' cpu ' ; t in [ dt_double ] device = ' cpu ' ; t in [ dt_complex64 ] device = ' cpu ' ; t in [ dt_complex128 ] device = ' cpu ' ; t in [ dt_bool ] device = ' cpu ' ; t in [ dt_string ] device = ' cpu ' ; t in [ dt_resource ] device = ' cpu ' ; t in [ dt_variant ] [ op : transpose ] ` ` `",0
tensorflow/tensorflow,android gpu delegate failed to build program executable <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> or <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? a bug happened ! i test gpu delegate on oppo r9 . tfliteinterpretermodifygraphwithdelegate will reture error value . error : failed to build program executable - build program failure <source> : <number> <time> : error : opencl extension ' cl_khr_3d_image_writes ' is not supported <hashtag> pragma </hashtag> opencl extension cl_khr_3d_image_writes : enable ^ error : compiler frontend failed ( error code <number> ) error : falling back to opengl error : tflitegpudelegate init : opengl - based api disabled error : tflitegpudelegate prepare : delegate is not initialized error : node number <number> ( tflitegpudelegatev2 ) failed to prepare . error original execution plan after delegate application failure . tflite gpu delegate create failed ! <number> # # # standalone code to reproduce the issue ` ` ` shell oppo r9 is produced in <number> year ` ` ` # # # relevant log output _no response_ </details>,0
tensorflow/tensorflow,"get_file raises exception if the content - length is unknown <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code no # # # os platform and distribution ubuntu <number> . <number> lts # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? calling get_file on a url that does not indicate the content - length causes an exception . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf p = tf . keras . utils . get_file ( fname = "" auto - mpg . csv "" , origin = "" <url> "" machine - learning - databases / auto - mpg / auto - mpg . data "" ) print ( p ) ` ` ` # # # relevant log output ` ` ` shell downloading data from <url> <number> / unknown - 0 s 0 us / steptraceback ( most recent call last ) : file "" / home / kent / env - ml / src / f3 . py "" , line <number> , in <module> p = tf . keras . utils . get_file ( fname = "" auto - mpg . csv "" , file "" / home / kent / env - ml / lib / python3 . <number> / site - packages / keras / utils / data_utils . py "" , line <number> , in get_file urlretrieve ( origin , fpath , dlprogbar ( ) ) file "" / home / kent / env - ml / lib / python3 . <number> / site - packages / keras / utils / data_utils . py "" , line <number> , in urlretrieve for chunk in chunk_read ( response , reporthook = reporthook ) : file "" / home / kent / env - ml / lib / python3 . <number> / site - packages / keras / utils / data_utils . py "" , line <number> , in chunk_read reporthook ( count , chunk_size , total_size ) file "" / home / kent / env - ml / lib / python3 . <number> / site - packages / keras / utils / data_utils . py "" , line <number> , in __call__ self . progbar . update ( self . progbar . target ) file "" / home / kent / env - ml / lib / python3 . <number> / site - packages / keras / utils / generic_utils . py "" , line <number> , in update bar = "" % 7 d / unknown "" % current typeerror : % d format real number is required , not nonetype ` ` ` </details>",0
tensorflow/tensorflow,"some parameters are missing type descriptions <details> <summary> click to expand </summary> # # # issue type documentation bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution _no response_ # # # mobile device _no response_ # # # python version _no response_ # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? < html xmlns <surprise> = "" urn : schemas - microsoft - com : office : office "" xmlns <kiss> = "" urn : schemas - microsoft - com : office : excel "" xmlns = "" <url> <head> < meta name = progid content = excel . sheet > < meta name = generator content = "" microsoft excel <number> "" > < link id = main - file rel = main - file href = "" file :/// c <annoyed> users / pigpi / appdata / local / temp / msohtmlclip1 / <number> / clip . htm "" > < link rel = file - list href = "" file :/// c <annoyed> users / pigpi / appdata / local / temp / msohtmlclip1 / <number> / clip_filelist . xml "" > <style> < ! - - table { mso - displayed - decimal - separator :""\\ . "" ; mso - displayed - thousand - separator :""\\ , "" ;} <user> { margin : . 7 5 in . 7 in . 7 5 in . 7 in ; mso - header - margin : . 3 in ; mso - footer - margin : . 3 in ;} . font5 { color : windowtext ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ;} tr { mso - height - source : auto ; mso - ruby - visibility : none ;} col { mso - width - source : auto ; mso - ruby - visibility : none ;} br { mso - data - placement : same - cell ;} td { padding - top : 1 px ; padding - right : 1 px ; padding - left : 1 px ; mso - ignore : padding ; color : black ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ; mso - number - format : general ; text - align : general ; vertical - align : bottom ; border : none ; mso - background - source : auto ; mso - pattern : auto ; mso - protection : locked visible ; white - space : nowrap ; mso - rotate : <number> ;} ruby { ruby - align : left ;} rt { color : windowtext ; font - size : <number> . 0 pt ; font - weight : <number> ; font - style : normal ; text - decoration : none ; font - family : 等线 ; mso - generic - font - family : auto ; mso - font - charset : <number> ; mso - char - type : none ; display : none ;} - - > </style> </head> < body link = "" <hashtag> 0 5 6 3 c1 </hashtag> "" vlink = "" <hashtag> 9 5 4 f72 </hashtag> "" > api | lack of type desciption params - - | - - tf . sparse . bincount | values tf . keras . layers . input | tensor tf . keras . applications . densenet121 | input_tensor tf . math . add_n | inputs tf . keras . applications . mobilenetv2 | input_tensor tf . device | device_name tf . keras . regularizers . get | identifier tf . metrics . rootmeansquarederror | metrics tf . expand_dims | input tf . keras . applications . densenet201 | input_tensor tf . signal . frame | signal tf . losses . mean_squared_error | y_true 、 y_pred tf . losses . cosine_similarity | y_true 、 y_pred tf . keras . metrics . binary_accuracy | y_true 、 y_pred tf . keras . losses . logcosh | y_true 、 y_pred tf . keras . applications . efficientnetb5 | input_tensor tf . metrics . binary_accuracy | y_true 、 y_pred tf . data . experimental . assert_cardinality | expected_cardinality tf . keras . metrics . sparse_top_k_categorical_accuracy | y_true 、 y_pred tf . losses . squared_hinge | y_true 、 y_pred tf . keras . utils . pack_x_y_sample_weight | x 、 y 、 sample_weight tf . keras . activations . softplus | x tf . nn . depth_to_space | input tf . gather_nd | params tf . zeros_like | input tf . keras . applications . efficientnetb2 | input_tensor tf . keras . metrics . meanabsoluteerror | metrics tf . stack | values tf . ensure_shape | x tf . roll | input tf . keras . activations . gelu | x tf . linalg . set_diag | input 、 diagonal 、 k tf . math . bincount | arr 、 weights tf . concat | values tf . keras . applications . efficientnetb6 | input_tensor tf . keras . losses . mean_squared_error | y_true 、 y_pred tf . random . categorical | logits tf . keras . backend . is_keras_tensor | x tf . keras . activations . tanh | x tf . pad | tensor tf . keras . initializers . identity | gain tf . keras . applications . efficientnetb0 | input_tensor tf . repeat | input tf . image . convert_image_dtype | image tf . split | value tf . keras . utils . to_categorical | y tf . scatter_nd | updates tf . keras . layers . concatenate | input tf . linalg . banded_triangular_solve | bands 、 rhs tf . ones_like | input tf . nest . flatten | structure tf . keras . activations . linear | x tf . linalg . tensor_diag_part | input tf . image . pad_to_bounding_box | image tf . config . set_visible_devices | devices tf . size | input tf . image . resize | images tf . tile | input tf . keras . applications . vgg16 | input_tensor tf . keras . metrics . top_k_categorical_accuracy | y_true 、 y_pred tf . initializers . identity | gain tf . image . stateless_random_brightness | image tf . ragged . range | starts 、 limits 、 deltas tf . reshape | tensor tf . losses . logcosh | y_true 、 y_pred tf . keras . applications . resnet50v2 | input_tensor tf . nn . moments | x tf . keras . applications . efficientnetb4 | input_tensor tf . image . adjust_jpeg_quality | image tf . keras . losses . sparse_categorical_crossentropy | y_true 、 y_pred tf . control_dependencies | control_inputs tf . image . grayscale_to_rgb | images tf . image . rgb_to_yiq | images tf . keras . applications . efficientnetb3 | input_tensor tf . ragged . boolean_mask | data 、 mask tf . image . random_hue | image tf . image . adjust_gamma | image tf . is_tensor | x tf . keras . initializers . orthogonal | gain tf . math . polyval | coeffs 、 x tf . io . serialize_tensor | tensor tf . image . stateless_random_saturation | image tf . one_hot | indices tf . linalg . diag_part | input 、 padding_value 、 tf . image . adjust_saturation | image tf . boolean_mask | tensor 、 mask tf . transpose | a tf . image . flip_up_down | image tf . keras . losses . binary_crossentropy | y_true 、 y_pred tf . broadcast_to | input tf . image . stateless_random_crop | value tf . losses . mean_absolute_percentage_error | y_true 、 y_pred tf . image . stateless_random_flip_left_right | image tf . image . random_flip_up_down | image tf . keras . activations . exponential | x tf . keras . applications . xception | input_tensor tf . identity | input tf . gather | params tf . keras . applications . inceptionv3 | input_tensor tf . keras . layers . masking | mask_value tf . losses . kullback_leibler_divergence | y_true 、 y_pred tf . linalg . band_part | input tf . keras . losses . cosine_similarity | y_true 、 y_pred tf . image . random_contrast | image tf . image . transpose | image tf . stop_gradient | input tf . strings . bytes_split | input tf . random . stateless_parameterized_truncated_normal | means 、 stddevs 、 minvals 、 maxvals tf . keras . losses . mean_absolute_error | y_true 、 y_pred tf . image . stateless_random_hue | image tf . keras . applications . densenet169 | input_tensor tf . keras . losses . categorical_crossentropy | y_true 、 y_pred tf . nn . embedding_lookup | params tf . math . reduce_variance | input_tensor tf . keras . utils . unpack_x_y_sample_weight | data tf . nn . l2_normalize | x tf . keras . losses . categorical_hinge | y_true 、 y_pred tf . keras . applications . efficientnetb1 | input_tensor tf . keras . constraints . get | identifier tf . initializers . orthogonal | gain tf . divide | x 、 y tf . math . top_k | input tf . keras . losses . kullback_leibler_divergence | y_true 、 y_pred tf . image . stateless_random_jpeg_quality | image tf . keras . losses . mean_absolute_percentage_error | y_true 、 y_pred tf . keras . applications . efficientnetb7 | input_tensor tf . clip_by_value | t tf . type_spec_from_value | value tf . losses . mean_squared_logarithmic_error | y_true 、 y_pred tf . tensor_scatter_nd_update | tensor 、 indices tf . equal | x 、 y tf . image . rgb_to_grayscale | images tf . image . stateless_random_contrast | image tf . image . rgb_to_hsv | images tf . convert_to_tensor | value tf . losses . sparse_categorical_crossentropy | y_true 、 y_pred tf . keras . activations . sigmoid | x tf . slice | input_ tf . image . adjust_hue | image tf . math . argmax | input tf . reverse_sequence | input tf . losses . categorical_crossentropy | y_true 、 y_pred tf . keras . losses . squared_hinge | y_true 、 y_pred tf . squeeze | input tf . math . equal | x 、 y tf . math . divide | x 、 y tf . unstack | value tf . keras . applications . mobilenet | input_tensor tf . keras . applications . resnet152v2 | input_tensor tf . keras . activations . softsign | x tf . keras . applications . nasnetmobile | input_tensor tf . keras . activations . swish | x tf . metrics . categorical_accuracy | y_true 、 y_pred tf . keras . metrics . sparse_categorical_accuracy | y_true 、 y_pred tf . metrics . sparse_top_k_categorical_accuracy | y_true 、 y_pred tf . losses . mean_absolute_error | y_true 、 y_pred tf . losses . binary_crossentropy | y_true 、 y_pred tf . keras . applications . resnet50 | input_tensor tf . image . random_jpeg_quality | image 、 min_jpeg_quality 、 max_jpeg_quality tf . keras . activations . hard_sigmoid | x tf . image . flip_left_right | image tf . keras . applications . resnet101v2 | input_tensor tf . nn . batch_normalization | x 、 mean 、 variance 、 offset 、 scale tf . math . reduce_min | input_tensor tf . keras . applications . resnet101 | input_tensor tf . math . not_equal | x 、 y tf . image . rot90 | image tf . keras . applications . vgg19 | input_tensor tf . image . stateless_random_flip_up_down | image tf . keras . applications . mobilenetv3large | input_tensor tf . keras . applications . resnet152 | input_tensor tf . keras . metrics . categorical_accuracy | y_true 、 y_pred tf . sparse . cross | inputs tf . keras . losses . mean_squared_logarithmic_error | y_true 、 y_pred tf . image . random_flip_left_right | image </body> </html> # # # standalone code to reproduce the issue ` ` ` shell many parameters are not for any type of value , so the allowed types should be clearly marked in the document . ` ` ` # # # relevant log output _no response_ </details>",0
tensorflow/tensorflow,"tensorboard histogram onehot operation causing resourceexhauseerror : oom <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? i am trying to train a vgg16 model . i am using a sample dataset of <number> 3 0 0 x300 images in <number> classes , and running my code on a google vm using an nvidia l4 gpu with 2 0 gb of memory . i am running python <number> , tf version <number> , and cuda version <number> . my data is stored in gcs . when i run the model with the following tensorboard callback : ` tensorboard_callback = tf . keras . callbacks . tensorboard ( log_dir = log_dir , histogram_freq = <number> ) ` i get this error at the end of the first epoch : ` ` ` <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / bfc_allocator . cc : <number> ] allocator ( mklcpu ) ran out of memory trying to allocate <number> . 9 7 gib ( rounded to <number> ) requested by op onehot if the cause is memory fragmentation maybe the environment variable ' tf_gpu_allocator = cuda_malloc_async ' will improve the situation . resourceexhaustederror : { { function_node _wrapped__onehot_device / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } oom when allocating tensor with shape [ <number> ] and type double on / job : localhost / replica : <number> / task : <number> / device : cpu : <number> by allocator mklcpu [ op : onehot ] ` ` ` the error traces back to the tensorboard histogram object : ` ` ` - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - resourceexhaustederror traceback ( most recent call last ) / var / tmp / ipykernel_5723 / <phone> . py in <module> <number> # fit model - - - - > <number> history = model . fit ( train_ds , validation_data = val_ds , epochs = <number> , callbacks =[ tensorboard_callback ] ) / opt / conda / lib / python3 . <number> / site - packages / keras / utils / traceback_utils . py in error_handler ( * args , * * kwargs ) <number> # to get the full stack trace , call : <number> # ` tf . debugging . disable_traceback_filtering ( ) ` - - - > <number> raise e . with_traceback ( filtered_tb ) from none <number> finally : <number> del filtered_tb / opt / conda / lib / python3 . <number> / site - packages / tensorboard / plugins / histogram / summary_v2 . py in histogram ( name , data , step , buckets , description ) <number> tensor = lazy_tensor , <number> step = step , - - > <number> metadata = summary_metadata , <number> ) <number> / opt / conda / lib / python3 . <number> / site - packages / tensorboard / util / lazy_tensor_creator . py in __call__ ( self ) <number> elif self . _tensor is none : <number> self . _tensor = _call_in_progress_sentinel - - - > <number> self . _tensor = self . _tensor_callable ( ) <number> return self . _tensor <number> / opt / conda / lib / python3 . <number> / site - packages / tensorboard / plugins / histogram / summary_v2 . py in lazy_tensor ( ) <number> <user> . lazytensorcreator <number> def lazy_tensor ( <sad> - - > <number> return _buckets ( data , buckets ) <number> <number> return tf . summary . write ( / opt / conda / lib / python3 . <number> / site - packages / tensorboard / plugins / histogram / summary_v2 . py in _buckets ( data , bucket_count ) <number> ) <number> - - > <number> return tf . cond ( is_empty , when_empty , when_nonempty ) / opt / conda / lib / python3 . <number> / site - packages / tensorboard / plugins / histogram / summary_v2 . py in when_nonempty ( ) <number> <number> return tf . cond ( - - > <number> has_single_value , when_single_value , when_multiple_values <number> ) <number> / opt / conda / lib / python3 . <number> / site - packages / tensorboard / plugins / histogram / summary_v2 . py in when_multiple_values ( ) <number> # see <url> for details . <number> one_hots = tf . one_hot ( - - > <number> clamped_indices , depth = bucket_count , dtype = tf . float64 <number> ) <number> bucket_counts = tf . cast ( resourceexhaustederror : { { function_node __wrapped__onehot_device_ / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } oom when allocating tensor with shape [ <number> ] and type double on / job : localhost / replica : <number> / task : <number> / device : cpu : <number> by allocator mklcpu [ op : onehot ] ` ` ` interestingly it seems to be calling tf . one_hot and blowing up the gpu memory with a massive tensor regardless of whether i train the model with integer labels and spare categorical cross entropy or if i train it with one hot labels and cross entropy . i do not really understand what the tensor contains because its dimensions neither relate to the number of training examples or classes that i am using . # # # standalone code to reproduce the issue ` ` ` shell tensorboard_callback = tf . keras . callbacks . tensorboard ( log_dir = log_dir , histogram_freq = <number> ) i get this error at the end of the first epoch : <number> - <number> - <number> <time> . <number> : w tensorflow / tsl / framework / bfc_allocator . cc : <number> ] allocator ( mklcpu ) ran out of memory trying to allocate <number> . 9 7 gib ( rounded to <number> ) requested by op onehot if the cause is memory fragmentation maybe the environment variable ' tf_gpu_allocator = cuda_malloc_async ' will improve the situation . resourceexhaustederror _wrapped__onehot_device / job : localhost / replica : <number> / task : <number> / device : cpu : <number> } } oom when allocating tensor with shape [ <number> ] and type double on / job : localhost / replica : <number> / task : <number> / device : cpu : <number> by allocator mklcpu [ op : onehot ] ` ` ` # # # relevant log output _no response_ </details>",0
tensorflow/tensorflow,"tf . test . gpu_device_name ( ) leads to soft lockup and unusable system <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source binary # # # tensorflow version <number> . <number> # # # custom code yes # # # os platform and distribution linux ubuntu <number> # # # mobile device _no response_ # # # python version <date> ( conda <number> ) # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version <number> # # # gpu model and memory nvidia t4 # # # current behaviour ? we found that running ` tf . test . gpu_device_name ( ) ` leads to a soft lockup and an unresponsive system . i apologise in advance , i do not know much about tensorflow . but several of my co - workers do and i am in charge of maintaining infrastructure . we have a virtual server for compute - intensive tasks where we train models and work with large datasets . yesterday at about <number> local time i found that i could not ssh into the server so i had to wait for it to forcibly restart the server . at around <number> local time the server was back up and i looked through the kernel logs to find that there was a soft lockup kernel bug . this means that the server was still running , but some process wasn ' t releasing the cpu for more than <number> seconds , which meant that no other process could fulfill its tasks . not even half an hour later and the server locked up again . i got a message from a co - worker who suspected that they were at fault for the server locking up because they started running a very compute - intensive python script on the server last night . they opened a python shell and typed in these two lines and watched the server lock up in real time . ` ` ` py import tensorflow as tf print ( ' default gpu device { } ' . format ( tf . test . gpu_device_name ( ) ) ) ` ` ` i provided the relevant log output below . it required another hard reset from it to get the server back into a working state . after the server restarted again i had another look at the kernel logs and found that the reported errors were the same , which means i have reason to believe that running ` tf . test . gpu_device_name ( ) ` consistently leads to a soft lockup on our infrastructure . as mentioned previously , we run our computations on a virtual server . the hypervisor is vmware . we are running on a nvidia t4 gpu with nvidia drivers in version <number> . <number> . these drivers were provided to us by it . installing another version , for some reason , is not possible and i prefer not to mess with that . the co - worker installed tensorflow in a conda environment using pip . since we are running on a virtual server , we depend on it in case things go very wrong . this means our means of reproducing this issue are somewhat limited . if the server locks up , we have to wait up to a couple hours for our service desk to escalate this issue enough so that it restarts the server . we cannot afford downtimes like these since we rely on the server for our computations . i am happy to provide any logs from previous runs but if possible i would like to not share them on github in their entirety since they might contain confidential info . # # # standalone code to reproduce the issue ` ` ` shell import tensorflow as tf print ( ' default gpu device { } ' . format ( tf . test . gpu_device_name ( ) ) ) ` ` ` # # # relevant log output ` ` ` shell # output from running the function in a python shell $ python3 python <date> ( default , <date> , <time> ) [ gcc <number> . <number> ] : : anaconda , inc . on linux type "" help "" , "" copyright "" , "" credits "" or "" license "" for more information . import tensorflow as tf <number> - <number> - <number> <time> . <number> : i tensorflow / tsl / cuda / cudart_stub . cc : <number> ] could not find cuda drivers on your machine , gpu will not be used . <number> - <number> - <number> <time> . <number> : i tensorflow / tsl / cuda / cudart_stub . cc : <number> ] could not find cuda drivers on your machine , gpu will not be used . <number> - <number> - <number> <time> . <number> : i tensorflow / core / platform / cpu_feature_guard . cc : <number> ] this tensorflow binary is optimized to use available cpu instructions in performance - critical operations . to enable the following instructions : avx2 fma , in other operations , rebuild tensorflow with the appropriate compiler flags . <number> - <number> - <number> <time> . <number> : w tensorflow / compiler / tf2tensorrt / utils / py_utils . cc : <number> ] tf - trt warning : could not find tensorrt > > > print ( ' default gpu device { } ' . format ( tf . test . gpu_device_name ( ) ) ) <number> - <number> - <number> <time> . <number> : i tensorflow / compiler / xla / stream_executor / cuda / cuda_gpu_executor . cc : <number> ] successful numa node read from sysfs had negative value ( - <number> ) , but there must be at least one numa node , so returning numa node zero . see more at <url> <number> - <number> - <number> <time> . <number> : w tensorflow / core / common_runtime / gpu / gpu_device . cc : <number> ] cannot dlopen some gpu libraries . please make sure the missing libraries mentioned above are installed properly if you would like to use gpu . follow the guide at <url> for how to download and setup the required libraries for your platform . skipping registering gpu devices . <repeated> message from syslogd <user> at <date> <time> . <repeated> kernel <sad> <number> ] watchdog : bug : soft lockup - cpu # <number> stuck for 2 2 s ! [ python <time> <number> ] # output from journalctl <date> <time> s050009088 kernel : nvidia - uvm : loaded the uvm driver , major device number <number> . <date> <time> s050009088 kernel : - - - - - - - - - - - - [ cut here ] - - - - - - - - - - - - <date> <time> s050009088 kernel : trying to vfree ( ) nonexistent vm area ( 0 0 0 0 0 0 0 0 f9521180 ) <date> <time> s050009088 kernel : warning : cpu : <number> pid : <number> at mm / vmalloc . c : <number> __vunmap + 0x 1 ff / 0x 2 1 0 <date> <time> s050009088 kernel : modules linked in : nvidia_uvm ( oe ) veth xt_nat xt_tcpudp xt_conntrack xt_masquerade nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_addrtype iptable_filter bpfilter br_netfilter bridge stp llc aufs overlay vmw_vsock_vmci_transport vsock dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua binfmt_misc nvidia_drm ( poe ) nvidia_modeset ( poe ) intel_rapl_msr nvidia ( poe ) vmw_balloon intel_rapl_common input_leds intel_powerclamp joydev rapl serio_raw vmw_vmci mac_hid sch_fq_codel msr ramoops reed_solomon efi_pstore ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear vmwgfx crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel drm_kms_helper syscopyarea aesni_intel sysfillrect crypto_simd sysimgblt mptspi cryptd fb_sys_fops glue_helper psmouse mptscsih drm mptbase ahci i2c_piix4 vmxnet3 scsi_transport_spi <date> <time> s050009088 kernel : libahci pata_acpi <date> <time> s050009088 kernel : cpu : <number> pid : <number> comm : python3 tainted : p oe <number> . <number> - <number> - generic # <number> - ubuntu <date> <time> s050009088 kernel : hardware name : vmware , inc . vmware virtual platform / 4 4 0 bx desktop reference platform , bios <number> <date> <date> <time> s050009088 kernel : rip : <number> : __vunmap + 0x 1 ff / 0x 2 1 0 <date> <time> s050009088 kernel : code : ff e8 d5 fc ff ff eb bf <number> <number> fe <number> c7 c7 a0 6 d b8 8 e e8 <number> ba <number> <number> 0 f 0 b eb b4 4 c <number> ee <number> c7 c7 c8 6 d b8 8 e e8 <number> ba <number> <number> <0f> 0 b eb a1 <number> <number> 2 e 0 f 1 f <number> <number> <number> <number> <number> <number> <number> <number> 0 f 1 f <number> <number> <date> <time> s050009088 kernel : rsp : <number> : ffffab66c32b7c00 eflags : <number> <date> <time> s050009088 kernel : rax : <number> rbx : <number> rcx : <number> <date> <time> s050009088 kernel : rdx : <number> rsi : <number> rdi : ffff8ce3bfb9c8c0 <date> <time> s050009088 kernel : rbp : ffffab66c32b7c28 r08 : 0 0 0 0 0 0 0 0 0 0 0 0 0 6 d2 r09 : 2 8 6 5 6 5 7 2 6 6 7 6 2 0 6 f <date> <time> s050009088 kernel : r10 : 2 8 6 5 6 5 7 2 6 6 7 6 2 0 6 f r11 : 6 9 7 8 6 5 6 e6f6e2029 r12 : <number> <date> <time> s050009088 kernel : r13 : ffff8ce22c348000 r14 : ffff8ce22c349000 r15 : <number> <date> <time> s050009088 kernel : fs : 0 0 0 0 7 fbf935e3180 ( <number> ) gs : ffff8ce3bfb80000 ( <number> ) knlgs : <number> <date> <time> s050009088 kernel : cs : <number> ds : <number> es : <number> cr0 : <number> <date> <time> s050009088 kernel : cr2 : 0 0 0 0 0 0 0 0 0 4 3 5 c3d0 cr3 : 0 0 0 0 0 0 1 e50400001 cr4 : 0 0 0 0 0 0 0 0 0 0 3 6 0 6 e0 <date> <time> s050009088 kernel : call trace : <date> <time> s050009088 kernel : __vfree + 0x 2 2 / 0x 6 0 <date> <time> s050009088 kernel : vfree + 0x 2 c / 0x 4 0 <date> <time> s050009088 kernel : os_free_mem + 0x 1 b / 0x 3 0 [ nvidia ] <date> <time> s050009088 kernel : os_unlock_user_pages + 0x 6 c / 0 xa0 [ nvidia ] <date> <time> s050009088 kernel : _nv000647rm + 0 xdc / 0x 1 6 0 [ nvidia ] <date> <time> s050009088 kernel : warning : kernel stack frame pointer at <phone> b18731 in python <time> <number> has bad value 0 0 0 0 0 0 0 0 f3874266 <date> <time> s050009088 kernel : unwind stack type : <number> next_sp : <number> mask :0 x2 graph_idx : <number> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c0e190e6 : ffffab66c32b7c40 ( 0 xffffab66c32b7c40 ) <date> <time> s050009088 kernel : <phone> c2bd79 : ffffffff8da6e972 ( __vfree + 0x 2 2 / 0x 6 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ea3bef41 : <number> ( 0x 2 0 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 d32e6150 : ffffab66c32b7c58 ( 0 xffffab66c32b7c58 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 e4c2b257 : ffffffff8da6e9dc ( vfree + 0x 2 c / 0x 4 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 8 b622c26 : ffff8ce22c348000 ( 0 xffff8ce22c348000 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 b443bda0 : ffffab66c32b7c68 ( 0 xffffab66c32b7c68 ) <date> <time> s050009088 kernel : <phone> f89fc7 : ffffffffc068ccdb ( os_free_mem + 0x 1 b / 0x 3 0 [ nvidia ] ) <date> <time> s050009088 kernel : <number> : ffffab66c32b7c98 ( 0 xffffab66c32b7c98 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 7 fb1b873 : ffffffffc068edfc ( os_unlock_user_pages + 0x 6 c / 0 xa0 [ nvidia ] ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 2 9 b0f28 : ffff8ce395ff60c8 ( 0 xffff8ce395ff60c8 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 3 a4ac7ef : <number> ( 0x 1 0 0 0 4 4 0 0 0 0 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 5 e9a76c9 : <number> ( 0x 2 0 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 5 fe4a88b : 0 0 0 0 0 0 0 0 aa000000 ( 0 xaa000000 ) <date> <time> s050009088 kernel : <phone> b18731 : ffff8ce3b3e92fb0 ( 0 xffff8ce3b3e92fb0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 bd345bcf : ffffffffc101e8dc ( _nv000647rm + 0 xdc / 0x 1 6 0 [ nvidia ] ) <date> <time> s050009088 kernel : <phone> f90db7 : ffff8ce395ff60c8 ( 0 xffff8ce395ff60c8 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c99c1977 : ffff8ce39726c0c0 ( 0 xffff8ce39726c0c0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 0 6 0 a662 : <number> ( 0x <happy> <date> <time> s050009088 kernel : <phone> ee1352 : ffffffffc101f3a3 ( _nv000723rm + 0 xa43 / 0 xa90 [ nvidia ] ) <date> <time> s050009088 kernel : <phone> b64c73 : <number> . <repeated> <date> <time> s050009088 kernel : <phone> c878cd : ffff8ce3b35bc000 ( 0 xffff8ce3b35bc000 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 e60fac70 : ffffffffc101f316 ( _nv000723rm + 0x 9 b6 / 0 xa90 [ nvidia ] ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 5 7 4 7 b0c : ffff8ce3b3e90000 ( 0 xffff8ce3b3e90000 ) <date> <time> s050009088 kernel : <phone> da2df1 : ffff8ce3a658b800 ( 0 xffff8ce3a658b800 ) <date> <time> s050009088 kernel : <phone> c0ac4e : ffffab66c32b7e48 ( 0 xffffab66c32b7e48 ) <date> <time> s050009088 kernel : <phone> ee515b : ffff8ce3b35b9400 ( 0 xffff8ce3b35b9400 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 c5468b7 : <number> ( 0x 2 7 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 5 2 8 1 df3 : ffffffffc1025204 ( rm_ioctl + 0x 5 4 / 0 xb0 [ nvidia ] ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 b419e233 : 0 0 0 0 0 0 3 8 8 da5fdb5 ( 0x 3 8 8 da5fdb5 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 f1b29a3 : ffff8ce39726c0c0 ( 0 xffff8ce39726c0c0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 4 6 5 f1196 : <number> ( 0x 8 0 0 0 0 0 0 0 0 0 0 0 0 0 2 7 ) <date> <time> s050009088 kernel : <phone> eadd83 : <number> ( 0x 1 0 4 7 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 e74dc1ab : 0 0 0 0 0 0 0 0 0 0 0 0 0 7 e9 ( 0x 7 e9 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f6d262ce : 0 0 3 d08dcf6746300 ( 0x3 d08dcf6746300 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 4 0 0 7 6 0 c5 : 0 0 3 d08dde4df8b00 ( 0x3 d08dde4df8b00 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c7874ca7 : 0 0 3 d08e3f2980f00 ( 0x3 d08e3f2980f00 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 4 6 d2608 : 0 0 3 d08dd6da9f700 ( 0x3 d08dd6da9f700 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 7 a9cecb7 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 0 e6b2400 : <number> ( 0x 2 0 0 ) <date> <time> s050009088 kernel : <phone> df682d : <number> ( 0x <phone> ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 0 5 9 7 2 aee : <number> ( 0x 1 0 4 7 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f3ab8670 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 4 3 3 9 5 ee6 : <number> ( 0x 1 ) <date> <time> s050009088 kernel : <phone> b81e91 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ce75a231 : ffff8ce39726c0c0 ( 0 xffff8ce39726c0c0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 3 3 6 aa9c4 : <number> ( 0x 3 8 ) <date> <time> s050009088 kernel : <phone> e174a9 : ffff8ce3b35b9400 ( 0 xffff8ce3b35b9400 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c453e6ee : ffff8ce3a658b800 ( 0 xffff8ce3a658b800 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ca8fc06a : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 7 3 8 8 a61 : ffffffffc068268f ( nvidia_ioctl + 0x 6 6 f / 0x 8 8 0 [ nvidia ] ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 1 f100b3c : ffff8ce3b3e90000 ( 0 xffff8ce3b3e90000 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 d70e2948 : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 e15ae1f : ffff8ce3b35b9498 ( 0 xffff8ce3b35b9498 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 d53d487 : 0 0 0 0 7 ffd00000027 ( 0x 7 ffd00000027 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 ea37e99 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 7 2 3 c86e7 : <number> ( 0x3 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 4 b070af4 : 3 c5520dc9b31e900 ( 0x3 c5520dc9b31e900 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 6 0 adce7 : ffff8ce3a37b1e01 ( 0 xffff8ce3a37b1e01 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f3ab64dd : ffff8ce3a37b1e00 ( 0 xffff8ce3a37b1e00 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 2 5 0 db0d : ffff8ce3a46feaa0 ( 0 xffff8ce3a46feaa0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 1 a439626 : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 d28c869f : ffff8ce3a37b1e00 ( 0 xffff8ce3a37b1e00 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 6 4 cc6e1 : ffffab66c32b7e58 ( 0 xffffab66c32b7e58 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c9cdb9ef : ffffffffc069191b ( nvidia_frontend_unlocked_ioctl + 0x3 b / 0x 5 0 [ nvidia ] ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 dc15a7de : ffffab66c32b7ed8 ( 0 xffffab66c32b7ed8 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 b8c3053 : ffffffff8dae8e77 ( do_vfs_ioctl + 0x 4 0 7 / 0x 6 7 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f2d02675 : <number> ( 0x 1 0 0 0 4 4 0 0 0 0 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c52e1241 : <number> . <repeated> <date> <time> s050009088 kernel : <phone> ba7e88 : ffffab66c32b7e78 ( 0 xffffab66c32b7e78 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 aaf3e429 : ffffab66c32b7e78 ( 0 xffffab66c32b7e78 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 fd0b1f6d : 3 c5520dc9b31e900 ( 0x3 c5520dc9b31e900 ) <date> <time> s050009088 kernel : <phone> d890d1 : <number> ( 0x 3 1 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 1 e5b54d6 : <number> ( 0x 2 0 0 0 0 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 bf7d038a : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 a021b6f5 : 3 c5520dc9b31e900 ( 0x3 c5520dc9b31e900 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 3 8 2 d5bbd : ffff8ce3a37b1e01 ( 0 xffff8ce3a37b1e01 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 5 6 9 1 9 9 6 c : <number> ( 0x 6 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 b0a9fae : 0 0 0 0 0 0 0 0 c0384627 ( 0 xc0384627 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ed0791ed : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 4 4 e63ee : ffff8ce3a37b1e00 ( 0 xffff8ce3a37b1e00 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 b8133399 : ffffab66c32b7f18 ( 0 xffffab66c32b7f18 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 b019d54 : ffffffff8dae9147 ( ksys_ioctl + 0x 6 7 / 0x 9 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 5 9 5 9 ce7 : 3 c5520dc9b31e900 ( 0x3 c5520dc9b31e900 ) <date> <time> s050009088 kernel : <phone> e92101 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 8 0 1 fc84 : ffffab66c32b7f58 ( 0 xffffab66c32b7f58 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 0 1 8 5 eb1 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 7 c1db1b7 : ffffab66c32b7f28 ( 0 xffffab66c32b7f28 ) <date> <time> s050009088 kernel : <phone> b8d84e : ffffffff8dae918a ( __x64_sys_ioctl + 0x 1 a / 0x 2 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 b8e8f5f : ffffab66c32b7f48 ( 0 xffffab66c32b7f48 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f2da7b14 : ffffffff8d804fd7 ( do_syscall_64 + 0x 5 7 / 0x 1 9 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 9 4 2 f1f70 : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f56a3750 : ffffffff8e4000a4 ( entry_syscall_64_after_hwframe + 0x 5 c / 0 xc1 ) <date> <time> s050009088 kernel : <phone> f467fe : 0 0 0 0 7 ffd116ceed0 ( 0x 7 ffd116ceed0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 8 3 1 b89bf : 0 0 0 0 0 0 0 0 6 4 8 af769 ( 0x 6 4 8 af769 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 0 5 9 7 aba6 : 0 0 0 0 7 ffd116cef88 ( 0x 7 ffd116cef88 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 a139f77e : <number> ( 0x 6 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 6 1 2 db4e : 0 0 0 0 0 0 0 0 c0384627 ( 0 xc0384627 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ab55bfe9 : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 0 b644430 : <number> ( 0x 2 4 6 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 d2d8779b : <number> . <repeated> <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 7 2 5 ab6b6 : 0 0 0 0 7 ffd116cef88 ( 0x 7 ffd116cef88 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 0 3 9 9 d500 : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 c0a877c1 : fda <elongated> ( 0 xffffffffffffffda ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 fb645881 : 0 0 0 0 7 fbf936f83ab ( 0x 7 fbf936f83ab ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 f6cd2602 : 0 0 0 0 7 ffd116cef60 ( 0x 7 ffd116cef60 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 6 fc36d91 : 0 0 0 0 0 0 0 0 c0384627 ( 0 xc0384627 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 e5058b78 : <number> ( 0x 6 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 2 7 7 7 ac42 : <number> ( 0x 1 0 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 d68cb82a : 0 0 0 0 7 fbf936f83ab ( 0x 7 fbf936f83ab ) <date> <time> s050009088 kernel : <phone> e7b064 : <number> ( 0x 3 3 ) <date> <time> s050009088 kernel : <phone> ef3830 : <number> ( 0x 2 4 6 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 fcba1669 : 0 0 0 0 7 ffd116ceec8 ( 0x 7 ffd116ceec8 ) <date> <time> s050009088 kernel : 0 0 0 0 0 0 0 0 ea0caaa1 : 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 b ( 0x 2 b ) <date> <time> s050009088 kernel : ? _nv000723rm + 0 xa43 / 0 xa90 [ nvidia ] <date> <time> s050009088 kernel : ? _nv000723rm + 0x 9 b6 / 0 xa90 [ nvidia ] <date> <time> s050009088 kernel : ? rm_ioctl + 0x 5 4 / 0 xb0 [ nvidia ] <date> <time> s050009088 kernel : ? nvidia_ioctl + 0x 6 6 f / 0x 8 8 0 [ nvidia ] <date> <time> s050009088 kernel : ? nvidia_frontend_unlocked_ioctl + 0x3 b / 0x 5 0 [ nvidia ] <date> <time> s050009088 kernel : ? do_vfs_ioctl + 0x 4 0 7 / 0x 6 7 0 <date> <time> s050009088 kernel : ? ksys_ioctl + 0x 6 7 / 0x 9 0 <date> <time> s050009088 kernel : ? __x64_sys_ioctl + 0x 1 a / 0x 2 0 <date> <time> s050009088 kernel : ? do_syscall_64 + 0x 5 7 / 0x 1 9 0 <date> <time> s050009088 kernel : ? entry_syscall_64_after_hwframe + 0x 5 c / 0 xc1 <date> <time> s050009088 kernel : - - - [ end trace be1a4a9ea080b7b8 ] - - - # # the following logs are then repeatedly printed , leading to the soft lockup <date> <time> s050009088 kernel : bug : bad page state in process python3 pfn : 1 e301a6 <date> <time> s050009088 kernel : page : ffffd052f8c06980 refcount : <number> mapcount : <number> mapping : ffff8ce230f6e0d8 index :0 x1 <date> <time> s050009088 kernel : shmem_aops name : "" dev / zero "" <date> <time> s050009088 kernel : flags : 0x 1 7 ffffc0000000 ( ) <date> <time> s050009088 kernel : raw : 0 0 1 7 ffffc0000000 dead000000000100 dead000000000122 ffff8ce230f6e0d8 <date> <time> s050009088 kernel : raw : <number> <number> 0 0 0 0 0 0 0 0 ffffffff <number> <date> <time> s050009088 kernel : page dumped because : non - null mapping <date> <time> s050009088 kernel : modules linked in : nvidia_uvm ( oe ) veth xt_nat xt_tcpudp xt_conntrack xt_masquerade nf_conntrack_netlink nfnetlink xfrm_user xfrm_algo iptable_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_addrtype iptable_filter bpfilter br_netfilter bridge stp llc aufs overlay vmw_vsock_vmci_transport vsock dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua binfmt_misc nvidia_drm ( poe ) nvidia_modeset ( poe ) intel_rapl_msr nvidia ( poe ) vmw_balloon intel_rapl_common input_leds intel_powerclamp joydev rapl serio_raw vmw_vmci mac_hid sch_fq_codel msr ramoops reed_solomon efi_pstore ip_tables x_tables autofs4 btrfs zstd_compress raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 multipath linear vmwgfx crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel drm_kms_helper syscopyarea aesni_intel sysfillrect crypto_simd sysimgblt mptspi cryptd fb_sys_fops glue_helper psmouse mptscsih drm mptbase ahci i2c_piix4 vmxnet3 scsi_transport_spi <date> <time> s050009088 kernel : libahci pata_acpi <date> <time> s050009088 kernel : cpu : <number> pid : <number> comm : python3 tainted : p w oe <number> . <number> - <number> - generic # <number> - ubuntu <date> <time> s050009088 kernel : hardware name : vmware , inc . vmware virtual platform / 4 4 0 bx desktop reference platform , bios <number> <date> <date> <time> s050009088 kernel : call trace : <date> <time> s050009088 kernel : dump_stack + 0x 6 d / 0x 8 b <date> <time> s050009088 kernel : bad_page . cold + 0x 8 0 / 0 xb1 <date> <time> s050009088 kernel : free_pages_check_bad + 0x 5 f / 0x 7 0 <date> <time> s050009088 kernel : free_pcppages_bulk + 0x 1 8 6 / 0x 6 b0 <date> <time> s050009088 kernel : free_unref_page_commit + 0 xb6 / 0 xd0 <date> <time> s050009088 kernel : free_unref_page_list + 0x 1 0 7 / 0x 1 9 0 <date> <time> s050009088 kernel : release_pages + 0x 3 8 d / 0x 4 0 0 <date> <time> s050009088 kernel : free_pages_and_swap_cache + 0 xb9 / 0 xd0 <date> <time> s050009088 kernel : tlb_flush_mmu + 0x3 a / 0x 1 4 0 <date> <time> s050009088 kernel : zap_pte_range . isra . <number> + 0x 5 6 3 / 0x 8 6 0 <date> <time> s050009088 kernel : ? __warn + 0x 9 d / 0 xe0 <date> <time> s050009088 kernel : unmap_page_range + 0x 2 e6 / 0x 5 6 0 <date> <time> s050009088 kernel : unmap_single_vma + 0x 7 f / 0 xf0 <date> <time> s050009088 kernel : unmap_vmas + 0x 7 9 / 0 xf0 <date> <time> s050009088 kernel : unmap_region + 0 xbc / 0x 1 6 0 <date> <time> s050009088 kernel : __do_munmap + 0x 2 aa / 0x 5 0 0 <date> <time> s050009088 kernel : mmap_region + 0x 2 4 8 / 0x 6 5 0 <date> <time> s050009088 kernel : do_mmap + 0x3 b4 / 0x 5 c0 <date> <time> s050009088 kernel : vm_mmap_pgoff + 0 xcb / 0x 1 2 0 <date> <time> s050009088 kernel : ksys_mmap_pgoff + 0x 1 2 5 / 0x 2 b0 <date> <time> s050009088 kernel : ? fput + 0x 1 3 / 0x 2 0 <date> <time> s050009088 kernel : ? ksys_ioctl + 0x 7 7 / 0x 9 0 <date> <time> s050009088 kernel : __x64_sys_mmap + 0x 3 3 / 0x 4 0 <date> <time> s050009088 kernel : do_syscall_64 + 0x 5 7 / 0x 1 9 0 <date> <time> s050009088 kernel : entry_syscall_64_after_hwframe + 0x 5 c / 0 xc1 <date> <time> s050009088 kernel : rip : <number> :0 x7fbf936fc8e6 <date> <time> s050009088 kernel : code : <number> <number> <number> <number> f3 0 f 1 e fa <number> f7 c1 ff 0 f <number> <number> <number> 2 b <number> <number> <number> fd <number> <number> cb <number> <number> ff <number> <number> <number> <number> da <number> <number> ef b8 <number> <number> <number> <number> 0 f <number> < <number> > 3 d <number> f0 ff ff <number> <number> 5 b 5 d c3 0 f 1 f <number> <number> <number> <number> <number> <number> 8 b <number> <date> <time> s050009088 kernel : rsp : 0 0 2 b : 0 0 0 0 7 ffd116cf078 eflags : <number> orig_rax : <number> <date> <time> s050009088 kernel : rax : fda <elongated> rbx : <number> rcx : 0 0 0 0 7 fbf936fc8e6 <date> <time> s050009088 kernel : rdx : <number> rsi : <number> rdi : <number> <date> <time> s050009088 kernel : rbp : <number> r08 : 0 0 0 0 0 0 0 0 ffffffff r09 : <number> <date> <time> s050009088 kernel : r10 : <number> r11 : <number> r12 : 0 0 0 0 0 0 0 0 0 4 4 be5e0 <date> <time> s050009088 kernel : r13 : <number> r14 : 0 0 0 0 0 0 0 0 0 4 7 d97c0 r15 : 0 0 0 0 0 0 0 0 0 4 3 0 b450 # # eventually the watchdog reports the soft lockup # # checking the pid against the one from the process that ran ` tf . test . gpu_device_name ( ) ` shows that it must ' ve caused this lockup <date> <time> s050009088 kernel : watchdog : bug lockup - cpu # <number> stuck for 2 2 s ! [ python <time> <number> ] ` ` ` </details>",0
tensorflow/tensorflow,"tf . data . dataset . map does not support randomization <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? did not try . # # # source binary # # # tensorflow version tf <number> . <number> # # # custom code yes # # # os platform and distribution macos # # # python version python <number> it seems like ` tf . data . dataset . map ` does not support randomization from ` numpy ` . for the sample code , it gives constant output from the ` randint ` function call , but if you switch the function from ` numpy . random . randint ` to ` tf . random . uniform ` ( toggle the comment ) , then you get good randomization behavior . i am wondering if this is expected . # # # standalone code to reproduce the issue ` ` ` shell a = [ { "" key "" : i , "" value "" : np . random . random ( ) } for i in range ( <number> ) ] dict_of_list = pd . dataframe . from_records ( a ) . to_dict ( orient = "" list "" ) keys = dict_of_list . keys ( ) dataset = tf . data . dataset . zip ( tuple ( [ tf . data . dataset . from_tensor_slices ( dict_of_list [ key ] ) for key in keys ] ) ) dataset = dataset . map ( lambda *x : { key : x[ i ] for i , key in enumerate ( keys ) } ) for i in range ( <number> <sad> for b in dataset : print ( "" before map :"", i , b ) def test_function ( x , f = "" key "" , low = <number> , high = <number> <sad> <hashtag> x </hashtag> [ f ] = tf . random . uniform ( shape =() , minval = <number> , maxval = <number> , dtype = tf . int32 ) # this is randomized x[ f ] = np . random . randint ( low = low , high = high ) # this is not randomized return x dataset = dataset . map ( test_function ) for i in range ( <number> <sad> for b in dataset : print ( "" after map :"", i , b ) ` ` ` # # # relevant log output ` ` ` shell before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } before map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor : shape =() , dtype = float32 , numpy = <number> > } after map : <number> { ' key ' : < tf . tensor : shape =() , dtype = int32 , numpy = <number> > , ' value ' : < tf . tensor dtype = float32 , numpy = <number> > } ` ` ` </details>",0
tensorflow/tensorflow,"functional bug ： could not interpret serialized activation function <details> <summary> click to expand </summary> # # # issue type bug # # # have you reproduced the bug with tf nightly ? no # # # source source # # # tensorflow version tf2 . <number> # # # custom code yes # # # os platform and distribution macos # # # mobile device _no response_ # # # python version <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? # # # # output ` ` ` could not interpret serialized activation function shape =( none , <number> , <number> , <number> ) , dtype = float32 ) ` ` ` # # # # document | ` activation ` | activation function to use . if you do not specify anything , no activation is applied ( see [ ` keras . activations ` ] ( <url> | | - - - - - - - - - - - - | - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - | # # # standalone code to reproduce the issue ` ` ` py x = keras . layers . conv2d ( filters = <number> , kernel_size = <number> , strides = <number> , activation = "" deserialize "" , padding = "" same "" , name = "" conv1a "" ) ( input_tensor ) ` ` ` ` ` ` py x = keras . layers . conv2d ( filters = <number> , kernel_size = <number> , strides = <number> , activation = "" serialize "" , padding = "" same "" , name = "" conv1a "" ) ( input_tensor ) ` ` ` # # # relevant log output _no response_ </details>",0
tensorflow/tensorflow,"image segmenter | tflite - suuport | attributeerror : type object ' segmentationoptions ' has no attribute ' outputtype ' <details> <summary> click to expand </summary> # # # issue type documentation bug # # # have you reproduced the bug with tf nightly ? yes # # # source source # # # tensorflow version tflite - support <number> . 0 a1 # # # custom code no # # # os platform and distribution macos ventura <number> # # # mobile device _no response_ # # # python version <number> . <number> # # # bazel version _no response_ # # # gcc / compiler version _no response_ # # # cuda / cudnn version _no response_ # # # gpu model and memory _no response_ # # # current behaviour ? the syntax provided for using [ image segmenter ] ( <url> did not execute ` ` ` processor . segmentationoptions ` ` ` in python environment . # # # possible fix : the line ` ` ` segmentation_options = processor . segmentationoptions ( output_type = processor . segmentationoptions . outputtype . category_mask ) ` ` ` should have been ` ` ` segmentation_options = processor . segmentationoptions ( output_type = processor . segmentationoptions . output_type . category_mask ) ` ` ` # # # standalone code to reproduce the issue ` ` ` shell <url> ` ` ` # # # relevant log output ` ` ` shell traceback ( most recent call last ) : file "" image_segmenter . py "" , line <number> , in <module> segmentation_options = processor . segmentationoptions ( output_type = processor . segmentationoptions . outputtype . category_mask ) attributeerror object ' segmentationoptions ' has no attribute ' outputtype ' ` ` ` </details>",0
microsoft/vscode,"when saving new file with some content in a readonly directory vscode creates an empty file instead type : <b> bug </b> <number> . use the following settings . json file : { "" files . readonlyinclude "" : { "" * * "" : true } } <number> . create temporary file with some content in editor : [ image ] ( <url> <number> . save file somewhere on the computer : ! [ image ] ( <url> <number> . the file created by vscode is empty , this is a bug : ! [ image ] ( <url> vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : unsupported <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i5 - 1 2 3 5 u ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 6 9 gb ( <number> . 7 8 gb free ) | | process argv || | screen reader | no | | vm | <percent> | </details> extensions <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> 9 b8hh23 <time> <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> copilotsettingt : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonmhint <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",0
microsoft/vscode,"can not use icon contributed using ttf file does this issue occur when all extensions are disabled ? : yes - vs code version : <number> . <number> - os version : macos monterey <number> steps to reproduce : <number> . created a ttf font file using <url> select any icon and font character ( i have selected g ) <number> . update the package . json to include the ttf file ` ` ` icons "" : { "" my - icon - id "" : { "" description "" : "" my icon "" , "" default "" : { "" fontpath "" : "" . / check . ttf "" , "" fontcharacter "" : "" g "" } } } ` ` ` <number> . create status bar item with text ` $( my - icon - id ) <number> . the icon needs to show up but the character g shows up instead [ image ] ( <url> attached my font file [ glyphter - font . zip ] ( <url> expected to show ! [ image ] ( <url>",0
microsoft/vscode,focus cell output with ctrl + down does not visually select the output found testing for <url> <number> . run a cell to get output <number> . press ctrl + down to focus the output : bug it looks like the cell container is focused .,0
microsoft/vscode,"terminal accessible view opens unexpectedly <number> . create a terminal <number> . quickly , use ` alt + f1 ` <number> . 🐛 the terminal accessible view opens , dynamic prompt data is shown",0
microsoft/vscode,"clarify what ` always ` means the comment for ` editor . codeactionsonsave just says "" always trigger code actions on save "" but that ' s not true because format on delay is not a trigger reason . this should be mentioned explicitly because other folks are confused . also , the ` <hashtag> setting </hashtag> . name # ` syntax could be used to link to the required setting",0
microsoft/vscode,"clear persistent tasks when task reconnection is disabled when one disables task reconnection , the persistent tasks should get cleared .",0
microsoft/vscode,comment widget jump to max height when switching editors testing # <number> <url>,0
microsoft/vscode,"' focus session ' button shows in debug toolbar in command center when there are multiple debug sessions , the dropdown action item shows in the debug toolbar here as a button that does not do anything . you can see it in the gif in <url> we could just hide that item when this setting is set",0
microsoft/vscode,"comments accessibility help nits testing # <number> the following commands are inconsistently treated in the accessibility help menu : <number> . run the command : go to previous commenting range , which is currently not triggerable via keybinding . < - - should just be listed as go to previous commenting range , aligning with go to next commenting range . also , should be ' triggerable via a keybinding ' <number> . run the command sticky scroll to focus the currently nested scopes . it is currently not triggerable by a keybinding . < - - should just be listed as focus sticky scroll [ image ] ( <url>",0
microsoft/vscode,"scrolling is blocked when mouse is on review comment testing # <number> i am able to see the text size grow fine with the existing comment , but realized scrolling freezes / blocked when my mouse cursor is on the comment section . when i move the mouse cursor to the actual document itself , i am able to scroll back and forth in the document fine , but scrolling seems to be stuck when cursor is in the comment block . for example , place the mouse cursor in the black comment section and try scrolling up and down the document , scrolling will freeze . however , having mouse cursor in outside the comment section . in this case the green section , will not freeze the scrolling . < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> pm "" src = "" <url>",0
microsoft/vscode,top tab border wrong size with pinned tab sizing shrink when scrolling / wrapping testing # <number> enable ` workbench . editor . highlightmodifiedtabs ` and make a pinned editor dirty . notice how the blue color appears over the entire tab bar : [ image ] ( <url> vs in stable,0
microsoft/vscode,"wrapping tabs needs a smaller threshold for disabling wrapping when <number> rows show testing # <number> see here when many tabs are wrapping and height is little , at once point we disable wrapping to make room for the editor . i think this limit now needs to account for when <number> tab rows are showing .",0
microsoft/vscode,active pinned tab has no separation to other tab row testing # <number> with the default light modern theme and this layout see how the active tab in the first row shows no border to the bottom row . i think i would have expected the border to go through here .,0
microsoft/vscode,"navigating to previous comment thread with shift + alt + f9 does not work on linux testing # <number> the accessibility help says this should work , but it does not appear to do anything <url>",0
microsoft/vscode,"do not observe any result from run recent command testing # <number> testing on windows <number> . - ` alt + f1 ` opens terminal accessibility help dialog correctly : [ image ] ( <url> - explore options , decide i want to try run recent command ( ` control + alt + r ` ) - ` alt + f2 ` to open terminal accessible view , opens correctly - try ` control + alt + r ` - ❓ i do not observe anything happen - but maybe i am misunderstanding what should happen or how i should set this command up ?",0
microsoft/vscode,"missing background color when showing inline chat inbetween * open inline chat on a mid sized method / function * the whole function gets included ( by the chat extension ) * 🐛 the background color is not contiguous < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",0
microsoft/vscode,can not set breakpoint in . c files testing # <number> i have the dwarf extension installed but i can not set breakpoint in ` . c ` files . should dwarf contribute the ability to set breakpoints in ` c ` files ? or am i missing something ?,0
microsoft/vscode,"scm sync : file change shows ` commit ^ ` testing # <number> although technically correct , we should use the actual commit ' s short hash , not the ` parent ^ ` syntax , for left side",0
microsoft/vscode,"make max comment area height depend on editor height ? testing # <number> the constant max height works well with large editors . when the editor area is small , the comment can overgrow it . maybe the max height could be constant unless the editor is very small .",0
microsoft/vscode,quick diff decorations not rendering at all or rendering the wrong state diff decorations are disappearing when switching to another file and back . [ recording <number> - <number> - <number> at <number> <number> <number> ] ( <url>,0
microsoft/vscode,go to prev / next commands keybindings are not working in terminal testing # <number> [ image ] ( <url> option + down / up arrow in terminal is not navigating between commands . these are working only in accessibility view . in terminal the keybindings are cmd + down / up arrow .,0
microsoft/vscode,"settings sync gets stuck at "" turning on settings sync . <repeated> "" after turning it off < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - - vs code version : version : <number> . <number> - insider ( user setup ) commit : 1 0 9 e1f8d8afb754ed31317f79937a44e98d5063b date : <number> - <number> - 2 5 t <time> . 2 9 5 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> - - os version : os : windows_nt x64 <number> . <number> steps to reproduce : <number> . turn on settings sync <number> . do not select anything to sync ( leave all checkboxes unchecked ) and click on "" sign in to sync "" <number> . sign in ( used my gh account , insiders ) <number> . get a notification that settings sync is on <number> . go back to the gear , turn off settings sync result the only way out of this is to restart vs code ( reload window does not fix it ) . once you do that , it appears to be turned off , everything is normal again .",0
microsoft/vscode,"[ accessibility ] take out "" exit this dialog ( escape ) . "" from terminal accessible view type : <b> bug </b> tested with terminal created in editor area . at the bottom of the terminal buffer ( accessible view ) , there is an instruction : > exit this dialog ( escape ) . users may not want to hear this instruction all the time . please consider removing this instruction when verbosity setting is off . vs code version : code - insiders <number> . <number> - insider ( 1 0 9 e1f8d8afb754ed31317f79937a44e98d5063b , <number> - <number> - 2 5 t <time> . 2 9 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 7 3 gb free ) | | process argv | . - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> aiprm - lang | aip | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> overleaf - workshop | iam | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttype : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> pythontestfixt : <number> pythonfb28095 <time> <number> pythonregdiag : <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,scm sync view actions are contributed to all views scm sync view refresh command,0
microsoft/vscode,debug controls in command center appear and rightaway disappear on window reload <number> . reload vscode window ( given you have setting to put debug controls in the command center <number> . 🐛 notice how the command center includes debug controls and then right away disappear see : <url> # version ` ` ` version : <number> . <number> - insider commit : c72447e8d8aaa7497c9a4bd68bc4301584b92beb date : <number> - <number> - 2 2 t <time> . 2 7 4 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os arm64 <number> . <number> ` ` `,0
microsoft/vscode,"profile icon missing type : <b> bug </b> in yesterday ' s insider i changed the icon for my documentation profile . atfer the update today it ' s gone . reloading the window did not help . < img width = "" <number> "" alt = "" image "" src = "" <url> vs code version : code - insiders <number> . <number> - insider ( c72447e8d8aaa7497c9a4bd68bc4301584b92beb , <number> - <number> - 2 2 t <time> . 2 7 4 z ) os version : darwin arm64 <number> . <number> modes : remote os version x64 <number> . <number> - <number> - azure < - - generated by issue reporter - - >",0
microsoft/vscode,next / previous commenting range command does not work i am on a mac . setting it to be ` option + f10 ` works though,0
microsoft/vscode,"[ accessibility ] alt + uparrow and alt + downarrow does not work as expected in terminal buffer accessible view type : <b> bug </b> cc <user> <number> . create a terminal in editor area <number> . type ` echo hello ; echo world ` in the terminal <number> . press alt + f2 to open accessible view <number> . press alt + uparrow and alt + downarrow to see if it moves between executed commands . * notes : alt + downarrow unexpectedly opens a new terminal view . vs code version : code - insiders <number> . <number> - insider ( 1 1 bfd76a61a299156a9f3138ecfad70937af3527 , <number> - <number> - 2 1 t <time> . 2 2 7 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 3 3 gb free ) | | process argv | - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> aiprm - lang | aip | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> overleaf - workshop | iam | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttype : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> pythontestfixt : <number> pythonfb28095 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,goto symbol cell decorations do not persist for multiple symbols in a single cell steps to reproduce : <number> . create a notebook with a markdown cell that has multiple headers e . g . ` ` ` # header # # sub header ` ` ` <number> . show the preview for the markdown cell - shift + enter <number> . open symbols quickpick ctrl + shift + o and arrow down through the symbols list : bug decoration appears for the first item but then is removed for the second,0
microsoft/vscode,"focusing a task terminal tab removes its status <number> . run a task , like ` vs code - build ` <number> . click on the terminal tab for one of the task terminals <number> . 🐛 the status is gone",0
microsoft/vscode,quick diff should not run in diff editor i think we had it disabled there,0
microsoft/vscode,issue reporter does not have a placeholder in the extension drop down [ image ] ( <url> this is needed for accessibility .,0
microsoft/vscode,"[ accessibility ] ctrl + downarrow in editor - type terminal buffer creates a new terminal view type : <b> bug </b> # # reproducible steps <number> . terminal : create new terminal in editor area <number> . type ` echo hello ` and hit enter . <number> . press alt + f2 to open accessible terminal buffer . <number> . in the accessible terminal buffer , press ctrl + downarrow to see if your focus goes back to the terminal input area . * notes : pressing ctrl + downarrow unexpectedly creates a new terminal view . vs code version : code - insiders <number> . <number> - insider ( 7 c7f7eee860e299499a3bd2915ad716f09f2d6a6 , <number> - <number> - 1 9 t <time> . 7 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 7 0 gb free ) | | process argv |c :\\\\ users \ \ \ \ jseo1005 \ \ \ \ onedrive - university of illinois - urbana \ \ \ \ desktop \ \ \ \ source . r - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> aiprm - lang | aip | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> overleaf - workshop | iam | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttype : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> pythontestfixt : <number> pythonfb28095 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,"` treeview . message = undefined ` should not block ` viewswelcome ` from appearing < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : macos <number> . <number> steps to reproduce : <number> . use vscode treeview with a ` viewswelcome ` , make sure the treeview is empty <number> . set the ` treeview . message ` to ` undefined ` <number> . the viewwelcome is not shown a minimal reproducible example is accessible on here : <url> expected behaviour the ` treeview . message ` to ` undefined ` should make vscode consider the message as empty and show the ` viewswelcome `",0
microsoft/vscode,add placeholder to terminal change icon and color [ image ] ( <url> ! [ image ] ( <url> feedback from <user> ( not daniel ),0
microsoft/vscode,"moved code false positive # # description this should not be reported as moved code # # playground example [ monaco editor playground repro ] ( <url> ( click on "" use latest dev "" to verify a future bug - fix ) [ another example ] ( <url>",0
microsoft/vscode,"aadsts500202 pass - thru error trying to sign - in with an msa using ' microsoft ' auth provider on github . dev the below code works fine in vs code desktop , and in <url> but fails on <url> * if <emphasis> * using an msa account . when i sign - in using my corp account ( i . e . ` * <user> . com ` ) is also works fine . ` ` ` ts log . debug ( "" querying for account workspaces "" ); / / * * * authenticate and retrieve tenants the user has azure resources for * * * / / for the msa case , you need to query the tenants first and get the underlying azuread / / tenant for the ' guest ' msa . see <url> const firstauth = await vscode . authentication . getsession ( "" microsoft "" , [ scopes . armmgmt ] , { createifnone : true } ); / / note : does not even get to this line , so the above must throw . if ( firstauth ) { log . error ( "" no authentication session returned "" ); return ; } ` ` ` below is the debug level output from the ` microsoft authentication ` channel : ` ` ` <number> - <number> - <number> <time> . <number> [ info ] reading sessions from secret storage . <repeated> <number> - <number> - <number> <time> . <number> [ info ] got <number> stored sessions <number> - <number> - <number> <time> . <number> [ info ] getting sessions for all scopes . <repeated> <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for all scopes . <repeated> <number> - <number> - <number> <time> . <number> [ info ] getting sessions for the following scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] getting sessions for the following scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ trace ] no session found with idtoken scopes . <repeated> using fallback scope list of : <url> <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] logging in for the following scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] exchanging login code for token for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ error ] error exchanging code for token ( for scopes email <url> offline_access openid profile ) : error : { "" error "" : "" invalid_grant "" , "" error_description "" : "" aadsts500202 : user account ' { emailhidden } ' from external identity provider ' live . com ' is not supported for api version ' <number> ' . microsoft account pass - thru users and guests are not supported by the tenant - independent endpoint . \ \ r \ \ ntrace id : 4 8 1 ada76 - a441 - 4 9 0 d - aec8 - 5 f6ab6141600 \ \ r \ \ ncorrelation id : c4c89d78 - 1 6 5 c - 4 eff - 9 c4e - <phone> f4 \ \ r \ \ ntimestamp : <number> - <number> - <number> <time> z "" , "" error_codes "" <sad> <number> ] , "" timestamp "" : "" <number> - <number> - <number> <time> z "" , "" trace_id "" : "" 4 8 1 ada76 - a441 - 4 9 0 d - aec8 - 5 f6ab6141600 "" , "" correlation_id "" :""c 4 c89d78 - 1 6 5 c - 4 eff - 9 c4e - <phone> f4 "" } ` ` ` using exactly the same code with the same msa account but working on ` <url> ( or vs code desktop ) it works fine and the log shows : ` ` ` <number> - <number> - <number> <time> . <number> [ info ] reading sessions from secret storage . <repeated> <number> - <number> - <number> <time> . <number> [ info ] got <number> stored sessions <number> - <number> - <number> <time> . <number> [ info ] getting sessions for all scopes . <repeated> <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for all scopes . <repeated> <number> - <number> - <number> <time> . <number> [ info ] getting sessions for the following scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] getting sessions for the following scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] logging in for the following scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] exchanging login code for token for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] exchanging login code for token ( for scopes : email <url> offline_access openid profile ) succeeded ! <number> - <number> - <number> <time> . <number> [ info ] setting token for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] login successful for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] token available from cache ( for scopes email <url> offline_access openid profile ) , expires in <number> milliseconds <number> - <number> - <number> <time> . <number> [ info ] stored token for scopes : email <url> offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] getting sessions for the following scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] refreshing token for scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] setting token for scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] token refresh success for scopes : email offline_access openid profile <number> - <number> - <number> <time> . <number> [ info ] got <number> sessions for scopes : email offline_access openid profile ` ` ` if interested , the extension i am using is published at <url> and you can see the code for it at <url> . < ! - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - >",0
microsoft/vscode,"notebook list view height changes after toggling the panel off / on <number> . open a notebook where two cells take up more than the scrollable region <number> . navigate between the two cells , note how far down the region scrolls <number> . toggle the panel off / on <number> . navigate between the two cells again 🐛 the scroll placement is different because ` listview . renderheight ` returns a different value . [ recording <number> - <number> - <number> at <number> <number> <number> ] ( <url>",0
microsoft/vscode,"backup file system provider can not do buffered write < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> - insiders - os version i was verifying <url> and found that large file piece tree buffers are still merged before write . turns out that backup is always doing ` dowriteunbuffered ` since it ' s using ` fileuserdataprovider ` , which does not have ` fileopenreadwriteclose ` capability so it always runs <url> and unfortunately unbuffered write always concatenates all ` buffers ` first <url> < img width = "" <number> "" alt = "" image "" src = "" <url>",0
microsoft/vscode,"since latest update , each time debugging , annoying popup type : <b> bug </b> i do not use github with vscode . since the latest update , each time i click the "" open browser devtools "" button while debugging a site , i get the following popup dialog and have to click "" cancel "" . "" the extension ' local tunnel port forwarding ' wants to sign in using github . "" i have searched for the extension and am not finding it . vs code version : code <number> . <number> <phone> d557a81c9d0b5f8a5a1e9274db5585 , <number> - <number> - 0 8 t <time> . 5 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i9 - 1 2 9 0 0 k ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 0 gb ( <number> . 2 0 gb free ) | | process argv || | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - intelephense - client | bme | <number> . <number> vscode - eslint | dba | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - browser - sync | jas | <number> . <number> classic - asp - html | jtj | <number> . <number> al - object - designer | mar | <number> . <number> better - folding | moh | <number> . <number> vscode - docker | ms - | <number> . <number> al | ms - | <number> . <number> vscode - edge - devtools | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> vscode - icons | vsc | <number> . <number> php - debug | xde | <number> . <number> php - pack | xde | <number> . <number> php - intellisense | zob | <number> . <number> </details> < - - generated by issue reporter - - > ! [ screenshot <number> - <number> - <number> <number> ] ( <url>",0
microsoft/vscode,"error messages from linters overlapping with chatprompt message * i have linters to ensure files have the right header , * when i created the empty ts file , i get the linter error as well as chat prompt * however when i add an empty line , you can see the caht prompt does not appear and linter error is in the second line [ screenshot <number> - <number> - <number> at <number> <number> <number> ] ( <url> ! [ screenshot <number> - <number> - <number> at <number> <number> <number> ] ( <url>",0
microsoft/vscode,"debug stop button does not work after restarting session - start debugging python ( not js because restart is handled inside the adapter ) - click restart - click stop - nothing happens this is from <url> similarly to <url> we reuse the debugsession when restarting , then these get disposed and never readded . maybe i can not really fix this until <url> is properly fixed . cc <user>",0
microsoft/vscode,"log files created should be created under the log home folder of the process by default a logger file created in the window ( renderer ) process should create the log file scoped to that window . for example , following should create the view log file under the current window ` ` ` ts loggerservice . createlogger ( ' view ' , { name : ' view ' } ) ` ` ` if a component wants to create the logger shared across windows , one should provide the log file while creating the logger as follows ` ` ` ts loggerservice . createlogger ( joinpath ( environmentservice . logshome , ` ${ log_id } . log ` ) , { id : log_id , name } ) ` ` `",0
microsoft/vscode,task status bar says ` building . <repeated> ` even after task statuses have been applied only happens for tasks that have been reconnected,0
microsoft/vscode,"` rpcprotocol . _remotecall ` leaks a listener this handler for token cancellation is never disposed found it with the help of the new commands from <url> to track disposables in the app , opening and closing editors .",0
microsoft/vscode,"copy and pasting issues type : <b> bug </b> when i am copying and pasting text from onenote into vs code editor it comes up like this ` [ alt text ] ( image . png ) ` . how do i correct this vs code version : code <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 4 3 8 z ) os version : windows_nt x64 <number> . <number> modes ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - better - comments | aar | <number> . <number> aws - toolkit - vscode | ama | <number> . <number> linkcheckmd | bla | <number> . <number> excel - to - markdown - table | csh | <number> . <number> vscode - markdownlint | dav | <number> . <number> docs - article - templates | doc | <number> . <number> docs - authoring - pack | doc | <number> . <number> docs - build | doc | <number> . <number> docs - images | doc | <number> . <number> docs - linting | doc | <date> docs - markdown | doc | <number> . <number> docs - metadata | doc | <number> . <number> docs - preview | doc | <number> . <number> docs - scaffolding | doc | <number> . <number> docs - yaml | doc | <number> . <number> gitlens | eam | <number> . <number> pythonsnippets | frh | <number> . <number> codespaces | git | <number> . <number> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> go | gol | <number> . <number> terraform | has | <number> . <number> vsc - python - indent | kev | <number> . <number> vscode - docker | ms - | <number> . <number> vscode - kubernetes - tools | ms - | <date> data - workspace - vscode | ms - | <number> . <number> mssql | ms - | <number> . <number> sql - bindings - vscode | ms - | <number> . <number> sql - database - projects - vscode | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> remote - server | ms - | <number> . <number> vscode - markdown - notebook | ms - | <date> vsliveshare | ms - | <number> . <number> vsliveshare - pack | ms - | <number> . <number> vscode - sanddance | msr | <number> . <number> autodocstring | njp | <number> . <number> vscode - yaml | red | <number> . <number> vscode - taskexplorer | spm | <number> . <number> code - spell - checker | str | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscodeintellicode - completions | vis | <date> vscode - icons | vsc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> <number> <time> <number> showlangstatbar : <number> a2ce337 <time> <number> 5 7 b7757 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < ! - - generated by issue reporter - - >",0
microsoft/vscode,"ui overlap with ` window . nativetabs : false ` , ` window . commandcenter : true ` , and tab per project type : <b> bug </b> <number> . have ` window . nativetabs : false ` and ` window . commandcenter : true ` . <number> . open a project . <number> . open a new tab ( i . e . , on macos , command + shift + n ) . <number> . open a new project in that tab ( i . e . , on macos , control + r then select one ) . <number> . notice overlap of command center atop tabs . | screenshot ( dark mode ) | | - | | [ vs code command center and tab per project ] ( <url> | | screenshot ( light mode ) | | - | | ! [ screenshot <number> - <number> - <number> at <number> <number> <number> pm ] ( <url> | <url> vs code version : code <number> . <number> ( universal ) ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 3 6 4 z ) os version : darwin arm64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | apple m2 ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 0 0 gb ( <number> . 1 2 gb free ) | | process argv | - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace - - crash - reporter - id 3 4 0 9 8 c29 - <number> - 4 ba7 - b025 - 6 e3e06616ace | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - yaml2json | ahe | <number> . <number> vscode - css - modules | and | <number> . <number> icons - carbon | ant | <number> . <number> chalice - icon - theme | art | <date> django - html | bib | <number> . <number> markdown - checkbox | bie | <number> . <number> markdown - footnotes | bie | <number> . <number> postcss | css | <number> . <number> vscode - markdownlint | dav | <number> . <number> mustache | daw | <number> . <number> vscode - eslint | dba | <number> . <number> gitlens | eam | <number> . <number> vscode - html - css | ecm | <number> . <number> editorconfig | edi | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - env | iro | <number> . <number> vscode - edit - csv | jan | <number> . <number> minifyall | jos | <number> . <number> python - sphinx - highlight | leo | <number> . <number> twig - language - <number> | mbl | <number> . <number> rainbow - csv | mec | <number> . <number> theme - monokai - pro - vscode | mon | <number> . <number> language - gettext | mro | <number> . <number> python | ms - | <number> . <number> color - highlight | nau | <number> . <number> autodocstring | njp | <number> . <number> vscode - yaml | red | <number> . <number> synthwave - vscode | rob | <date> jinjahtml | sam | <number> . <number> markdown - to - confluence - vscode | t - n | <number> . <number> solarized - high - contrast - light | tin | <number> . <number> vscode - css - custom - properties | toc | <number> . <number> simple - rst | tro | <number> . <number> vscode - mdx | uni | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vscaat : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> ecj1e33 <time> <number> pythonfmttext : <number> f8hc823 <time> <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",0
microsoft/vscode,"update to <number> . <number> , vscode takes more time to kill processes after quit type : <b> bug </b> in this video , i quit vscode at <number> seconds , but it ( dock icon ) was killed at <number> seconds . <url> vs code version : code <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 3 6 4 z ) os version : darwin x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 1 0 3 8 ng7 cpu @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 0 0 gb ( <number> . 4 3 gb free ) | | process argv | - - crash - reporter - id 2 5 3 3 7 f27 - 0 fd6 - 4 ad2 - a343 - d23e45e01d53 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - astro - vscode | ast | <number> . <number> vscode - language - pack - zh - hans | ms - | <number> . <phone> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,"terminal link underline flickers and is re - evaluated when the buffer changes ( but not on the current line ) repro run vscode tests <number> . scroll up <number> . hover a link , 🐛 it will flicker and be difficult to ctrl / cmd + click not sure yet if this is upstream or vscode .",0
microsoft/vscode,terminal dom renderer : selection background is drawn incorrectly when unfocused upstream,0
microsoft/vscode,"cannot connect to the server after update the updated version is : version : <number> . <number> ( universal ) os : darwin arm64 <number> . <number> the issue is "" cannot reconnect . please reload the window . "" but as i reload the window , it shows "" waiting for port forwarding to be ready "" = = = = = = = = = = = = = = = = = update : i uninstalled the extension "" wsl "" and it solves the problem . = = = = = = = = = = = = = = = = = update # <number> , changing the ssh extension to the pre - release version can also help",0
microsoft/vscode,toggle collapse unchanged regions button is showing on every editor even ones without changes . should not it only show it diffs ?,0
microsoft/vscode,"error traces show unrendered html ` href ` that makes them hard to read < - - please search existing issues to avoid creating duplicates . - - > # # environment data - vs code version : <number> . <number> - jupyter extension version ( available under the extensions sidebar ) : v1 . <number> - python extension version ( available under the extensions sidebar ) : v2023 . <number> - os ( windows | mac | linux distro ) and version : linux ( debian <number> ) - python and / or anaconda version : conda <number> . <number> ( python <number> ) - type of virtual environment used ( n / a | venv | virtualenv | conda | . <repeated> <sad> conda - jupyter server running : local # # steps to reproduce : <number> . open an interactive jupyter window . <number> . execute some python code that would trigger an exception in a library - example : ` ` ` python from sklearn . linear_model import logisticregression logisticregression ( ) . fit ( <number> ) ` ` ` # # expected behaviour should show me a readable error trace like it does in regular terminal - based python and regular jupyter . # # actual behaviour shows me an error trace in which each line that references a line of code from an external library shows up with what looks like unrendered html tags and does not let me change the "" presentation format "" anymore for the cell outputs like it used to do in the past .",0
microsoft/vscode,"[ bug ] to expand diff editor appears to be shorter than full height does not seem to happen with other editors < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",0
microsoft/vscode,"regression : port forwarding fails while working with remote ssh this is the 4 th time this issue has occurred and fixed , then regressed within the last <number> months . <repeated> such frequent major bugs make vscode really unstable to use <annoyed> does this issue occur when all extensions are disabled ? : yes ` ` ` version : <number> . <number> ( user setup ) commit : 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 date : <number> - <number> - 0 6 t <time> . 4 3 8 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os : windows_nt x64 <number> . <number> ` ` ` steps to reproduce : ( see <url> ) <number> . connect to a remote ssh host via remote explorer <number> . start a server on <number> . <number> . <time> <number> <number> . port forward it <number> . try to access it on the current pc . error logs in ` shared ` window : ( could be irrelevant since i get the log below on both <number> . <number> and <number> . <number> but no issue is faced on <number> . <number> ) ` ` ` <number> - <number> - <number> <time> . <number> [ error ] [ uncaught exception in sharedprocess ] : cannot read properties of undefined ( reading ' then ' <sad> typeerror : cannot read properties of undefined ( reading ' then ' ) at v . s ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at v . q ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at c . value ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . w ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . x ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . fire ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . te ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . emit ( node : events : <number> <time> ) at messageportmain . _internalport . emit ( node : electron / js2c / utility_init : <time> <number> ) <number> - <number> - <number> <time> . <number> [ error ] [ uncaught exception in sharedprocess ] : cannot read properties of undefined ( reading ' then ' <sad> typeerror : cannot read properties of undefined ( reading ' then ' ) at v . s ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at v . q ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at c . value ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . w ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . x ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . fire ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . te ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . emit ( node : events : <number> <time> ) at messageportmain . _internalport . emit ( node : electron / js2c / utility_init : <time> <number> ) <number> - <number> - <number> <time> . <number> [ error ] [ uncaught exception in sharedprocess ] : cannot read properties of undefined ( reading ' then ' <sad> typeerror : cannot read properties of undefined ( reading ' then ' ) at v . s ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at v . q ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at c . value ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . w ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . x ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at b . fire ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . te ( c :\\ users \ \ ashesh \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ code \ \ node \ \ sharedprocess \ \ sharedprocessmain . js : <time> <number> ) at messageportmain . emit ( node : events : <number> <time> ) at messageportmain . _internalport . emit ( node : electron / js2c / utility_init : <time> <number> ) ` ` ` launch . json : ` ` ` { "" version "" : "" <number> . <number> "" , "" configurations "" : [ { "" name "" : "" next . js : debug server - side "" , "" type "" : "" node - terminal "" , "" request "" : "" launch "" , "" command "" : "" npm run dev "" } , { "" name "" : "" next . js : debug client - side "" , "" type "" : "" chrome "" , "" request "" : "" launch "" , "" url "" : "" http :// localhost : <number> "" } , { "" name "" : "" next . js : debug full stack "" , "" type "" : "" node - terminal "" , "" request "" : "" launch "" , "" command "" : "" npm run dev "" , "" serverreadyaction "" : { "" pattern "" : "" started server on . + , url : ( https ? :// . + ) "" , "" uriformat "" : "" %s "" , "" action "" : "" debugwithchrome "" } , "" sourcemaps "" } ] } ` ` ` [ image ] ( <url>",0
microsoft/vscode,"regresion : terminal group relative size not preserved < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : stable and from sources - os version : steps to reproduce : <number> . open terminal panel <number> . split terminal and resize <number> . reload window <number> . : bug relative sizes are not preserved",0
microsoft/vscode,"updater wastes large amount of disk space logging once per second while waiting for vs code to close type : <b> bug </b> the vs code update process appears to write to log files named ` % temp %/ setup log [ date ] #[ number ] . txt ` . it appends a new line once every second as it waits for a running instance of vs code to be closed . here ' s a snippet from a current file on my machine : ` ` ` <number> - <number> - <number> <time> . <number> application is still running , waiting <number> - <number> - <number> <time> . <number> application is still running , waiting <number> - <number> - <number> <time> . <number> application is still running , waiting <number> - <number> - <number> <time> . <number> application is still running , waiting <number> - <number> - <number> <time> . <number> application is still running , waiting ` ` ` i basically always have vs code running , and sometimes do not close it for weeks at a time , and just found that i had over 2 0 0 mb of log files mostly filled with lines like the above . vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",0
microsoft/vscode,consider changing dim usage in test terminal this : <url> now results in <number> / <number> the minimum contrast requirements which is a soft accessibility issue . it seems like these messages are special vscode messages which use special formatting in the real terminal,0
microsoft/vscode,test view output needs a space separator between messages these <number> do not have a space or new line between them [ image ] ( <url>,0
microsoft/vscode,test results view misses separator between editor and list there ' s no separator line between the two components . [ image ] ( <url>,0
microsoft/vscode,"build task status indicator not reappearing on window reload steps to reproduce i use the default build task in ` vscode ` on a multi root workspace <number> . i see a "" <number> "" appearing in the status bar , indicating the <number> runs ( see pic [ <number> ] ) <number> . i reload the window <number> . everything is fine <number> . i open <number> more terminals ( see pic [ <number> ] ) <number> . i hide the panel <number> . i reload window => 🐛 there is no more a "" <number> "" in my status bar [ <number> ] [ image ] ( <url> ! [ image ] ( <url> [ <number> ] ! [ image ] ( <url>",0
microsoft/vscode,themes warn on unknown theme key activitybaritem . profilesbackground [ image ] ( <url>,0
microsoft/vscode,"processing artifacts fails with error about blob already existing <url> ` ` ` creating asset . <repeated> size : <number> sha1 : fbe6dc51e3e5a0de9bf6b6b43042a855100d2b05 sha256 : d1403e8a597b5787ba9524069de397175d81f76401dbfc0caf42b73c46184132 / mnt / vss / _work / <number> / s / build / azure - pipelines / common / createasset . js : <number> uploadpromises . push ( promise . reject ( new error ( ` blob ${ quality } , ${ blobname } already exists , not publishing again . ` ))); ^ error insider , b254a4c05a4b7625970e7fee3cbcb3dbf2862bba / vscode_cli_linux_x64_cli . tar . gz already exists , not publishing again . at main ( / mnt / vss / _work / <number> / s / build / azure - pipelines / common / createasset . js : <number> <time> ) at process . processticksandrejections ( node : internal / process / task_queues : <number> : <number> ` ` `",0
microsoft/vscode,"null error in sticky scroll test when running the sticky scroll unit tests , the following error was noticed : ` ` ` "" uncaught ( in promise ) typeerror : cannot read properties of null ( reading ' coordinatesconverter ' ) "" , source ( <number> ) ` ` `",0
microsoft/vscode,pasting is not selecting the newly pasted text this happens after a regular ctrl + v : [ image ] ( <url> it ' s very easy to reproduce . 버전 : <number> . <number> - insider ( user setup ) 커밋 : f1302be1e67e3af5fbeb8bbb2ea784de7bc96150 날짜 : <number> - <number> - 0 1 t <time> . 4 1 4 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os x64 <number> . <number>,0
microsoft/vscode,"terminal cursor does not reflect that the terminal is focused < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes # description terminal has focus , but the cursor displayed does not look like it does - - it ' s not filled white # # actual <url> # # expected with stable <number> . <number> : <url> # steps to reproduce : i do not have any , but i can repro even with a new vscode window without any workspace folders . # version ` ` ` version : <number> . <number> - insider commit : f1302be1e67e3af5fbeb8bbb2ea784de7bc96150 date : <number> - <number> - 0 1 t <time> . 5 2 3 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os arm64 <number> . <number> ` ` ` can not seem to repro with <number> . <number> stable . within my vscode integrated terminal ` ` ` $ zsh - - version zsh <number> (x 8 6 _64 - apple - darwin22 . <number> ) ` ` ` it ' s weirdly x86_64 even though i am on arm64 , but it seems ok - <url>",0
microsoft/vscode,"` vscode . treeview . dispose ` does not clean up in the renderer i went after some later callers of ` disposable <hashtag> register </hashtag> ` ( see [ dev - discuss ] ( <url> and i noticed that ` $ registertreeviewdataprovider ` uses the disposable of its container object , not one specific to the tree view ( id ) . it ' s likely a theoretical bug but it seems that extensions calling dispose of their view do not achieve anything and that repeated create calls for the same id could leak some memory",0
microsoft/vscode,"is moved code range correct ? <user> not sure if it a bug report or question , but i notcied that moved lines range is not correct why it is written that lines are moved from <number> - <number> ? should not they be marked as "" moved from <number> - <number> "" ? the same question is about "" moved to "" .",0
microsoft/vscode,"pre launch task does not run if similar task is already running in different workspace folder < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : * yes <emphasis> * < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - insider - os version : macos ventura <number> issue description : if the prelaunchtask for a debug configuration in workspace a is already running in workspace b , the task is not started in workspace a . vs code thinks the prelaunchtask is already running , even though it ' s running in a different workspace folder . the expected behavior is to launch the task in the workspace folder containing the debug configuration . seems similar to <url> i have created a repository which reproduces the bug . the repo contains two node express hello world apps , and corresponding launch / tasks . json configs for debugging the apps . the debug config is a simple launch program and each has a ` prelaunchtask ` that runs ` tsc - - watch ` as a background task with a problem matcher . steps to reproduce : <number> . clone <url> <number> . open the ` workspace . code - workspace ` workspace file in vs code . < img width = "" <number> "" alt = "" image "" src = "" <url> <number> . run ` npm install ` in each workspace folder . each workspace folder contains a hello world express server . <number> . run the "" npm task in workspace - <number> . < img width = "" <number> "" alt = "" image "" src = "" <url> <number> . launch "" launch program ( workspace - <number> ) "" from the debug menu . it will not run the prelaunchtask and it will not work . < img width = "" <number> "" alt = "" image "" src = "" <url>",0
microsoft/vscode,quick chat layout issue on resize on the latest insiders on windows <number> . open quick chat . <number> . close it . <number> . zoom in or out of the editor . <number> . open quick chat again . <number> . : bug issue,0
microsoft/vscode,"improve consistency , lessen redundancy of accessibility help dialog text as a result of the discussion in # <number> , we should format each command , description , and keybinding more consistently and limit redundancy .",0
microsoft/vscode,"invoking choice from ` show codelens commands for current line ` may require multiple enters or clicks type : <b> bug </b> <number> . set ` "" typescript . referencescodelens . enabled "" : true ` <number> . open a . ts file . <number> . put cursor on a line that has an ` n references ` codelens above it . <number> . open command palette and run ` show codelens commands for current line ` . <number> . press enter to try and run the selected ( only ) entry ( n references ) . : bug : nothing happens until you press enter a second time ( or more . <repeated> ) . results of using a mouse - click to try and run the codelens from the quickpick is the same . first click does nothing , and a second click is required . this also happens in <number> . <number> so it is not a <number> regression . vs code version : code - insiders <number> . <number> - insider ( e7756c8870ee1df7360e6624e220534174039b02 , <number> - <number> - 3 1 t <time> . 6 9 8 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",0
microsoft/vscode,rendersidebysideinlinebreakpoint invalid default value [ image ] ( <url>,0
microsoft/vscode,"after the command is localized , it cannot be searched by keywords . add issue description here version : <number> . <number> commit : 6 c3e3dba23e8fadc360aed75ce363ba185c49794 user agent : mozilla / <number> ( macintosh ; intel mac os x 1 0 _15_7 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> embedder : vscode . dev < - - generated by web issue reporter - - > ! [ bug3 ] ( <url> steps open vscode . dev or vs code app <number> . switch to a language other than the default , for example , simplified chinese . <number> . search command by shortcut key , for example , search ` toggle ` , some matching commands will appear . <number> . but i search the keyword directly ( incomplete command keyword ) , nothing will be matched .",0
microsoft/vscode,"[ serve - web ] in - app github sign - in link rotates ports and cannot be preset with - - port , breaking ssh local forwarding summary : the sign - in link for github in vscode running through serve - web rotates ports in the 5 4 xxx range , breaking any pre - set port forwarding for users connecting via ssh . this occurs regardless of any port option set with - - port ( taken from my comment in # <number> ) does this issue occur when all extensions are disabled ? : yes - vs code version : insiders <number> . <number> - os version : mac os <number> steps to reproduce : <number> . connect to the device hosting code via ssh , with local port forwarding for port <number> enabled <number> . on the remote device , launch serve - web with ` ` ` code - insiders serve - web ` ` ` <number> . open a browser window and navigate to the url given by serve - web <number> . turn on settings sync , showing a github sign - in url . note the port is different than the default , requiring you to reconnect and add a new forwarded port expected behavior the port with - - port should ( a ) also determine the port used for sign - in , or ( b ) be a separate option so that ssh port forwarding does not require updating ssh config and reconnecting each time you need to sign in",0
microsoft/vscode,"terminal buffer is not updating information type : <b> bug </b> in the insiders version , i noticed that the buffer is no longer automatically reflecting terminal information . for example , when executing the mvn clean install command in my project , the information of the build that starts to run is updated from time to time in the buffer , sometimes it is only necessary to give a tab to go to the terminal and give a shift tab that i have the information updated while the build is running and printing various information . in the insiders version , this update no longer occurs . i have to keep pressing alt + f2 , but i could not be sure if i still have all the updated information . vs code version : code - insiders <number> . <number> - insider ( 3 5 be9bf683eace09796e59d54f1f225bbc3a7866 , <number> - <number> - 3 0 t <time> . 9 7 2 z ) os version : windows_nt x64 <number> . <number> modes : remote os version : linux x64 <number> . <number> - microsoft - standard - wsl2 <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 8 3 6 5 u cpu @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 7 2 gb ( <number> . 4 4 gb free ) | | process argv | - - crash - reporter - id dd13835f - 4 8 8 a - <number> - bdf9 - 2 1 fedc4249ef | | screen reader | yes | | vm | <percent> | | item | value | | - - - | - - - | | remote | wsl | os | linux x64 <number> . <number> - microsoft - standard - wsl2 | | cpus | intel ( r ) core ( tm ) i5 - 8 3 6 5 u cpu @ <number> . 6 0 ghz ( <number> x <number> )| | memory ( system ) | <number> . 6 2 gb ( <number> . 2 2 gb free ) | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - remote - containers | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cucumber - official | cuc | <number> . <number> gitlens | eam | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> vscode - docker | ms - | <number> . <number> vscode - language - pack - pt - br | ms - | <number> . <phone> postman - for - vscode | pos | <number> . <number> vscode - thunder - client | ran | <number> . <number> java | red | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - boot - dev - pack | vmw | <number> . <number> vscode - spring - boot | vmw | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> vscode - spring - boot - dashboard | vsc | <number> . <number> vscode - spring - initializr | vsc | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod8 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttypecf : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,"port forwarding log has formatting issues testing # <number> on linux there are ansi escape sequences as well as empty lines : ` ` ` <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ] starting cli <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ] [ <number> - <number> - <number> <time> ] debug no code server tunnel found , creating new one <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m [ <number> - <number> - <number> <time> ] info creating tunnel with the name : ubuntu - linux - <number> - <number> - desktop <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m [ <number> - <number> - <number> <time> ] debug starting tunnel to server . <repeated> <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m [ <number> - <number> - <number> <time> ] debug connected to tunnel endpoint : tunnelrelaytunnelendpoint { base : tunnelendpoint { connection_mode : tunnelrelay , host_id : "" ad01adb3 - 1 b36 - 4 c76 - 8 0 3 c - eb1b09e822ff "" , host_public_keys : [ ] , port_uri_format : some ( "" <url> tunnel_uri : some ( "" <url> port_ssh_command_format : some ( "" ssh 0 1 bkhf70 - { port } <user> . eun1 . devtunnels . ms "" ) , tunnel_ssh_command : some ( "" ssh 0 1 bkhf70 <user> . eun1 . devtunnels . ms "" ) , ssh_gateway_public_key : none } , host_relay_uri : some ( "" wss :// eun1 - data . rel . tunnels . api . visualstudio . com / api / v1 / host / connect / 0 1 bkhf70 "" ) , client_relay_uri } <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ error ] [ forwarding ] <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m [ <number> - <number> - <number> <time> ] info forwarding port <number> at private <number> - <number> - <number> <time> . <number> [ error ] [ forwarding ] <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 2 m [ <number> - <number> - <number> <time> ] trace found token in keyring <number> - <number> - <number> <time> . <number> [ info ] [ forwarding ]  [ 0 m [ <number> - <number> - <number> <time> ] info forwarding port <number> at private <number> - <number> - <number> <time> . <number> [ error ] [ forwarding ] ` ` ` [ image ] ( <url>",0
microsoft/vscode,"quick text search results when file is open testing # <number> - open the vscode repo - open command palette , type ` % findtheme ` - pick the first entry in ` workbenchthemeservice ` . the file opens - again , open command palette , type ` % findtheme ` you see duplicated results [ image ] ( <url>",0
microsoft/vscode,"progressive rendering freezes sometimes [ image ] ( <url> i thought i noticed this happening from time to time after we made changes to how rendering works , but in this case with my test extension it ' s pretty obvious , i will investigate",0
microsoft/vscode,align quick question code block style with chat view testing # <number> not sure why we would the backgrounds different ! [ image ] ( <url>,0
microsoft/vscode,"bad input box styling in quick chat ui [ image ] ( <url> i am not sure how i got to this point , but resizing the window just makes it worse",0
microsoft/vscode,"icons in accessible view do not seem right testing # <number> [ image ] ( <url> the bell icon for manage extension does not seem correct here , gear icon might be better . the shown next and show previous should swap places . ` > ` and ` < ` should probably be ` < ` and ` > ` i would expect ` x ` to be at the end .",0
microsoft/vscode,"confusing accessibility help message in an editor <number> . open settings . json <number> . run open accessibility help <number> . see this message <number> . i do not really understand what "" disable the aria hint label to open this "" means [ image ] ( <url>",0
microsoft/vscode,"i cannot click on the left hand side to expand : i cannot click on the left hand side to expand also , is the hover color themable ? _originally posted by <user> in <url>",0
microsoft/vscode,dim unfocused setting link does not work forked off from <url> the link in settings does not work : [ image ] ( <url> i am pretty sure i am following this reference exactly though : <url> config is defined here,0
microsoft/vscode,"consider to revert dynamic grow from scroll when reopening testing # <number> i think that growing quick chat when you scroll is an interesting idea , but i would reset the size back to how it was when you reopen quick chat because its harder to manage the size to make it small again . you cannot just scroll to get back where you were .",0
microsoft/vscode,"port forwarding attempts wrong path to tunnel binary on linux testing # <number> <number> ) use latest snap installation of insiders <number> ) attempt to forward a port and after sign in flow , see the notification with following error < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> <number> ) path should have been ` / snap / code - insiders / <number> / usr / share / code - insiders / bin / code - tunnel - insiders `",0
microsoft/vscode,"moved code widget does not get highlighted when is moved over cursor testing # <number> steps to reproduce create two copies of the same lines . one of this copy will be connected with arrow <number> . select the connected copy and past some symbols , so that arrow jumps to another copy <number> . remove those symbols , so the arrow jumps again over the block where cursor is located <number> . . 🐛 now the moved lines frame is located so that the cursor is inside of it but it is not highlighted untill the cursor is moved <url> [ example ] ( <url>",0
microsoft/vscode,welcome editor does not dim testing # <number> [ image ] ( <url>,0
microsoft/vscode,settings editor does not fully dim testing # <number> [ image ] ( <url>,0
microsoft/vscode,"clicking "" compare "" in another moved section does not compare testing # <number> <number> . move <number> blocks of code <number> . click "" compare "" on one of them . <number> . scroll to the other block and click "" compare on that . <number> . i would expect the compare the move to the second block , but instead it just untoggles the compare [ recording <number> - <number> - <number> at <number> <number> <number> ] ( <url> [ monaco editor playground repro ] ( <url>",0
microsoft/vscode,missing jsdoc for ` globalenvironmentvariablecollection ` * the ` globalenvironmentvariablecollection ` type lacks a leading jsdoc comments * the ` getscoped ` function lacks a ` <user> ` annotation,0
microsoft/vscode,"clicking on search result sets the selection in the cell and not in the output testing # <number> * open <url> * enable ` "" search . experimental . closednotebookrichcontentresults "" : true ` * search for ` [ <number> , <number> , <number> , <number> , <number> ] ` * click on a result in ` <number> - sorting . ipynb ` * observe the selection is set on the input code and not on the output <url> ` ` ` version : <number> . <number> - insider commit : ebd67244fb2da33ab078bb2baa96106fda29f336 date : <number> - <number> - 2 9 t <time> . 2 1 8 z electron : <number> . <number> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os arm64 <number> . <number> ` ` `",0
microsoft/vscode,"notification and notification buttons lack border radius <number> . if the notification content is sufficiently long , insufficient height is allocated for the action buttons to grow , consequently they appear cramped and lack border radius <number> . when a notification is focused the focus border radius is truncated [ image ] ( <url>",0
microsoft/vscode,` ctrl + uparrow ` does not open accessible buffer on windows <number> . enable screen reader mode <number> . focus a terminal <number> . ` ctrl + uparrow ` <number> . 🐛 the accessible buffer is not entered this is because it ' s getting sent to the shell . adding it to commands to skip shell fixes this . reported by <user>,0
microsoft/vscode,"unable to ssh to remote host type : <b> bug </b> note : i was unable to create a bug report after i choose the "" an extension "" option in the dropdown in the "" help : report issue . <repeated> "" window as it does not show any of my extensions ( i do have extensions for whatever reason the bug report does not show them ) after installing <number> . <number> ( insiders ) , i started to get the following error message : notificationsalerts . ts : <number> failed to connect to the remote extension host server ( error : wrappederror ( wrappederror { message : "" server exited without writing socket "" , original : "" channel closed "" } ) ) and could not fetch remote environment when i use an older version ( via regular vscode ) this issue does not happen . vs code version : code - insiders <number> . <number> - insider ( f7973f357e7c316a88e8817886f41a843021fe74 , <number> - <number> - 2 8 t <time> . 1 3 1 z ) os version : windows_nt x64 <number> . <number> modes : connection to ' ssh : devlab . linepulse . ca ' could not be established wrappederror ( wrappederror { message : "" server exited without writing socket "" , original : "" channel closed "" } ) <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 8 6 6 5 u cpu @ <number> . 9 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 7 2 gb ( <number> . 9 9 gb free ) | | process argv | - - crash - reporter - id 8 5 7 fed38 - 4 fa8 - 4 9 ff - b2f3 - bbf104e17732 | | screen reader | no | | vm | <percent> | connection to ' ssh : devlab . linepulse . ca ' could not be established wrappederror ( wrappederror { message : "" server exited without writing socket "" , original : "" channel closed "" } ) </details> extensions <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod8 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> vscrp : <number> 2 i9eh26 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonmpsinfo : <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,"avoid unnecessary virtual scroll bar area refreshing during typing - vs code version : <number> . <number> - os version : windows <number> 2 2 h2 ( <number> ) steps to reproduce : <number> . open a text file . turn on developer tools \ \ rendering \ \ paint flashing <number> . type a few characters without changing line . observe the screen refresh areas . with each character typing , the virtual scroll bar area is updated . expected : because there is no line change , i ' d expect the scroll bar area should not update . impact think this behavior leads to [ issue <number> ] ( <url> which causes underlying chromium to combine and refresh the entire whole editor screen . this kind of behavior is not optimized for remote desktop experience such as dev box .",0
microsoft/vscode,"toggling command center does not resize the title bar toggling the command center works , but the title bar height is stuck until the window is resized . i also noticed some issues where i had to click twice to open the command center once enabled . the command center input then disappears while the command center quick pick opens . <url>",0
microsoft/vscode,"[ accessibility ] accept suggestion button disappeared in the latest accessible view type : <b> bug </b> press alt + f2 on any inline suggestion and press shift + tab key where you can find related action buttons , such as go to symbols , go to next / previous suggestion . there is no "" accept suggestion "" button anymore . vs code version : code - insiders <number> . <number> - insider ( 0 8 3 fca132543aa91a7e1de2dc23857d70ea56dd3 , <number> - <number> - 2 5 t <time> . 8 7 8 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 6 6 gb free ) | | process argv |c :\\\\ users \ \ \ \ jseo1005 \ \ \ \ onedrive - university of illinois - urbana \ \ \ \ desktop \ \ \ \ email . md - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <date> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonmpsinfo : <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",0
microsoft/vscode,"macos : window menu does not show opened windows in some cases steps to reproduce have <number> windows opened , e . g . one on ` vscode ` and an empty window <number> . quit <number> . start <number> . verify <number> windows open => 🐛 the "" window "" menu only shows <number> window for me [ image ] ( <url>",0
microsoft/vscode,"[ accessibility ] cannot clear terminal buffer using shortcut key type : <b> bug </b> if you assign a keybinding for clear terminal command and press the key in the accessible terminal buffer , it does not work . ` keybindings ` : ` ` ` json [ { "" key "" : "" ctrl + l "" , "" command "" : "" workbench . action . terminal . clear "" , "" when "" : "" terminalfocus & & terminalhasbeencreated || terminalaccessiblebufferfocus & & terminalhasbeencreated "" } ] ` ` ` * note : ` ctrl + l ` only works in terminal input , not terminal buffer . vs code version : code - insiders <number> . <number> - insider ( aad333b878b4cfce2f4152d48552fb6f980d7daf , <number> - <number> - 2 8 t <time> . 5 4 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 0 2 gb free ) | | process argv | - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> aiprm - lang | aip | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> overleaf - workshop | iam | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> vscode - bard | jim | <date> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttype : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> pythontestfixt : <number> pythonfb28095 <time> <number> pythonregdiag : <number> pythoncet <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",1
microsoft/vscode,"have the "" ask chat "" button in the command palette default to the chatview originally this option , which can be seen if you have a chat provider installed like copilot chat would open the query in quick chat . however , i got feedback that quick chat may be too hard for newer users of vs code and that instead it should open in chat view . <repeated> but there should be a setting to change this behavior to quick chat . we should do this .",1
microsoft/vscode,"[ accessibility ] ctrl + k , ctrl + shift + n does not open actionable notification items type : <b> bug </b> the recently added keybinding ctrl + k , ctrl + shift + n does not seem to do what screen reader users want . if there are some notification items that require user interactions , pressing this key does not move user focus to the actionable notification item list . linking this keybinding to ` "" notifications . showlist "" ` command seems more reasonable for the accessibility purpose . vs code version : code - insiders <number> . <number> - insider ( 1 1 bfd76a61a299156a9f3138ecfad70937af3527 , <number> - <number> - 2 1 t <time> . 2 2 7 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 4 5 g7 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 1 3 gb free ) | | process argv | - - crash - reporter - id b05b88e5 - <number> - <number> - ae34 - fa034ebddea9 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - android - dev - ext | ade | <number> . <number> aiprm - lang | aip | <number> . <number> bookmarks | ale | <number> . <number> openscad | ant | <number> . <number> spellright | ban | <date> zoterolatex | bna | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> doxdocgen | csc | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> vscode - quick - select | dba | <number> . <number> vscode - deno | den | <number> . <number> gitlens | eam | <number> . <number> editorconfig | edi | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - google - translate | fun | <date> codespaces | git | <number> . <number> copilot | git | <number> . <number> copilot - chat | git | <number> . <phone> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> overleaf - workshop | iam | <number> . <number> cslpreview | igo | <number> . <number> easy - snippet | inu | <number> . <number> path - autocomplete | ion | <number> . <number> latex - workshop | jam | <number> . <number> lilypond - syntax | jea | <number> . <number> scheme | jea | <number> . <number> better - cpp - syntax | jef | <number> . <number> google - search | kam | <number> . <number> vscode - lua - format | koi | <number> . <number> lilypond - formatter | lhl | <number> . <number> lilypond - pdf - preview | lhl | <number> . <number> lilypond - snippets | lhl | <number> . <number> vslilypond | lhl | <number> . <number> zotero | mbl | <date> git - graph | mhu | <number> . <number> vscode - docker | ms - | <number> . <number> black - formatter | ms - | <number> . <number> flake8 | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - github - issue - notebooks | ms - | <date> vscode - selfhost - test - provider | ms - | <date> vscode - serial - monitor | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> autodocstring | njp | <number> . <number> pandocciter | not | <number> . <number> shiny - python | pos | <number> . <number> shinyuieditor | pos | <number> . <number> quarto | qua | <number> . <number> r - debugger | rde | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> r | red | <number> . <number> multi - command | ryu | <number> . <number> vscode - deepl | soe | <number> . <number> abc - music | sof | <number> . <number> lua | sum | <number> . <number> latex - utilities | tec | <date> cmake | twx | <date> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - arduino | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> markdown - all - in - one | yzh | <number> . <number> grammarly | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod805cf : <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> ecj1e33 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> welcomedialog : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> pythonlinttype : <number> pythonmpsinfo : <number> dsvsc0 <time> <number> pythontestfixt : <number> pythonfb28095 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",1
microsoft/vscode,"debug : surface stackframe moduleid in the debug call stack ui go debug adapter ( delve ) has been using the fully qualified go function names as the stack frame names . these names include the full package path ( e . g . ` example . com / foo / bar ` ) and the function name ( e . g . ` pkg . acoolfuncname ` ) , and many users requested to shorten the names presented in the call stack view . for example , <url> <url> one way to handle this is to shorten the name by dropping the full package path . for example , our contributor is proposing to control this by adding a new launch config setting . <url> i think it makes sense to show the function name , but i wonder if we can still carry the package info ( "" module "" in other languages ) . ` stackframe ` has the ` moduleid ` field . the current vs code does not seem to utilize this field at all . how about utilizing the ` moduleid ` field and surfacing it in the ui ? i think the tooltip shown when hovering over the stackframe name is a good option . screenshot with [ a small change in callstackview . ts ] ( <url> and a delve - side change <number> - <number> - <number> at <number> <number> <number> pm ] ( <url> one concern is it will be surprising for other language debug adapters that use the ` moduleid ` field already for other purposes . cc <user> <user> <user>",1
microsoft/vscode,indentsize api finalization this api adds ` indentsize ` as a property to ` texteditoroptions ` <url>,1
microsoft/vscode,"windows is ignore terminal titles unless they look like paths from <user> in <url> > i have been trying to programmatically set the title of a bash terminal without success , and glad that i chanced upon this issue . > > here is the repro : > > system : windows <number> . > git bash installed . > vscode version : <number> . <number> > > vscode workspace settings : > > [ image ] ( <url> > > > * * . bashrc * * > ` ` ` shell > echo - en "" \ \ <number> ] <number> ; new terminal title \ \ a "" > ` ` ` > > the title of the bash terminal window remains stubbornly unchanged as ` bash myworkspacefolder ` . > > i am assuming this issue will fix my use case . > > thanks - - - this section is setting it to undefined we want to keep the path trimming behavior above , but only when it looks like a path .",1
microsoft/vscode,"allow the debug toolbar to show in the command center today the debug toolbar can be hidden , floating around , or be docked inside the viewlet . this is about adding it to the command center . implemented by <url> via ` "" debug . toolbarlocation "" <url>",1
microsoft/vscode,support icons for profiles support icons for profiles - <url> - <url>,1
microsoft/vscode,adopt accessible view in terminal this change should be seamless for screen reader users and we will want to test it thoroughly to ensure that .,1
microsoft/vscode,"theme names should have transated name and untranslated name like the command palette see in the list below : - dark + ( no localization ) - kimbie 어둡게 ( only dark translated ) - 다크 모던 ( all translated with loan words "" daku modeon "" ) [ image ] ( <url> ideally this would act like commands where we provide the original if they differ",1
microsoft/vscode,"moved code should scan previous / next lines # # description these moves could be one : [ image ] ( <url> and this move could be bigger # # playground example [ monaco editor playground repro <number> ] ( <url> [ monaco editor playground repro <number> ] ( <url> ( click on "" use latest dev "" to verify a future bug - fix )",1
microsoft/vscode,"[ accessibility ] consider removing "" terminal . integrated . tabfocusmode "" type : <b> feature request </b> now that tabindex is removed from the terminal buffer area , i was wondering if "" terminal . integrated . tabfocusmode "" would be still useful . if not , please consider removing this option to officially deprecate this . vs code version : code - insiders <number> . <number> - insider ( 3 cd6f481266dcbd2ca2fcff43b4465d747c78e2f , <number> - <number> - 3 1 t <time> . 9 1 6 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",1
microsoft/vscode,"add support for grouping root workspace folders < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i ' d like to nest workspace folders like this : ` ` ` json "" folders "" : [ { "" name "" : "" e2e "" , "" folders "" : [ { "" name "" : "" app1 "" , "" path "" : "" "" } , { "" name "" "" "" } ] } ] , ` ` ` functionally the folders can act the same as if the list were flat , but just in ui the nested structure is shown . my immediate need would also be solved by <url> but i have used this feature a bit in visual studio to group shared library projects where for other reasons the folder structure on disk needed to be different .",1
microsoft/vscode,[ improvement ] icons to expand and contract the hidden lines should be placed on top of each other in the diff view the icon to expand the hidden lines is to the right of the editor gutter . the icon to contract the hidden lines is to the left of the editor gutter . perhaps these icons could be positioned in the same spot so that the user can contract the hidden lines after the last expansion without moving the mouse .,1
microsoft/vscode,make comment editor expandable the editor in the comment widget does not get any larger when the content does not fit . this makes editing long comments painful,1
microsoft/vscode,make quickchat styles closer to quickpick this was done across multiple prs to eventually get to this experience <url> <url>,1
microsoft/vscode,"[ css ] text - wrap : balance is css ( unknownproperties ) < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : ventura <number> < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> steps to reproduce : <number> . write text - wrap <number> . observer error",1
microsoft/vscode,"add ` preserveinput ` for quick search there is currently a way to preserve the input for quick open and the command palette , but not for quick search . add a setting for this .",1
microsoft/vscode,"go to next change does not show in command palette in diff editors steps to reproduce open a git change in the scm view <number> . try using ` move to next change ` in the command to navigate to a diff in the file * bug <emphasis> * ` move to next change ` does not show in the command palette , only in the toolbar . i see the command in normal text files related # <number>",1
microsoft/vscode,"support binding to a unix domain socket instead of a tcp address in serve - web > it does not look like this new cli supports serving on a unix domain socket . could we please get that added in too , like in <url> we are going to need it in order to handle auth in a proxy . ( i am happy to open a new issue for that if you need ) _originally posted by <user> in <url>",1
microsoft/vscode,diff editor align just whitespace [ image ] ( <url> [ repro ] ( <url>,1
microsoft/vscode,"explore relatedinformation provider < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > this would replace [ semanticsimilarity ] ( <url> proposal : ` ` ` ts /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * copyright ( c ) microsoft corporation . all rights reserved . * licensed under the mit license . see license . txt in the project root for license information . * - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ declare module ' vscode ' { / / <url> export enum relatedinformationtype { symbolinformation = <number> , commandinformation = <number> , searchinformation = <number> , settinginformation = <number> } interface relatedinformationbaseresult { type : relatedinformationtype ; weight : number ; } / / todo : symbols and search export interface commandinformationresult extends relatedinformationbaseresult { type : relatedinformationtype . commandinformation ; command : string ; } export interface settinginformationresult extends relatedinformationbaseresult { type : relatedinformationtype . settinginformation ; setting : string ; } export type relatedinformationresult = commandinformationresult | settinginformationresult ; export interface relatedinformationprovider { providerelatedinformation ( query : string , token : cancellationtoken ) : providerresult < relatedinformationresult [ ]>; } export interface embeddingvectorprovider { provideembeddingvector ( strings : string [ ] , token : cancellationtoken ) : providerresult < number [ ][]>; } export namespace ai { export function getrelatedinformation ( query : string , types : relatedinformationtype [ ] , token : cancellationtoken ) : thenable < relatedinformationresult [ ]>; export function registerrelatedinformationprovider ( type : relatedinformationtype , provider : relatedinformationprovider ) : disposable ; export function registerembeddingvectorprovider ( model : string , provider : embeddingvectorprovider ) } } ` ` `",1
microsoft/vscode,"diff editor v2 : setting for number of untouched lines around unchanged regions there is now diffeditor . hideunchangedregions . contextlinecount . verification steps it to a value ( e . g . <number> ) , open a diff where there are many unchanged lines and verify that the distance between a changed line and the closest collapsed code block is at least as many lines as specified .",1
microsoft/vscode,"quick text search open files first related to <url> since quick picks do not support streaming results via callback , the ' quick text search ' seems a bit slow . if we show some synchronous results ( from open editors ) ahead of time using the ` fastandslowpicks ` , it might give the user something initial to look at .",1
microsoft/vscode,"[ themes ] follow ` window . autodetectcolorscheme ` when switching profiles < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : windows11 2 3 h2 <number> steps to reproduce switching to the default profile <number> . switching the windows to dark theme <number> . switching to the other profile , but the vs code remains the light theme",1
microsoft/vscode,"show extension dependents < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i just discovered that the [ . net install tool for extension authors ] ( <url> was installed without my interaction . i had no idea where this came from or why it was suddenly added . it wasn ' t until i tried to uninstall it that i saw a warning that the c # extension depends on it , i had no other way of knowing . please add a tab to the extension info page to show what extensions depend on a given extension . ~ ~ it would also be great to see what a given extension depends on . ~ ~ i never noticed the dependencies tab on extensions until i was looking into this issue and now it ' s very strange that there is not the "" reverse "" for installed dependencies .",1
microsoft/vscode,"diff giving more priority to blank lines < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > version : <number> . <number> ( user setup ) commit : 6 c3e3dba23e8fadc360aed75ce363ba185c49794 date : <number> - <number> - 0 9 t <time> . 1 7 5 z os : windows_nt x64 <number> . <number> steps to reproduce : <number> . open two new / blank editors <number> . compare both the editors ( open editors ` ctrl - k e ` > select both ` shift - up / down ` > right click > compare selected ) <number> . paste this on one side , and that on the other ( mind the empty lines ) <number> . now , on "" that "" side : try to make it match the left side by say : * removing one empty line from starting , or * putting ` # ` before ` async ` <details> <summary> this and that </summary> this : ` ` ` # async def handle ( request ) : # keyvals = request . match_info # return web . response ( text = keyvals ) async def fetch_name ( request ) : pass async def insert_name ( request ) : pass ` ` ` that : ` ` ` async def handle ( request ) : keyvals = request . match_info return web . response ( text = keyvals ) ` ` ` </details> wrong diff result : < img src = "" <url> width = 5 5 0 px / > after the step <number> given above - i . e . on increasing similarity ; it detects common part correctly src = "" <url> width = 5 5 0 px / > <url>",1
microsoft/vscode,"dim terminal text should have half the minimum contrast ratio see <url> for an example of why this is an issue repro : - set a high minimum contrast ratio in settings - run ` echo - e ' \\x 1 b [ 3 1 mnormal \\x 1 b [ <number> ; 2 mdim \ \x 1 b [ 0 m ' ` , 🐛 provided the minimum contrast ratio is not too high , the colors should differ actual : [ image ] ( <url> expected",1
microsoft/vscode,"side by side compare ( diffs ) should leverage editor groups instead of nested "" subgroups "" developers often work with two editor groups ( side by side ) . when switching between editing and comparing your changes to the existing code , the ux is not great . i prefer to use side - by - side diffs rather than inline . when opening a compare , the side - by - side compare editor is squeezed into whichever editor group is active , which means the "" before "" and "" after "" sides of the compare editor end up with <number> / <number> of the total available editor width . even on a 5 k monitor this is cramped width = "" <number> "" alt = "" image "" src = "" <url> so i end up merging all of the editor groups . when i close the compare and switch back to editing , i have lost my editor organization . would it be possible for split compares to just open two linked editors ? the left and right editors would have their own tabs in side - by - side editor groups . activating / closing one would activate / close the other , scrolling would still be linked between the two , etc . this could have other advantages , for example breadcrumbs and stickiness today is only supported for the right half of the compare editor ( even though these features are left - aligned ) . if two adjoining groups do not exist , a new group could be added with some kind of "" transient "" flag , such that it goes away when it becomes empty ( if the user explicitly moves something into that group , it maybe clears that flag ? )",1
microsoft/vscode,"rendersidebyside : never | auto | always instead of rendersidebyside : boolean , we could have rendersidebyside : never | auto | always . as a downside , when a user switches from the inline view to the side by side view , they probably switch to "" rendersidebyside and not "" auto "" , even though "" never "" does not make much sense .",1
microsoft/vscode,"notebook toolbar foreground color cannot be modified by custom styles : the unavailable foreground color has been marked with a red arrow , please see the image expected behavior : notebook toolbar foreground color can be modified through custom styles . unexpected behavior : notebook toolbar foreground color cannot be modified through custom styles . vs code version : <number> | <number> os version",1
microsoft/vscode,consider adding ` ctrl / cmd + shift + . ` as go to symbol keybinding in accessible view cc <user>,1
microsoft/vscode,"[ accessibility ] add a custom setting always moving focus to terminal buffer after sending selection to terminal type : <b> feature request </b> for screen reader users , terminal buffer is the great landing area where they can verify executed commands . currently , the focus always moves to terminal input when we use the following command : > terminal : run selected text in active terminal given the immediate need of accessing the terminal buffer for screen reader users , please add a custom setting to always move focus in terminal buffer area instead of the terminal input . this will greatly improve the accessibility , navigability , and alertability for screen reader users . vs code version : code - insiders <number> . <number> - insider ( 8 3 fc3ad9bd553869e8fea3d370d1d2bd438f9e54 , <number> - <number> - 0 4 t <time> . 0 2 3 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",1
microsoft/vscode,update search results after enter pressed in files to include / exclude text fields . < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > when the user has a high debounce delay and changes the files to include / exclude text fields it can take a couple of seconds for the search results to update . this could be fixed by giving the user the option to skip this delay by pressing enter . this already happens with the search text field . it seams like calling triggerquerychange ( ); after the user presses enter in one of the fields would fix this problem . maybe somewhere in setsearchparameters ( line <number> ) at src / workbench / contrib / search / browser / media / searchview . ts ?,1
microsoft/vscode,"support public / private port toggle in tunnel forwarding <user> > while testing [ microsoft / vscode - internalbacklog # <number> ] ( <url> via the instructions in [ microsoft / vscode - remote - release # <number> ] ( <url> i noticed that switching port visibility has no effect and the port stays "" private "" . > > public visibility would be helpful for scenarios such as testing webhooks . > > _also , there are a few vs code repos but hopefully this is an appropriate one to file this . _ > > cc",1
microsoft/vscode,"rich content results for naive output in closed notebooks currently , we show rich content results for inputs in closed notebooks . it would be nice to show some output results info before we enable this experimental feature for insiders , since some people might want to search in outputs ( even if it ' s not fully a "" rich content result "" )",1
microsoft/vscode,"menu bar unnecessarily changes to hamburger when window width is reduced this feels similar to # <number> but that ' s marked as closed . i do not like the hamburger menu , i want to always see the normal menu bar . it already supports overflowing , but for some reason as soon as the go menu overflows , it seems to be entirely replaced by the hamburger . i do not understand why this is ( when there ' s already an overflow ) , and i would prefer it to just move more items into the overflow . the hamburger adds an extra click to get to the file menu ( which is almost always the menu i am using the mouse for ) . <url> i have tried changing various settings , including setting "" menu bar visibility "" to "" visible "" , but nothing seems to fix this ( except disabling command center , but i feel like that should not be necessary because a ) there ' s an overflow for the menu and b ) the command center appears to basically just be a mouse hit target , so could probably shrink in size ? ) .",1
microsoft/vscode,"[ feature ] collapse region in sticky scroll < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > in the sticky scroll section , it would be nice to be able to collapse those regions . this would allow us to move to the next region more quickly when we are finished with the current one .",1
microsoft/vscode,"render terminal cursor in the webgl canvas incoming change from xterm . js <url> to verify , check all combinations of ` terminal . integrated . cursor * ` , moving the cursor around in the terminal a bit and ensure they work .",1
microsoft/vscode,"[ accessibility ] add keybindings going between terminal input and output buffer back and forth just like copilot chat view type : <b> feature request </b> currently , to navigate between terminal input and output , users are required to use the tab and shift + tab keys . while this does offer a method for moving around , it ' s not an ideal solution . the tab cycle can include other tabbable elements which may not be relevant to the user ' s immediate needs . i would like to point out the new navigation model introduced in the copilot chat view . this new navigation system is both efficient and user - friendly . by pressing ctrl + uparrow while in the chat input , users ' focus is shifted directly to the chat output , regardless of how many times the ctrl + uparrow key is pressed . to return to the chat input from the output view , users simply press ctrl + downarrow . by implementing a similar system for navigating between terminal input and output , vscode could greatly improve the user experience . the intuitive and user - friendly design of the copilot chat view ' s navigation system makes it a perfect candidate to be extended into other areas of vscode ui . i believe many screen reader users would appreciate this change , as it would streamline their interactions with the terminal input and output buffer . vs code version : code - insiders <number> . <number> - insider ( 9 8 0 0 cf6dd6bf4634889d60720ef46a400f3a7298 , <number> - <number> - 2 8 t <time> . 4 7 2 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",1
microsoft/vscode,""" restart running task "" command should not show quick pick if there ' s only one task to restart < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - >",1
microsoft/vscode,add snippets as a language there ' s a good community grammar for snippets at <url> which would improve our snippets experience . we could include this as one of our built in languages .,1
microsoft/vscode,"allow specifying prompt prefix via environment variable collection < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > ` ` ` typescript export interface environmentvariablecollection { /* * * a string that will be prepended to the existing prompt in the terminal . keep it short and only use it if * it ' s really needed , as it modifies all terminals , even those not related to the extension . * * note this only applies if shell integration is enabled . */ promptprefix } ` ` ` the idea is to use shell integration to apply this parameter . this can be used by python extension to indicate an environment is activated in powershell or fish where there is no other way to set the prompt .",1
microsoft/vscode,"mouse click shortcut to split view right sublime text <number> has a great feature that allows us to simply "" ctrl + left click "" on an editor tab to split view that tab with the currently selected open editor tab . right now the only way to do this in vs code is by right clicking the tab and selecting "" split right "" . to demonstrate what i am asking for , let us say we currently have <number> open editors , file <number> , file <number> , and file <number> . let us say the currently selected open editor tab is file <number> . what i ' d like to do now is to split right file <number> . in order to do this with vs code , i must right click file <number> and select "" split right "" . what i ' d like is the ability to do this exact same scenario by simply holding ctrl + left clicking file <number> ' s tab . vs code v1 . <number> os x64 <number> . <number>",1
microsoft/vscode,commands do not work in single question mode testing # <number> looks like they are not supported,1
microsoft/vscode,"settings editor does not respect ctrl + up and ctrl + down testing # <number> not sure if the settings editor is intended to support this , but it seems very similar to the other places that do support it .",1
microsoft/vscode,"add tunnel activation event with <url> extensions can now contribute tunnel factories . however , extensions doing this need to get activated on ` * ` / startupfinished to be shown there . it ' d be nice if there was some kind of "" tunnel "" activation event that would only pick up when a tunnel or requested , or perhaps if the ports view is interacted with , to avoid this . mentioned in <url>",1
microsoft/vscode,allow terminal quick fix providers to call vscode commands currently quick fixes can run a command in the terminal or open a uri . to fix with copilot we need to run a command which will then do the work in order to suggest the result ( which could take some time ) .,1
microsoft/vscode,disable autosave while debugging < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i would like to be able to disable autosave while debugging . it ' s a problem if you are using hot reload .,1
microsoft/vscode,"open preview to support spawning new tabs < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > currently , when opening for example a markdown file in preview the vs code editor only uses the first tab to present the markdown . as shown below it would be nice to make it possible to open multiple files in preview mode and spawn new window tabs .",1
microsoft/vscode,"diff editor v2 : hide unchanged code command < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > in the new diff editor you can hide unchanged code , but there ' s no way to fold or unfold all regions at once . they all come folded but you must unfold each region manually , and must also fold them back . it would be nice to have a command to un / fold all regions at once , just like with code . ! [ image ] ( <url> there ' s no need for all code folding functionalities but at least fold and unfold all would be very useful",1
microsoft/vscode,"[ json ] set a user agent when attempting to retrieve ` $ schema ` json schemas < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : darwin x64 <number> . <number> steps to reproduce : <number> . start a webserver at ` http :// localhost : <number> ` serving a single schema at ` foo . json ` containing ` { "" required "" : [ "" bar "" ] } ` ( the precise contents do not really matter ) <number> . open a file bar . json , and paste in : ` ` ` json { "" $ schema "" } ` ` ` <number> . place the cursor between the quotes just after the trailing slash <number> . type foo . json , and look at the webserver logs , observing no user agent was sent",1
microsoft/vscode,"add accessible view hint to aria label for hovers we can pretty easily do this for editor hovers , but i am not sure how straightforward this will be to apply to the aria labels of extension contributed ones via the context view service",1
microsoft/vscode,pressing enter on deleted lines should take me to old file verification steps open diff editor * press f7 to open accessible diff viewer * select a deleted line and press enter * verify focus is in original editor * press f7 again * select a modified line and press enter * verify focus is in modified editor,1
microsoft/vscode,"turning off debug settings focus editor on break , focus window on break , or set the open debug to neveropen , does not work type : <b> bug </b> i have ` main . js ` and import it to ` main . debug . js ` . in ` main . debug . js ` i have a breakpoint , either by clicking the breakpoint dot or by the ` debugger ` line . in ` launch . json ` i set : ` ` ` js "" program "" : "" main . debug . js "" , ` ` ` if i open ` main . js ` and hit f5 , then when the debugger hit the breakpoint on ` main . debug . js ` , the editor will automatically switch to that file . is there a way to set this off ? in the setting i have tried to turn of focus editor on break , focus window on break , and set the open debug to neveropen , but it still switches . [ ] ( <url> ! [ ] ( <url> vs code version : code <number> . <number> ( 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 , <number> - <number> - 1 2 t <time> . 6 5 1 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 1 0 2 1 0 u cpu @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 4 gb ( <number> . 5 3 gb free ) | | process argv | - - crash - reporter - id a6dbf1b7 - 4 d72 - 4 9 e8 - badf - 0 7 a408420f74 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - bookmarks | ale | <number> . <number> htmlplay | bia | <date> markdown - mermaid | bie | <number> . <number> mermaid - markdown - syntax - highlighting | bpr | <number> . <number> vscode - search - long - paths | car | <number> . <number> regex | chr | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> githistory | don | <date> gitlens | eam | <number> . <number> runonsave | eme | <number> . <number> url - encode | fle | <number> . <number> ftp - simple | hum | <number> . <number> graphviz - previewer - web | ijm | <number> . <number> copy - markdown - as - html | jer | <number> . <number> vscode - graphviz | joa | <number> . <number> vscode - autohotkey - plus - plus | mar | <number> . <number> json | mee | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> powershell | ms - | <number> . <number> vscode - markdown - paste - image | tel | <number> . <number> graphviz - interactive - preview | tin | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - mermaid - preview | vst | <number> . <number> quokka - vscode | wal | <date> wordpress - toolbox | wor | <date> javascriptsnippets | xab | <number> . <number> yautohotkey | yed | <number> . <number> esm - cjs - converter | zha | <number> . <number> vitest - explorer | zix | <date> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vscaac : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> h7j2d46 <time> <number> h0f3276 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",1
microsoft/vscode,"adopt ` next / previous ` in chat response accessible view useful so a screen reader user does not have to escape the view , navigate to the next element , and reopen the view . this is possible now in notifications and should also be possible for notebook output cc <user>",1
microsoft/vscode,"while editing multiline using ctrl + d if a right import suggestion is accepted , it will only effect on the first occurrence ' s spelling . < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : not sure while editing multiline using ctrl + d if a right import suggestion is accepted , it will only affect on the first occurrence ' s spelling . < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : windows <number> steps to reproduce for example , in a file , i have a variable type called ` userservice ` and i want to change it to ` customuserservice ` and there are at least two mentions of it , the type . <number> . i select both of them using ` ctrl + d ` and start typing ` customuserservice ` * *( notice the small letter u and s in the name ) . * * after that i ` ctrl + . ` it for suggestions and when i accept the suggestion it will update the spelling of the first occurrence only and will leave the subsequent occurrences just like that .",1
microsoft/vscode,"the dismissed search results appear after search term changes even a letter < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > when i use multi - file search ( by ctrl + shift + f ) , i may use the dismiss button . but after adding another letter to the term , the dismissed item shows up again . ! [ vs - code - search - dismiss - functionality ] ( <url> i started not using this functionality ( the dismiss button ) any more , because it shows up right after a new letter . proposed solution can you please add the dismissed items to "" files to exclude "" part after clicking on dismiss button - or the dismissed items can stay dismissed until whole word gets deleted or changes ( this may require another button to stop dismissing )",1
microsoft/vscode,"diff editor shows funny word alignments * compare the two versions of the file * the word alignment is confusing < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> [ monaco editor repro ] ( <url>",1
microsoft/vscode,"firefox : support opening local files and folders < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > as stated in [ the docs ] ( <url> opening local folders in vscode . dev requires the file system access api support of the browser . according to [ file system access api - browser compatibility ] ( <url> as of <number> / <number> / <number> firefox <number> + added support for the file system access api . so , could the support for firefox be added ?",1
microsoft/vscode,"allow trusting the session instead of the folder there needs to be a way to trust "" just once "" for the current open editor . i do not want to have to maintain a list of trusted folders . _originally posted by <user> in <url>",1
microsoft/vscode,selectable soft dependencies < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > able to select what dependencies should not install while installing an extension .,1
microsoft/vscode,diff editor : show progress indicator <url> to verify : * open the diff editor and notice that a progress indicator appears when diffing big files ( appears after 1 s of waiting ),1
microsoft/vscode,fallback to release version if the pack / dependency extension does not have compatible pre - release fallback to release version if the pack / dependency extension does not have compatible pre - release,1
microsoft/vscode,"parent function identifier on "" go to references "" window < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > in the image below i have functions <number> and <number> , they are found inside different functions p1 and p2 – "" parent functions "" . i ' d like to see the proper parent function name on the top bar at the end of the file path and before "" references ( <number> ) "" when i have a reference focused . ! [ image ] ( <url>",1
microsoft/vscode,"surface dev tunnel usage limits more clearly - check dev tunnel limits asynchronously when we try to connect to a tunnel , and show a modal notification if they are exceeded - have a "" show usage limits "" command for users to monitor their limits - link to documentation both places with more information",1
microsoft/vscode,image preview on hover < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > it would be awesome to have ( optional ) image previews for e . g . . jpeg and . png images when hovering over these files in the explorer or the quick open promt . this would allow to find images faster not just by their name but also visually .,1
microsoft/vscode,"when chosen to install pre - release install the latest release version if there is no compatible pre - release version available <number> . install vs code <number> <number> . install copilot chat pre - release <number> . get this error message < img width = "" <number> "" alt = "" image "" src = "" <url> <number> . install copilot chat stable <number> . try to switch to pre - release <number> . : bug description view optimistically switches to pre - release view , but i am still on stable < img width = "" <number> "" alt = "" image "" src = "" <url>",1
microsoft/vscode,add an explicit toggle for terminal bracketed paste mode support the sub - shell experience can have problems without a toggle .,1
microsoft/vscode,"ctrl + p on selected text must fill search input "" search file by name . <repeated> "" , such as happens in ctrl + f < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > i am working on a legacy huge project that has many files references inside other files , and it ' s not able to be clickable to navigate in because it ' s using a different aproach to import files , so to navigate inside the referenced file , i have to copy the file name and then press ctrl + p to open search input , then i paste the copied file name . it ' s good for small changes , but when i am doing big changes it ' s can be a few exhausting . the suggested behavior is like happens with ctrl + f the file name ( any text ) , then press ctrl + p and the search input must be fullfilled with the selected text with the search ' s results already selected the first match , to hit enter and navigate into the selected file . <repeated> < ! - - describe the feature you ' d like . - - >",1
microsoft/vscode,moved code polishing ` ` ` [ tasklist ] # # # tasks - [ ] # <number> - [ ] <url> - [ ] # <number> ` ` ` moved out [ ] # <number> - [ ] # <number>,1
microsoft/vscode,[ feature ] auto unfold code when navigate symbols [ image ] ( <url>,1
microsoft/vscode,"improve run and debug < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > hi , i wonder whether it would be possible to improve output in run and debug pane and make it more dev - friendly ? ( i could not find any similar issue / feature request on this , though i might have missed . ) shortly background of the feature request - i come from another php ide - phped by nusphere , which uses own php debugger client & server module ( not xdebug ) . the debugging output is much more developer - friendly than in vscode . i am not using phped anymore on new projects because it has memory issues and no development or new features last few years . vscode works much better , but debugging experience cannot really compare to phped ' s . i could not find any extension that would make debugging output more friendly . so here is what i would love to have vscode improved in pane "" run and debug "" ( note : this strictly applies to php / xdebug , since i do not have experience with other languages ) : * possibility to move "" run and debug "" into bottom of screen to allow wider layout * possibility to have separate tabs for locals ( variables ) - globals - immediate evaluation - call stack * formatting variables into a table with <number> columns ( name - value - type ) - see screen bellow * less cluttered output , better readable / legible - e . g . change font size , less padding , increase / decrease line height * possibility to sort variables by name ( e . g . modes : not_sorted , asc , desc ) * highlight changed values between <number> breakpoints ( or against previous debug step ) if there already exists such an extension , please provide link . if not , how difficult would it be to implement features above - how much time would it take ? i would be willing to pay for it , if someone could provide costs estimate . screenshot vscode debug output vs . phped output < img src = ' <url> <url> < img src = ' <url>",1
microsoft/vscode,"replace preview : add "" jump to this line "" feature < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i am in the middle of global find - and - replace with <number> changes to make . but i want to manually edit one of the lines that is about to get changed . in this instance i am trying to review all usage of title case and switch to sentence case . in the replace preview window , it would be nice if i can right click and "" jump to this line "" . this would open the original file at that line so that i could edit that line however i want . < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",1
microsoft/vscode,"suggestion window styling / snippet styling < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > can we get suggestion styling . the suggestion windows on the right where is the description . i am new to the github sorry for messing up . if this feature already exists could you give me a link or examples . ! [ image ] ( <url> here you type the code and the suggestion pops up and when you open description i saw some styling for the suggestion box but not for the description box . would be nice to be able to change text color , size , font weight . very general formatting when writing snippets .",1
microsoft/vscode,"information on updates prior to using "" restart to update "" < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > it would be great if users could know what changes an update will apply to vscode before we choose "" restart to update "" . a link to a "" what ' s new "" or some log would probably suffice . < img width = "" <number> "" alt = "" image "" src = "" <url>",1
microsoft/vscode,"allow search to be sorted by shortest match . < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > when using a regex search in a project , it ' s often likely that the shortest match is the one closest to what you need . it would be nice if there was some way to make this a search option in the ctrl + shift + f menu .",1
microsoft/vscode,"pull new css grammar for colors , lengths , etc . this issue will describe the changes pulled in by prs on ` vscode - css ` ( <url> <url> <url> <url> this updates highlighting for colors ( gradients and ` color - mix ` function ) , length units , math functions , and adding the ` text - decoration - thickness ` property . to verify that this is working , paste this string into a css file . ` ` ` css body { background - image : linear - gradient ( to right in oklch longer hue , red <percent> <percent> ) ; } h1 { font - size : 1 0 vb ; text - decoration - line : underline ; text - decoration - thickness : 3 0 px ; } . shape { width : calc ( 1 0 px * pow ( <number> , <number> )); height : 1 0 0 px ; background - color : <hashtag> f <elongated> </hashtag> ; } ` ` ` if you are using the default dark theme , then it should look like this . < img width = "" <number> "" alt = "" image "" src = "" <url> in particular make sure that that ` hue ` is the same color as ` longer ` and ` oklch ` . - ` 1 0 vb ` is all one color , like how you would see ` 1 0 px ` . - ` text - decoration - thickness ` is the same color as the other properties - ` pow ( ) ` is formatted as a css function like ` calc ( ) ` is",1
microsoft/vscode,"developer tools settings sync < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > ! [ image ] ( <url> the button does not do anything . why not integrate it into vscode ' s existing settings sync ? i do have a lot of preferences set there , so it ' d be helpful since i frequently have to diagnose lots of extensions in the same way across machines .",1
microsoft/vscode,"add command to manually trigger proxy login dialog < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > # # the problem my company has a proxy password that changes daily . because vscode caches the proxy password , if vscode is left open past midnight it loses the ability to access the internet . this causes a variety of issues , including inability to install extensions , "" xhr failed "" errors , etc . the current workaround is to quit and reopen vscode , but this is a sledgehammer where a scalpel would suffice . <repeated> # # proposed solution i would like to propose the addition of a command to the command palette that manually triggers the [ proxy login dialog ( proxyauthhandler ( ) vscode : openproxyauthenticationdialog ) ] ( <url>",1
microsoft/vscode,"allow better extension of the search ui < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i was trying to build out some features to extend the search results ui since it is a very natural place for the work i was doing . in general there were two limitations i encountered <number> that has been flagged and delayed and <number> that has not . <number> . context menu actions or additional action buttons like ` replace ` are not able to be added . being able to add context menu actions to search can enable workflows that are more context specific to that result but possibly more intelligent . for instance being able to right click a result and look at the quick actions for that result inline . what if we wanted to add an option for each result to process it in some way , say convert it to a functional component from a class component ? if we could have a way to specify ` when ` a result matches some criteria show this additional action it would be awesome . enabling the context menu would also achieve this . <number> . replacements are <number> : <number> with find , it would be very powerful if you could instead suggest a custom replacement for each found result in the api . when triggering the command ` workbench . action . findinfiles ` you are allowed to pass a ` replace ` value . this value has all the power of the ui ' s ` replace ` field but is pretty limiting when trying to build a stronger tool here . it would be awesome if you could provide one of the following as well # # pass a replacement function ` ` ` tsx replace : string | async ( current : string ) => string ` ` ` this function could be executed when rendering the ui for the replacement and would allow a custom replacement to be computed from the original . this means you could have all the power of javascript to compute these replacements instead of being limited to only regex style computations . you could load the file into an ast and make a decision for instance ? # # allow re - use of the existing search ui if you allowed instead for the extension to just provide a list of results it would keep a similar ui experience but provide similar power . ` ` ` tsx vscode . commands . executecommand ( ' workbench . action . showinfindandreplace ' , { query : { type : ' custom ' , representation : "" ast query "" } , results : [ { match : { file : ' . / file . ts ' , line : <number> , start : <number> , end : <number> } , replacement : "" foo "" } ] , triggersearch : true , preservecase : true , useexcludesettingsandignorefiles : true , isregex : true , iscasesensitive : false , matchwholeword : false , filestoinclude : "" . /* */ * . ts , . /* */ * . tsx "" , } ) ` ` ` this could be made even more powerful if you actually let extension developers leverage the search functionality in the extension as well . something like ` ` ` tsx vscode . commands . executecommand ( ' workbench . action . findinfiles ' , ( matches : match [ ] ) => ( { query : { type : ' custom ' , representation : "" ast query "" } , results : matches . map ( v => computereplacement ( match ) ) , triggersearch : true , preservecase : true , useexcludesettingsandignorefiles : true , isregex : true , iscasesensitive : false , matchwholeword : false , filestoinclude } ) ) ` ` `",1
microsoft/vscode,"custombuiltinextensions fromlocations will not automatically load the correct ` package . nls . { locale } . json ` <url> the code here , when loading the extension through ` additionalbuiltinextensions ` , does not pass ` packagenlsuris ` parameters , is it because it cannot read all nls - related file lists for matching ?",1
microsoft/vscode,"add accessible view provider for inline chat response cc <user> panel chat responses can be read char by char , line by line via ` alt + f2 ` . we should do the same for the inline chat response .",1
microsoft/vscode,"worst usability in "" preventing dirty writes "" . please fix it ( ) hello i hope you rate it right away that if the usability is terrible that this is a product defect because it makes the product no longer reasonably useful . < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> ( system setup ) - os version : windows_nt x64 <number> . <number> steps to reproduce : <number> . create a situation , in which vs code offers the annoying [ preventing dirty writes ] ( <url> mode <number> . select yes , so that vs code allows you to merge changes now the problems start for probably more than <percent> of the users : <number> . * * it is terribly difficult to see what content is displayed in the left and right windows . * * this is the most important use case for any merge process : users can see at a glance ( without having to scan the screen ) which data he can see . but vs code displays this information in small printed text . because it ' s not obvious which data is on the left / right side , it happens again and again that users merge the wrong way around and they ask _ "" where is the backup ? <repeated> "" _ . <number> . this is a shame : * * basic merge commands are missing * * . of course , one often uses the command to merge all <emphasis> changes to the left or to the right . but because vs code allows to navigate between the individual changes , it also means that sometimes customers want to merge individual changes in one direction . but those commands are missing or somewhere hidden ! <number> . * * if something fails : where is the backup ? * * it can happen that someone synchronizes the data in the wrong direction due to a mistake . therefore , it should be configurable and enabled by default that a backup is created automatically or that the merge process asks whether a backup should be created . <number> . * * please allow us to disable this new "" feature "" preventing dirty writes * * in the end , in the last decade , i have never once had the problem of a ' dirty write ' and when i ask our many dozens of engineers , they felt the same way code solves a problem that pretty much only occurs in very rare / special situations . but vs code now forces users to merge data every now and then , even though it was never necessary before . please allow to re - enable the old mode , where everything worked perfectly . thanks a lot , kind regards , thomas",1
microsoft/vscode,"toggle inline diff not discoverable in inline chat followup to <url> i spent quite some time messing with the inline chat preview mode setting to get the diff to appear , without success . it did not occur to me to look in the discard dropdown for the toggle inline diff action , since i do not associate diffing with discarding . have we considered surfacing this another way , maybe as a separate icon or through a keybinding with some helper text below the result ?",1
microsoft/vscode,"add context key for when cursor is at end of line in cell < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > having the same issue as [ this post ] ( <url> i decided to try to create my own keybindings within jupyter vscode in order to replicate the behavior of the old jupyter interface , regarding the effect of ` downarrow ` when the cursor is not at the bottom of a cell , move the cursor down <number> . when the cursor is at the bottom of a cell , but not at the end of the line , move cursor to the end of the line <number> . when the cursor is at the bottom of a cell and at the end of the line , move to the next cell in order to do this , however , i need to use a "" when clause context "" to detect when the cursor is at the end of a line . i can not seem to find such a context key . ( there is , however , ` notebookcursoreditoratboundary = = ' bottom ' ` , which i believe detects if the cursor is at the bottom of a cell . )",1
microsoft/vscode,"feature request : show code variations < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > in this type of feature if i select a block of code , it will display different and optimized variation to execute a particular block of code . for example javascript code for mapping ` ` ` javascript const inputarray = [ <number> , <number> , <number> , <number> , <number> ]; const outputarray = []; for ( let i = <number> ; i < inputarray . length ; i + + ) { const mappedvalue = inputarray [ i ] * <number> ; outputarray . push ( mappedvalue ) ; } console . log ( outputarray ) ; ` ` ` optimized javascript code variation ` ` ` javascript const inputarray = [ <number> , <number> , <number> , <number> , <number> ]; const outputarray = inputarray . map ( ( value ) => value * <number> ); console . log ( outputarray ) ; ` ` `",1
microsoft/vscode,"extend serverreadyaction to support startdebugging with provided debugconfiguration < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > ` serverreadyaction ` only seems to support [ ` name ` ] ( <url> for starting a new browser debug session . this feature request is to extend it to support a new entry called ` config ` that will be used for the ` vscode . debug . startdebugging ` call . e . g . ` ` ` jsonc "" name "" : { "" type "" : "" object "" , "" markdowndescription "" : "" % debug . server . ready . config . description %"", "" default "" : { "" name "" : "" launch browser "" , "" type "" : "" edge "" , "" request "" : "" launch "" } , "" properties "" /* vscode . debugconfiguration */ } } , ` ` `",1
microsoft/vscode,"difference review , improve voice over testing # <number> maybe this is related to <url> but as i tried it i got different results from that mode but it never seemed to read what i expected i enable screenreader mode and voiceover , and every time i press f7 , this is what it reads . i do not really know what the first part is telling me , and also it says "" <number> lines changed "" but this was a file with <number> change chunks , each with just one changed line , so no idea where it is getting <number> . < img width = "" <number> "" alt = "" image "" src = "" <url>",1
microsoft/vscode,""" collapse unchanged regions "" looks disabled testing # <number> < img width = "" <number> "" alt = "" image "" src = "" <url> should use a toggle state instead , or a new icon . you can see it looks the same as the other disabled action",1
microsoft/vscode,"add accessibility view and accessibility help menu for copilot inline suggestions testing # <number> if you already have an issue for future plans feel free to close this one . i think the copilot inline ( ghost text ) suggestions could also profit from the accessibility view . currently users are depending on the copilot view on the side which might get deprecated in the future . and i think we need a native solution for this , so intellicode also profits . i am not sure how exactly to integrate this with the inline suggestion hover . a potential start would be . whenever there is an inline suggestion , we play an audio cue ( as we do today ) , and a user can open accessibility view to inspect the exact suggestion . fyi <user>",1
microsoft/vscode,inform sr users about ` open accessible view ` maybe as an aria hint for hovers and in the chat accessibility help menu,1
microsoft/vscode,support trailing colon in - - goto it ' d be useful if ` code - - goto file : <number> : <number> : ` works with a trailing comma . this ' d be useful because many tools add a comma after file [ : line [ : col ] ] . for example : clang : ` ` ` src / main . rs : <number> : <number> : error : unknown type name ' fn ' ` ` ` gcc : ` ` ` src / main . rs : <number> : <number> : error : unknown type name ‘ fn ’ ` ` ` ld : ` ` ` ld : src / main . rs : file format not recognized ; treating as linker script ` ` ` bash : ` ` ` src / main . rs : line <number> : syntax error near unexpected token ` ( ' ` ` ` awk : ` ` ` awk : src / main . rs : <number> : ( filename = - fnr = <number> ) fatal : function ` main ' not defined ` ` ` make : ` ` ` src / main . rs : <number> missing separator . stop . ` ` ` and so on .,1
microsoft/vscode,add compare feature on tab menu < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > can add compare feature on tab menu ? ! [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url>,1
microsoft/vscode,"filetype specific autosave < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > in my project , i have a mixture of javascript and python files . the javascript files are connected to an auto - reloading react app , thus i would like to retain autosave on them . the python files control a running server ( also auto - reload ) . however , making a change to the python file takes longer , so i would like to control the saving of those more directly . is it possible to have ` files . autosave ` be settable on a filetype granularity ?",1
microsoft/vscode,"allow to disable editor group maximize on double click < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > occasionally a mouse click turns into a double click ( which is hard to avoid ) , but i do not want to maximize the editing group and have to get it back every time i accidentally maximize it . i can not find the relevant settings in the settings , i hope this feature can be turned off , thank you very much !",1
microsoft/vscode,"tab sizing option fixed type : <b> feature request </b> i am very happy that you introduced this feature , for quickly closing many tabs : ` workbench . editor . tabsizingfixedmaxwidth ` however , with fixed width , the tabs become so small that it is hard to distinguish files with the same first <number> or <number> characters . can you also add a minimum width setting ? ` workbench . editor . tabsizingfixedminwidth ` this would allow each developer to set a minimum width based on their file naming convention ( e . g . if they have many files starting with the first <number> characters , they can set the minimum to see at least <number> or <number> characters ) . thank you . vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",1
microsoft/vscode,"[ fr ] search editor syntax highlighting based on extensions < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > currently , the search editor uses a fixed list to do syntax highlighting , which cannot be dynamically changed based on which extensions are installed . it would be nice to support that . ref",1
microsoft/vscode,"keyboard shortcut to move tabs left and right < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i am a very visual person , and i work better when i drag similar tabs together . i can do that using the cursor , but i ' d love to be able to do that using the keyboard . ideally , there would be keyboard shortcuts to move a tab to the left - to the right - to the further left side - to the further right side basically , i ' d love to be able to do in vscode what i can do in chrome using [ this extension ] ( <url>",1
microsoft/vscode,"move off of keytar with [ node - keytar ] ( <url> now archived and unmaintained , we need a path forward for securely storing secrets and additionally , reflect the fact that we do not need to follow the shape of keytar anymore . on desktop , we will take advantage of electron ' s [ safestorage ] ( <url> api . we will also get rid of ` vscode - encrypt ` because it would be providing no benefit in this new world . additionally , we will remove the keytarshim that has been in the product . this was the solution to secrets before the [ secretstorage api ] ( <url> outlined are the steps to get this done : ` ` ` [ tasklist ] # # # tasks - [x ] implement new ` encryptionservice ` & ` secretstorageservice ` services with a migration story . use new services for extension secretstorage api - [ ] <url> - [x ] write new docs for troubleshooting keyring issues & update remote docs - [x ] notify top x extensions of keytarshim that we will remove it - [x ] make discussion post - [ ] <url> ` ` `",1
microsoft/vscode,"started vs - code type : <b> bug </b> arch linux . trying to get vs - code to find yarn berry packages for intellisense vs code version : code - oss <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 1 1 t <time> . 9 5 9 z ) os version : linux x64 <number> . <number> - arch1 - <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 5 3 5 0 u cpu @ <number> . 8 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : disabled_software <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 6 6 gb ( <number> . 6 1 gb free ) | | process argv | - - unity - launch | | screen reader | no | | vm | <percent> | | desktop_session | plasma | | xdg_current_desktop | kde | | xdg_session_desktop | kde | | xdg_session_type |x 1 1 | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - zipfs | arc | <number> . <number> - rc . <number> better - toml | bun | <number> . <number> editorconfig | edi | <number> . <number> vscode - npm - script | eg2 | <date> python | ms - | <number> . <number> vscode - twoslash - queries | ort | <number> . <number> rome | rom | <number> . <number> rust - analyzer | rus | <date> volar | vue | <date> ( <number> theme extensions excluded ) </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"error : ebusy busy or locked filed to save “ html file ” : unable to write file ‘ disk \ \ folder \ \ htmlfile ’ ( unknown ( filesystemerror ) : error : ebusy : resource busy or locked , open ‘ disk ;\\ folder \ \ htmlfile ’ ) suddenly this error occured when i make change in the code and save '",2
microsoft/vscode,"running a selection of code is not working while it runs when the program is executed type : <b> performance issue </b> i am using python ide when i run some part of my code , i get errors like nameerrors , syntaxerrors , but when the program is run as a whole , the code executes as it should . how do i fix it ? i ' d like to keep running portions of the program without necessarily executing the whole program . section of code : a = int ( input ( "" enter number a : "" ) ) b = int ( input ( "" enter number b : "" ) ) sum = <number> if a > b : a , b = b , a start = a while start < = b : sum + = start start + = <number> print ( f "" the sum between { a } and { b } is , { sum } "" ) vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 6 3 0 0 u cpu @ <number> . 4 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 4 2 gb ( <number> . 3 4 gb free ) | | process argv | - - crash - reporter - id 5 0 ce0f66 - 2 b2b - 4 d87 - 9 8 c3 - dad345dd3f4c | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> crashpad - handler <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> shared - process <number> <number> <number> window [ <number> ] ( ● countup problems . py - python_exercises - visual studio code ) <number> <number> <number> gpu - process <number> <number> <number> utility - network - service <number> <number> <number> ptyhost <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ python \ \ python311 \ \ python . exe "" <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node c :\\ users \ \ lenovo \ \ . vscode \ \ extensions \ \ ms - python . vscode - pylance - <number> . <number> \ \ dist \ \ server . bundle . js - - cancellationreceive = file : abcf6af201ba576b327857c8d9c8120d3b66f81992 - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> window [ <number> ] ( issue reporter ) ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( ● countup problems . py - python_exercises - visual studio code ) | folder ( python_exercises ) : <number> files | file types gitignore ( <number> ) | conf files : ; ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - pull - request - github | git | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> <number> <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> copilotsettingc : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - > ` ` ` [ tasklist ] # # # tasks ` ` `",2
microsoft/vscode,"inlay hints do not show when using workspace typescript version < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : macos <number> . <number> steps to reproduce : <number> . have a ` . vscode / settings . json ` with ` ` ` json { "" typescript . tsdk "" : "" node_modules / typescript / lib "" , } ` ` ` <number> . command palette - > ' typescript : select typescript version ' . choose between workspace + default vs code version when default ts version is selected ( <number> . <number> <sad> < img width = "" <number> "" alt = "" image "" src = "" <url> when workspace ts is selected ( <number> . <number> <sad> < img width = "" <number> "" alt = "" image "" src = "" <url> however , ts definitely knows what type it is , since when i hover over , it displays the type correctly width = "" <number> "" alt = "" image "" src = "" <url>",2
microsoft/vscode,"non existent issue with url mapping [ screenshot <number> - <number> - <number> <number> ] ( <url> ! [ screenshot <number> - <number> - <number> <number> ] ( <url> ! [ screenshot <number> - <number> - <number> <number> ] ( <url> type : <b> bug </b> vsc is currently giving me <number> messages when i run the development server . despite me correctly mapping out the urls to the relavant pages , your piece of shit software of giving me error <number> ( webpage not found ) messages . end of my teather with your product . sort it out ! vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 9 4 0 0 f cpu @ <number> . 9 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 5 gb ( <number> . 0 6 gb free ) | | process argv | - - crash - reporter - id ae970a20 - b9e5 - <number> - b646 - c75742ce9184 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - intelephense - client | bme | <number> . <number> composer - php - vscode | dev | <number> . <number> phptools - vscode | dev | <number> . <number> profiler - php - vscode | dev | <number> . <number> compilemql4 | kei | <number> . <number> mql - tools | l - i | <number> . <number> file - downloader | min | <date> vscode - html - format | moh | <number> . <number> vscode - azureresourcegroups | ms - | <number> . <number> vscode - azurestorage | ms - | <number> . <number> python | ms - | <number> . <number> azure - account | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> live - server | ms - | <number> . <number> powershell | ms - | <number> . <number> mq4 | ner | <number> . <number> mql - over - cpp | nic | <number> . <number> quick - html - template | pen | <number> . <number> bitmagic | yaz | <date> html - css - class - completion | zig | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 e4cg34 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingt : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",2
microsoft/vscode,"python input ( ) function does not read input ( ssh remote vm ) i am using vs code to ssh into a remote vm . when running python scripts ( in the terminal window ) that request input . e . g . ` query = input ( "" \ \ nenter a query it does not read the provided input but waits forever .",2
microsoft/vscode,"msys2 ucrt64 terminal profile can not integrate under terminal tab < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : microsoft windows <number> 家庭中文版 steps to reproduce : <number> . installed msys2 . <number> . adding following in settings . json : ` ` ` javascript "" terminal . integrated . profiles . windows "" : { "" ucrt64 "" : { "" path "" : "" d :\\\\ program files \ \ \ \ msys64 \ \ \ \ ucrt64 . exe "" } } , "" terminal . integrated . defaultprofile . windows "" : "" ucrt64 "" , ` ` ` <number> . click ' ucrt64 ' in terminal profiles dropdown <number> . the ucrt64 terminal run but not under the terminal tab ! [ image ] ( <url>",2
microsoft/vscode,"unable to resolve your shell environment exit code from spawned shell ( code <number> , signal null ) < img width = "" <number> "" alt = "" image "" src = "" <url> i am facing this issue in viscose on my mac . <repeated> i have removed node_options from the . zshrc file still the error is not going away . <repeated> need fix asap",2
microsoft/vscode,"music store database type : <b> bug </b> details about customers , city , amount and most preferred employee vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : restricted <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 3 2 5 0 u with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 9 5 gb ( <number> . 5 8 gb free ) | | process argv |c :\\\\ users \ \ \ \ sajna \ \ \ \ downloads \ \ \ \ music store database analysis project on postgresql - - crash - reporter - id 8 0 2 0 f3a6 - a5dd - 4 b63 - 8 5 d2 - d5897bf9dd78 | | screen reader | no | | vm | <percent> | </details> extensions <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingc : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"markdown format problem < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > can you add specific icon to avoid plugin or anything else aiming at formating autoly deleting the space at the end of line ? i need double space at the end of line to elegant line break . anyhting else , if there ' s another way to elegant line break gracefully , please tell me",2
microsoft/vscode,"breakpoints not working on vscode extension development type : <b> bug </b> i am developing an extension for vscode and breakpoints do not work . i can set them without any problem , but when i launch f5 , it just will not stop on them . i have tried disabling all the extensions and it still happens . related to <url> vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i7 - 1 1 8 0 0 h @ <number> . 3 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 9 gb ( <number> . 1 2 gb free ) | | process argv | - - crash - reporter - id ccedd11e - cbe3 - 4 aab - 8 3 5 a - b81d8cc295d7 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - codesnap | adp | <number> . <number> tsl - problem - matcher | amo | <number> . <number> ng - template | ang | <number> . <number> cssrem | cip | <number> . <number> continue | con | <date> vscode - eslint | dba | <number> . <number> es7 - react - js - snippets | dsz | <number> . <number> gitlens | eam | <number> . <number> prettier - vscode | esb | <number> . <number> fetch - client | gan | <number> . <number> todo - tree | gru | <date> angular2 | joh | <number> . <number> vscode - peacock | joh | <number> . <number> vscode - colorize | kam | <number> . <number> git - graph | mhu | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> live - server | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> vscode - versionlens | pfl | <number> . <number> postman - for - vscode | pos | <number> . <number> vscode - thunder - client | ran | <number> . <number> fabric8 - analytics | red | <number> . <number> java | red | <number> . <number> vscode - xml | red | <number> . <number> vscode - yaml | red | <number> . <number> vs - code - prettier - eslint | rve | <number> . <number> partial - diff | ryu | <number> . <number> errorlens | use | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> vscode - icons | vsc | <number> . <number> javascriptsnippets | xab | <number> . <number> markdown - all - in - one | yzh | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 3 biah6 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> copilotsettingc : <number> synctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"intellisense type : <b> bug </b> when i press tab to complete the line of code like : ` ` ` const btn = docoment . queryselector ( "" ` ` ` did not finish the hole line with brackets vs code version : code <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 4 3 8 z ) os version : windows_nt x64 <number> . <number> modes : unsupported <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i7 - 1 1 8 0 0 h @ <number> . 3 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : unavailable_software <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : disabled_software <br> multiple_raster_threads : enabled_on <br> opengl : disabled_off <br> rasterization : disabled_software <br> raw_draw : disabled_off_ok <br> video_decode : disabled_software <br> video_encode : disabled_software <br> vulkan : disabled_off <br> webgl : unavailable_software <br> webgl2 : unavailable_software <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 3 1 gb free ) | | process argv | - - crash - reporter - id 2 0 bc9b53 - fa17 - 4 9 c9 - <number> - dcdebd4c9d7e | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - codesnap | adp | <number> . <number> vscode - css - formatter | aes | <number> . <number> auto - add - brackets | ali | <number> . <number> laravel - extra - intellisense | ami | <number> . <number> bootstrap5 - vscode | anb | <number> . <number> ng - template | ang | <number> . <number> browse - lite | ant | <number> . <number> vite | ant | <number> . <number> musicplayer | arj | <number> . <number> blackbox | bla | <date> auto - align | bla | <date> vscode - intelephense - client | bme | <number> . <number> bracket - pair - color - dlw | bra | <number> . <number> vscode - tailwindcss | bra | <number> . <number> exe - runner | bra | <number> . <number> phpserver | bra | <number> . <number> simple - react - snippets | bur | <number> . <number> multi - cursor - case - preserve | car | <number> . <number> turbo - console - log | cha | <number> . <number> npm - intellisense | chr | <number> . <number> path - intellisense | chr | <number> . <number> codeium | cod | <date> logical - properties | cod | <date> laravel - goto - view | cod | <number> . <number> postcss | css | <number> . <number> php - namespace - resolver | ctf | <number> . <number> vs - phpclassgen | dam | <number> . <number> vscode - jq | dan | <number> . <number> dart - code | dar | <number> . <number> flutter | dar | <number> . <number> vscode - eslint | dba | <number> . <number> composer - php - vscode | dev | <number> . <number> phptools - vscode | dev | <number> . <number> profiler - php - vscode | dev | <number> . <number> css - minify | die | <date> chatgpt - code | dog | <number> . <number> jquerysnippets | don | <number> . <number> es7 - react - js - snippets | dsz | <number> . <number> chatgpt - gpt4 - gpt3 - vscode | eas | <number> . <number> vscode - html - css | ecm | <number> . <number> vscode - great - icons | emm | <date> prettier - vscode | esb | <number> . <number> auto - close - tag | for | <date> auto - complete - tag | for | <number> . <number> auto - rename - tag | for | <date> code - runner | for | <number> . <number> vscode - mysql | for | <number> . <number> cypress - runner | g - f | <number> . <number> chatgpt - sql - vscode - plugin | gao | <number> . <number> remotehub | git | <number> . <number> php - awesome - snippets | hak | <number> . <number> cppsnippets | har | <date> vue - snippets | hol | <number> . <number> cypress - snippets | ijs | <number> . <number> fontawesome - autocomplete | jan | <number> . <number> css - snippets | joy | <number> . <number> code - background | kat | <number> . <number> vsc - python - indent | kev | <number> . <number> pretty - php | lkr | <date> next - js - ts - snippets | loc | <number> . <number> json | mee | <number> . <number> php - namespace - resolver | meh | <number> . <number> fluent - icons | mig | <date> vscode - docker | ms - | <number> . <number> csharp | ms - | <number> . <number> vscode - dotnet - runtime | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> azure - repos | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> live - server | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> vscode - typescript - next | ms - | <number> . <number> awesome - flutter - snippets | nas | <number> . <number> vetur | oct | <number> . <number> laravel - blade | one | <number> . <number> laravel5 - snippets | one | <number> . <number> convert - css - in - js | pau | <number> . <number> phpstorm - snippets | phi | <number> . <number> material - icon - theme | pki | <number> . <number> vscode - css - peek | pra | <number> . <number> vscode - yaml | red | <number> . <number> format - html - in - php | rif | <number> . <number> liveserver | rit | <number> . <number> vs - code - prettier - eslint | rve | <number> . <number> laravel - artisan | rya | <date> vscode - javascript - booster | sbu | <number> . <number> vue - vscode - snippets | sdr | <number> . <number> background | sha | <date> vscode - cy - helper | she | <number> . <number> vscode - blade - formatter | shu | <number> . <number> vscode - scss - formatter | sib | <number> . <number> html5 - boilerplate | sid | <number> . <number> indenticator | sir | <number> . <number> js - jsx - snippets | sky | <number> . <number> music - time | sof | <date> react - next - js - code - snippets | soh | <date> html - to - css - autocompletion | sol | <number> . <number> css - auto - prefix | spo | <number> . <number> autoimport | ste | <number> . <number> tabnine - vscode | tab | <number> . <number> vscode - pets | ton | <number> . <number> cmake | twx | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> properties - validator | viv | <number> . <number> volar | vue | <date> vscode - typescript - vue - plugin | vue | <date> quokka - vscode | wal | <date> wordpress - toolbox | wor | <date> eno | wsc | <date> qf | wsc | <date> php - debug | xde | <number> . <number> php - pack | xde | <number> . <number> reactcssautocomplete | zac | <number> . <number> tailwind - snippets | zar | <number> . <number> material - theme | zhu | <number> . <number> html - css - class - completion | zig | <number> . <number> classnametocss | zit | <number> . <number> php - intellisense | zob | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 3 biah6 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingc : <number> synctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"unable to save file type : <b> bug </b> failed to save ' app . component . html ' : insufficient permissions . select ' retry as admin ' to retry as administrator . vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 5 2 0 0 u cpu @ <number> . 2 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 2 gb ( <number> . 7 1 gb free ) | | process argv | d :\\\\ angular projects \ \ \ \ proclient - - crash - reporter - id 2 9 3 4 8 f64 - 4 7 9 a - 4 fdb - be59 - b118dc28dd1d | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - ng - template | ang | <number> . <number> npm - intellisense | chr | <number> . <number> path - intellisense | chr | <number> . <number> prettier - vscode | esb | <number> . <number> auto - rename - tag | for | <date> code - runner | for | <number> . <number> copilot | git | <number> . <number> liveserver | rit | <number> . <number> html - css - class - completion | zig | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 3 biah6 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> copilotsettingt : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"code not working correctly for a specific number <number> type : <b> bug </b> ` <hashtag> include </hashtag> <iostream> using namespace std ; void frequency ( int arr [ ] , int n ) { int count = <number> ; for ( int i = <number> ; i < n ; i + + ) { if ( arr [ i ] = = arr [ i + <number> ] ) { count + + ; } else { cout < < arr [ i ] < < "" : "" < < count < < endl ; count = <number> ; } } } int main ( ) { int n = <number> ; int arr [ n ] = { <number> , <number> }; frequency ( arr , n ) ; return <number> ; } ` above is the code to print frequency of each element of sorted array , for the test case int n = <number> ; int arr [ n ] ={ <number> , <number> }; output : ps c :\\ users \ \ himanshu > cd "" c :\\ users \ \ himanshu \ \ desktop \ \ dsa \ \ cpp_vscode \ \ "" ; if ($ ? ) { g + + gfg . cpp - o gfg } ; if ($ ? ) { . \ \ gfg } <number> : <number> <number> : <number> the vs code is not giving right output else everywhere it working fine ; and also for the test case int n = <number> ; int arr [ n ] ={ <number> , <number> }; output : ps c :\\ users \ \ himanshu \ \ desktop \ \ dsa \ \ cpp_vscode > cd "" c :\\ users \ \ himanshu \ \ desktop \ \ dsa \ \ cpp_vscode \ \ "" ; if ($ ? ) { g + + gfg . cpp - o gfg } ; if ($ ? ) { . \ \ gfg } <number> : <number> <number> : <number> <number> : <number> it is working fine vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 3 5 g7 @ <number> . 4 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 7 gb ( <number> . 7 0 gb free ) | | process argv | - - crash - reporter - id 3 be531c0 - eff7 - 4 6 2 a - 8 9 a9 - a053520d62c8 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - html - end - tag - labels | ant | <number> . <number> prettier - vscode | esb | <number> . <number> auto - rename - tag | for | <date> code - runner | for | <number> . <number> copilot | git | <number> . <number> unibeautify - vscode | gla | <number> . <number> next - js - ts - snippets | loc | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> cpptools | ms - | <number> . <number> liveserver | rit | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 9 6 2 ge76 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"react type : <b> feature request </b> react is not installed vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 3 9 0 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"problem with "" unused variable "" type : <b> bug </b> sometimes there is such an error , when reopening it may not happen again . idx <emphasis> declared but not used vs code version : code <number> . <number> ( abd2f3db4bdb28f9e95536dfa84d8479f1eb312d , <number> - <number> - 1 4 t <time> . 9 8 1 z ) os version : linux x64 <number> . <number> - <number> - generic modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 9 3 0 0 h cpu @ <number> . 4 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : disabled_software <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 6 0 gb ( <number> . 5 6 gb free ) | | process argv | - - unity - launch - - crash - reporter - id a927cb51 - 7 c20 - 4 5 d0 - b062 - ab7a5fc00084 | | screen reader | no | | vm | <percent> | | desktop_session | cinnamon | | xdg_current_desktop |x - cinnamon | | xdg_session_desktop | cinnamon | | xdg_session_type |x 1 1 | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - common | clo | <date> github - authentication | clo | <number> . <number> doxdocgen | csc | <number> . <number> gdb - debug | dam | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - pull - request - github | git | <number> . <number> go | gol | <number> . <number> todo - tree | gru | <date> vscode - drawio | hed | <number> . <number> plantuml | jeb | <number> . <number> better - cpp - syntax | jef | <number> . <number> cmake - language - support - vscode | jos | <number> . <number> vscode - docker | ms - | <number> . <number> vscode - language - pack - ru | ms - | <number> . <phone> vscode - dotnet - runtime | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> rust - analyzer | rus | <date> crates | ser | <number> . <number> cmake | twx | <date> vscode - lldb | vad | <number> . <number> vscode - todo - highlight | way | <number> . <number> debug | web | <number> . <number> markdown - all - in - one | yzh | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> showlangstatbar : <number> 9 6 2 ge76 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"cursor control type : <b> performance </b> cursor speed is very high it scroll the whole page at once it is very difficult to navigate because the whole page is scroll at once and i am unable to scroll a single line or double line vs code version : code <number> . <number> <phone> d557a81c9d0b5f8a5a1e9274db5585 , <number> - <number> - 0 8 t <time> . 5 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i3 - 6 0 0 6 u cpu @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 1 gb ( <number> . 8 6 gb free ) | | process argv |c :\\\\ workspace \ \ \ \ test_projects \ \ \ \ tourofheroes - - crash - reporter - id 6 6 1 6 f29d - de03 - 4 7 a4 - 9 5 d4 - ba60bdab8111 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - ng - template | ang | <number> . <number> csdevkit | ms - | <date> csharp | ms - | <date> vscode - dotnet - runtime | ms - | <number> . <number> vscodeintellicode - csharp | ms - | <date> vs - keybindings | ms - | <number> . <number> liveserver | rit | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes5 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> showlangstatbar : <number> 9 6 2 ge76 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"open a file in a new tab < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce when i double click a file to open it in a new tab , it ' s not <number> . then i go back in navigation history , the elder file replaced by the new one is opened again in a new tab . i tried to do a quick video with camtasia , and then , recording , the problem disappears . 🤔 i am using the fact to open new files in the same project when right - clicking on a file . and not opening a new project ( default behavior ) . i do not know if it ' s linked , but just to tell . <repeated>",2
microsoft/vscode,"svg path is not working type : <b> bug </b> whenever i paste svg code in vs code then svg path is not coming an decrypted code coming in vs code while pasting . please help me and resolve my problem as soon as possible . vs code version : code <number> . <number> <phone> d557a81c9d0b5f8a5a1e9274db5585 , <number> - <number> - 0 8 t <time> . 5 7 5 z ) os version : windows_nt x64 <number> . <number> modes : unsupported <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 5 5 0 0 u with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 9 gb ( <number> . 0 3 gb free ) | | process argv |c :\\\\ users \ \ \ \ asus \ \ \ \ desktop \ \ \ \ snkr plts - - crash - reporter - id fd6ca2a9 - f709 - 4 3 bb - <number> - 1 2 c2822e613a | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - django | bat | <number> . <number> vscode - tailwindcss | bra | <number> . <number> java - run | cao | <number> . <number> python - environment - manager | don | <number> . <number> python - extension - pack | don | <number> . <number> vscode - html - css | ecm | <number> . <number> gitpod - desktop | git | <date> beautify | hoo | <number> . <number> vsc - python - indent | kev | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> autodocstring | njp | <number> . <number> java | red | <number> . <number> liveserver | rit | <number> . <number> background | sha | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - gradle | vsc | <number> . <number> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> vscode - icons | vsc | <number> . <number> jinja | who | <number> . <number> html - css - class - completion | zig | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> vscrpc : <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"command line - - help does not show cli help type : <b> bug </b> hello vs code crew 🤘 # # found : the documentation <url> shows one shall type ` code - - help ` to get help for the command line interface . but this seems broken . what i get is : ` ` ` c :\\ users \ \ eric \ \ appdata \ \ local \ \ programs \ \ microsoft vs code > code - - help c :\\ users \ \ eric \ \ appdata \ \ local \ \ programs \ \ microsoft vs code > [ <number> <time> <number> / <number> : error : cache_util_win . cc ( <number> ) ] unable to move the cache : access is denied . ( 0x 5 ) [ <number> <time> <number> / <number> : error : disk_cache . cc ( <number> ) ] unable to create cache ` ` ` and then a new vs code window opens . the short ` - h ` works just the same way . # # expected : * the [ cli options ] ( <url> are printed * no window opens * no confusing errors vs code version : code <number> . <number> <phone> d557a81c9d0b5f8a5a1e9274db5585 , <number> - <number> - 0 8 t <time> . 5 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 1 0 6 5 g7 cpu @ <number> . 3 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 4 gb ( <number> . 5 0 gb free ) | | process argv | - - crash - reporter - id acdf514f - 7 def - <number> - 8 8 b6 - fa6e0a8d7631 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - office | cwe | <number> . <number> vscode - autohotkey - plus - plus | mar | <number> . <number> perforce | mjc | <number> . <number> black - formatter | ms - | <number> . <number> pylint | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> hexeditor | ms - | <date> material - theme | zhu | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> showlangstatbar : <number> 9 6 2 ge76 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"modules type : <b> feature request </b> i installed the external module pandas using pip install pandas command in terminal . in problems its showing import "" pandas "" could not be resolved pylance vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"terminal issue type : <b> performance issue </b> why is my terminal not running my python code vs code version : code <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 4 3 8 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd 3 0 2 0 e with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : disabled_off <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 4 gb ( <number> . 2 7 gb free ) | | process argv |c :\\\\ users \ \ \ \ chdnk \ \ \ \ appdata \ \ \ \ local \ \ \ \ programs \ \ \ \ microsoft vs code \ \ \ \ code . exe - - crash - reporter - id 5 8 aadbbe - dca1 - <number> - af33 - 2 6 e4cdf0cc2d | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> window [ <number> ] ( demo . py - visual studio code ) <number> <number> <number> ptyhost <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ chdnk \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> conpty - agent <number> <number> <number> conpty - agent <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ chdnk \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ chdnk \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> gpu - process <number> <number> <number> shared - process <number> <number> <number> crashpad - handler <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ chdnk \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node c :\\ users \ \ chdnk \ \ . vscode \ \ extensions \ \ ms - python . vscode - pylance - <number> . <number> \ \ dist \ \ server . bundle . js - - cancellationreceive = file : 1 9 aab41bae3689461d83bca46521ed33608166c89f - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> window [ <number> ] ( demo . py - visual studio code ) <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ chdnk \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node c :\\ users \ \ chdnk \ \ . vscode \ \ extensions \ \ ms - python . vscode - pylance - <number> . <number> \ \ dist \ \ server . bundle . js - - cancellationreceive = file : 1 2 0 0 e540246f3ac587b529c6690b8c613b2181dee2 - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> utility - network - service <number> <number> <number> window [ <number> ] ( issue reporter ) ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` ; ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> showlangstatbar : <number> a2ce337 <time> <number> 7 ij388 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> synctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc015cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"coke project wrong checking error cs50 coke project checking is giving wrong results . i think my program is correct but checking shows error however those errors were directed to next prompts even though the project asks for next prompt . <url> version : <number> . <number> commit : 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 user agent : mozilla / <number> ( windows nt <number> ; win64 ; x64 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> edg / <number> . <number> embedder < - - generated by web issue reporter - - >",2
microsoft/vscode,"error while compiling / running c code type : <b> bug </b> <hashtag> include </hashtag> < stdio . h > <hashtag> include </hashtag> < stdlib . h > <hashtag> include </hashtag> < time . h > int main ( ) { const int min = <number> ; const int max = <number> ; int guess ; int guesses ; int answer ; srand ( time ( <number> )); answer = rand ( ) % max + min ; do { printf ( "" enter your guess : "" ); scanf ( "" % d "" , & guess ) ; if ( guess > answer ) { printf ( "" too high \ \ n "" ); } else if ( guess < answer ) { printf ( "" too low ! \ \ n "" ); } else { printf ( "" correct ! you got this \ \ n "" ); } guesses + + ; } while ( guess ! = answer ) ; printf ( "" * * * * * * * * * * * * * *\\ n "" ); printf ( "" answer : % d \ \ n "" , answer ) ; printf ( "" tries : % d \ \ n "" , guesses ) ; printf ( "" * * * * * * * * * * * * * *"") ; return <number> ; } the result must provide me with the answer and the total amount of tries ( guesses ) , i get the answer printed correctly but the number of tries is not accurate , i tried running the same code on online compilers to see if my code had some problem , the online compilers gave me the correct answer and the number of tries but there seems to be something wrong with vscode when it comes to the "" tries "" part vs code version : code <number> . <number> ( 8 b617bd08fd9e3fc94d14adb8d358b56e3f72314 , <number> - <number> - 0 6 t <time> . 4 3 8 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 4 6 0 0 h with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : enabled_on <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 4 2 gb ( <number> . 5 7 gb free ) | | process argv | - - crash - reporter - id f807df9d - ec96 - 4 8 b8 - <number> - 9 0 b59ddd654e | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - code - runner | for | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> vscrpc : <number> showlangstatbar : <number> 0 bi6i64 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",2
microsoft/vscode,"zsh : killed type : <b> bug </b> i have this problem in my terminal on my mac and in the terminal on visual studio code . when i type python3 and python3 - version it gives me zsh : killed . im new to coding and it didnt do this before im not sure how to fix it . < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> pm "" src = "" <url> vs code version : code <number> . <number> ( universal ) ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 9 2 4 z ) os version : darwin arm64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | apple m2 ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> metal : disabled_off <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 0 0 gb ( <number> . 0 5 gb free ) | | process argv | - - crash - reporter - id 6 0 b21ea6 - cb30 - 4 5 b4 - 8 a90 - 5 2 1 0 e66abae0 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes263cf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> synctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"zoom challenge < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce <number> . ! [ vsc ] ( <url>",2
microsoft/vscode,"error in calculating <number> cube to check weather <number> is armstrong number or not type : <b> bug </b> i wrote a code to check weather <number> is a armstrong number or not ( it is an armstrong number ) as per a question in vs code . when i print cube of every digit using pow it show <number> cube as <number> instead of <number> . while same code run perfectly on devc + + software . but other cubes are correct code is provided below for your ease to confirm my problem <hashtag> include </hashtag> < stdio . h > <hashtag> include </hashtag> < math . h > int main ( ) { int i = <number> , a , ans = <number> , n = <number> ; a = i ; while ( a > <number> ) { n = pow ( ( a % <number> ) , <number> ); printf ( "" cube of each digits \ \ n % d "" , n ) ; ans + = n ; a / = <number> ;} if ( i = = ans ) { printf ( "" \ \ n % d "" , ans ) ;} return <number> ;} vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i3 - 1 1 1 5 g4 @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 5 gb ( <number> . 1 9 gb free ) | | process argv | - - crash - reporter - id 1 0 b0f727 - de92 - 4 2 e6 - 8 eb1 - a3bfc470a2bb | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - code - runner | for | <number> . <number> c - cpp - runner | fra | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> vscode - lldb | vad | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"coming out of program type : <b> bug </b> my code automatically comes out after taking only one input vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i7 - 1 2 6 5 0 h ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 6 8 gb ( <number> . 8 6 gb free ) | | process argv | - - crash - reporter - id 5 5 5 9 ba75 - 7 fa7 - 4 6 c0 - 8 7 ef - 6 5 b447fd9419 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - blackbox | bla | <date> vscode - html - css | ecm | <number> . <number> editorconfig | edi | <number> . <number> vsc - material - theme | equ | <number> . <number> vsc - material - theme - icons | equ | <number> . <number> prettier - vscode | esb | <number> . <number> code - runner | for | <number> . <number> chatgpt - vscode | gen | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> java | red | <number> . <number> java - generate - setters - getters | soh | <number> . <number> code - spell - checker | str | <number> . <number> tabnine - vscode | tab | <number> . <number> cmake | twx | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> javascriptsnippets | xab | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes5 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> 9 6 2 ge76 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"i can not run the code type : <b> bug </b> when i try to run my come , vs shows "" timed out waiting launcher to connect "" i search for all info in internet but nothing helped vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 5 6 0 0 h with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 6 gb ( <number> . 2 0 gb free ) | | process argv | - - crash - reporter - id fbfc43f9 - 8 b65 - 4 5 8 d - bbea - 1 e1d06f60383 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - gc - excelviewer | gra | <date> json2csv | kha | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> atom - keybindings | ms - | <number> . <number> vscode - icons | vsc | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> vscrpc : <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> a2ce337 <time> <number> 5 7 b7757 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"markdown files does not show as expected type : <b> feature request </b> i am testing intellij after using vs code for a year and i have noticed that md - files display more like i expect in intellij . that ' s a pitty . both are not opimal though since marked lists are not very visible i intellij and absent in vs code . <url> example - [x ] write the press release - [ ] update the website - [ ] contact the media definition lists : definition vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"extension name too lang cause ui not good < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > see this picture , when extension name too lang ， ui seems ! [ image ] ( <url>",2
microsoft/vscode,"same program , different outputs type : <b> bug </b> <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> <cmath> using namespace std ; int main ( ) { cout < < "" enter a number : "" ; int n ; cin > > n ; int i = <number> , bit = <number> , binary = <number> ; cout < < n ; while ( n = <number> ) { bit = n & <number> ; / / to store a number in same order binary = ( bit*pow <censored> ( <number> , i ) ) + binary ; i + + ; n = n > > <number> ; } cout < < "" in binary is "" < < binary < < endl ; } /* let us take n = <number> , then the output should be <number> , but on visual studio this program gives it to be <number> . i copied the same program and ran it on some online c + + compiler and got the desired output i . e <number> */ vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i5 - 1 2 3 5 u ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 3 gb ( <number> . 9 2 gb free ) | | process argv | - - crash - reporter - id 7 4 6 3 add3 - 7 d3b - <number> - a81a - 0 3 f05de84b90 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - c - cpp - compile - run | dan | <date> code - runner | for | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> java | red | <number> . <number> cmake | twx | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 bi6i64 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",2
microsoft/vscode,"settings . json without noticing i removed something in the code , in settings . json and it now shows incorrect type . expected "" null "" . [ ln4 , col <number> ] . and as a beginner in coding i have no idea how to correct that . help me please . i just need the original to copy and paste it back . [ <number> ] ( <url> ! [ <number> ] ( <url> { "" python . defaultinterpreterpath "" : "" c :\\\\ users \ \ \ \ hp \ \ \ \ appdata \ \ \ \ local \ \ \ \ programs \ \ \ \ python \ \ \ \ python311 \ \ \ \ python . exe "" , "" files . autosave "" : "" onfocuschange "" , "" terminal . integrated . automationprofile . windows "" : { } , "" code - runner . runinterminal "" : true , "" code - runner . executormap "" : { "" javascript "" : "" node "" , "" java "" : "" cd $ dir & & javac $ filename & & java $ filenamewithoutext "" , "" c "" : "" cd $ dir & & gcc $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" zig "" : "" zig run "" , "" cpp "" : "" cd $ dir & & g + + $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" objective - c "" : "" cd $ dir & & gcc - framework cocoa $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" php "" : "" php "" , "" python "" : "" python - u "" , "" perl "" : "" perl "" , "" perl6 "" : "" perl6 "" , "" ruby "" : "" ruby "" , "" go "" : "" go run "" , "" lua "" : "" lua "" , "" groovy "" : "" groovy "" , "" powershell "" : "" powershell - executionpolicy bypass - file "" , "" bat "" : "" cmd / c "" , "" shellscript "" : "" bash "" , "" fsharp "" : "" fsi "" , "" csharp "" : "" scriptcs "" , "" vbscript "" : "" cscript / / nologo "" , "" typescript "" : "" ts - node "" , "" coffeescript "" : "" coffee "" , "" scala "" : "" scala "" , "" swift "" : "" swift "" , "" julia "" : "" julia "" , "" crystal "" : "" crystal "" , "" ocaml "" : "" ocaml "" , "" r "" : "" rscript "" , "" applescript "" : "" osascript "" , "" clojure "" : "" lein exec "" , "" haxe "" : "" haxe - - cwd $ dirwithouttrailingslash - - run $ filenamewithoutext "" , "" rust "" : "" cd $ dir & & rustc $ filename & & $ dir $ filenamewithoutext "" , "" racket "" : "" racket "" , "" scheme "" : "" csi - script "" , "" ahk "" : "" autohotkey "" , "" autoit "" : "" autoit3 "" , "" dart "" : "" dart "" , "" pascal "" : "" cd $ dir & & fpc $ filename & & $ dir $ filenamewithoutext "" , "" d "" : "" cd $ dir & & dmd $ filename & & $ dir $ filenamewithoutext "" , "" haskell "" : "" runghc "" , "" nim "" : "" nim compile - - verbosity : <number> - - hints : off - - run "" , "" lisp "" : "" sbcl - - script "" , "" kit "" : "" kitc - - run "" , "" v "" : "" v run "" , "" sass "" : "" sass - - style expanded "" , "" scss "" : "" scss - - style expanded "" , "" less "" : "" cd $ dir & & lessc $ filename $ filenamewithoutext . css "" , "" fortranfreeform "" : "" cd $ dir & & gfortran $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" fortran - modern "" : "" cd $ dir & & gfortran $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" fortran_fixed - form "" : "" cd $ dir & & gfortran $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" fortran "" : "" cd $ dir & & gfortran $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" sml "" : "" cd $ dir & & sml $ filename "" } , "" terminal . integrated . defaultprofile . windows "" : "" command prompt "" , "" code - runner . clearpreviousoutput "" : true , "" code - runner . executormapbyfileextension "" : { "" . vb "" : "" cd $ dir & & vbc / nologo $ filename & & $ dir $ filenamewithoutext "" , "" . vbs "" : "" cscript / / nologo "" , "" . scala "" : "" scala "" , "" . jl "" : "" julia "" , "" . cr "" : "" crystal "" , "" . ml "" : "" ocaml "" , "" . zig "" : "" zig run "" , "" . exs "" : "" elixir "" , "" . hx "" : "" haxe - - cwd $ dirwithouttrailingslash - - run $ filenamewithoutext "" , "" . rkt "" : "" racket "" , "" . scm "" : "" csi - script "" , "" . ahk "" : "" autohotkey "" , "" . au3 "" : "" autoit3 "" , "" . kt "" : "" cd $ dir & & kotlinc $ filename - include - runtime - d $ filenamewithoutext . jar & & java - jar $ filenamewithoutext . jar "" , "" . kts "" : "" kotlinc - script "" , "" . dart "" : "" dart "" , "" . pas "" : "" cd $ dir & & fpc $ filename & & $ dir $ filenamewithoutext "" , "" . pp "" : "" cd $ dir & & fpc $ filename & & $ dir $ filenamewithoutext "" , "" . d "" : "" cd $ dir & & dmd $ filename & & $ dir $ filenamewithoutext "" , "" . hs "" : "" runhaskell "" , "" . nim "" : "" nim compile - - verbosity : <number> - - hints : off - - run "" , "" . csproj "" : "" dotnet run - - project "" , "" . fsproj "" : "" dotnet run - - project "" , "" . lisp "" : "" sbcl - - script "" , "" . kit "" : "" kitc - - run "" , "" . v "" : "" v run "" , "" . vsh "" : "" v run "" , "" . sass "" : "" sass - - style expanded "" , "" . cu "" : "" cd $ dir & & nvcc $ filename - o $ filenamewithoutext & & $ dir $ filenamewithoutext "" , "" . ring "" : "" ring "" , "" . sml "" $ dir & & sml $ filename "" } }",2
microsoft/vscode,"intellisense type : <b> performance issue </b> in my laptop the intellisense is not working even after re installing it from the website vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 8 2 5 0 u cpu @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 8 7 gb ( <number> . 6 4 gb free ) | | process argv | - - crash - reporter - id c9ec38c8 - 1 f08 - 4 5 ed - bf55 - 4 e3bd456d151 | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> utility - network - service <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node c :\\ users \ \ user \ \ . vscode \ \ extensions \ \ formulahendry . auto - rename - tag - <date> \ \ packages \ \ server \ \ dist \ \ servermain . js - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ json - language - features \ \ server \ \ dist \ \ node \ \ jsonservermain "" - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node c :\\ users \ \ user \ \ . vscode \ \ extensions \ \ pranaygp . vscode - css - peek - <number> . <number> \ \ server \ \ out \ \ server . js - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> crashpad - handler <number> <number> <number> shared - process <number> <number> <number> gpu - process <number> <number> <number> ptyhost <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> conpty - agent <number> <number> <number> conpty - agent <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ user \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> window [ <number> ] ( ● app . js - practice - visual studio code ) <number> <number> <number> window [ <number> ] ( issue reporter ) ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( ● app . js - practice - visual studio code ) | folder ( practice ) : <number> files | file types : js ( <number> ) json ( <number> ) png ( <number> ) css ( <number> ) gitignore ( <number> ) ico ( <number> ) html ( <number> ) | txt ( <number> ) md ( <number> ) svg ( <number> ) | conf files ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - simple - react - snippets | bur | <number> . <number> es7 - react - js - snippets | dsz | <number> . <number> vscode - html - css | ecm | <number> . <number> prettier - vscode | esb | <number> . <number> auto - rename - tag | for | <date> vscode - css - peek | pra | <number> . <number> liveserver | rit | <number> . <number> html5 - boilerplate | sid | <number> . <number> open - in - browser | tec | <number> . <number> vscode - icons | vsc | <number> . <number> javascriptsnippets | xab | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"multiple file does not open in new tab type : <b> feature request </b> when a file is open then if i wanna open a new file it replace the tab with older one not in the new tab . vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"all multi - cursors now disappear when navigating by word type : <b> bug </b> take any test with multiple lines . use ctrl + shift + alt - down to create a multi - cursor , one per line . navigating by character ( left / right ) works as expected : all cursors navigate . navigating by line ( up / down or home / end ) also works as expected , and all cursors do it . navigating by word ( ctrl + left / right ) does not . all cursors but one disappear as soon as you do this . this just started happening out of the blue , possibly after an update . i do not know ( just came back from vacation ) . vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : connection to ' ssh : ttdrazzle ' could not be established canceled <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 7 9 5 0 x <number> - core processor ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 1 4 gb ( <number> . 8 0 gb free ) | | process argv | - - crash - reporter - id 5 1 1 5 2 fc6 - 5 7 cf - 4 5 c0 - ba26 - 7 ec617922cec | | screen reader | yes | | vm | <percent> | connection to ' ssh could not be established canceled </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - midl3 - language - server | ale | <date> vscode - research | dev | <number> . <number> vulnerability - extension | dev | <number> . <number> xml | dot | <number> . <number> gitlens | eam | <number> . <number> x8664assembly | fre | <number> . <number> vscode - pull - request - github | git | <number> . <number> todo - tree | gru | <date> vscode - graphviz | joa | <number> . <number> tdpcode | mic | <number> . <number> vscode - azdo - codereview | mic | <number> . <number> wavework | mic | <number> . <number> csharp | ms - | <date> vscode - dotnet - runtime | ms - | <number> . <number> sarif - viewer | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> remote - server | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> windbg - debug | rez | <number> . <number> cmake | twx | <date> markdown - all - in - one | yzh | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd2263cf : <number> vscaat : <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> asynctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"java compilation error type : <b> bug </b> java compilation error vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i3 - 1 1 1 5 g4 @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 0 gb ( <number> . 4 3 gb free ) | | process argv | - - crash - reporter - id efb292f6 - 6 bae - 4 3 c1 - 9 ac1 - d943ac88d6ec | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - bracket - pair - colorizer - <number> | coe | <number> . <number> prettier - vscode | esb | <number> . <number> auto - rename - tag | for | <date> vscode - javac | geo | <date> vscode - power - mode | hoo | <number> . <number> remote - wsl | ms - | <number> . <number> powershell | ms - | <number> . <number> vscode - thunder - client | ran | <number> . <number> java | red | <number> . <number> liveserver | rit | <number> . <number> p5 - vscode | sam | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> es7 - react - js - snippets | woo | <number> . <number> reactsnippets | xab | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vscaac : <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"how to detect if there is a snippets list in the editor ? < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > how to detect if there is a snippets list in the editor as user type keystrokes ? by meaning snippets list , i mean dropdown menu in the picture . thanks . i am building a completion provider which want to trigger only when there is no dropdown suggestions list . how to achieve it ? ! [ screenshot <number> - <number> - <number> at <number> <number> <number> pm ] ( <url>",2
microsoft/vscode,"about winerror <number> when i import eel lib on my python code it keeps shown me this error on "" eel . start ( ' index . html ' ) "" is there a problem with my code or it ' s a bug ?",2
microsoft/vscode,"transform function not working type : <b> bug </b> i am using css transform function which is not working vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 2 5 0 0 u with radeon vega mobile gfx ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 6 gb ( <number> . 3 1 gb free ) | | process argv | - - crash - reporter - id 7 ddaedf5 - adeb - 4 e7a - aa82 - 0 2 d910600357 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - tailwindshades | bou | <number> . <number> turbo - console - log | cha | <number> . <number> es7 - react - js - snippets | dsz | <number> . <number> vscode - html - css | ecm | <number> . <number> prettier - vscode | esb | <number> . <number> vscode - typescript - next | ms - | <number> . <number> sqltools | mtx | <number> . <number> liveserver | rit | <number> . <number> vs - code - prettier - eslint | rve | <number> . <number> ayu | tea | <number> . <number> javascriptsnippets | xab | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> 7 ij388 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> synctok : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"problema com salvar as comfigurações do vscode type : <b> bug </b> eu instalai a extensão dracula , porém quando eu fecho o vscode ela simplesmente sai e eu tenho que reabilitar ela vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 5 4 0 0 u with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 1 gb ( <number> . 3 2 gb free ) | | process argv |c :\\\\ users \ \ \ \ familia \ \ \ \ onedrive \ \ \ \ documentos \ \ \ \ estudos \ \ \ \ html - css \ \ \ \ exercicios \ \ \ \ ex014 - - crash - reporter - id 3 c8e9e65 - b787 - 4 6 5 e - baa9 - ba80177a6eea | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - codesnap | adp | <number> . <number> bracket - pair - color - dlw | bra | <number> . <number> vscode - language - pack - pt - br | ms - | <number> . <phone> color - highlight | nau | <number> . <number> indent - rainbow | ode | <number> . <number> material - icon - theme | pki | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vscaac : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"<hashtag> include </hashtag> path error type : <b> bug </b> my include path error is coming again and again . kindly resolve my issue . i have just done everything changing its include path like everything which i can do form my side now pls help me to resolve this issue . vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i3 - 1 1 1 5 g4 @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 3 gb ( <number> . 8 2 gb free ) | | process argv | - - crash - reporter - id 7 a7d5bee - 7 e26 - 4 6 9 e - b5f5 - 4 f06501ac25c | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - exe - runner | bra | <number> . <number> code - runner | for | <number> . <number> icie | mik | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyonecf : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"how can i close the auto save i want to control saving ? add issue description here version : <number> . <number> commit : 6 c3e3dba23e8fadc360aed75ce363ba185c49794 user agent : mozilla / <number> ( windows nt <number> ; win64 ; x64 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> edg / <number> . <number> embedder < - - generated by web issue reporter - - >",2
microsoft/vscode,"in my vs code t is not working whenever i enter t key vs code give an error by saying the key combination ( t , t ) is not command please help me < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce <number> . ` ` ` [ tasklist ] # # # tasks ` ` `",2
microsoft/vscode,"> sign has accidentally replaced $ sign at cursor in terminal window of vs code . how can i get $ sign back at cursor add issue description here version : <number> . <number> commit : 6 c3e3dba23e8fadc360aed75ce363ba185c49794 user agent : mozilla / <number> ( windows nt <number> ; win64 ; x64 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> embedder < - - generated by web issue reporter - - >",2
microsoft/vscode,"issues in c + + program running i am running c + + program and i it not running properly , i have tried run in terminal only it is not running at all . { fix this }",2
microsoft/vscode,"bug in in - built function pow ( power function ) . type : <b> bug </b> hello developers hope you are doing well . so while solving my class asignment problem i have found bug in following program as i am getting unexpected output for certain value that is for input = <number> ; for other values it is working fine but on entering input <number> it is printing unexpected output . code - <hashtag> include </hashtag> < stdio . h > <hashtag> include </hashtag> < math . h > int power ( int a ) ; int main ( ) { int a = <number> ; int k = power ( a ) ; printf ( "" pow is = % d "" , k ) ; } int power ( int a ) { int b = pow ( a , <number> ); return b ; } vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 3 5 g7 @ <number> . 4 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 6 gb ( <number> . 9 4 gb free ) | | process argv | - - crash - reporter - id fa9f32f2 - 7 2 d6 - 4 8 d1 - 9 4 8 e - bdac79ea866a | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - exe - runner | bra | <number> . <number> prettier - vscode | esb | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> indent - rainbow | ode | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vscaac : <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"armstrong3 . c we have written the needed data into your clipboard because it was too large to send . please paste type : <b> performance issue </b> <hashtag> include </hashtag> < stdio . h > <hashtag> include </hashtag> < math . h > int main ( ) { int n , rem , p , s = <number> ; printf ( "" \ \ nenter number :"") ; scanf ( "" % d "" , & n ) ; p = n ; while ( n > <number> ) { rem = n % <number> ; s = s + pow ( rem , <number> ); n = n / <number> ; } if ( s = = p ) { printf ( "" \ \ n % d is a armstrong number "" , p ) ; } else { printf ( "" \ \ nsum % d "" , s ) ; printf ( "" \ \ nnot a armstrong number "" ); } return <number> ; } the output is showing me <number> and i even tried the same source code on other ide ' s they worked well and gave me expected output as <number> please solve this issue , i talked to my friend and he experienced the same thing why is this happening . vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i5 - 1 1 3 0 0 h @ <number> . 1 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 7 9 gb ( <number> . 2 2 gb free ) | | process argv | - - crash - reporter - id 0 0 c502d4 - 7 eb0 - 4 e69 - 8 ccb - <phone> eb | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> window [ <number> ] ( issue reporter ) <number> <number> <number> gpu - process <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ ms - vscode . cpptools - <number> . <number> - win32 - x64 \ \ bin \ \ cpptools . exe "" <number> <number> <number> c :\\ windows \ \ system32 \ \ conhost . exe 0x 4 <number> <number> <number> "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ ms - vscode . cpptools - <number> . <number> - win32 - x64 \ \ bin \ \ cpptools . exe "" <number> <number> <number> "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ ms - vscode . cpptools - <number> . <number> - win32 - x64 / bin / cpptools - srv . exe "" <number> { 3 c644793 - b351 - 4 5 b1 - 9 7 3 c - cf71be990fc8 } <number> <number> <number> c :\\ windows \ \ system32 \ \ conhost . exe 0x 4 <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ formulahendry . auto - rename - tag - <date> \ \ packages \ \ server \ \ dist \ \ servermain . js "" - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ redhat . java - <number> . <number> - win32 - x64 \ \ jre \ \ <number> . <number> - win32 - x86_64 \ \ bin \ \ java "" - - add - modules = all - system - - add - opens java . base / java . util = all - unnamed - - add - opens java . base / java . lang = all - unnamed - - add - opens java . base / sun . nio . fs = all - unnamed - declipse . application = org . eclipse . jdt . ls . core . id1 - dosgi . bundles . defaultstartlevel = <number> - declipse . product = org . eclipse . jdt . ls . core . product - djava . import . generatesmetadatafilesatprojectroot = false - dfile . encoding = utf8 - xx : + useparallelgc - xx : gctimeratio = <number> - xx : adaptivesizepolicyweight = <number> - dsun . zip . disablememorymapping = true - xmx1g - xms100m - xlog : disable "" - javaagent <sad> :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ redhat . java - <number> . <number> - win32 - x64 \ \ lombok \ \ lombok - <date> . jar "" - xx : + heapdumponoutofmemoryerror "" - xx : heapdumppath =c :\\ users \ \ tushal dewasi \ \ appdata \ \ roaming \ \ code \ \ user \ \ workspacestorage \ \ bfd811242260c75c11defd2d2b60bc80 \ \ redhat . java "" - daether . dependencycollector . impl = bf - jar "" c :\\ users \ \ tushal dewasi \ \ . vscode \ \ extensions \ \ redhat . java - <number> . <number> - win32 - x64 \ \ server \ \ plugins \ \ org . eclipse . equinox . launcher_1 . <number> . v20230717 - <number> . jar "" - configuration "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ roaming \ \ code \ \ user \ \ globalstorage \ \ redhat . java \ \ <number> . <number> \ \ config_win "" - data "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ roaming \ \ code \ \ user \ \ workspacestorage \ \ bfd811242260c75c11defd2d2b60bc80 \ \ redhat . java \ \ jdt_ws "" <number> <number> <number> utility - network - service <number> <number> <number> window [ <number> ] ( armstrong3 . c - untitled ( workspace ) - visual studio code ) <number> <number> <number> crashpad - handler <number> <number> <number> shared - process <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ bootstrap - fork "" ms - vscode . cppdbg "" { \ \ "" common . vscodemachineid \ \"": \ \ "" cd7ed736ee8427943b89b704cff0647a508ec907f33e7a6c4ad5b0f79a12bd09 \ \ "" , \ \ "" common . vscodesessionid \ \"": \ \ "" f87104dd - dd6c - 4 d67 - a458 - 7 5 6 de437d35a1691844211630 \ \ "" } "" 0 c6ae279ed8443289764825290e4f9e2 - 1 a736e7c - <number> - <number> - be46 - fc2a58ae4d14 - <number> ) <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> ptyhost <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ tushal dewasi \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> conpty - agent ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( armstrong3 . c - untitled ( workspace ) - visual studio code ) | folder ( coding ) : <number> files | file types exe ( <number> ) py ( <number> ) cpp ( <number> ) class ( <number> ) java ( <number> ) | conf files : ; ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - django | bat | <number> . <number> python - environment - manager | don | <number> . <number> python - extension - pack | don | <number> . <number> vscode - html - css | ecm | <number> . <number> prettier - vscode | esb | <number> . <number> auto - close - tag | for | <date> auto - rename - tag | for | <date> code - runner | for | <number> . <number> vsc - python - indent | kev | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> autodocstring | njp | <number> . <number> java | red | <number> . <number> liveserver | rit | <number> . <number> cmake | twx | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> jinja | who | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"cant run my code properly type : <b> feature request </b> respected mam / sir i am humbly requesting kindly fix my vs code via any online connecting souces like anydesk its showing "" open . launch . json "" . kindly help me out i am stuck in this loop for this i will be forever thankful to you guys vs code version : code <number> . <number> ( 6 c3e3dba23e8fadc360aed75ce363ba185c49794 , <number> - <number> - 0 9 t <time> . 1 7 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"custom view container menu action contribute < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i want to be able to add actions to custom view container . i have tried : ` ` ` json "" commands "" : [ { "" command "" : "" test_command_id "" , "" title "" : "" test command "" , "" icon "" : "" $( zap ) "" } ] , "" viewscontainers "" : { "" activitybar "" : [ { "" id "" : "" custiom_view_container "" , "" title "" : "" test title "" , "" icon "" : "" $( zap ) "" } ] } , "" menus "" : { "" view / title "" : [ { "" command "" : "" test_command_id "" , "" when "" : "" view = = custiom_view_container "" , "" group "" : "" navigation "" } ] , "" custiom_view_container / title "" : [ { "" command "" : "" test_command_id "" , "" when "" : "" true "" , "" group "" : "" navigation "" } ] } ` ` ` non of this works , however i am able to add command for built in containers : ` ` ` json "" menus "" : { "" scm / title "" : [ { "" command "" : "" test_command_id "" , "" when "" : "" true "" , "" group "" } ] } ` ` `",2
microsoft/vscode,"bug in codespace type : <b> bug </b> in codespace jupyter template : when i create a new jupyter template and run the first python clock in notebooks / image - classifier . ipynb , it shows an error : modulenotfounderror : no module named ' _lzma ' ( caused by the second line "" import torchvision "" ) vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 4 8 0 0 u with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 7 gb ( <number> . 0 7 gb free ) | | process argv | - - crash - reporter - id 8 0 6 7 c5ac - fbff - <number> - b384 - 5 ccac3278cfc | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - codespaces | git | <date> vscode - pylance | ms - | <number> . <number> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cpptools | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"subscribing to the response of cli command i am developing an extension , and there is a cli command which subscribes to server and keep listening to it until stopped manually . i want to use this command to implement a feature in the extension , so that it will keep listening to the server and whenever there is some event happened on server side it will give a response and we can read it and use it . this command will get fired as soon as the extension activates and will stop when extension deactivates . i tried executing with spawn ( ) but it executed the command as there was not response at that time from the cli command . it got exit how can i implement this ?",2
microsoft/vscode,"import type : <b> bug </b> i have installed polars but it is not importing in the code says module not found error vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 8 5 5 0 u cpu @ <number> . 8 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 0 gb ( <number> . 9 5 gb free ) | | process argv | - - crash - reporter - id 7 3 8 5 7 6 e0 - ec7f - <number> - af <elongated> - 2 e7fa748845c | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - matlab - formatter | aff | <number> . <number> matlab - interactive - terminal | apo | <number> . <number> matlab - extension - pack | bat | <number> . <number> matlab - code - run | bra | <number> . <number> sld - docker - builder | del | <number> . <number> vscode - npm - script | eg2 | <date> docker - explorer | for | <number> . <number> docker - extension - pack | for | <number> . <number> docker - run | geo | <number> . <number> matlab | gim | <number> . <number> composer | ika | <number> . <number> mongodb - vscode | mon | <number> . <number> vscode - docker | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> docker - compose |p 1 c | <number> . <number> vsc - octave - debugger | pau | <date> sas | pvp | <number> . <number> sas - lsp | sas | <number> . <number> sasjs - for - vscode | sas | <number> . <number> matlab - complete | sla | <number> . <number> octave - debug | tia | <number> . <number> intellicode - api - usage - examples | vis | <number> . <number> vscode - java - debug | vsc | <number> . <number> wordup - code | wor | <number> . <number> php - debug | xde | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> f6dab26 <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"test results tab in vs code keep popping up even if closed after each test run type : <b> bug </b> * * context : * * - playwright version : <number> . <number> - operating system : mac - node . js version : v20 . <number> - visual studio code version : version : <number> . <number> commit : 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 date : <number> - <number> - 0 2 t <time> . 7 2 2 z electron : <date> electronbuildid : <number> chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os : darwin arm64 <number> . <number> - playwright for vscode extension version : v1 . <number> when running a test - the tab with test results always pops - up . this started happening with recent vs code update . no matter what i do - it keeps popping - up . before it was never showed up if not on demand . vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 7 2 2 z ) os version : darwin arm64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | apple m1 pro ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> metal : disabled_off <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 0 0 gb ( <number> . 4 0 gb free ) | | process argv | - - crash - reporter - id 3 dc3f68f - 2 3 e6 - 4 d9e - a0b7 - 6 1 4 e6fcd461a | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - atlascode | atl | <number> . <number> vscode - generate - getter - setter | dsk | <number> . <number> gitlens | eam | <number> . <number> prettier - vscode | esb | <number> . <number> code - runner | for | <number> . <number> rainbow - csv | mec | <number> . <number> template - string - converter | meg | <number> . <number> vscode - azureresourcegroups | ms - | <number> . <number> vscode - azurevirtualmachines | ms - | <number> . <number> playwright | ms - | <date> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> azure - account | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> remote - server | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> vscode - thunder - client | ran | <number> . <number> vscode - yaml | red | <number> . <number> code - spell - checker | str | <number> . <number> icons | tal | <number> . <number> tcv - typescript - constructor - generator | toa | <number> . <number> colonize | vms | <number> . <number> gistfs | vsl | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> 7 ij388 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - > ! [ image ] ( <url>",2
microsoft/vscode,"i want to put my name in output type : <b> feature request </b> i want to my name in output in default vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"problems with the fetch api type : <b> bug </b> i am not sure if this is really a problem with vs code or not . i use the fetch api in javascript . when i run the fetch directly in javascript with a chrome browser , i have no problems . the fetch api executes fine . a very important point is that i do not see an options request being sent . however , when i run the same fetch using exactly the same javascript ( using vs code ) i do see an options being sent . even worse , the fetch ( under javascript , under chrome , under vs code ) does not wait for the options to complete . the fetch just fails with a ' failed to fetch ' error . i do not understand this . why should vs code make any difference ? what am i doing wrong ? what additional information should i provide ? how should this problem be debugged ? this is not a question . this is ( probably ) a bug . vsc has a behavior that native javascript does not . either vsc should be fixed or some sort of circumvention should be devised . vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 4 7 0 2 hq cpu @ <number> . 2 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : unavailable_off <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 9 1 gb ( <number> . 0 5 gb free ) | | process argv | - - folder - uri file :/// c % 3 a / users / peter / documents / visual % 2 0 studio % 2 0 code / projects / webapplication5 / webapplication5 - - crash - reporter - id 7 c17602d - d116 - 4 1 bf - 9 eeb - 6 f288773aaef | | screen reader | no | | vm | <percent> | </details> extensions <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"problems with the fetch api type : <b> bug </b> i am not sure if this is really a problem with vs code or not . i use the fetch api in javascript . when i run the fetch directly in javascript with a chrome browser , i have no problems . the fetch api executes fine . a very important point is that i do not see an options request being sent . however , when i run the same fetch using exactly the same javascript ( using vs code ) i do see an options being sent . even worse , the fetch ( under javascript , under chrome , under vs code ) does not wait for the options to complete . the fetch just fails with a ' failed to fetch ' error . i do not understand this . why should vs code make any difference ? what am i doing wrong ? what additional information should i provide ? how should this problem be debugged ? vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 4 7 0 2 hq cpu @ <number> . 2 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : unavailable_off <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 1 gb ( <number> . 7 9 gb free ) | | process argv | - - folder - uri file :/// c % 3 a / users / peter / documents / visual % 2 0 studio % 2 0 code / projects / webapplication5 / webapplication5 - - crash - reporter - id 7 c17602d - d116 - 4 1 bf - 9 eeb - 6 f288773aaef | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - copilot | git | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> live - server | ms - | <number> . <number> liveserver | rit | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> f6dab26 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"webpages did not load in chrome [ type :]( url ) <b> performance issue </b> when i run the html , css , js code it shows again and again localhost : <number> , localhost refused error i cant load my webpages i feel great if you reply me vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 1 th gen intel ( r ) core ( tm ) i3 - 1 1 1 5 g4 @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 6 5 gb ( <number> . 5 4 gb free ) | | process argv | - - crash - reporter - id 6 3 f06613 - <number> - 4 4 e6 - <number> - e04554d095e2 | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ program files \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node - - max - old - space - size = <number> "" c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js "" - - useinferredprojectperprojectroot - - enabletelemetry - - cancellationpipename c :\\ users \ \ hp \ \ appdata \ \ local \ \ temp \ \ vscode - typescript \ \ 6 1 2 6 cc3695a9be66c6ba \ \ tscancellation - b10b20c4d0a60e94376a . tmp * - - locale en - - nogeterronbackgroundupdate - - validatedefaultnpmlocation - - usenodeipc ) <number> <number> <number> electron - nodejs ( "" c :\\ program files \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c <annoyed> program files / microsoft vs code / resources / app / extensions / node_modules / typescript / lib / typingsinstaller . js "" - - globaltypingscachelocation c <annoyed> users / hp / appdata / local / microsoft / typescript / <number> - - enabletelemetry - - typesmaplocation "" c <annoyed> program files / microsoft vs code / resources / app / extensions / node_modules / typescript / lib / typesmap . json "" - - validatedefaultnpmlocation ) <number> <number> <number> electron - nodejs ( "" c :\\ program files \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node - - max - old - space - size = <number> "" c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js "" - - servermode partialsemantic - - useinferredprojectperprojectroot - - disableautomatictypingacquisition - - cancellationpipename c :\\ users \ \ hp \ \ appdata \ \ local \ \ temp \ \ vscode - typescript \ \ 6 1 2 6 cc3695a9be66c6ba \ \ tscancellation - ce1d0f50734ce3c1f720 . tmp * - - locale en - - nogeterronbackgroundupdate - - validatedefaultnpmlocation - - usenodeipc ) <number> <number> <number> shared - process <number> <number> <number> window [ <number> ] ( app . js - js - visual studio code ) <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> crashpad - handler <number> <number> <number> window [ <number> ] ( issue reporter ) <number> <number> <number> gpu - process <number> <number> <number> utility - network - service ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( app . js - js - visual studio code ) | folder ( js ) : <number> files | file types html ( <number> ) | conf files : ; ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - auto - close - tag | for | <date> code - runner | for | <number> . <number> remote - wsl | ms - | <number> . <number> liveserver | rit | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"error type : <b> bug </b> user microsoft windows [ versión <number> . <number> ] ( c ) microsoft corporation . todos los derechos reservados . c :\\ users \ \ primo \ \ onedrive \ \ escritorio \ \ otro \ \ clontwitter > rails s => booting puma => rails <number> . <number> application starting in development => run ` bin / rails server - - help ` for more startup options * * * sigusr2 not implemented , signal based restart unavailable * * * sigusr1 not implemented , signal based restart unavailable ! * * * sighup not implemented , signal based logs reopening unavailable ! puma starting in single mode . <repeated> * version <number> . <number> ( ruby <number> . <number> - p185 ) , codename : spoony bard * min threads : <number> , max threads : <number> * environment : development exiting c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` initialize ' : only one usage of each socket address ( protocol / network address / port ) is normally permitted . - bind ( <number> ) for "" <number> . <number> "" port <number> ( errno : : eaddrinuse ) from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` new ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` add_tcp_listener ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` block in add_tcp_listener ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` each ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` add_tcp_listener ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` block in parse ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` each ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / binder . rb : <number> : in ` parse ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / runner . rb : <number> : in ` load_and_bind ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / single . rb : <number> : in ` run ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / puma / launcher . rb : <number> : in ` run ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / puma - <number> . <number> / lib / rack / handler / puma . rb : <number> : in ` run ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / rack - <number> . <number> / lib / rack / server . rb : <number> : in ` start ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / commands / server / server_command . rb : <number> : in ` start ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / commands / server / server_command . rb : <number> : in ` block in perform ' from < internal : kernel > : <number> : in ` tap ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / commands / server / server_command . rb : <number> : in ` perform ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / thor - <number> . <number> / lib / thor / command . rb : <number> : in ` run ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / thor - <number> . <number> / lib / thor / invocation . rb : <number> : in ` invoke_command ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / thor - <number> . <number> / lib / thor . rb : <number> : in ` dispatch ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / command / base . rb : <number> : in ` perform ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / command . rb : <number> : in ` invoke ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / railties - <number> . <number> / lib / rails / commands . rb : <number> : in ` <main> ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / bootsnap - <number> . <number> / lib / bootsnap / load_path_cache / core_ext / kernel_require . rb : <number> : in ` require ' from c <annoyed> ruby31 - x64 / lib / ruby / gems / <number> . <number> / gems / bootsnap - <number> . <number> / lib / bootsnap / load_path_cache / core_ext / kernel_require . rb : <number> : in ` require ' from bin / rails : <number> : in ` <main> ' vs code version : code <number> . <number> ( 2 ccd690cbff1569e4a83d7c43d45101f817401dc , <number> - <number> - 2 7 t <time> . 9 0 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 9 7 5 0 h cpu @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 8 gb ( <number> . 8 5 gb free ) | | process argv |c :\\\\ users \ \ \ \ primo \ \ \ \ onedrive \ \ \ \ escritorio \ \ \ \ ejercitando - - crash - reporter - id ddbab503 - a4a8 - 4 7 c1 - aa4e - da7cea102e39 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - rails - partial | aki | <number> . <number> ruby - and - rails - snippets | cja | <number> . <number> codespaces | git | <date> remotehub | git | <number> . <number> vscode - github - actions | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> beautify | hoo | <number> . <number> rails - snippets | hri | <number> . <number> vscode - peacock | joh | <number> . <number> vscode - language - pack - es | ms - | <number> . <phone> remote - wsl | ms - | <number> . <number> azure - repos | ms - | <number> . <number> live - server | ms - | <number> . <number> remote - repositories | ms - | <number> . <number> indent - rainbow | ode | <number> . <number> sqlite - viewer | qwt | <number> . <number> vscode - thunder - client | ran | <number> . <number> ruby | reb | <number> . <number> liveserver | rit | <number> . <number> docxreader | sha | <number> . <number> vscode - icons | vsc | <number> . <number> vscode - ruby | win | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> <number> <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonfmttext : <number> pythoncmvfstrcf : <number> 9 b8hh23 <time> <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxptcf : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",2
microsoft/vscode,"can not connect emulators type : <b> bug </b> i have tries all posible answers but no luck . path ; re - installation , etc . i can not test my apps . i have windows <number> home edition . vs code version : code <number> . <number> ( 6 4 4 5 d93c81ebe42c4cbd7a60712e0b17d9463e97 , <number> - <number> - 0 2 t <time> . 4 8 5 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 4 2 1 0 u cpu @ <number> . 7 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 1 gb ( <number> . 8 8 gb free ) | | process argv | - - crash - reporter - id 7 7 e77cea - 8 9 3 a - 4 2 b9 - ad6c - 1 c3dd2e42ab1 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - dart - code | dar | <number> . <number> flutter | dar | <number> . <number> emulate | die | <number> . <number> gitlens | eam | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyonecf : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofb : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"trying to get to srcpf on pub400 type : <b> bug </b> i am getting an odd error trying to access my srcpf ' s on pub400 . getting cpf2207 : not authorized to ise object ileditor in library qsys type * lib . can anyone assist ? vs code version : code <number> . <number> ( 2 ccd690cbff1569e4a83d7c43d45101f817401dc , <number> - <number> - 2 7 t <time> . 9 0 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i5 - 1 2 4 5 u ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 6 9 gb ( <number> . 1 5 gb free ) | | process argv | - - crash - reporter - id 9 0 5 bb75d - c64b - <number> - <number> - c54522ac4b5c | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - ibmi - languages | bar | <date> cobol | bit | <date> vscode - rpgfree | bri | <date> ibmi - snippets | for | <number> . <number> copilot | git | <number> . <number> code - for - ibmi | hal | <number> . <number> ibm - i - development - pack | hal | <number> . <number> vscode - db2i | hal | <number> . <number> vscode - displayfile | hal | <number> . <number> vscode - ibmi - notebooks | hal | <number> . <number> vscode - ibmi - walkthroughs | hal | <number> . <number> vscode - rpgle | hal | <number> . <number> ibmidebug | ibm | <number> . <number> vscode - clle | ibm | <number> . <number> ibm - i - run - sql - from - acs | nie | <number> . <number> errorlens | use | <number> . <number> vscode - todo - highlight | way | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> <number> <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc014cf : <number> cmakesidepanelv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"failed to install the app . < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - - vs code version : <number> . <number> - - os version : windows_nt x64 <number> . <number> steps to reproduce : <number> . create a project regardless of reaction native cli / expocli . ex ) npx create - expo - app my - app <number> . run the project on the android emulator . ex ) expo start - > a ( run on emulator ) if you run an android app project in the current visual studio , the emulator will run properly , but the app installation will continue to fail . is there any additional configuration that i should set ? currently , i have set the sdk / platform - tools / jdk / android home environment variable . even though it runs normally in android studio , it does not run only in visual studio code . i do not think i can make a java machine properly , can you help me ? i also posted an issue on the react native collar , but i did not get any help . i am attaching the link together just in case . <url> this is my error ` ` ` npx react - native run - android info starting js server . <repeated> * daemon not running ; starting now at tcp : <number> * daemon started successfully info launching emulator . <repeated> info successfully launched emulator . info installing the app . <repeated> unrecognized option : - xmx64m error : could not create the java virtual machine . error : a fatal exception has occurred . program will exit . error failed to install the app . make sure you have the android development environment set up : <url> error : command failed : gradlew . bat app : installdebug - preactnativedevserverport = <number> unrecognized option : - xmx64m error fatal exception has occurred . program will exit . at makeerror ( d :\\ android \ \ awesomeproject \ \ node_modules \ \ <user> - native - community \ \ cli - platform - android \ \ node_modules \ \ execa \ \ index . js : <number> : <number> ) at d :\\ android \ \ awesomeproject \ \ node_modules \ \ <user> - native - community \ \ cli - platform - android \ \ node_modules \ \ execa \ \ index . js : <number> <time> at processticksandrejections ( node : internal / process / task_queues : <number> : <number> ) at async runonalldevices ( d :\\ android \ \ awesomeproject \ \ node_modules \ \ <user> - native - community \ \ cli - platform - android \ \ build \ \ commands \ \ runandroid \ \ runonalldevices . js : <number> : <number> ) at async command . handleaction ( d :\\ android \ \ awesomeproject \ \ node_modules \ \ <user> - native - community \ \ cli \ \ build \ \ index . js : <number> : <number> ) info run cli with - - verbose flag for more details . ` ` `",2
microsoft/vscode,"code - oss configure display language is invalid < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : windows_nt x64 <number> . <number> about info : version : <number> . <number> ( user setup ) commit : unknown date : <number> - <number> - 3 1 t <time> . 8 5 3 z electron : <date> electronbuildid : undefined chromium : <number> . <number> node . js : <number> . <number> v8 : <number> . <number> - electron . <number> os : windows_nt x64 <number> . <number> steps to reproduce build vscode source ( yarn gulp vscode - win32 - x64 ) <number> . install chinese extension ( zh - cn ) <number> . configure display language and select to zh - cn , reload it ' s still englist ( es ) <number> . same as yarn gulp vscode - win32 - x64 - user - setup the argv . json had set locale to "" zh - cn "" ! [ image ] ( <url>",2
microsoft/vscode,"snippet requires extra tab to move to next placeholder type : <b> bug </b> this is my snippets file : ` ` ` { / / place your global snippets here . each snippet is defined under a snippet name and has a scope , prefix , body and / / description . add comma separated ids of the languages where the snippet is applicable in the scope field . if scope / / is left empty or omitted , the snippet gets applied to all languages . the prefix is what is / / used to trigger the snippet and the body will be expanded and inserted . possible variables are : / / <money> , <money> for tab stops , <money> for the final cursor position , and ${ <number> : label } , ${ <number> : another } for placeholders . / / placeholders with the same ids are connected . / / example : / / "" print to console "" : { / / "" scope "" : "" javascript , typescript "" , / / "" prefix "" : "" log "" , / / "" body "" : [ / / "" console . log ( ' <money> ' ); "" , / / "" <money> "" / / ] , / / "" description "" : "" log output to console "" / / } "" cdrl "" : { "" scope "" : "" html "" , "" prefix "" : "" cdrl "" , "" body "" : [ "" < a class =\\ "" ${ <number> | pdf , xls , mpp , doc , zip , ppt , sam |}\\ "" href =\\ "" / doc / cdrls / a <money> <number> / <money> \ \ "" > <money> </a> "" ] , "" description "" : "" insert href for cdrls "" } , "" fldr "" <sad> "" scope "" : "" html "" , "" prefix "" : "" fldr "" , "" body "" : [ "" < div class =\\ "" pkg \ \ "" > <h3> <strong> <money> </strong> </h3> "" , "" \ \ t < div class =\\ "" pkg - body \ \ "" > "" , "" \ \ t \ \ t < - - cdrl placeholder - - > "" , "" \ \ t </div> "" , "" </div> "" ] , "" description "" : "" creates a new ' folder ' "" } } ` ` ` steps to reproduce : <number> - create a new text file and chose html as the language . <number> - type cdrl & press [ tab ] - - snippet is displayed <number> - press [ tab ] on 1 st placeholder to select pdf . <number> - type <number> & press [ tab ] to move to 3 rd placeholder <number> - type test1 and press [ tab ] then [ enter ] to go to new line . <number> - repeat steps <number> - <number> . must enter a second tab to move to 3 rd placeholder if you repeat this process again but use "" <number> "" instead of "" <number> "" it will work fine , unless you have used <number> before . this wasn ' t an issue in the may or <date> release , but started during the <date> release . i verified using the insiders version with no extentions loaded . just to note i have folders named a001 - a047 that have new files added monthly so i use this snippet often . vs code version : code - insiders <number> . <number> - insider ( 9 8 0 0 cf6dd6bf4634889d60720ef46a400f3a7298 , <number> - <number> - 2 8 t <time> . 4 7 2 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - <number> cpu @ <number> . 4 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : unavailable_off <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : unavailable_off <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : unavailable_off <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 9 2 gb ( <number> . 7 5 gb free ) | | process argv | - - crash - reporter - id be61bc57 - 8 ced - 4 0 aa - bcc2 - ba594673b68d | | screen reader | no | | vm | <percent> | </details> extensions <summary> a / b experiments </summary> ` ` ` vsliv69 <time> <number> vsins8 <time> <number> vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> pythontb : <number> pythonptprofiler : <number> vshan8 <time> <number> vscod8 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> <number> <time> <number> pythonfmttext : <number> pythoncmvfstr : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> i26e353 <time> <number> gsofa : <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> copilotsettingc : <number> e537b57 <time> <number> h0f3276 <time> <number> asynctokenver : <number> dsvsc0 <time> <number> dsvsc0 <time> <number> diffeditorv <time> <number> ` ` ` </details> < ! - - generated by issue reporter - - >",2
microsoft/vscode,"not executing code type : <b> bug </b> please make it correct sometimes my c program is compiled and executed but sometimes it dont neither compiled and executed vs code version : code <number> . <number> ( 2 ccd690cbff1569e4a83d7c43d45101f817401dc , <number> - <number> - 2 7 t <time> . 9 0 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i5 - 1 2 4 0 p ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 1 3 gb free ) | | process argv | - - crash - reporter - id 8 e9450ea - 3 bf5 - 4 5 2 f - a58b - bb9d7be856e5 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - node - snippets | chr | <number> . <number> vscode - eslint | dba | <number> . <number> code - runner | for | <number> . <number> live - server - preview | neg | <number> . <number> javascriptsnippets | xab | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes263cf : <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyonecf : <number> 2 e4cg34 <time> <number> <number> <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosmt <time> <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"can we activate vs code extension through activation event by reading url from particular file < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce we a config file inside folder . git <number> . we need to read the url inside that file and activate the extension",2
microsoft/vscode,"click on file tab shows folder of file < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > hey guys , good job but i am missing the use case of the subject on the pane "" git "" is open . i continue to code and want to check other files of one active file . so i would like to click on the tab of the file and then the file - pane should open and the active folder is the one i clicked on . maybe it makes sense to offer this feature as an option and to explain it with clicked the first time on it ? best robert",2
microsoft/vscode,"tabs showing green line above them . [ problemwithvscode ] ( <url> < ! - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no this occurs regardless of whether all the extensions are disabled . i do not think it ' s an extension issue . < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : <number> . <number> build <number> - microsoft windows <number> home steps to reproduce do not know what caused it . i just opened it today and it looked like this now .",2
microsoft/vscode,"mongodb atlas i plan to work on a project in github codespaces . however , i cannot find any good documentation on how to connect an express . js application to mongodb atlas . can anyone help ? version : <number> . <number> commit : 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 user agent : mozilla / <number> ( macintosh ; intel mac os x 1 0 _15_7 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> embedder < - - generated by web issue reporter - - >",2
microsoft/vscode,"program not running type : <b> bug </b> showing bellow erorr again and again : c <annoyed> mingw / bin / . <repeated> / lib / gcc / mingw32 / <number> . <number> / . <repeated> / . <repeated> / . <repeated> / libmingw32 . a ( main . o ) <sad> . text . startup + 0 xa0 ) : undefined reference to ` winmain <user> ' collect2 . exe : error : ld returned <number> exit status vs code version : code <number> . <number> ( 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 , <number> - <number> - 1 2 t <time> . 6 5 1 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i5 - 1 2 4 0 p ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 1 gb ( <number> . 1 3 gb free ) | | process argv | - - crash - reporter - id 9 ab42a03 - 3 f6e - 4 d15 - a0d2 - 4 9 5 bc5fbc45a | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - exe - runner | bra | <number> . <number> dscodegpt | dan | <date> code - runner | for | <number> . <number> python | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> vscode - lldb | vad | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl49 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> <number> <time> <number> showlangstatbar : <number> vsctsb : <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> i26e353 <time> <number> pythonnosm12tcf : <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> h0f3276 <time> <number> dsvsc013cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"error comes node . jc file not found type : <b> bug </b> <hashtag> include </hashtag> < isotream ? using namespace std ; int main ( ) { count < < <number> + <number> < < endl ; return <number> ; } i am a beginner i start code c + + and save this file with cpp format but when i got to run file error come node . jc not found vs code version : code <number> . <number> ( 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 , <number> - <number> - 1 2 t <time> . 6 5 1 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 3 7 0 0 x <number> - core processor ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 9 5 gb ( <number> . 2 5 gb free ) | | process argv | - - crash - reporter - id 1 caba4ac - 3 bc7 - <number> - b129 - 6 d <phone> | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnocebcf : <number> e537b57 <time> <number> dsvsc0 <time> <number> dsvsc0 <time> <number> cmakesidepanelv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"import pptx not working type : <b> feature request </b> why can not i install pptx on vscode to do my powerpoint presentations vs code version : code <number> . <number> ( 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 , <number> - <number> - 1 2 t <time> . 6 5 1 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"explorer has disappeared and the shortcut key ( ctrl + shift + e ) in view menu is also gone explorer has disappeared and the shortcut key ( ctrl + shift + e ) in view menu is also gone as shown below . < img width = "" <number> "" alt = "" image "" src = "" <url> the context menu ( mouse right click ) of the activity bar also does not have the explorer option . < img width = "" <number> "" alt = "" image "" src = "" <url> < img width = "" <number> "" alt = "" image "" src = "" <url> please advise how i can get the explorer back ?",2
microsoft/vscode,"javascript , websocket type : <b> bug </b> hello , how are you ? i ' am am programmer and i am having a issue what i can not solve on my own anymore . it ' s probably too offtopic and i still count good people helping a vegan out . i logic to work with code , excellent software , no problems at all , working on linux lite <number> ubuntu . i do program a computer game , browser based , java backend and javascript frontend , webgl for rendering . right now i ' am writing the interface and having a issue with one of the fundamental functions of javascript . hopefully you have some experience with websockets , if not then i might have to ask google next . usualy the payload - length is calculated by the send method of the websocket object . unfortunetily the byte representation of the payload length does never match the length of the submitted string . you at microsoft are very smart people , i like you alot , and if you have some idea how to solve that , i would very appreciate that . for a experienced programmer the string is sent by the index . js and received by the class transmissions , building the websocket there before creating a seperate object for it . everything else is working fine . it is that project : <url> if my imagination work , than thank you in advance bill . kind regards , indetale marcel otte vs code version : code <number> . <number> ( 7 4 f6148eb9ea00507ec113ec51c489d6ffb4b771 , <number> - <number> - 1 2 t <time> . 2 5 7 z ) os version : linux x64 <number> . <number> - <number> - generic modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 cpu m <number> @ <number> . 5 3 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : disabled_software <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 4 3 gb ( <number> . 5 7 gb free ) | | process argv | - - crash - reporter - id 6 4 f67b50 - 8 3 2 e - <number> - befc - 3 cd3ea0198a8 | | screen reader | no | | vm | <percent> | | desktop_session | xfce | | xdg_current_desktop | xfce | | xdg_session_desktop | xfce | | xdg_session_type |x 1 1 | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - java | red | <number> . <number> vscode - java - debug | vsc | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 e4cg34 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnoceb : <number> e537b57 <time> <number> dsvsc013cf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"task definition does not work < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce <number> .",2
microsoft/vscode,"setting "" workbench . editorassociations "" not work type : <b> bug </b> i just installed "" office viewer "" plugin from market , after setting "" workbench . editorassociations "" ， “ * . md ” ' s first use plugin ， it not work 。 so . <repeated> i try to setting “ * . md ” to the original vscode preview ， and it not work ， everytime i open a markdown file ， always open in raw editor . <repeated> everything can not change that vs code version : code <number> . <number> ( 6 6 0 3 9 3 deaaa6d1996740ff4880f1bad43768c814 , <number> - <number> - 0 4 t <time> . 4 0 7 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 4 8 0 0 h with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 7 gb ( <number> . 5 0 gb free ) | | process argv | e :\\\\ 笔记本 \ \ \ \ office \ \ \ \ word . md - - crash - reporter - id <number> - <number> - 4 6 5 b - b795 - 5 ae7eab81d84 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - better - comments | aar | <number> . <number> shell | bbe | <number> . <number> doxdocgen | csc | <number> . <number> vscode - office | cwe | <number> . <number> gitlens | eam | <number> . <number> prettier - vscode | esb | <number> . <number> code - runner | for | <number> . <number> latex - workshop | jam | <number> . <number> better - cpp - syntax | jef | <number> . <number> kite | kit | <number> . <number> rainbow - csv | mec | <number> . <number> vscode - language - pack - zh - hans | ms - | <number> . <phone> csharp | ms - | <number> . <number> anaconda - extension - pack | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> java | red | <number> . <number> vscode - yaml | red | <number> . <number> latex - support | tor | <number> . <number> cmake | twx | <date> vscode - java - debug | vsc | <number> . <number> vscode - java - dependency | vsc | <number> . <number> vscode - java - pack | vsc | <date> vscode - java - test | vsc | <number> . <number> vscode - maven | vsc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscorecescf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnocebcf : <number> e537b57 <time> <number> cmakesidepanelv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"frequently closes itself type : <b> performance issue </b> i am trying to build a website , writing some html , css files , and it keeps on closing . vs code version : code <number> . <number> ( 6 6 0 3 9 3 deaaa6d1996740ff4880f1bad43768c814 , <number> - <number> - 0 4 t <time> . 4 0 7 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 1 0 3 5 g1 cpu @ <number> . 0 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 7 8 gb ( <number> . 5 4 gb free ) | | process argv | - - crash - reporter - id 4 9 6 0 5 8 d9 - 3 8 1 b - 4 2 de - b2fb - fc9b00570406 | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> crashpad - handler <number> <number> <number> window [ <number> ] ( issue reporter ) <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c :\\ users \ \ lenovo \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ json - language - features \ \ server \ \ dist \ \ node \ \ jsonservermain "" - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> shared - process <number> <number> <number> gpu - process <number> <number> <number> ptyhost <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> window [ <number> ] ( detect . html - untitled ( workspace ) - visual studio code ) <number> <number> <number> utility - network - service ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( detect . html - untitled ( workspace ) - visual studio code ) | folder ( newfolder ) : <number> files | file types : py ( <number> ) pyc ( <number> ) ipynb ( <number> ) txt ( <number> ) | conf files : | folder ( cropdiseasedetector - hustlers ) : <number> files | file types ipynb ( <number> ) js ( <number> ) css ( <number> ) | conf files : ; ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - django | bat | <number> . <number> django - html | bib | <number> . <number> vscode - intelephense - client | bme | <number> . <number> doxdocgen | csc | <number> . <number> vscode - html - css | ecm | <number> . <number> code - runner | for | <number> . <number> gc - excelviewer | gra | <date> better - cpp - syntax | jef | <number> . <number> vscode - django - support | jun | <date> vscode - docker | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> material - icon - theme | pki | <number> . <number> liveserver | rit | <number> . <number> html5 - boilerplate | sid | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh931cf : <number> vshan8 <time> <number> vstes263cf : <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> vscaat : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 3 biah6 <time> <number> pyind77 <time> <number> f6dab26 <time> <number> pythonsymbol <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> 5 7 b7757 <time> <number> pythonfmttext : <number> pythoncmv : <number> f8hc823 <time> <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnocebcf : <number> h7j2d46 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"i3wm vscode cannot open appimage formate browser like edge , chrome when i sync github , vscode just have open browser , but my system not have install browser , i use appimage format browser . i cannot load vscode sync . or i install firefox - esr , and it can not open too , it always jump to new tab ,",2
microsoft/vscode,"pip can not be installed type : <b> bug </b> i can not able to install pip package in terminal . i have tried several ways but failed . vs code version : code <number> . <number> ( 6 6 0 3 9 3 deaaa6d1996740ff4880f1bad43768c814 , <number> - <number> - 0 4 t <time> . 4 0 7 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 5 5 0 0 u with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 3 gb ( <number> . 4 2 gb free ) | | process argv | - - crash - reporter - id a876346b - 0 6 4 e - 4 bde - b064 - bdd2003e0923 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd2263cf : <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosmt <time> <number> pythonidxpt : <number> pythonnocebcf : <number> cmakestatusbarv <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"ts server fatal error : unc host ' <number> . <number> ' access is not allowed type : <b> bug </b> ❗ ️ ❗ ️ ❗ ️ please fill in the sections below to help us diagnose the issue ❗ ️ ❗ ️ ❗ ️ * * typescript version : * * <number> . <number> * * steps to reproduce crash * * <number> . <number> . <number> . * * ts server log * * ❗ ️ server logging disabled . to help us fix crashes like this , please enable logging by setting : ` ` ` json "" typescript . tsserver . log "" : "" verbose "" ` ` ` after enabling this setting , future crash reports will include the server log . * * ts server error stack * * server : ` semantic ` ` ` ` error [ err_unc_host_not_allowed ] : unc host ' <number> . <number> ' access is not allowed at object . watchfile ( node : fs : <number> <time> ) at fswatchfileworker ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> at createsinglewatcherpername ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at pollingwatchfile ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at watchfile2 ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at watchmissingfilesystementry ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at fswatchhandlingexistenceonhost ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> : <number> ) at c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> at createsinglewatcherpername ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at fswatch ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at object . watchfile2 [ as watchfile ] ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at watchfile ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> : <number> ) at object . watchfile ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> <number> ) at c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> <number> at _projectservice . foreachconfigfilelocation ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at _projectservice . startwatchingconfigfilesforinferredprojectroot ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at inferredproject2 . addroot ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at _projectservice . assignorphanscriptinfotoinferredproject ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at _projectservice . assignprojecttoopenedscriptinfo ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> : <number> at flatmap ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at _projectservice . applychangesinopenfiles ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at updateopen ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> : <number> at ipciosession . executewithrequestid ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at ipciosession . executecommand ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at ipciosession . onmessage ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at process . <anonymous> ( c :\\ program files \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js : <number> <time> ) at process . emit ( node : events : <number> <time> ) at emit ( node : internal / child_process : <number> <time> ) at process . processticksandrejections ( node : internal / process / task_queues : <number> <time> ) ` ` ` vs code version : code <number> . <number> ( b3e4e68a0bc097f0ae7907b217c1119af9e03435 , <number> - <number> - 1 0 t <time> . 2 4 8 z ) os version : windows_nt x64 <number> . <number> modes : sandboxed : yes <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i7 - 1 2 6 0 p ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 7 0 gb ( <number> . 5 0 gb free ) | | process argv | - - crash - reporter - id d532a383 - f5a0 - 4 ea2 - 9 9 af - 5 8 b07f7daa19 | | screen reader | yes | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - intelephense - client | bme | <number> . <number> ini - for - vscode | dav | <number> . <number> vscode - html - css | ecm | <number> . <number> inifmt | lkr | <number> . <number> vscode - autohotkey - plus - plus | mar | <number> . <number> nc - gcode | ml2 | <number> . <number> sqltools | mtx | <number> . <number> log - booster | nvi | <number> . <number> vscode - powerquery | pow | <date> vscode - xml | red | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> pythongtdpath : <number> dh2dc7 <time> <number> pythonidxpt : <number> pythondjangotscf : <number> pythonnoceb : <number> e537b57 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"<hashtag> include </hashtag> < cs50 . h > add issue description here version : <number> . <number> commit : 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 user agent : mozilla / <number> ( windows nt <number> ; win64 ; x64 ) applewebkit / <number> ( khtml , like gecko ) chrome / <number> . <number> safari / <number> embedder < - - generated by web issue reporter - - >",2
microsoft/vscode,"tkinter type : <b> feature request </b> why is not defined tk ( ) ? vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"intellisense font is incredibly small . < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : * yes <emphasis> * < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : ` <number> . <number> ` - os version : fedora <number> ( ` <number> . <number> - <number> . fc38 . x86_64 ` ) - device : framework laptop ( 2 2 5 6 x1504 ) steps to reproduce open vscode <number> . write some code such that vscode can provide a suggestion <number> . press ctrl + space to open intellisense <number> . observe the intellisense font is incredibly small ! [ screenshot from <number> - <number> - <number> <number> - <number> - <number> ] ( <url>",2
microsoft/vscode,"my terminal is splitting a file name into seperated parts and then showing directory not found . type : <b> bug </b> say my file name is hello world . c . when i run it on my terminal its showing : - gcc . exe : error : hello : no such file or directory gcc . exe : error : world . c : no such file or directory gcc . exe : error : world : no such file or directory gcc . exe : fatal error : no input files compilation terminated . what to do vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 6 5 0 0 u cpu @ <number> . 5 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 9 gb ( <number> . 6 0 gb free ) | | process argv | - - crash - reporter - id cd1c98e5 - 0 ad5 - 4 1 1 a - a3b0 - 5 fa596baad88 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - code - runner | for | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh93 <time> <number> vshan8 <time> <number> vstes263cf : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 e4cg34 <time> <number> pyind77 <time> <number> f6dab26 <time> <number> pythonsymbol <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> a2ce337 <time> <number> ecj1e33 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> pythongtdpath : <number> bgfeh9 <time> <number> pythonnosm12tcf : <number> pythonidxptcf : <number> pythonnocebcf : <number> e537b57 <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"commande "" . / hello "" ne fonctionne pas good morning , i need support , because when i type de commande "" . / hello "" , the notification i receive is "" permission denied "" . please help me .",2
microsoft/vscode,add a method to switch between open windows < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > developers often need to keep multiple vs code windows open at the same time . providing a method to switch between the open windows would be greatly useful . the method can be a ui to show open windows ( e . g . like open editors ) - a keyboard shortcut,2
microsoft/vscode,"cant see terminal text < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version what is happening ? it ' s been a week haha . i have tried all kinds of terminal but same issue . ! [ image ] ( <url>",2
microsoft/vscode,"need a quick way to run or debug the last test i ran < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > i often repeatedly run or debug a test as i work on it . i ' d like a quick way to run or debug the last test i ran . ideally something i can assign to a single keystroke . currently , i have to find the test ' s first line in my code , in the testing panel or in test results .",2
microsoft/vscode,"can not run any code on vs code type : <b> performance issue </b> i am new to vs code and i can not run any of my code on vs code . so please help me out as this is so irritating . vs code version : code <number> . <number> ( universal ) ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 9 2 z ) os version : darwin arm64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | apple m1 ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> metal : disabled_off <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | <number> , <number> , <number> | | memory ( system ) | <number> . 0 0 gb ( <number> . 1 2 gb free ) | | process argv | - - crash - reporter - id 7 a18136b - e19f - 4 e1f - b9a7 - cc052e7734ec | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> gpu - process <number> <number> <number> utility - network - service <number> <number> <number> window [ <number> ] ( main . c — pythonn . py ) <number> <number> <number> ptyhost <number> <number> <number> shared - process <number> <number> <number> / bin / ps - ax - o pid =, ppid =, pcpu =, pmem =, command = <number> <number> <number> filewatcher [ <number> ] <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> / users / rakeshkumarjagdev / . vscode / extensions / ms - vscode . cpptools - <number> . <number> - darwin - arm64 / bin / cpptools <number> <number> <number> window [ <number> ] ( issue reporter ) ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( main . c — pythonn . py ) | folder ( pythonn . py ) : <number> files | file types : json ( <number> ) c ( <number> ) plist ( <number> ) python ( <number> ) | conf files tasks . json ( <number> ); ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - html - css | ecm | <number> . <number> code - runner | for | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> liveserver | rit | <number> . <number> cmake | twx | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python38 <time> <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> 2 i9eh26 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpath : <number> dh2dc7 <time> <number> pythonidxpt : <number> pythondjangotscf : <number> pythonnoceb : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"vscode js issue : jsdoc intellisense behavior in array variables are weird after version <number> . <number> + type : <b> bug </b> ` ` ` /* * <user> { [ { a : string , b : string } ] } */ let arr = [ { a : "" abc "" , b : "" def "" } , { / / intellisense not working here } ] ` ` ` it works normally for the first element , but start to show intellisense of array ( at , reduce , . <repeated> ) instead of normal behavior ( a , b ) it works fine at version <number> . <number> vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | 1 2 th gen intel ( r ) core ( tm ) i9 - 1 2 9 0 0 f ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 6 gb ( <number> . 8 0 gb free ) | | process argv | - - crash - reporter - id ee865e3c - 1 7 be - <number> - 9 1 aa - 0 8 0 e6ea972ea | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - gitlens | eam | <number> . <number> git - graph | mhu | <number> . <number> custom - code - template | yin | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh93 <time> <number> vshan8 <time> <number> vstes26 <time> <number> vscoreces : <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa593cf : <number> pythonvs93 <time> <number> py29gd226 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 3 biah6 <time> <number> pyind77 <time> <number> f6dab26 <time> <number> pythonsymbol <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> showindicator : <number> pythongtdpath : <number> gsof <time> <number> dh2dc7 <time> <number> pythonidxpt : <number> pythondjangotscf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"intellisense for mongodb doesnt working type : <b> bug </b> well i am working on project using mongodb / atlas , and i created regular model and controller for it . it was working in model . <repeated> but when i started typing functions in controller it stops . <repeated> also i tried everything like : updates for vsc , extensions , deleting models , packages and restarting vsc . still doesnt working please help me ! vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes generated by issue reporter - - >",2
microsoft/vscode,"add keyboard shortcut for "" select language mode "" < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > the little thing in the bottom - right corner where you can select language mode would be a useful candidate for adding a keyboard shortcut . it does not even have to have a real shortcut by default , but in my case i would find it very useful to have the ability to assign a shortcut to it . here ' s an example workflow where it would come in handy . ( i find myself doing this a lot and wish there were a way to add a shortcut . ) <number> . graph some code from somewhere ( a sql query from slack , or a graphql query from the network tab , or some yaml printed from a debug log ) and copy - paste it into a new tab in vscode . <number> . vscode does not correctly figure out what language the code is in . <number> . go to the bottom right corner , click the ( sometimes very small ) button to ` select language mode ` . ( this is the part i want a keyboard shortcut for . ) <number> . select the correct language . <number> . hit ` alt - shift - f ` to format the code . and you are done . basically these steps allow you to take code from some random source , where it ' s probably unformatted and may not be syntax - highlighted , and allow you to format & inspect it in vscode . if step <number> above were able to be automated via a keyboard shortcut , it would make this process quite a bit more ergonomic . what do you think ? if there ' s already a way to do this , i am open to hearing it . as of right now , i open ` keyboard shortcuts ` and search for ` select language mode ` but nothing pops up , so it appears that a new keyboard shortcut needs to be added .",2
microsoft/vscode,"problem with vsc [ img_20230622_205601 ] ( <url> < ! - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : - os version : steps to reproduce hello , i have a problem with vsc . today i saw that when i write code in react , plugins turn off . i will not format the code well , the program does not tell me the names of components when writing the code . thank you in advance for your help <number> . ! [ img_20230622_211443 ] ( <url> ! [ img_20230622_205601 ] ( <url>",2
microsoft/vscode,"removed keybinding not trigger global hotkey < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> - os version : win <number> steps to reproduce remove keybinding ` f1 ` ! [ image ] ( <url> <number> . press ` f1 ` to trigger my global screenshot hotkey ( snipaste app default hotkey ) , but it is not work ! [ keyboard troubleshooting ] ( <url>",2
microsoft/vscode,"addeventlistener not calling function type : <b> bug </b> i am currently doing a tutorial with a game logic . the game has buttons which are meant to call on specific functions however upon writing : . <repeated> btn . addeventlistener ( ' click ' , function ) function remains blank in color indictacting it has not been declared vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i5 - 7 2 0 0 u cpu @ <number> . 5 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 8 5 gb ( <number> . 5 9 gb free ) | | process argv | - - crash - reporter - id 5 0 2 cda83 - bb40 - 4 0 5 c - 9 8 b6 - 9 a6402af3c74 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - project - manager | ale | <number> . <number> linkcheckmd | bla | <number> . <number> gitignore | cod | <number> . <number> vscode - markdownlint | dav | <number> . <number> vscode - eslint | dba | <number> . <number> docs - article - templates | doc | <number> . <number> docs - authoring - pack | doc | <number> . <number> docs - build | doc | <number> . <number> docs - images | doc | <number> . <number> docs - linting | doc | <date> docs - markdown | doc | <number> . <number> docs - metadata | doc | <number> . <number> docs - preview | doc | <number> . <number> docs - scaffolding | doc | <date> docs - yaml | doc | <number> . <number> git - extension - pack | don | <number> . <number> githistory | don | <date> es7 - react - js - snippets | dsz | <number> . <number> gitlens | eam | <number> . <number> prettier - vscode | esb | <number> . <number> auto - close - tag | for | <date> auto - rename - tag | for | <date> dotenv | mik | <number> . <number> vscode - edge - devtools | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> cpptools | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> vscode - react - native | msj | <number> . <number> material - icon - theme | pki | <number> . <number> vscode - yaml | red | <number> . <number> liveserver | rit | <number> . <number> easysass | spo | <number> . <number> code - spell - checker | str | <number> . <number> vscode - icons | vsc | <number> . <number> quokka - vscode | wal | <date> javascriptsnippets | xab | <number> . <number> material - theme | zhu | <date> vscode - open - in - github | ziy | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv368cf : <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vswsl492cf : <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdc : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> f6dab26 <time> <number> pythonsymbol <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixshowwlkth : <number> pythongtdpath : <number> bgfeh9 <time> <number> dh2dc7 <time> <number> pythonidxpt : <number> pythondjangotscf : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"npm start is not starting my react app again i dont know if i mistakenly delete something type : <b> performance issue </b> help me restore the deleted files on my project because npm start is not starting my application again . vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) pentium ( r ) cpu n3710 @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu : enabled | | load ( avg ) | undefined | | memory ( system ) | <number> . 8 5 gb ( <number> . 6 5 gb free ) | | process argv | - - crash - reporter - id e69cf78d - 0 6 f6 - 4 4 cc - 9 9 3 a - c6fd4a9d96f1 | | screen reader | no | | vm | <percent> | </details> <details> <summary> process info </summary> ` ` ` cpu % mem mb pid process <number> <number> <number> code main <number> <number> <number> ptyhost <number> <number> <number> console - window - host ( windows internal process ) <number> <number> <number> c :\\ windows \ \ system32 \ \ windowspowershell \ \ v1 . <number> \ \ powershell . exe - noexit - command "" try { . \ \ "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ out \ \ vs \ \ workbench \ \ contrib \ \ terminal \ \ browser \ \ media \ \ shellintegration . ps1 \ \ "" } catch { } "" <number> <number> <number> crashpad - handler <number> <number> <number> shared - process <number> <number> <number> window [ <number> ] ( issue reporter ) <number> <number> <number> gpu - process <number> <number> <number> extensionhost [ <number> ] <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node - - max - old - space - size = <number> "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js "" - - servermode partialsemantic - - useinferredprojectperprojectroot - - disableautomatictypingacquisition - - cancellationpipename c :\\ users \ \ dafe \ \ appdata \ \ local \ \ temp \ \ vscode - typescript \ \ 2 e05939c857e1e12a620 \ \ tscancellation - c6fb6f8df58661951fc6 . tmp * - - locale en - - nogeterronbackgroundupdate - - validatedefaultnpmlocation - - usenodeipc ) <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ json - language - features \ \ server \ \ dist \ \ node \ \ jsonservermain "" - - node - ipc - - clientprocessid = <number> ) <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node - - max - old - space - size = <number> "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ resources \ \ app \ \ extensions \ \ node_modules \ \ typescript \ \ lib \ \ tsserver . js "" - - useinferredprojectperprojectroot - - enabletelemetry - - cancellationpipename c :\\ users \ \ dafe \ \ appdata \ \ local \ \ temp \ \ vscode - typescript \ \ 2 e05939c857e1e12a620 \ \ tscancellation - 0 0 5 0 db3ad7f4a29fd639 . tmp * - - locale en - - nogeterronbackgroundupdate - - validatedefaultnpmlocation - - usenodeipc ) <number> <number> <number> electron - nodejs ( "" c :\\ users \ \ dafe \ \ appdata \ \ local \ \ programs \ \ microsoft vs code \ \ code . exe "" - - ms - enable - electron - run - as - node "" c <annoyed> users / dafe / appdata / local / programs / microsoft vs code / resources / app / extensions / node_modules / typescript / lib / typingsinstaller . js "" - - globaltypingscachelocation c <annoyed> users / dafe / appdata / local / microsoft / typescript / <number> - - enabletelemetry - - typesmaplocation "" c <annoyed> users / dafe / appdata / local / programs / microsoft vs code / resources / app / extensions / node_modules / typescript / lib / typesmap . json "" - - validatedefaultnpmlocation ) <number> <number> <number> utility - network - service <number> <number> <number> window [ <number> ] ( home . jsx - arc . js ( workspace ) - visual studio code ) <number> <number> <number> filewatcher [ <number> ] ` ` ` </details> <details> <summary> workspace info </summary> ` ` ` | window ( home . jsx - arc . js ( workspace ) - visual studio code ) | folder ( arc . js ) : <number> files | file types : png ( <number> ) css ( <number> ) jpg ( <number> ) jsx ( <number> ) js ( <number> ) json ( <number> ) webp ( <number> ) | txt ( <number> ) map ( <number> ) ico ( <number> ) | conf files ` ` ` </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - dart - code | dar | <number> . <number> flutter | dar | <number> . <number> vscode - language - babel | mgm | <date> vscode - react - native | msj | <number> . <number> </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vsdfh93 <time> <number> vshan8 <time> <number> vstes26 <time> <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> vsc <elongated> : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> 0 3 d3595 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixhidewlkth : <number> hideindicator : <number> pythongtdpath : <number> e440d66 <time> <number> pythondjangots : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,"localhost error type : <b> bug </b> i am aways getting localhost refused to connect error on my laptop whenever i am trying to make a html file on vs code terminal to use java script vs code version : code <number> . <number> ( 6 9 5 af097c7bd098fbf017ce3ac85e09bbc5dda06 , <number> - <number> - 1 4 t <time> . 3 7 9 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | intel ( r ) core ( tm ) i7 - 5 6 0 0 u cpu @ <number> . 6 0 ghz ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 6 9 gb ( <number> . 8 1 gb free ) | | process argv | - - crash - reporter - id 6 d89a0c5 - <number> - <number> - 8 d2a - 4 0 5 9 6 e2895f4 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - dart - code | dar | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> sqltools | mtx | <number> . <number> material - theme | zhu | <date> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vslsvsres3 <time> <number> vserr24 <time> <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod8 <time> <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc012cf : <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> <number> <time> <number> pythonsymbol <time> <number> a9j8j15 <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> pythoncmv : <number> fixhidewlkth : <number> hideindicator : <number> pythongtdpath : <number> dh2dc7 <time> <number> pythondjangots : <number> ` ` ` </details> < - - generated by issue reporter - - >",2
microsoft/vscode,eu quero setar o arquivo a ser executado quando clicar em play . < - - ⚠️ ⚠️ do not delete this ! feature_request_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct - - > < ! - - please search existing issues to avoid creating duplicates . - - > < ! - - describe the feature you ' d like . - - > ! [ image ] ( <url>,2
microsoft/vscode,"bug - - file saving error < - - ⚠️ ⚠️ do not delete this ! bug_report_template ⚠️ ⚠️ - - > < ! - - please read our rules of conduct : <url> - - > < ! - - 🕮 read our guide about submitting issues : <url> - - > < ! - - 🔎 search existing issues to avoid creating duplicates . - - > < ! - - 🧪 test using the latest insiders build to see if your issue has already been fixed : <url> - - > < ! - - 💡 instead of creating your report here , use ' report issue ' from the ' help ' menu in vs code to pre - fill useful information . - - > < ! - - 🔧 launch with ` code - - disable - extensions ` to check . - - > does this issue occur when all extensions are disabled ? : yes / no < ! - - 🪓 if you answered no above , use ' help : start extension bisect ' from command palette to try to identify the cause . - - > < ! - - 📣 issues caused by an extension need to be reported directly to the extension publisher . the ' help > report issue ' dialog can assist with this . - - > - vs code version : <number> . <number> ( universal ) - os version : macos steps to reproduce : <number> . save the file <number> . get error width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> am "" src = "" <url>",2
microsoft/vscode,"markdown preview does not work type : <b> bug </b> step <number> : try to open any . md file . step <number> : try to show markdown preview by any of the available command . result : markdown preview does not work . expected result : markdown preview should ( according to vscode ) work out of the box . things i tried : apart from trying to use default set of commands i found on the vscode documentation and on the internet ; i tried reinstalling various third party extension for markdown preview . i restarted vscode itself and my pc as well . vs code version : code <number> . <number> ( 4 cb974a7aed77a74c7813bdccd99ee0d04901215 , <number> - <number> - 1 2 t <time> . 1 0 2 z ) os version : windows_nt x64 <number> . <number> modes : <details> <summary> system info </summary> | item | value | | - - - | - - - | | cpus | amd ryzen <number> 4 6 0 0 h with radeon graphics ( <number> x <number> )| | gpu status | 2 d_canvas : enabled <br> canvas_oop_rasterization : disabled_off <br> direct_rendering_display_compositor : disabled_off_ok <br> gpu_compositing : enabled <br> multiple_raster_threads : enabled_on <br> opengl : enabled_on <br> rasterization : enabled <br> raw_draw : disabled_off_ok <br> video_decode : enabled <br> video_encode : enabled <br> vulkan : disabled_off <br> webgl : enabled <br> webgl2 : enabled <br> webgpu | load ( avg ) | undefined | | memory ( system ) | <number> . 3 6 gb ( <number> . 6 3 gb free ) | | process argv | - - crash - reporter - id 5 9 c088fe - <number> - <number> - ae22 - 2 5 d7a3fc3048 | | screen reader | no | | vm | <percent> | </details> <details> <summary> extensions ( <number> ) </summary> extension | author ( truncated ) | version - - - | - - - | - - - vscode - sqlite | ale | <number> . <number> tailwind - docs | aus | <number> . <number> vscode - django | bat | <number> . <number> vscode - intelephense - client | bme | <number> . <number> vscode - tailwindcss | bra | <date> simple - react - snippets | bur | <number> . <number> npm - intellisense | chr | <number> . <number> path - intellisense | chr | <number> . <number> doxdocgen | csc | <number> . <number> dart - code | dar | <number> . <number> flutter | dar | <number> . <number> vscode - eslint | dba | <number> . <number> python - environment - manager | don | <number> . <number> gitlens | eam | <number> . <number> vscode - html - css | ecm | <number> . <number> vsc - material - theme | equ | <number> . <number> vsc - material - theme - icons | equ | <number> . <number> prettier - vscode | esb | <number> . <number> copilot | git | <number> . <number> vscode - pull - request - github | git | <number> . <number> gc - excelviewer | gra | <date> vscode - auto - open - markdown - preview | hnw | <number> . <number> vscode - power - mode | hoo | <number> . <number> ionic | ion | <number> . <number> ionide - fsharp | ion | <number> . <number> better - cpp - syntax | jef | <number> . <number> cmake - language - support - vscode | jos | <number> . <number> vsc - python - indent | kev | <number> . <number> wsl - path | kgr | <number> . <number> gitlab - notifications | log | <number> . <number> magicpython | mag | <number> . <number> vscode - language - babel | mgm | <date> document | min | <number> . <number> vscode - docker | ms - | <number> . <number> csharp | ms - | <number> . <number> vscode - dotnet - pack | ms - | <date> vscode - dotnet - runtime | ms - | <number> . <number> black - formatter | ms - | <number> . <number> isort | ms - | <number> . <number> python | ms - | <number> . <number> vscode - pylance | ms - | <number> . <number> jupyter | ms - | <number> . <phone> jupyter - keymap | ms - | <number> . <number> jupyter - renderers | ms - | <date> vscode - jupyter - cell - tags | ms - | <number> . <number> vscode - jupyter - slideshow | ms - | <number> . <number> remote - containers | ms - | <number> . <number> remote - ssh | ms - | <number> . <number> remote - ssh - edit | ms - | <number> . <number> remote - wsl | ms - | <number> . <number> vscode - remote - extensionpack | ms - | <number> . <number> cmake - tools | ms - | <date> cpptools | ms - | <number> . <number> cpptools - extension - pack | ms - | <number> . <number> js - debug - nightly | ms - | <number> . <number> live - server | ms - | <number> . <number> powershell | ms - | <number> . <number> remote - explorer | ms - | <number> . <number> remote - server | ms - | <number> . <number> vscode - typescript - next | ms - | <number> . <number> vsliveshare | ms - | <number> . <number> vscode - react - native | msj | <number> . <number> sqltools | mtx | <number> . <number> sqltools - driver - sqlite | mtx | <number> . <number> autodocstring | njp | <number> . <number> vscode - versionlens | pfl | <number> . <number> gcc - md | pha | <number> . <number> material - icon - theme | pki | <number> . <number> sqlite - viewer | qwt | <number> . <number> r - debugger | rde | <number> . <number> liveserver | rit | <number> . <number> html5 - boilerplate | sid | <number> . <number> code - spell - checker | str | <number> . <number> node - pack | swe | <date> vscode - djaneiro | the | <number> . <number> pdf | tom | <number> . <number> cmake | twx | <date> intellicode - api - usage - examples | vis | <number> . <number> vscodeintellicode | vis | <date> vscodeintellicode - completions | vis | <date> vscodeintellicode - insiders | vis | <date> jinja | who | <number> . <number> vue | wsc | <date> php - debug | xde | <number> . <number> vue | znc | <number> . <number> ( <number> theme extensions excluded ) </details> <details> <summary> a / b experiments </summary> ` ` ` vsliv36 <time> <number> vsreu68 <time> <number> python383cf : <number> vspor87 <time> <number> vspor7 <time> <number> vspor36 <time> <number> vstes6 <time> <number> vslsvsres3 <time> <number> vserr242cf : <number> pythontb : <number> vsjup5 <time> <number> pythonptprofiler : <number> vshan8 <time> <number> vstes26 <time> <number> vscod805cf : <number> binariesv6 <time> <number> bridge07 <time> <number> bridge07 <time> <number> vsaa59 <time> <number> pythonvs93 <time> <number> vsclangdf : <number> c4g489 <time> <number> dsvsc0 <time> <number> pynewext5 <time> <number> azure - dev_surveyone : <number> 2 8 2 f87 <time> <number> pyind77 <time> <number> f6dab26 <time> <number> pythonsymbol <time> <number> showlangstatbar : <number> vsctsb : <number> pythonms3 <time> <number> a2ce337 <time> <number> pythonfmttext : <number> pythoncmvfstrcf : <number> fixshowwlkth : <number> hideindicator : <number> pythongtdpathcf : <number> dh2dc7 <time> <number> pythonnosmt <time> <number> ` ` ` </details> < - - generated by issue reporter - - >",2
bitcoin/bitcoin,<hashtag> f </hashtag> #,1
bitcoin/bitcoin,<hashtag> f </hashtag> #,1
bitcoin/bitcoin,"a native token protocol # # # please describe the feature you ' d like to see added . a native token protocol can be implemented on btc a utxo - based blockchain structure with a native token method , which i call "" dye coins . "" <number> . the transaction output class needs to add a new field called "" color "" . <number> . the block header needs to add a new field called "" color "" . <number> . the validation of ordinary transactions needs to add additional validation logic . <number> . the validation of coinbase transactions needs to add additional validation logic . <number> . the validation of blocks needs to add new validation logic . specific logic is as follows : <number> . after the transaction output has the color tag field , there are only two results , <number> ( all <number> ) indicates not marked , non - <number> indicates marked . <number> . to verify that a normal transaction ( non - coinbase ) is valid , additional validation logic needs to be added . specifically , it means that the input and output of all transactions must be traversed to ensure that the output values of all colors are less than or equal to the input values of the corresponding colors . <number> . miners need to verify that all or some of the normal transactions in the transaction pool that need to be packaged meet <number> , and calculate the difference between all the corresponding colors in all these transactions as utxo ( can be single or multiple ) added to the coinbase output . <number> . a coinbase transaction is a special transaction that essentially has no input and only output . in addition to the utxo generated by the block miner fee , the coinbase transaction itself has a certain number of native token rewards . <number> . the structure of a coinbase transaction that does not participate in color marking is as follows : * assuming that the current block reward is <number> white coins , and the transaction output generated by the miner fee contains <number> red coins , <number> green coins , and <number> white coins . * when constructing a coinbase transaction , <number> red coin outputs ( <number> red coin outputs total <number> ) can be output , <number> green coin outputs ( <number> green coin outputs total <number> ) and <number> white coin output ( output quantity is equal to <number> ) <number> . the structure of a coinbase transaction that participates in color marking is as follows assuming that the current block reward is <number> white coins , and the transaction output generated by the miner fee contains <number> red coins , <number> green coins , and <number> white coins . * when constructing a coinbase transaction , <number> red coin outputs ( <number> red coin outputs total <number> ) can be output , <number> green coin outputs ( <number> green coin outputs total <number> ) , and the remaining <number> white coins will be dyed yellow as a single output ( or multiple outputs ) . however , <number> white coins cannot be dyed yellow , <number> blue , and white coins can only be dyed the same color and a color that has not appeared before ( how to determine if the color exists will be mentioned later ) , otherwise the transaction will fail . <number> . next , the miner will pack all the transactions to generate the merkle tree , and assign the color value of the dye in the coinbase transaction to the color field in the block header , and then start proof of work to generate the block . <number> . after the miner generates the block , it will broadcast the block to the entire network . <number> . other miners will verify the validity of the block . this includes verifying the validity of each transaction , especially the coinbase transaction . it must be ensured that the only color that can be dyed in the coinbase transaction is one color ( not that there is only one color of output ) , and that this color has never appeared before ( it can be obtained by traversing all the block headers or caching all the colors locally ) . otherwise , the transaction will fail . if there is no dyeing ( meaning that the output of the coinbase transaction does not produce a new color utxo , and the number of utxo tokens of other colors does not increase ) , the traversal process can be skipped . <number> . verify whether the color value in the block header ( <number> if no dyeing ) is equal to the new color value in the coinbase transaction . if equal , the block is valid , the block is added to the blockchain , and the next block is mined . if not equal , the block is invalid and the block is rejected . <number> . [ note ] there is no inflation problem here , because inflation can be achieved simply by adding one more color utxo , for example , green and red are marked as us dollars . if interested , pls contact me <email> # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like _no response_ # # # describe any alternatives you have considered _no response_ # # # please leave any additional context _no response_",1
bitcoin/bitcoin,"add bech32 . cpp locateerrors case to check for minimum length . # # # please describe the feature you ' d like to see added . add a "" bech32 string too short "" case to bech32 . cpp locateerrors to reduce ambiguity and collisions with "" invalid separator position "" case . # # # is your feature related to a problem , if so please describe it . error representation when calling locateerrors within ` ` ` src / test / bech32_tests . cpp ` ` ` is ambiguous and incorrect in some cases because of a missing bech32 string to short case . not having a too short case causes a fall though into the conditions of ` ` ` "" invalid separator position "" || "" missing separator "" ` ` ` # # # describe the solution you ' d like simply add a check for a minimum bech32 string size after or during the check for the maximum size which is already implemented . cascading effects of this would be seen in decodedestination if <url> was patched to not consider bech32 invalid then attempt to base58decode it becuase of the incorrect hrp for the current chain . referenced in <url> pr <url> # # # describe any alternatives you have considered you could case this into other logic on a case by case basis in code . example make a speical case inside decodedestination to display the correct error message to the user by if locateerrors returns ' invalid ` ` ` "" separator position "" || "" missing separator "" ` ` ` then you check to see if the string is to short . # # # please leave any additional context as an aside there is already a check for string length greater than <number> ( max len ) in src / test / bech32_tests . cpp both [ "" 1 0 a06t8 "" , "" 1 qzzfhee "" ] produce ` ` ` { "" invalid separator position "" , { <number> } } ` ` ` the former is non conformant to bip173 due not even having enough chars to represent a base32 string and the second genuinely has a wrong separator position .",1
bitcoin/bitcoin,"add maxrelaytxfee # # # please describe the feature you ' d like to see added . just an example ( this feature request is not related to mutinynet in any way ) - <url> let us suppose the miner wants to recycle coins ( and shrink the total number of utxos ) by making an all - fee transaction and eventually mining it . any other miner would be happy to mine such a transaction if it is relayed to the network . # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like add ` maxrelaytxfee ` parameter , similar to its ` min ` equivalent which is present at least since 9 d14e689c8 , later it became a configurable . the value could be in percentage of the amount sent . an absolute number ( like the other similar configurables use ) may be less helpful here when tho point is to stop broadcasting mostly all - fee transactions . # # # describe any alternatives you have considered the usual way how utxos work is that they retain their full history back to the block from whose coinbase they come . # # # please leave any additional context in case of large - scale recycling maybe the chainstate can get back below <number> gib ( at the moment it is <number> gib big ) . also the "" ordinals theory "" may require some rethinking .",1
bitcoin/bitcoin,"change wording # # # please describe the feature you ' d like to see added . "" if you have chosen to limit block chain storage ( pruning ) , the historical data must still be downloaded and processed , but will be deleted afterward to keep your disk usage low . "" change to "" if you have chosen to limit block chain storage ( pruning ) , the historical data will be downloaded , processed , and deleted , to keep your disk usage low . "" # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like _no response_ # # # describe any alternatives you have considered _no response_ # # # please leave any additional context _no response_",1
bitcoin/bitcoin,"use semantic analysis in lint - logs . py # # # please describe the feature you ' d like to see added . currently ` test / lint / lint - logs . py ` tries to parse the c + + manually which is prone to flaws . for example it does not support expressions that span more than one line and would be happy if there is ` \ \ n "" ` anywhere ( on the single line it checks ) . for example : ` ` ` cpp logprintlevel ( bclog : : proxy , bclog : : level : : debug , "" this is fine , but lint - logs . py will report error \ \ n "" ); ` ` ` ` ` ` cpp logprintlevel ( bclog : : proxy , bclog : : level : : debug , "" not ok \ \ n "" "" but will be reported as ok "" ); ` ` ` ` ` ` cpp logprintlevel ( bclog : : proxy , bclog : : level : : debug , "" even more /* continued */ no newline , but is ok "" ); ` ` ` ` ` ` cpp /* logprintlevel ( ) inside comment causes an error */ ` ` ` # # # is your feature related to a problem , if so please describe it . ` lint - logs . py ` requires to either write everything on line line or add ` /* continued */ ` comment . # # # describe the solution you ' d like it should be possible to use [ libclang ] ( <url> to properly parse the c + + code . then the check should be something like : ` ` ` for each call to ` logprintlevel ( ) ` : get the third argument should be a string its last two chars should be ` \ \ ` and ` n ` ` ` ` # # # describe any alternatives you have considered _no response_ # # # please leave any additional context _no response_",1
bitcoin/bitcoin,"option to prevent sleep # # # please describe the feature you ' d like to see added . there should be option to start core which enable system to sleep only when net . act . is disabled # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like _to [ notify ] ( <url> the system that your application is busy , use the . <repeated> _ # # # describe any alternatives you have considered _constant disk activity about 5 _ _mb / sec , occasionally it may drop to a smaller value but in general this <number> mb / sec_ [ activity ] ( <url> continues for very long periods of time . _ _creating a customized power plan_ is also not option because that would affect system even when core is not running hours would be better , and best of all would be if you can run your node continuously . _ # # # please leave any additional context _no response_",1
bitcoin/bitcoin,rpc importing wallets by data instead of by filename # # # please describe the feature you ' d like to see added . right now we can only import wallets by filename . this means you can only execute that rpc if you are executing it from the system where the bitcoin node is running . but it could be that you are sending the rpc request from outside the node . in that case you do not have access to the file system . so it would be handy to allow passing the content of a wallet dump file instead and also allowing dumping the content not in a file but in the rpc result . it might be a good idea to check for other cases where the same is being done .,1
bitcoin/bitcoin,"consider removing message signing signing arbitrary messages with bitcoin private keys was always a bit strange . it was designed to handle only the narrow case of single - sig keys , and is not well - defined for new address types . worse , it does not allow for signing messages with more arbitrary scripts , and its not clear what the semantics for such things would be . sadly , the message signing feature is now being abused to "" verify wallets "" as a part of withdraw flows . this prevents bitcoin users from using anything but an incredibly narrow set of scripts ( in many cases only <emphasis> p2pkh , not even modern script formats , and definitely not multisig ) . rather than trying to rework it to avoid these things breaking bitcoin , it seems like it ' d be much better to simply remove the message signing logic entirely . if someone comes along with a proposal that considers a broader set of scripts and a more sensible design that could be considered .",1
bitcoin/bitcoin,"millisecond log timestamp ( option ) # # # please describe the feature you ' d like to see added . for some events , like two blocks at the same height being announced by multiple peers at the same time , it ' s nice to have millisecond precision on the log entries . # # # is your feature related to a problem , if so please describe it . e . g . one of the forkmonitor nodes saw two blocks at height <number> within the same second : <url> other nodes saw those blocks in the opposite order . # # # describe the solution you ' d like something like ` - logtimestampprecision = ` which would ` second ` by default , with ` millisecond ` as an option . log entries would look like <number> - <number> - 2 2 t <time> z ` ` ` # # # describe any alternatives you have considered a more fancy solution would allow a custom date formatter . # # # please leave any additional context _no response_",1
bitcoin/bitcoin,"feature request : alert pr author in case of ci failure would be nice to have a means of quickly getting alerted about the ci failing on a pr you authored . it ' s a bit cumbersome to keep an eye out on it manually , and a freshly created pr typically gets more eyes on it so having it in failing state is a bit of a waste of everyone ' s time . creating this issue after brief [ irc discussion ] ( <url> i think the main requirements are : - be able to opt out or easily ignore / hide - only get a single notification per run , not for every single job failing - low - touch , easy to maintain some potential approaches i see use github actions as [ recommended ] ( <url> by cirrus ci documention , e . g . here ' s a [ sample email workflow ] ( <url> - can we get this to work so the pr author is notified ? <number> . use [ ` bitcoin - git ` ] ( <url> irc bot to dm the nickname that corresponds to the pr author ' s github nickname - people with a different username on irc and github would not get notifications - to opt - out , i think users could simply use ` / ignore bitcoin - git ` ? <number> . use [ drahtbot ] ( <url> to tag the author as assignee when ci fails ( and remove them as assignee upon force push ) - have not come up with an elegant way for people to opt - out of this , but arguably it ' s also the least intrusive option and e . g . could be filtered on the mailbox level ?",1
bitcoin/bitcoin,"add support for sighash flags in psbt ( like single | anyonecanpay ) * * is your feature request related to a problem ? please describe . * * when using bitcoin core ' s gui to sign a psbt that has a utxo with ` sighash = single | anyonecanpay ` , it throws following error : ` ` ` specified sighash value does not match value stored in psbt ` ` ` * * describe the solution you ' d like * * i ' d like it to successfully sign psbts containing ` sighash = single | anyonecanpay ` * * describe alternatives you have considered * * the current non - idea workaround is to use the console window as described here : <url> * * additional context * * example psbt chnidp8baf4caaaaaxeythren2bak3exrpqguix / zzznz3icqcl0hdiynbraaaaaaad / / / / / arckaaaaaaaailegvwmppjtgis8behetziizvhop7lskzagxa50kvksv / 2 iaaaaaaaeayaiaaaabtliktgpzhuz4feenzimfd1gmwpdzvivt3fs9je62veqaaaaaakcwraigsren / 8 1 qq0c5mvytzqjra8zkxzv4tiancxbqguhequecicinpefntasthdmzmoryuqfwuhvj8gua3o0blski8krxaseds2nwiid + uhhkaiisgni + bs + c0mplqzolte4vz8l4lop9 / / / / awweaaaaaaaailegvwmppjtgis8behetziizvhop7lskzagxa50kvksv / 2 ll3qsaaqmegwaaaaaa ` ` ` ( taken from <url>",1
bitcoin/bitcoin,feature inscriptions automatically bitcoin core is used a lot for inscriptions but they could be spent accidentally so lets freeze it . we could not do it in past but <user> helped fix it in <url> lets lock inscriptions with by default it to help users,1
bitcoin/bitcoin,"decodescript could infer miniscript from given script in "" segwit "" context i was playing around with a probably - too - cute script for ln channel smart contract , and was looking for something to decide for me if miniscript can infer the structure of the script . f . e . ` ` ` decodescript 8 2 0 1 2 0 8 8 a914ffffffffffffffffffffffffffffffffffffffff8820ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffad51b2 { "" asm "" : "" op_size <number> op_equalverify op_hash160 f <elongated> op_equalverify f <elongated> op_checksigverify <number> op_checksequenceverify "" , "" desc "" : "" raw ( 8 2 0 1 2 0 8 8 a914ffffffffffffffffffffffffffffffffffffffff8820ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffad51b2 ) <hashtag> 5 guaahuv </hashtag> "" , "" type "" : "" nonstandard "" , "" p2sh "" : "" 3 3 yyjgxsjywsi8wbp7tacsfaujboufywdq "" , "" segwit "" : { "" asm "" : "" <number> 9 9 f5d0b69f9828fea9363198cf812e95d7fc5e94c7881092b8e3f43984c1bd5c "" , "" desc "" : "" addr ( bc1qn86apd5lnq50a2fkxxvvlqfwjhtlch55c7yppy4cu06rnpxph4wqrar3md ) <hashtag> wtlf0gwk </hashtag> "" , "" hex "" : "" 0 0 2 0 9 9 f5d0b69f9828fea9363198cf812e95d7fc5e94c7881092b8e3f43984c1bd5c "" , "" address "" : "" bc1qn86apd5lnq50a2fkxxvvlqfwjhtlch55c7yppy4cu06rnpxph4wqrar3md "" , "" type "" : "" witness_v0_scripthash "" , "" p2sh - segwit "" } } ` ` ` since it ' s evaluated at "" top level for ` desc ` it cannot get any meaningful structure from it . perhaps in ` segwit ` it could try infering the miniscript in the segwit context",1
bitcoin/bitcoin,"expose addrman size info via rpc if # <number> gets merged , addrman will maintain counts of addresses stored on the new & tried tables , broken down by network . this is useful information for monitoring the state of our addrman when we run it with different network configs on mainnet . currently when running a node , we can get a rough estimate into this information using the cli argument ` - addrinfo ` . however , this is just an estimate because it uses the rpc endpoint ` getnodeaddresses ` , which calls through to ` addrman : : getaddr ` . although this logic path skips the addr caching logic we apply for results propagated to the p2p network , it still applies some logic to filter results . for example it removes addresses that meet the ` isterrible ` attributes . it also doesn ’ t distinguish between the new and tried tables . so , i recommend we expose the raw counts to provide a better picture of addrman ’ s workings , especially since addrman & network integrations are particularly hard to test via our automated tooling . we can breakdown the counts by network & new vs tried . there are a few different options for how we can expose this information . i expect this feature to primarily be used by bitcoin core contributors or super - users that understand bitcoind ’ s inner workings . thus , i suggest we expose it as a new , hidden rpc . that enables exposing the information to devs without having to guarantee a stable rpc interface over future versions . the name of the rpc can be something generic like ` addrmaninfo ` , to allow for future extensions for other debugging information . for more historic context around bitcoin cli ’ s ` - addrinfo ` and conversation about cli vs rpc , see [# <number> ] ( <url> if we do end up implementing a new rpc endpoint , we could consider reworking the cli to use these values directly rather than trying to compute them with partial information .",1
bitcoin/bitcoin,"pathing guide for installation using macos * * is your feature request related to a problem ? please describe . * * yes . one common problem with software setups is the pathing is corrupted . proper pathing is one of the most important parts of getting started with any new software because if the path is depreciated the software will not properly run . this is the problem i am currently working on with bitcoin . i downloaded the software but am unable to interact with the network . ` ` ` # in bitcoin - cli getblocktemplate ' { "" rules "" : [ "" segwit "" ] } ' # out zsh : command not found : bitcoin - cli ` ` ` * * describe the solution you ' d like * * ideally , there would be clear instructions on how to install and get started using bitcoin software that did not require extensive knowledge of software systems architecture and pathing . a great installation method would automatically install the software along the right path to prevent the pathing problem . a generalized solution would be really good , providing a clear guide to solving the pathing problem on any machine . * * describe alternatives you have considered * * i have researched solving the problem in the [ bitcoin forum ] ( <url> and on [ stackoverflow ] ( <url> however , because this common problem is so unique to each local machine , a clear or generalized solution is hard to develop . one specific solution i tried from my research was to add additional context to the command . ` ` ` # in > curl - - user username - - data - binary ' { "" jsonrpc "" : "" <number> "" , "" id "" : "" curltest "" , "" method "" : "" getblocktemplate "" , "" params "" : [ { "" rules "" : [ "" segwit "" ] } ] } ' - h ' content - type : text / plain ; ' <url> # out > curl : ( <number> ) failed to connect to <number> . <number> port <number> refused ` ` ` however , the command resulted in a connection refusal . i found a possible solution in the [ bitcoin forum ] ( <url> but the problem with the proposed solution is that there is no ` bitcoin . conf ` file in the main bitcoin repository . i found a ` config ` folder in the ` src ` directory , but there is no ` bitcoin . conf ` file . * * additional context * * i am running on macos . thank you in advance for your attention and advice .",1
bitcoin/bitcoin,"erlay status in getpeerinfo when running with ` - txreconciliation ` ( added in # <number> , but off by default and hidden under ` - - help - debug ` ) it ' s currently rather tedious to see which of your peers have this enabled . it basically involves looking <elongated> at the p2p log . it would be nice to have erlay status under ` getpeerinfo ` .",1
bitcoin/bitcoin,"include estimated time to synchronize blockchain in bitcoind * * is your feature request related to a problem ? please describe . * * no * * describe the solution you ' d like * * currently , the estimated time to synchronize to the bitcoin blockchain is only shown in the qt gui . it would be useful to display the estimated remaining time to synchronize to the blockchain via the bitcoin daemon ' s logs as well . * * describe alternatives you have considered * * display estimated time in the bitcoin daemon synchronizing logs .",1
bitcoin/bitcoin,"setminrelayfee i think this is something within my ability to add . * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > the ability to change minrelaytxfee without restarting . * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > i would like the ability to set the relayfee by a bitcoin - cli rpc call . ` ` ` setminrelayfee amount set the transaction fee rate in btc / kvb for this wallet . overrides the global - setminrelayfee config parameter . can be deactivated by passing <number> as the fee . in that case automatic default min fee will be used . arguments : <number> . amount ( numeric or string , required ) the transaction fee rate in btc / kvb result : true | false ( boolean ) returns true if successful examples : > bitcoin - cli setminrelayfee <number> > curl - - user myusername - - data - binary ' { "" jsonrpc "" : "" <number> "" , "" id "" : "" curltest "" , "" method "" : "" setminrelayfee "" , "" params "" : [ <number> ] } ' - h ' content - type <url> ` ` ` * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > * * additional context * * < ! - - add any other context or screenshots about the feature request here . - - > i encountered this while hooking up a lightning network node . my relay fee was too high from bad conf settings and was blocking funding transactions . i wasn ' t able to change this value and had to restart bitcoind instead of just making an rpc call .",1
bitcoin/bitcoin,"dumpprivkey does not work in descriptor wallet in a descriptor wallet it returns "" this type of wallet does not support this command "" instead of the private key . even though the wallet does not store the key directly it obviously knows how to build it since it can spend the funds so it should do that and return it . there are just too many cases where a private key needs to be dumped and it should not be a nightmare of rpc calls and 3 rd party scripts to try to get it , that just does not make sense usability - wise . just a common case point for us for example , we still use p2sh - segwit for our users since too many places still do not support bech32 and pretty regularly people still send ltc to our btc addresses ( smh , i know ) and we need to easily be able to dump the key to recover it for them .",1
bitcoin/bitcoin,"rpc : ` listunspent ` does not return immature coinbase outputs * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > i need to know all the utxos i have in my wallet , including the immature ones . when using listunspent , the immature coinbases are not returned : ` ` ` ❯ bitcoin - cli listtransactions [ { "" address "" : "" bcrt1q0epv47eaxhyenhqcjp8v7wytjx26k35r6w2py6 "" , "" category "" : "" immature "" , "" amount "" : <number> , "" label "" : "" "" , "" vout "" : <number> , "" confirmations "" : <number> , "" generated "" : true , "" blockhash "" : "" 3 3 4 9 9 6 db387320711e1176f2591dfc16fe8d92f617af47ae65b97c2a817758b0 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 4 8 0 f0340a98755cfb095743b5464e87d0cf1b0200d204507ec8448962bcafbe6 "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" } ] ❯ bitcoin - cli listunspent [ ] ` ` ` * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > what do you think of adding a ` include_immature_coinbase ` ( default to false ) parameter to ` listunspent ` ? if you think it ' d be useful , i can start working on this myself . * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > at the moment i am calling ` listtransaction ` and adding to the utxo list every immature output i see ( <url> this works just fine , but having the ` include_immature ` parameter would make the code a bit cleaner ( and it would save one rpc call ! ) .",1
bitcoin/bitcoin,add ensurewalletisunlocked to rescanblockchain rpc ? it seems that it would otherwise silently skip the rescan for pre - backup used keys that are still waiting to be generated into the keypool ? see also # <number>,1
bitcoin/bitcoin,"progress for opening wallet while there is progress for importing private keys and rescanning , there is good reason to also add progress for wallet opening . now you need to look into debug . log and trace it for progress , so adding a progress bar for opening wallet ( it is rescan in the end ) would be a very nice feature .",1
bitcoin/bitcoin,"allow groups of accounts to access the rpc cookie file * context <emphasis> * i have multiple accounts on the same box that need to run clients of bitcoind . in particular it ' s lnd and electrs . bitcoind itself runs in a separate account . i have other accounts that i do not want to have access to bitcoin grafana ( graphs / charts web ui ) . so , the cookie file can be shared amount the relevant accounts via a group . on the file system a group read permissions can be set on each file . i call the group "" bitcoinclients "" . i setup a directory where new files will inherit the bitcoinclients group and will be created with the u = rw , g = r , o= permission . this means bticoin account can write the file , bitcoinclinets group can read it , and no one else has access . * * is your feature request related to a problem ? please describe . * * the problem is that when ` bitcoind ` is restarted it drops the group read permission on the cookie file so clients in other unix accounts can not read the file . * * describe the solution you ' d like * * there should be a config that allows controlling permissions of the cookie file . just a boolean that sets the read permissions on the group would be sufficient . * * describe alternatives you have considered * * i setup a [ chronos workaround ] ( <url> but i bet more people will have to re - invent this wheel as the cookie file gets wider adoption . * * additional context * * [ cookie file ] ( <url> has [ more security ] ( <url> over user + password auth . user + password auth is being deprecated in favor of the cookie file .",1
bitcoin/bitcoin,"gettransaction does not contain the field "" abandoned "" for abandoned receiving tx in specter <url> a user can abandon an * receiving <emphasis> * tx that was evicted from the mempool ( for example because it was double spent ) . the tx is marked via [ abandontransaction ] ( <url> as abandoned in bitcoin core . however [ gettransaction ] ( <url> and [ listtransactions ] ( <url> do not return the field ` abandoned ` , even though this is visible in the gui : [ image ] ( <url> * consequence <emphasis> * : specter cannot get the ` abandoned ` information from bitcoin rpc , and will fetch the abandoned tx back again via [ listtransactions ] ( <url> after it was deleted in specter . * * describe the solution you ' d like * * always return the ` abandoned ` field in <url> and not only in this special case <url> that should then automatically solve [ gettransaction ] ( <url> via <url> * * describe alternatives you have considered * * if specter can not get the information about abandoned tx from bitcoin , it would have to keep an internal list . that is possible , but redundant and can lead to many edge cases . * * steps to reproduce * * - wallet a : send <number> btc to wallet b ( rbf ) - wallet a : replace tx with rbf to send all funds to wallet a - wallet b : abandon tx ( because no longer in mempool ) - wallet b will not show the ` abandoned ` field",1
bitcoin/bitcoin,"export a watch wallet only ( with descriptors and without private keys ) for an air gap setup * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > for long time i have seen users looking for a way to do an air gap setup with bitcoin core , since there were no easy way people tend to go with other solutions such as armory , electrum which offers an easy and friendly way to do it . since bitcoin core v <number> and with the command listdescriptors it can be done easily pc - create a wallet with descriptors = true , export descriptors with "" listdescriptors "" . online pc - create a wallet without privatekeys , descriptors = true , and importdescriptors to have a functional watch only wallet for receiving funds and create unsigned transactions . * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > instead of creating two wallets , one offline and a watch only and manually have to export / import descriptors , the wallet containing the private keys could offer an option like "" exportwatchonly "" that would generate a "" watch_wallet . dat "" without private keys and all descriptors already imported . making very easy and user friendly the process to create an air gap setup .",1
bitcoin/bitcoin,"ci , tracing tracepoint interface tests in the ci the first tracepoint tests were merged in # <number> . however , they are skipped in the ci . i have already done some experiments running them in the ci in <url> however i have not been successful with it yet . the main issue was the limited capabilities inside the cirrusci docker container . an option would be to ask cirrusci for help , another would be to run this part in a vm , if it ' s not to much overhead .",1
bitcoin/bitcoin,"enable consistency checks by default with ` - - enable - debug ` now that most consistency checks have been made a runtime option instead of configure option ( see commit 8 0 3 ef70fd9f65ef800567ff9456fac525bc3e3c2 ) , it could make sense to enable them on ` - - enable - debug ` . otherwise they are only run when explicitly passed the runtime setting .",1
bitcoin/bitcoin,"isstandard should check the witnessprogram is a valid point on the field * * is your feature request related to a problem ? please describe . * * a v1 taproot witness program must be a valid point on secp256k1 otherwise it is essentially a black hole . * * describe the solution you ' d like * * add a check in isstandard that will check the witnessprogram in the case where witness_v1 is returned . if the witness program is not the x value of a valid point , return false . ( <url>",1
bitcoin/bitcoin,"edit src / index / base . cpp * * is your feature request related to a problem ? please describe . * * txindex sync message will print more than exactly twice a minute because current_time and last_log_time are in seconds , it jitters by one second <url> * * describe the solution you ' d like * * change ' < ' with ' <= ' makes the message appear [ twice exactly a minute ] [ <number> ] , in <url> for consistency , probably as well in <url> ( among others ? ) but i am not interested in analyzing implications of changing that line <number> * * describe alternatives you have considered * * i suppose there ' s a way to change this by using millisec resolution in these time variables . * * additional context * * thanks [ <number> ]",1
bitcoin/bitcoin,ci testing with minimum supported versions it would be nice if we could get a ci that always tests our minimum supported versions . maybe we can use guix for it ?,1
bitcoin/bitcoin,"feature request : automatically upgrade legacy to hd wallet since bitcoin core <number> the startup option ` upgradewallet ` has been available , allowing users to upgrade old legacy wallets to hd wallets . however many users with non - hd wallets are yet to upgrade their wallet , to reduce complexity , and or various issues related to non - hd wallets , i am wondering if it may be a good idea to require users to upgrade . two different ways of implementing this ( off the top of my head ) when all addresses in a keypool are marked used , automatically upgrade wallet and notify user to make a backup . - on startup upgrade wallet and notify user to make a backup are there any reasons for why bitcoin core does not already do this ?",1
bitcoin/bitcoin,"[ rpc ] removal of signmessage and verifymessage those two features are present for a long in bitcoin core , but has been scarcely used over the years . however , this is going to change with [ aopp ] ( <url> who directly link at [ this code ] ( <url> in the protocol ' s implementation .",1
bitcoin/bitcoin,"proof of advanced delegation if we remove the consensus protocol ( blockchain and mining ) what do we get ? we are left with what satoshi nakamoto had before he added the consensus protocol . no blockchain , no mining , just the transaction history with nothing to protect it from double spend attacks . here i present what i call "" proof of advanced delegation "" ( pad ) . pad eliminates double spends by spending them in advance . when an output is created , the transaction that spends it is created too and it ' s txid is embedded in the output before it gets published . the output of the last transaction is not yet spent so it remains unpublished . beginning with an arbitrary transaction in the ledger , a , it ' s output is a1 . the unpublished transaction that spends it is b and it ' s output is b1 . to make a payment , the sender : <number> . creates a new transaction c that splits b1 between two new outputs , c1 payment and c2 change . <number> . generates the txid of c and embeds it in b1 . <number> . creates another transaction e that spends c2 and embeds it ' s txid in c2 . <number> . publishes transactions b ( advanced delegation ) . <number> . sends transaction c with sigs to the receiver in secret . the receiver then sees transaction b on the network which validates and binds it to transaction c . <number> . creates two new transactions of his own , d and f . d spends c1 and f spends d1 . <number> . publishes c and d as a pair and saves f until he ' s ready to spend it . since every published output is bound to the txid of the unpublished transaction that spends it , there is no opportunity to double spend them therefore no consensus mechanism is required .",1
bitcoin/bitcoin,"fuzz : prototype for cross - language differential fuzzing since differential fuzzing is strongest when different programming languages are involved , i ’ d like to fuzz cpp code against python code . * solution <emphasis> * : use sockets to establish a tcp client / server connection . - * server <emphasis> * - a python file which listens and accepts client communications from the fuzz target , performs the required operations in python and sends back the computed python output as a response back to the client . - * client <emphasis> * - a fuzzing harness which can send requests to the python server to perform certain operations , receives python output computed in the server file and compares cpp output with the python output to see if they match . since this requires a slight architectural change in the fuzzing harness to allow python subprocesses , i ’ d like to know if this solution is acceptable . * * additional context * * i ’ ve implemented a prototype which performs cross - language differential fuzzing of chacha20 on [ this branch ] ( <url> and would love to hear opinions / feedback on how to proceed . * * test instructions * * <number> . start server : ` python3 src / test / fuzz / script . py ` <number> . run the fuzz test src / test / fuzz / fuzz `",1
bitcoin/bitcoin,"add ability to remove ( dust ) utxos * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > problem : i have a lot of <number> sat utxos ( after flooding the testnet lol ) * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > an rpc call to remove utxos for example removeutxos ` with an argument to remove everything with less than the specified amount * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > making a new wallet , sending coins from the old one to the new one , remove the old wallet * * additional context * * < ! - - add any other context or screenshots about the feature request here . - - >",1
bitcoin/bitcoin,"improve test framework time with run order directive if you could leave a directive in one of the commits in your tree , e . g . ` ` ` > run - priority ` ` ` and we could use that to instruct the ci to run that test _first_ , that would be pretty helpful in speeding up ci if there ' s a particular e . g . new or impacted test that should see failures before others . it will not help with passing , but it could help with ' fail fast ' .",1
bitcoin/bitcoin,"allow positional and named arguments in ` bitcoin - cli ` * * is your feature request related to a problem ? please describe . * * ` bitcoin - cli sendtoaddress "" bc1q09vm5lfy0j5reeulh4x5752q25uqqvz34hufdl "" <number> "" "" "" "" true false <number> unset true <number> true ` or ` bitcoin - cli - named sendtoaddress address = "" bc1q09vm5lfy0j5reeulh4x5752q25uqqvz34hufdl "" amount = <number> verbose = true ` ? * * describe the solution you ' d like * * ` bitcoin - cli sendtoaddress bc1q09vm5lfy0j5reeulh4x5752q25uqqvz34hufdl <number> verbose = true ` add the ability to use positional and named arguments in one rpc call",1
bitcoin/bitcoin,"make rescans faster this issue aggregates ideas for making wallet rescans substantially faster . using compact block filters , when available , could substantially speed up wallet rescans although it is not clear to me how this might work for non - descriptor - based ( legacy ) wallets . currently , # <number> proposes to add an rpc that will use compact block filters to scan for blocks relevant to a given script descriptor . this gives a listing of blocks that correspond to the descriptor , but the user then has to do something with that list of blocks to find transactions relevant to their wallet . it would be nice to integrate scanblocks ' use of block filters opaquely into ` rescanblockchain ` ( wallet rescanning ) so that if block filters are available , the faster method of consulting them could be used instead of scanning the entire chain , or forcing the user to do something more roundabout with the output of ` scanblocks ` . from <user> : > regarding faster rescanning , i think there is some lower - hanging fruit , like ( a ) rescanning all wallets at startup in parallel rather than one by one and ( b ) building a set of scriptpubkeys to look for and using that for ( legacy wallet ) rescanning , instead of calling ismine for each ( especially with achow101 ' s migrate - to - descriptor code , i think there should be a way of computing that set for legacy wallets too ) # # # possible changes - [ ] integrate compact block filter usage into ` rescanblockchain ` to allow faster rescans when the compact block index is available , otherwise falling back to the standard scanning method . a first approach could possibly use filters only when dealing with a non - legacy descriptor wallet - currently proposed in <url> - [ ] allow pruned nodes with the block filter index to rescan quickly by getting a list of relevant blocks to the wallet , requesting the download of blocks , and then parsing the blocks for relevant transactions - [ ] rescanning all wallets at startup in parallel ( instead of one - by - one ) - [ ] building a set of scriptpubkeys to look for when using a legacy wallet based on <user> ' s migrate - to - descriptor code",1
bitcoin/bitcoin,"add benchmark to test large scale of hardware i am not sure if there are any benchmarks that could be easily used to test large scale of hardware wrt type of bottleneck ( cpu / ssd bound ) . i recall i saw something on twitter mentioned by <user> but not sure now what it was about . benching cpus , rams and ssds is my daily job and i have been wanting to have some bitcoin related benchmarks in my suite . last month i was rebenching <number> cpus and i wish i had relevant data on their performance in bitcoin software . it would be best if we could have such benchmarks as part of <url> and <url> so we could bench all kind of hardware and different versions of bitcoin core with single command . that way we could have a lot of open data which i am sure could be useful to development .",1
bitcoin/bitcoin,"tracing tracepoints in guix builds there has been light conceptual agreement on including the usdt based tracepoints in bitcoin core release builds . this , for example , enables user to hook into production deployments , if they need to . binaries do not have to be switched out . this is possible because we do not do [ expensive computations ] ( <url> only for the tracepoints and the tracepoints are nops when not used . there is a slight chance that the guix build on the current master already includes the tracepoints . i have not done an guix build on a branch with the tracepoints merged yet . this can be tested on a ` bitcoind ` binary using one of the methods mentioned in the [ tracing documentation section "" listing avaliable tracepoints "" ] ( <url> if not present , making the ` systemtap ` headers ( ` sys / sdt . h ` ) available during the guix build should build ` bitcoind ` with the tracepoints .",1
bitcoin/bitcoin,"return fee in ` getrawtransaction ` similar to how the fee is returned in ` getblock ` (# <number> ) , it should be returned in ` getrawtransaction ` . # # # # useful skills of undo data , the rpc interface and the functional tests . # # # # want to work on this issue ? for guidance on contributing , please read [ contributing . md ] ( <url> before opening your pull request .",1
bitcoin/bitcoin,rpc fee bumping with external inputs external inputs are currently only supported for initial transaction creation functions . extend ` solving_data ` arguments as necessary to support fee bumping wallet transactions with non - wallet inputs . <url>,1
bitcoin/bitcoin,"other platform support for linux syscall sandboxing now that # <number> is merged , it would be nice if syscall sandboxing was extended to the other linux platforms that have release binaries as well [x ] ` x86_64 - linux - gnu ` - [ ] ` arm - linux - gnueabihf ` - [ ] ` aarch64 - linux - gnu ` - [ ] ` riscv64 - linux - gnu ` - [ ] ` powerpc64 - linux - gnu ` - [ ] ` powerpc64le - linux - gnu `",1
bitcoin/bitcoin,"plumb "" too - long - mempool - chain "" to rpc error for send / sendtoaddress * * is your feature request related to a problem ? please describe . * * if coin selection fails as part of ` send ` or ` sendtoaddress ` , the error message returned to the rpc client is ` insufficient funds ` , even if the actual cause is due to ` too - long - mempool - chain ` . this is likely to alarm users unnecessarily , as they will believe this means their funds have permanently vanished rather than being temporarily inaccessible . * * describe the solution you ' d like * * if coin selection fails due to ` too - long - mempool - chain ` , the ` send ` and ` sendtoaddress ` methods should pass that error back to the rpc client , instead of ` insufficient funds ` . * * describe alternatives you have considered * * it would be even nicer from a ux standpoint if the limit on mempool chain length were removed , but aiui [ that ' s a dos vector ] ( <url> so is not going to happen . * * additional context * * apparently this is [ already done for ` sendrawtransaction ` ] ( <url> the discussion in <url> is relevant ( it appears that the pr was withdrawn for reasons unrelated to my request ) .",1
bitcoin/bitcoin,"auto - generate msvc version information right now , bumping the version involves updating the version in two places : - ` configure . ac ` - ` build_msvc / bitcoin_config . h ` ( in multiple places — see for example commit f277b1782c5aae0fab83e3d678189c150ae0f263 ) i think it should be doable to parse the information from ` configure . ac ` in ` msvc - autogen . py ` , so that this is no longer something error - prone that has to be done manually . if you do this , also update [ the release process ] ( <url> # # # # useful skills msvc # # # # want to work on this issue ? for guidance on contributing , please read [ contributing . md ] ( <url> before opening your pull request .",1
bitcoin/bitcoin,"fuzz python interpreter from a c + + file i am interested in reimplementing a python version of the new [ chacha20poly1305 <user> aead ] ( <url> and fuzzing it against the c + + implementation . however since it involves invoking a python interpreter from inside a c + + file , i am confused on how to proceed . * * possible approaches * * <number> . using pipes to call the python script from the c + + file . <number> . using [ pybind11 ] ( <url> to create the python c + + interface ( idea from [ this pr ] ( <url> however the python to c + + code for the aead would not be very readable here . would method <number> be ok ? i ' d love to hear your thoughts on how to proceed .",1
bitcoin/bitcoin,"rpc : add level <number> verbosity to getblock rpc call (# <number> modified ) author of # <number> expressed [ time issues ] ( <url> in the original pr . given that # <number> has received a lot of review * , i have decided to open this new pull request with [ modifications required to get ack from luke - jr ] ( <url> and a few nits of mine . # # # original pr description > display the prevout in transaction inputs when calling getblock level <number> verbosity . this pr affects the existing ` / rest / block ` api by adding a ` prevout ` fields to tx inputs . this is mentioned in the change to the release notes . > > i added some functional tests that > > * checks that the rpc call still works when txundo can not be found > > * does not display the "" value "" or "" scriptpubkey "" of the previous output when at a lower verbosity level > > > this "" completes "" the issue # <number> # # # possible improvements * <url> - i can include even this commit to this pr if deemed useful or i can leave it for a follow - up pr . see <url> for more context . # # # examples examples of the ` getblock ` output with various verbose levels . note that ` 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f682b188971cc1a121546be4e9d5baf22934fdc7f538288d5 ` contains only <number> transactions . ( see : <url> # # # # verbose level <number> ` ` ` bash . / bitcoin - cli - testnet getblock 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f682b188971cc1a121546be4e9d5baf22934fdc7f538288d5 <number> ` ` ` # # # # # verbose level <number> ` ` ` bash . / bitcoin - cli - testnet getblock 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f682b188971cc1a121546be4e9d5baf22934fdc7f538288d5 <number> ` ` ` # # # # # verbose level <number> ` ` ` bash . / bitcoin - cli - testnet getblock 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f682b188971cc1a121546be4e9d5baf22934fdc7f538288d5 <number> ` ` ` # # # # # verbose level <number> ` ` ` bash . / bitcoin - cli - testnet getblock 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f682b188971cc1a121546be4e9d5baf22934fdc7f538288d5 <number> ` ` ` # # # # rest ` ` ` bash curl - h "" content - type : text / plain ; "" <url> ` ` ` <sub> * . <repeated> and my everyday obsessive checking of my email inbox whether the pr moves forward . </sub> edit laanwj at symbol from message , and large example output to prevent it from all ending up in the commit message .",1
bitcoin/bitcoin,"rpc / wallet : add simulaterawtransaction rpc ( note was originally titled "" add analyzerawtransaction rpc "" ) this command iterates over the inputs and outputs of the given transactions , and tallies up the balance change for the given wallet . this can be useful e . g . when verifying that a coin join like transaction does not contain unexpected inputs that the wallet will then sign for unintentionally . i originally proposed this to elements ( <url> and it was suggested that i propose this upstream . there is an alternative # <number> to instead add this info to ` getbalances ` when providing an optional transaction as argument .",1
bitcoin/bitcoin,"fuzz : investigate test adequacy ( unit and fuzz ) via mutation testing <user> / <user> i am starting some mutation analysis using <url> a couple of questions at random i picked ` tx_verify . cpp ` as a file with high coverage that is likely critical , where any missed non - equivalent mutants should be carefully investigated . is this a good choice to get the workflow going on ? - is there any mapping from fuzz targets to code expected to be covered ? i can run all targets , of course , but knowing which ones might possibly detect ` tx_verify . cpp ` mutants would be helpful .",1
bitcoin/bitcoin,"createwallet with external signer always picks the first external signer , regardless of how many are connected * * is your feature request related to a problem ? please describe . * * while testing ` <number> . 0 rc2 ` external signer feature , i noticed that if multiple hw wallets are connected to the computer , ` createwallet ` will always pick the first one without alerting the user in any way . this led to the following unexpected behavior attach a trezor and create a wallet using ` bitcoin - cli createwallet ` <number> . attach a coldcard while the trezor is still connected and create a second wallet with ` bitcoin - cli createwallet ` both wallets were created fine with no errors , but on further inspection i realized both wallets were connected to the trezor , despite having different names . this is related to # <number> * * describe the solution you ' d like * * ideally , if multiple external signers are returned from ` enumeratesigners ` and i then try to create a wallet , i would want ` createwallet ` to fail and warn me there are multiple external signers . additionally , an option to specify the fingerprint of the device would be useful , but i think simply failing would be sufficient for now . * * describe alternatives you have considered * * granted , this is a rare case where a user would have multiple hw ' s connected while creating a single sig wallet , so perhaps just documenting that you should only have one connected at a time is a viable solution .",1
bitcoin/bitcoin,"netinfo addr_ { processed , rate_limited , relay_enabled } and relaytxes data update cli - netinfo to display the getpeerinfo ` addr_processed ` , ` addr_rate_limited ` , ` addr_relay_enabled ` and ` relaytxes ` data with auto - adjusting column widths . ` ` ` $ . / src / bitcoin - cli - netinfo help txn time since last novel transaction received from the peer and accepted into our mempool , in minutes "" * "" - the peer requested we not relay transactions to it ( relaytxes is false ) addrp total number of addresses processed , excluding those dropped due to rate limiting "" . "" - we do not relay addresses to this peer ( addr_relay_enabled is false ) addrl total number of addresses dropped due to rate limiting ` ` ` [ screenshot from <number> - <number> - <number> <date> ] ( <url>",1
bitcoin/bitcoin,"sub wallet for dirty coins * * is your feature request related to a problem ? please describe . * * - unable to spend unconfirmed utxo - output_group_max_entries = <number> the combination of these two things affects privacy which is discussed in detail here : <url> * * describe the solution you ' d like * * [ image ] ( <url> we are already marking used addresses as ' grey ' in <url> can improve this by marking ' red ' for addresses used twice or more for receiving bitcoin . grey color for addresses used once . once we have dirty coins ( received bitcoin twice or more ) , they should be locked automatically , dump private keys associated with the addresses and import them in a new wallet ( sub - wallet for dirty coins ) . not sure if there should be a sub wallet for each address or all dirty coins go in one sub wallet . * * describe alternatives you have considered * * use other wallets * * additional context * * this will improve following things no wallet fingerprinting <number> . keep things separate <number> . manage [ forced address reuse ] ( <url> in a better way <number> . neither utxos being confirmed nor value for ` output_group_max_entries ` will matter anymore . the user can decide how to spend dirty coins in future . either use all in one transaction or do coinjoin or something else . looking for concept acks and discussion about this solution so that implementation can be planned if enough people agree to add this feature . thanks <user> for suggesting idea about sub - wallets in <url>",1
bitcoin/bitcoin,"nulldata treated as nonstandard bitcoind returns ' nonstandard ' addresses from scripts started from op_return . e . g . in first <number> kiloblocks ( bk . tx . vout ) : - <number> . <number> - <number> . <number> - <number> . <number> - <number> . <number> - <number> . <number> - <number> . <number> as an example - vout <number> . <number> ( block height <number> , tx <number> , vout <number> <sad> ` ` ` json { "" value "" : <number> , "" n "" : <number> , "" scriptpubkey "" : { "" asm "" : "" op_return op_dup op_hash160 cd2b3298b7f455f39805377e5f213093df3cc09a op_equalverify op_checksig "" , "" hex "" : "" 6 a76a914cd2b3298b7f455f39805377e5f213093df3cc09a88ac "" , "" type "" } } ` ` `",1
bitcoin/bitcoin,"wallet : add destination ( output ) and bump fee this would allow ad hoc transaction batching . when a wallet transaction is unconfirmed , i ' d like to be able to add an additional destination / output . [ bip <number> ] ( <url> requires that such a transaction also increases the fee rate . one approach would be to create a new wallet rpc ` addoutput , which takes a transaction hash , address , amount and fee rate ( difference ) . a better approach imo would be to add a new argument ` extra_outputs ` to ` bumpfee ` ( and ` psbtbumpfee ` ) . this would have the same format as ` outputs ` in [ send ] ( <url> two complications come to mind : <number> . if the original transaction used ` subtract_fee_from_outputs ` we do not store that information ( see also # <number> ) , so either the user has to provide that array again for us to subtract the additional fee correctly ; or b ) the extra fee is paid entirely from change <number> . if the previous transaction confirms , rather than the new one , the user has to figure this out and manually craft a new transaction",1
bitcoin/bitcoin,"enable reading of pchcommand before complete ( ) i always found it useful to be able to see what message was incoming from another node as soon as the header had been received , but since # <number> it no longer appears to be possible to discern the pchcommand until the complete message has been received . is there a way to determine the message as soon as the header has been received ? this could be useful functionality in order to be able to reduce bandwidth , and for logging and debugging .",1
bitcoin/bitcoin,"broadcast a transaction to specific nodes * * is your feature request related to a problem ? please describe . * * there is no argument in [ ` sendrawtransaction ` ] ( <url> to mention the peers * * describe the solution you ' d like * * add an argument in ` sendrawtransaction ` that accepts one or more addresses for nodes . * * describe alternatives you have considered * * <url> * * additional context * * > note that the transaction will be sent unconditionally to * * all peers * * , so using this for manual rebroadcast may degrade * privacy <emphasis> * by leaking the transaction ' s origin , as nodes will normally not rebroadcast non - wallet transactions already in their mempool . <sup> <url> rebroadcast thing mentioned above will be fixed in <url> however this option will still be useful .",1
bitcoin/bitcoin,"develop watch - only wallet * * is your feature request related to a problem ? please describe . * * i am trying to create a watch only wallet on bitcoin - qt with bitcoin core . * * describe the solution you ' d like * * password / pin protected , qr - code capability .",1
bitcoin/bitcoin,"style guide enum bool thoughts in many places we use ` bool ` parameters to denote some effect . i suggest that throughout the codebase , we make an effort to ( where feasible / touched ) replace these with enum class . it ' s a bit more verbose , but it ' s already a norm to do ` f ( /* confusing bool */ true ) ` , and it would be nice if our compiler helped us out . further , it makes it easier should later code extend the functionality to add more / different options . in order to reduce clutter we could make a macro ` boolenum ( argument , enabledname , disabledname ) ` not high importance , so i do not think we should make a project out of it , but i was looking at some code recently where it would have been nice . con do not need more structs .",1
bitcoin/bitcoin,"how to tell compilers to not drop the lock stack when using assume / assert ? it would be nice if there was a way to tell compilers not to drop the lock stack and thus issue warnings when accessing a symbol that needs a lock inside ` assume ` / ` assert ` . hit by ( at least ) , <user> , me .",1
bitcoin/bitcoin,"add mitigation for disk fill via logging attacks short version : it would be nice if we could render "" disk fill via logging "" vulnerabilities unexploitable by introducing a mitigation like the one suggested in the "" up for grabs "" pr # <number> . that pr was concept ack : ed by <user> , <user> and <user> , but unfortunately closed due to lack of time . volunteers welcome - - - long version : a disk fill attack is an attack where an untrusted party ( such as a peer ) is able to cheaply make your node log to disk excessively . the excessive logging may fill your disk and thus make your node crash either cleanly ( best case : if disk fill rate is relatively slow ) or uncleanly ( worst case : if disk fill rate is relatively fast ) . it is easy to accidentally introduce a disk fill vulnerability it takes is a ` logprintf ` in a code path which is easily and cheaply triggered by a remote attacker . it would be nice if we could kill this vulnerability bug class by introducing a general mitigation mechanism which would remove the ability exploit such such a misplaced ` logprintf ` . ( our first line of defence would obviously be to never misplace a ` logprintf ` , but realistically logging mistakes happen and that ' s where mitigations kick in as a _second_ line of defence . ) one possible mitigation was suggested in pr # <number> which received concept acks from <user> , <user> and <user> . the reviewers came up with some good ideas for improvements which need to be implemented . unfortunately i do not have time to implement those changes myself , but if someone is looking for "" up for grabs "" prs then # <number> would be a very good choice . it is seldom one gets the chance to kill an entire vulnerability bug class <happy> i ' d be glad to review and help out . the solution suggested in the referenced pr is one of many possible solutions , but regardless of which solution we choose i think we need _some_ disk fill attack mitigation to kill this bug class once and for all <happy>",1
bitcoin/bitcoin,"contrib : current systemd usage does not allow for dependencies the current [ systemd service ] ( <url> does not allow gracefully for services to depend on ` bitcoind ` . all dependent units will be launched immediately , regardless of current status . so if it is to work at all , they need an inefficient polling loop . in the service file the service type is specified as ` type = forking ` but we do not heed the behavior specified . from the ` systemd . service ( <number> ) ` man page : > if set to forking , it is expected that the process configured with execstart = will call fork ( ) as part of its start - up . the parent process is expected to exit when start - up is complete and all communication channels are set up . a possible solution is # <number> , which adds ` - daemonwait ` so that the control is only returned to the parent process when startup is complete . in combination with ` type = forking ` , i think this achieves the proper functionality start dependent units only when rpc functionality is working . with that the only change to the service file needed would be to use ` - daemonwait ` instead of ` - daemon ` . an alternative would be to implement support for ` type = notify ` and do readyness notification through ` sd_notify ` . but i opted for daemonwait because it seems more generally useful . the advantage of using a native notification mechanism , however , would be that it is still possible to have logging through stdout to the journal . the drawback that it introduces an optional dependency on ` libsystemd ` .",1
bitcoin/bitcoin,"solve for % <number> attact * * is your feature request related to a problem ? please describe . * * yes , this problem is security problem for pow consensus protocol . * * describe the solution you ' d like * * solution is basic , the new consensus algorithm safe proof of work , blocks will have a two hash block , and miner select from random , first hash mining or second hash mining . this is changed all mining proccess . and add into the hash , the merkle root of the blocks of the blockchain the miners wants to add . if first hash blocks merkler root not equal second hash blocks merkle root , all nodes and miners restart process . * * describe alternatives you have considered * * if you want to know more , you can check the my github repository . ( onuratakan / spow ) i published with mozilla public license .",1
bitcoin/bitcoin,"guix : prints "" g + + : not found "" when building depends steps to reproduce : start a guix build on a system without gcc . ` ` ` / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin18 / native / bin / clang ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin18 / native / bin / clang ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin18 / native / bin / clang + + ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin18 / native / bin / clang + + ' : no such file or directory found macos sdk at ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / sdks / xcode - <number> . <number> - 1 1 c505 - extracted - sdk - with - libcxx - headers ' , using . <repeated> make : entering directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found make [ <number> <sad> entering directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin / native / bin / clang ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin / native / bin / clang ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin / native / bin / clang + + ' : no such file or directory env : ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends / x86_64 - apple - darwin / native / bin / clang + + ' : no such file or directory make [ <number> <sad> leaving directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' make [ <number> <sad> entering directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found make [ <number> <sad> leaving directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' make [ <number> <sad> entering directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' / bin / sh : <number> : gcc : not found / bin / sh : <number> : gcc : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : g + + : not found / bin / sh : <number> : x86_64 - w64 - mingw32 - gcc : not found / bin / sh : <number> : x86_64 - w64 - mingw32 - gcc : not found / bin / sh : <number> : x86_64 - w64 - mingw32 - g + + : not found / bin / sh : <number> : x86_64 - w64 - mingw32 - g + + : not found make [ <number> <sad> leaving directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' make : leaving directory ' / home / micap / temp / scratch / guix / bitcoin / bitcoin / depends ' info commit a4903f747ccd for platform triple x86_64 - linux - gnu :",1
bitcoin/bitcoin,"refactor ( move - only ) into a "" block storage "" module see <url> * <url>",1
bitcoin/bitcoin,"add ranges to to output descriptors [ output descriptors ] ( <url> do not contain support for ranges . bip44 / <number> / <number> account uses two chains ( for receiving addresses and for change addresses ) . having possibility to express ranges would allow us to capture the essence of bip44 / <number> / <number> accounts in one output descriptor , e . g . ` ` ` pkh ( [ d34db33f / <number> ' / <number> ' / <number> ' ] xpub6erapfzwunrhlckdtchtcxd75rbzs1ed54g1lkbuhqvhqkqhmkhgbmjbzrkrgzw4koxb5jahwky4alhy2grbgrjadmzqlcgjvljuzzvrcel / [ <number> - <number> ]/* ) ` ` ` ( note the ` [ <number> - <number> ]/* ` at the end ) .",1
bitcoin/bitcoin,"github < > slack integration * * is your feature request related to a problem ? please describe . * * no * * describe the solution you ' d like * * can you add the slack github integration to this repo ( <url> so that changes can be easily tracked in slack * * describe alternatives you have considered * * i understand that email notifications can be used , but in my opinion slack notifications provides easier team organisation . * * additional context * * none",1
bitcoin/bitcoin,"feature : add editorconfig # # # motivation developers are supposed to follow [ coding style ] ( <url> however , [ from time to time ] ( <url> a pr is created and then its author is asked to change tabs to spaces , for example . introducing an ` . editorconfig ` file can mitigate these formatting issues : # # # user story a contributor wants to create a new pr . she clones bitcoin core repo , opens her editor , the editor loads ` . editorconfig ` rules and writes her patch with correct formatting rules . less coding style issues is then discovered in the pr review process and thus less ci runs are needed . # # # what is editorconfig file ? <url> provides very well and concise explanation : > what is editorconfig ? > editorconfig helps maintain consistent coding styles for multiple developers working on the same project across various editors and ides . the editorconfig project consists of a file format for defining coding styles and a collection of text editor plugins that enable editors to read the file format and adhere to defined styles . editorconfig files are easily readable and they work nicely with version control systems . # # # support ` . editorconfig ` is supported in many ides and text editors . sometimes , the support is out - of - box and sometimes a plugin is needed . however , for example , vs code detects ` . editorconfig ` presence and automatically offers you to install the missing plugin . see <url> for details on support . visual studio is supported , vs code and intellij idea are supported and many others . # # # my editor does not support ` . editorconfig ` then nothing really changes for you . # # # ` . editorconfig ` vs ` . clang - format ` as explained [ here ] ( <url> > note that visual studio also supports editorconfig , which works in a similar way . clangformat , however , has a [ much larger variety of style options ] ( <url> than editorconfig , including some very c + + specific rules that can be set , and it is already used by c + + developers today . having both ` . editorconfig ` and ` . clang - format ` in a project , may not always work correctly though , i think . as some editors may have a plugin for ` . editorconfig ` and a plugin for ` clang - formatter ` which may not work correctly in unison . # # # proposed ` . editorconfig ` this proposal is based on [ developer notes ] ( <url> ` ` ` yaml # this is the top - most editorconfig file . root = true # for all files . [* ] charset = utf - <number> end_of_line = lf insert_final_newline = false # shell scripts [* . sh ] indent_size = <number> indent_style = space trim_trailing_whitespace = true # c + + files [* . { h , cpp } ] indent_size = <number> indent_style = space trim_trailing_whitespace = true # python files [* . py ] indent_size = <number> indent_style = space trim_trailing_whitespace = true # makefiles [ makefile , * . am ] indent_style = tab trim_trailing_whitespace = true # markdown files [* . md ] trim_trailing_whitespace = true # . cirrus . yml , . appveyor . yml , . fuzzbuzz . yml , etc . [* . yml ] indent_style = space indent_size = <number> trim_trailing_whitespace = true ` ` ` note that the syntax can be much shorter but it is not my goal here , i prefer clarity at this point . so what is this specifying : * ` charset ` : use ` utf - <number> ` in all files . * ` end_of_line ` : line endings should be ` lf ` by default in all files . * ` insert_final_newline ` : do not add a blank line at the end of a file by default . and then there are rules for various types of files . the rules does not cover everything . one can see that there actually many different file types ( meaning , unique file extensions ) using this simple powershell script : ` ` ` powershell get - childitem - recurse | % {$ _ . extension . tolower ( ) } | sort | unique ` ` ` with the following output . <number> . ac . adoc . am . bash - completion . bat . bmp . c . cc . cert . cfg . clang_complete . clang - format . cmake . cmd . cnf . com . conf . cpp . css . csv . doxyfile . dtd . empty . exe . exp . gci . gitattributes . github . gitignore . gitmodules . guess . h . hex . hpp . html . icns . ico . idb . ilk . in . include . ini . init . ipp . jam . js . json . lastbuildstate . lib . list . log . m . m4 . md . mk . mm . moc . obj . openrc . openrcconf . patch . pc . pdb . pl . plist . png . po . pro . py . python - version . qbk . qm . qml . qrc . raw . rb . rc . recipe . res . s . sage . sass . scm . scss . service . sgml . sh . sln . spec . sub . supp . svg . targets . td . tlog . ts . tx . txt . ui . user . v2 . vcxproj . verbatim . vscode . xml . xpm . xsl . y . yapf . yml . yy ` ` ` what do you think ?",1
bitcoin/bitcoin,modify ` testmempoolaccept ` to handle client ' s ` max_raw_tx_fee ` check failure see <url>,1
bitcoin/bitcoin,"log : add debug log category prefix i wanted to log ( especially incoming ) onion connections . was unsure if category ` net ` or ` tor ` would do that . in the debug log you do not see which debug message is of what category , or if it is of no debug category at all ( standard log ) . i would like if debug log categories are logged e . g . with a ` [ < debug log category > ] ` prefix . e . g . for ` net ` debug category log line , the line is currently logged like this ( i hope i am right that this belongs to ` net ` category , so this is an example of the uncertainty that results in the current absence of the category logging ) : ` <number> - <number> - 0 1 t <time> z socks5 connected zncs3yn6h4l7c3z7 . onion ` my wish would be that it would be logged like this [ net ] socks5 connected zncs3yn6h4l7c3z7 . onion ` if that would have any drawback like log gets bloated , an option to toggle debug log category prefix logging could be introduced , e . g . ` debugcategorylogging =< <number> | <number> > `",1
bitcoin/bitcoin,"rpc field to indicate which of the conflicted wallet transactions is in the mempool ` listtransactions ` / ` listsinceblock ` do not currently indicate which of the conflicted unconfirmed wallet transactions are considered ' active ' ( in the node ' s mempool ) and which are not . to determine this , one currently has to issue an additional ` getmempoolentry ` rpc call and check if that fails or not . it would be useful if this information was more readily available , without requiring additional calls . ~ ~ additionally , it appears that there ' s an inconsistency between ` gettransaction ` and ` listtransactions ` . normally , ` listtransactions ` returns a flattened concatenated list of the ` details ` for all the wallet transactions , with one entry per ` detail ` entry . but in this case , ` gettransaction ` returns an empty ` details ` array for ' non active ' transactions , which should to my understanding translate to the transaction not showing up in ` listtransactions ` at all , yet it does . ~ ~ ( this is wrong , it was a pebmac ) my preference would be for ' non active ' unconfirmed transactions to show up in both , but with a flag that indicates their status ( similarly to ` confirmations < <number> ` for transactions that conflict with confirmed transactions ) .",1
bitcoin/bitcoin,"threshold signing for releases for signing the sha256sums . asc in the distribution i would like to move away from using a signing key that i solely , personally possess . i am not entirely sure how , threshold signing is only a possibility there might be others . i think it would be ideal if we could distribute the process over multiple people , like a m out of n scheme . for example , a few ( relatively ) trusted gitian builders . the resulting signature should ideally be verifiable in the same way as it is now ( with ` gnupg ` ) , getting people to adapt a custom tool for validation is going to be difficult .",1
bitcoin/bitcoin,"better errors for address use from wrong network i spent too long to figure out i was putting mainnet addresses to regtest node , errors were not helpful at ` getaddressinfo ` and ` validateaddress ` . seems like a good first issue to augment the error messages when prefix / hrp do not match current network .",1
bitcoin/bitcoin,"` decodescript ` returns wrong reqsigs for complex scripts this issue may be related to <url> but i do not think it is , because i get the expected reqsigs for other p2sh / p2wsh scripts . create a redeem script that includes both op_checkmultisig and some other condition such as op_checklocktimeverify feed the redeem script into the ` decodescript ` rpc * * expected behavior * * i expect reqsigs to match the number corresponding to the op_checkmultisig portion of the redeem script . * * actual behavior * * the output has "" reqsigs "" set to <number> . * * to reproduce * * example script : ` ` ` $ . / bitcoin - cli - testnet - named decodescript hexstring = 0 3 bcef1cb175532102e47b8a3f7cd012c08369814af843cbb545cf9ba7f8e037d385e6213678a7f272210364c573ac74edd46bfa3e6ae1cd3aad2c6607af0a8d6ff3751147a8d73c6968d52102173becdbff83c43653d0273838a774d6ae42e1ea63ad7b9af53bd7b65bbbb04321033eb39dedc8527c796a9d619ba3ed3e9f611ef3fbb76a33b42806c0d138124a0021037a40c9aaa3c402ac95b0072dfe6e48486e30c0c0138aef7544e325d0b8faaa4e55ae { "" asm "" : "" <number> op_checklocktimeverify op_drop <number> 0 2 e47b8a3f7cd012c08369814af843cbb545cf9ba7f8e037d385e6213678a7f272 0 3 6 4 c573ac74edd46bfa3e6ae1cd3aad2c6607af0a8d6ff3751147a8d73c6968d5 0 2 1 7 3 becdbff83c43653d0273838a774d6ae42e1ea63ad7b9af53bd7b65bbbb043 0 3 3 eb39dedc8527c796a9d619ba3ed3e9f611ef3fbb76a33b42806c0d138124a00 0 3 7 a40c9aaa3c402ac95b0072dfe6e48486e30c0c0138aef7544e325d0b8faaa4e <number> op_checkmultisig "" , "" type "" : "" nonstandard "" , "" p2sh "" : "" 2 mtwvwxqieehaunb6ygawdo2filb5pivgrj "" , "" segwit "" : { "" asm "" : "" <number> 9 7 6 7 ecace7e49ced3cfa047924760497cb4717473b1ef35d3227d6a23360fa4a "" , "" hex "" : "" 0 0 2 0 9 7 6 7 ecace7e49ced3cfa047924760497cb4717473b1ef35d3227d6a23360fa4a "" , "" reqsigs "" : <number> , "" type "" : "" witness_v0_scripthash "" , "" addresses "" : [ "" tb1qjan7et88ujww6086q3ujgasyjl95w9688v00xhfjylt2yvmqlf9qzpqkl5 "" ] , "" p2sh - segwit "" } } ` ` ` * * system information * * bitcoin core <number> . <number> ubuntu <number> using bitcoin - cli against local node .",1
bitcoin/bitcoin,"can we create a non proof - of - childs documentation to install and run properly this repo ? i have tried several times to install and run this repo in a fresh ubuntu <number> operative system . all the documentation i have found is outdated . and there is not an evident clear path to successfully install this repo in at least an os . i would like to suggest if possible , an exhaustive doc that explains how to install this repo in any linux distribution .",1
bitcoin/bitcoin,"please split the ` - msghand ` thread into multiple threads . it seems when using good storage that this thread act as bottleneck in a way that other threads are waiting completion from it a the cpu level when performing initial full node synchronisation . as a result , bitcoin is mostly single threaded in that case ( the ` - msghand ` thread take <percent> of a single core leaving other cores idle even on a core duo cpu ) . this what currently prevents syncing a full node with ` - txindex ` in less than <number> hour .",1
bitcoin/bitcoin,"partially signed bitcoin transaction ( psbt ) gui for bitcoin core request dear developers , do you fine people plan on adding a gui to bitcoin core so that users can create a psbt and also sign a psbt for cold storage users ? i would love this feature ! <repeated> i think you can do this with command line but it ' s risky and not simple . i ask because i only use bitcoin core as my wallet and do not have easy cold storage spending method without bringing all online . thank you and i love you all ! * * is your feature request related to a problem ? please describe . * * no * * describe the solution you ' d like * * an easy to use gui to create a psbt and to sign a psbt for cold storage . * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > * * additional context * * < ! - - add any other context or screenshots about the feature request here . - - >",1
bitcoin/bitcoin,"add "" walletpassphrasechange "" command in bitcoin - wallet . exe this is an underrated useful tool . dear devs , please add this "" feature "" . thanks .",1
bitcoin/bitcoin,add macos arm build ( universal or additional binaries ) first arm based macs are available now . see the porting guide <url> and the architectural differences : <url> things to do [x ] fix depends build on arm with macos <number> ( done in merged # <number> ) * [x ] fix crc32 build issue ( fix in # <number> ) * [x ] update macos build readme ( if required ) * [x ] allow cross compiling arm64 mac on linux / gitian * [x ] fix ` make deploy ` on arm64,1
bitcoin/bitcoin,"gui - feedback on calculating transaction fees i stumbled across fee calculation messing up units . please take a look at this [ discussion ] ( <url> which is related [ to ] ( <url> therefore , i ' d like to suggest the following use case selects manually editing fee and user asks for sending . next bitcoin - qt calculates the fee added to the transaction then gui shows to user the calculated fee and ask for confirmation . if ok , then send transaction else edit fee calculation and repeat .",1
bitcoin/bitcoin,"hardcoded seeds for torv3 addresses for the <number> release we will want to start hardcoding torv3 addresses instead of ( or in addition to ) torv2 . the biggest challenge here is not how to hardcode them , although that will require a few minor script and code changes , but how to collect them . the release process is to get the addresses from dns seeders , however these currently do not support the ` addrv2 ` message nor torv3 addresses in the first place . nor do they really have a need to .",1
bitcoin/bitcoin,"dandelion + + * * is your feature request related to a problem ? please describe . * * + bip <number> never got implemented in bitcoin core which could improve privacy : <url> + reasons : ( tldr : denial of service ) <url> <url> + dandelion + + proposed for bitcoin : <url> + implemented in monero : <url> * * describe the solution you ' d like * * implement dandelion + + in bitcoin core and if it involves any risks , let us test , try to find solutions or workarounds . * * describe alternatives you have considered * * tor * * additional context * * not sure how much is this pr and associated project related to this issue : <url> also there is no option to specify tor for ` sendrawtransaction ` rpc command",1
bitcoin/bitcoin,"support tor v3 seeding * * is your feature request related to a problem ? please describe . * * dns seeding for tor v3 edwards curve nodes is not possible in the current a / a <elongated> design . * * describe the solution you ' d like * * support for tor v3 seeding : by creating a bip that outlines how that can be done . until we have that : add at least some fixed v3 node addresses in the vseeds list . dns seeding for tor v3 nodes is not possible in the current a / a <elongated> design . when v2 fades out seeders are unable to hand over learned known v3 onions encoded in there dns a / a <elongated> answer records . not that enable dns seeding while using tor is a good privacy idea anyway , but since some users are behind chains of nat ' s and just like or must bootstrap over tor , we should support that . with the next release , users will be reachable by v3 onions the node created and gossip them but be unable to get also those onion v3 seeds by the build in seeders or there own dns tor v2 / ipv4 / <number> seeders in the usual fashion on a clean bootstrap . so to mitigate against fragmentation , at least we should add some build in v3 node addresses i . e . from the known seeder nodes . so nodes have a chance to stay solely in tor and v3 and are not forced to leak . or / and think about building for and using something like the lightning mechanism to support also seeding like we used to now also for tor v3 onions . <url> general caveat of long v3 onion names is that we can no longer bootstarp in out of ip4 / ip6 by use of a and a <elongated> records that fit the now long onion address , but might be forced to support dns srv calls for the long names but srv dns calls are not supported by tor and the socks5 proxy . so the task is that we must find a way to bootstrap in a private maner before tor v2 fades out . * * describe alternatives you have considered * * until we have that at least some fixed v3 node addresses in the vseeds list . * * additional context * * <url>",1
bitcoin/bitcoin,document supported methods for backups and restore existing literature on backing up and restoring are on external sites and forums and may become outdated . they may also be recommending methods that could result in wallet file corruption . we should document the supported backup and restoration methods in repo or on the website so that people can refer to those as the officially supported methods . this also lets us update them as files and folder structures change .,1
bitcoin/bitcoin,"validate user input and keep path in a separate argument for importwallet , createwallet and dumpwallet * * is your feature request related to a problem ? please describe . * * wallets with weird names possible which can be exploited in vulnerable web applications that use bitcoin core and allow the users to create and import / export wallet ` . <repeated> / testwallet ` for linux ` . <repeated> \ \ testwallet ` for windows tried on bitcoin core v <number> . <number> <url> * * describe the solution you ' d like * * + keep two arguments : ` wallet_name ` and ` wallet_path ` for ` createwallet ` ` filename ` and ` filepath ` for ` dumpwallet ` ` importwallet ` + validate user input . name should not contain special characters . path should be optional and if not specified use a default value . * * describe alternatives you have considered * * web developers should create secure web apps that use bitcoin core . * * additional context * * [ image ] ( <url> i understand its not a vulnerability in bitcoin core and only affects vulnerable web apps that use bitcoin core . however we can at least consider it a bug that may affect something in future or other projects that use bitcoin core . basic checks for user input can improve the security . in <number> i had a website in which someone had reported a vulnerability that could be used to change price of flight tickets and book with almost zero bitcoin . it was an issue with the website and we had to fix it although nobody could exploit it because the third party apis that we were using to book the fight tickets were validating all the things . so a ticket could not be processed after tampering and changing the price by attacker . similarly , if any web developer makes a mistake and using bitcoin core for the web app would still be unaffected if bitcoin core itself does not allow such things for wallet names . there have been lot of directory traversal related vulnerabilities , recently one was reported in facebook android app",1
bitcoin/bitcoin,"use orange icon for bitcoin account currently the account that hosts this repo and several others uses the black bitcoin icon which ( as far as i am concerned ) is the bitcoin core brand icon . would it not be more on - brand to use the orange icon for this account and the black bitcoin core icon for the bitcoin - core account ? a discussions was had several weeks ago around branding here <url> visualized below <number> ] ( <url> [ here ] ( <url> is the orange icon on figma that can be exported ( as svg , png , jpg etc . ) and used . posted an issue over at <url> regarding changing the orange icon to black for the bitcoin - core account .",1
bitcoin/bitcoin,"wallet : reenable - fallbackfee by default for regtest and signet ( and maybe testnet too ) ? this [ pr ] ( <url> merged in <date> disabled ` - fallbackfee ` by default across all chains ( mainnet , testnet , regtest ) . the only motivation for doing this for regtest ( and maybe testnet too ) that i can see is : > now it is <number> by default for all chains , thus there ' s no need to call params ( ) . ( [ comment ] ( <url> however , this does mean that if you do not set a fee or enable ` - fallbackfee ` you get this error . ` ` ` error code : - <number> error message : fee estimation failed . fallbackfee is disabled . wait a few blocks or enable - fallbackfee . ` ` ` any thoughts on reenabling ` - fallbackfee ` by default for regtest , signet ( and maybe testnet too ) ? i think it makes sense to continue to have it disabled by default for real money on mainnet . if i am missing context please direct me to it and i will close this . bitcoin stackexchange post",1
bitcoin/bitcoin,"split policy / error consensus codes for cleanstack , minamalif discussion here <url>",1
bitcoin/bitcoin,""" good first review "" label i would like to suggest a small idea that i had , to make it easier for beginner to review their first prs . analogous to the "" good first issue "" label , i am suggesting a "" good first review "" label . as a general guideline , labeled pull requests should have some ( ideally all ) of the following characteristics : - complexity level low - a tentative small change ( < <number> loc ) - a detailed description of the pr background if needed - general instructions / hints on how to test / reproduce examples of pulls that generally tend to be good first reviews a newly added rpc that can be easily manually tested to verify functionality - a pr that only adds new functional test coverage since these tend to be self - documenting - a documentation change that can be verified to be correct with manual testing similar to other labels the creator of the pull could ask for the label to be applied or contributors with the power to give labels could apply the proactively .",1
bitcoin/bitcoin,"onion address in bitcoin core wallet * * is your feature request related to a problem ? please describe . * * i am not sure if there is a rpc method available to directly get the onion address used so i checked it in ` localaddresses ` returned by ` getnetworkinfo ` <url> it was also mentioned in debug . log file : [ image ] ( <url> * * describe the solution you ' d like * * i would love to see an onion address and maybe qr code in the wallet if it is listening on tor similar to [ abcore ] ( <url> * * describe alternatives you have considered * * check the onion address in debug . log or ` localaddresses ` returned for ` getnetworkinfo ` * * additional context * * linked to <url> because if we add such option to check the onion address , it will have to be added in the [ tor documentation ] ( <url>",1
bitcoin/bitcoin,"fatal performance when importing 1 5 1 m private keys earlier i was importing several thousands of private keys to bitcoin core and it took much time , especially rescan of the blockchain . few hours , > <number> surely . but now , when i am importing 1 5 1 m private keys ( <number> gb file with keys ) the import process is now taking <number> days and i have <percent> for now . how badly bitcoin core is written is unbelivable . there is no other way to scan such an amount of keys , because other apps are for private , limited use and do not offer scanning the blockchain with such a large amount of keys . will this be fixed or even rewritten ?",1
bitcoin/bitcoin,revisit the chain view cache policy oops wrong repo . sorry <happy>,1
bitcoin/bitcoin,"rpc : pipe support for submitblock if you use the ` submitblock ` rpc with a large enough block you will run into "" argument list too long "" . using a pipe should fix this , at least on linux / macos . the ` submitblock ` rpc can be useful to check validity of a stale block that never made it into the main chain ( and that your node never fetched ) ` invalidateblock ` on its same - height - sibling and then feed it the raw block . easiest workaround is to use a library that connects directly to ` bitcoind ` via rpc ( be careful to avoid a line break character at the end ) . using the gui console fails with "" block decode failed "" for me , but have not tested that extensively .",1
bitcoin/bitcoin,"manual - pruning cursor rewind * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > setting a lower - than - current prune height via ` pruneblockchain ` currently requires a full resync . on slower machines , this can take a very long time . it is also unnecessary . * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > since all block headers are retained for pow and chain reorg purposes , bitcoind could feasibly re - download the raw blocks that are lower than the old <emphasis> cursor , and above the new <emphasis> cursor , and check their validity against the retained headers .",1
bitcoin/bitcoin,"decouple build system from source . use cmake to help newcomers and ecosystem # # # problem # # # # new developers unable to quickly get up and going in their ide the current build system is very brittle and the custom tailored nature of it leaves the development team having to address constant breaking changes and new architectures etc . it also leaves new comers to the project in a state of despair when they realize that the current ide cannot pick up the project . for example in this issue # <number> , the user asked "" where i could find the guide on how to install an ide and debug the code ? "" because his top of the line ide , clion works by using cmake . the other well known c / c + + ides that are currently used are able to and appear to prefer the cmake build system . sadly , the sarcastic response ( i hope ) from <user> was "" most developers do not use ides as far as i know "" . for the windows users out there , in issue # <number> , the issue got alot of support to the idea of adding cmake , as one is able to quickly work on c / c + + projects given visual studio ' s and vscode support for cmake projects . # # # # improve ecosystem by having option to build shared libraries this last point will be a simple theory , but if cmake is added , one could imagine that the ecosystem to grow and flourish and bring in new support and tooling with it . for instance , if i wanted to instead wrap around the c + + code using cffi and call directly from python / js / whatever i am unable to as where would i even begin to get started on compiling a library as opposed to the executable in a neat manner ? right now the best way to interact with bitcoin is via the rpc it seems , but why not directly ? the other direct benefits of having a "" simple "" cmake build system is we can undo the tight coupling of what should be the "" build "" with the source code . when developing , the sanitizers can be added and help ensure there is strong checks with proper memory usage and addressing etc . # # # # proper sanitizing and bug catching for instance , i found a buffer overflow in the tests within the ctaes library ( inside crypto ) which was found trivially after setting up a proper standalone cmake file for it . this was found within hours of me poking around and was there for a number of years . makes one wonder how many other bugs are simple low hanging fruit , just waiting to be picked . # # # solution according to [ the developer ecosystem in <number> c + + is doing ] ( <url> , the c + + community is moving with conviction to cmake as their build system of choice . over the last couple years its gone from <percent> to <percent> of developers choosing it over makefiles or visual studio . if the trend continues , it is imperative the project invests in improving the current status quo . begin adding cmake files to cover the major oss windows , osx , linux on the major processors x86_64 . then expand out to arm and the smaller targets like raspberry pis and the like . eschew the many optimizations and custom - tailored functions / modules for specific architectures and just add the generic standard library versions until the entire project builds and tests properly . while this is a "" monumental task "" as noted by <user> , the hardest part will be decoupling of the many build steps with source code , but thats more tedious and repetitive than anything . i can and already have a couple cmake files for some modules like crypto and ctaes to start off . what i am not sure of exactly is the flow of direction between the libraries in the "" bitcoin - core "" project and this one . which one follows which ?",1
bitcoin/bitcoin,"testnet reset ? i was thinking , since it looks like the testnet is very hard to mine and has already had many halvings ( i know they are faster ) . what are your thoughts on a testnet reset for the next release of bitcoin core ? the testnet i believe should get a refresher as it ' s been many years since the launch of testnet3 and many altcoins are on testnet4 . if you all like this , i will submit a pr to reset the testnet .",1
bitcoin/bitcoin,"provide skeleton bitcoin . conf during datadir generation * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > it would be helpful to provide the example bitcoin . conf provided in the source code to the users that just download the binary / install from their package manager . secondly , a new bitcoind user may not be aware that bitcoin . conf file is an available option . while bitcoin . conf is mentioned in the man page , one would have to know what to look for or read a bit to get to the - conf section . by providing a skeleton file , a new user is more exposed to such an option . * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > automatically generate a skeleton bitcoin . conf with something like the one found at ` ` ` share / examples / bitcoin . conf ` ` ` when first starting bitcoind . * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > * * additional context * * < ! - - add any other context or screenshots about the feature request here . - - >",1
bitcoin/bitcoin,"cve - <number> - <number> , possible code and performance improvement ok so , in my effort to create a fullnode from scratch both for learning purpose and need ( not a topic for discussion , thanks ) , i was looking at the code that check merkle root to not be vulnerable to cve - <number> - <number> . that vulnerability basically allows you to create a block that has a valid merkle root but contains duplicate transactions , causing a node that receive such block before the correct one , to be stuck on a fork ( because that block will be flagged as incorrect and having the same hash of a correct block will prevent the node to ask again for that block , causing a node to be stuck . anyway , this is an old cve that was fixed back in <number> and code changed during time . now in bitcoin core the code is not that pretty and the method to compute merkle root are polluted from a boolean parameter that when read back reflects if the block has been mutated ( malleated ) or not . the code is this ` ` ` uint256 computemerkleroot ( std : : vector <uint256> hashes , bool * mutated ) { bool mutation = false ; while ( hashes . size ( ) > <number> ) { if ( mutated ) { for ( size_t pos = <number> ; pos + <number> < hashes . size ( ); pos + = <number> ) { if ( hashes [ pos ] = = hashes [ pos + <number> ] ) mutation = true ; } } if ( hashes . size ( ) & <number> ) { hashes . push_back ( hashes . back ( )); } sha256d64 ( hashes [ <number> ] . begin ( ) , hashes [ <number> ] . begin ( ) , hashes . size ( ) / <number> ); hashes . resize ( hashes . size ( ) / <number> ); } if ( mutated ) * mutated = mutation ; if ( hashes . size ( ) = = <number> ) return uint256 ( ); return hashes [ <number> ]; } ` ` ` now thinking about the problem , i think i found a better approach that is o ( log n ) and allows to have a better code too ( without having to pass the ` mutated ` flag around ) . <repeated> if it works and the logic is not flawled . <repeated> so i ' d like to know your thought about this . my paint skill can show you some of the logic : [ [ \ \ [ <time> \ \ ] ] [ <number> ] ] [ <number> ] a , b , c . <repeated> etc . <repeated> are transactions the vertical line is what i call "" safe point "" , basically all transactions before that safe point are guaranteed to be not duplicable ( this is one of the assumption i do and one thing to check if it ' s correct ) then , based on how merkle root is computed and how the cve uses that to do nasty things , you can see some example of malleated blocks i created then a gist , containing a linqpad code that can be run as it is and produce outputs to see if the logc is correct and can spot malleated blocks . <url> this is an output example of that gist [ ! [ enter image description here ] [ <number> ] ] [ <number> ] the core logic relies into checking the last transaction against the previous , moving exponentially to the left at each iteration ( see the picture with the vertical blue arrows showing which element it checks , or see the linqpad result where it ' s explicitly detailed ) i handle both the cases where the tx count is even or odd . to me it seems to work but would like to have some more eyes on that to see if the logic sounds correct or not . note how for <number> tx i have just to compare <number> transactions instead of thousands like current bitcoin code is doing . apart from that in my case it has another pros that allow me to split the markle computation and markle cve check in two different places , without having to use a solution like that ` mutated ` boolean parameter ( actually i can not setup an environment to check if my intuition is correct and benchmark it so i can not provide numbers , but i am more interested about the check logic itself ) [ <number> <sad> <url> [ <number> ]",1
bitcoin/bitcoin,"change address selected randomly causes problem with hw . wallet using core with hardware wallets ( coldcard in this case ) comes with a problem : when you create a psbt with walletcreatefundedpsbt it pulls a * random <emphasis> * change address . problem is : coldcard ( i think justly ) says : it is out of order and displays a strong warning . like in our case in a pool of <number> addresses coldcard scans and accepts the first <number> . * * solution : * * it would be nice to have an option ( default ? ) to pull change addresses in the derivation order . * * alternative : * * right now we can importmulti <number> addresses , so any random address falls into the accepted first <number> . problem is easy to run out of that <number> . i believe pulling random change addresses form a hd wallet gives no extra privacy .",1
bitcoin/bitcoin,"boundary limit ? <user> - jr <user> <user> <user> be here in code "" less equal than "" i . e . "" <= "" for determination if an uxto is to be considered "" dust "" ? <url>",2
bitcoin/bitcoin,"bitcoin - wallet reported as wacatac malware # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour running gpg verified binaries leads to "" an active ' wacatac ' malware was prevented "" # # # expected behaviour not triggering the incident # # # steps to reproduce follow <url> curl - o <url> - zxf bitcoin - <number> - arm64 - apple - darwin . tar . gz - cd downloads & & gpg - - verify sha256sums . asc - tar - zxf bitcoin - <number> - arm64 - apple - darwin . tar . gz - sudo cp bitcoin - <number> / bin / bitcoin * / usr / local / bin / . - run bitcoind # # # relevant log output an active ' wacatac ' malware was prevented # # # how did you obtain bitcoin core pre - built binaries # # # what version of bitcoin core are you using ? <number> - arm64 - apple - darwin # # # operating system and version macos ventura <number> . <number> # # # machine specifications apple silicone chip",2
bitcoin/bitcoin,"what is depends build_id_salt ? is it unused ? if not , how can it be used ? if yes , can it be removed ? references : ` ` ` $ git grep ' \ \ < build_id \ \ > ' depends / makefile <seallips> they rely on the build_id variables depends / makefile : build_id : =$( shell env cc ='$ ( build_cc ) ' c_standard ='$ ( c_standard ) ' cxx ='$ ( build_cxx ) ' cxx_standard ='$ ( cxx_standard ) ' ar ='$ ( build_ar ) ' ranlib ='$ ( build_ranlib ) ' strip ='$ ( build_strip ) ' sha256sum ='$ ( build_sha256sum ) ' debug ='$ ( debug ) ' lto ='$ ( lto ) ' no_harden ='$ ( no_harden ) ' . / gen_id ' $( build_id_salt ) ' ' guix_environment =$( realpath $( guix_environment ) ) ' ) ` ` ` the cache is not invalidated $ ( cd depends / & & build_id_salt = salt make boost & & echo done ) done $ ( cd depends / & & build_id_salt = salty make boost & & echo done ) done",2
bitcoin/bitcoin,"regtest mode loses unspents after day # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour testing in regtest mode , i have noticed that after <number> day of send some funds from a wallet to b , the unspent inputs disappear from wallet b . * * bitcoin . conf * * ` ` ` bash # generated by <url> # this config should be placed in following path : # ~ / . bitcoin / bitcoin . conf # [ chain ] # run this node on its own independent test network . equivalent to - chain = regtest regtest = <number> # [ core ] # specify a non - default location to store blockchain and other data . datadir <annoyed> home / debian / . bitcoin # reduce storage requirements by only storing most recent n mib of block . this mode is incompatible with - txindex and - coinstatsindex . warning : reverting this setting requires re - downloading th > prune = <number> # [ wallet ] # bech32 addresstype = bech32 # bech32 changetype = bech32 # specify wallet database path . can be specified multiple times to load multiple wallets . path is interpreted relative to <walletdir> if it is not absolute and will be created if it does not ex > wallet = default # [ sections ] # most options automatically apply to mainnet , testnet , and regtest networks . # if you want to confine an option to just one network , you should add it in the relevant section . # exceptions : the options addnode , connect , port , bind , rpcport , rpcbind and wallet # only apply to mainnet unless they appear in the appropriate section below . # options only for mainnet [ main ] # options only for testnet [ test ] # options only for regtest [ regtest ] # accept command line and json - rpc commands . server = <number> # bind to given address to listen for json - rpc connections . this option is ignored unless - rpcallowip is also passed . port is optional and overrides - rpcport . use [ host ] : port notation for ipv6 . > rpcbind = <number> . <number> # listen for json - rpc connections on this port rpcport = <number> # allow json - rpc connections from specified source . valid for <ip> are a single ip ( e . g . <number> . <number> ) , a network / netmask ( e . g . <number> . <number> / <number> . <number> ) or a network / cidr ( e . g . <number> . <number> / <number> ) . this optio > rpcallowip = <number> . <number> # username and hashed password for json - rpc connections . the field <userpw> comes in the format : <username> <sad> salt >$< hash > . rpc clients connect using rpcuser =< username > / rpcpassword =< password > ar > rpcauth = bitcoin : 6 e0efb08ebd20eff65959edc38d17bc <money> 9 bc0e273f35e583d9b70071cfd71dc78034ff639d1900c780e3412d7011aab1f ` ` ` # # # expected behaviour obtain unspent inputs # # # steps to reproduce ` ` ` bash / / send unspent bitcoin - core . cli - named send outputs ='{ "" bcrt1qugd904ne5z0ks45fgmdcne2qe37s2fv7jqra24 "" fee_rate = <number> / / generate blocks bitcoin - core . cli generatetoaddress <number> bcrt1qpg03lyd93mfvz56p92rl6e0mxzasrfskccn77k / / list unspent bitcoin - core . cli listunspent <number> <number> [ \ \ "" bcrt1qugd904ne5z0ks45fgmdcne2qe37s2fv7jqra24 \ \ "" ] ` ` ` after day , trying again to list with ` listunspent ` and the result is empty . # # # relevant log output _no response_ # # # how did you obtain bitcoin core package manager # # # what version of bitcoin core are you using ? v0 . <number> # # # operating system and version debian <number> # # # machine specifications _no response_",2
bitcoin/bitcoin,"fuzz : crc32c : : extendarm64 undefined symbols for architecture arm64 : # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour configure success make failed # # # expected behaviour make failed # # # steps to reproduce . / configure - - enable - fuzz - - disable - asm . <repeated> build options : with external callbacks = no with benchmarks = no with tests = yes with ctime tests = no with coverage = no with examples = no module ecdh = no module recovery = yes module extrakeys = yes module schnorrsig = yes module ellswift = yes asm = no ecmult window size = <number> ecmult gen prec . bits = <number> valgrind = no cc = gcc cppflags = secp_cflags = - o2 - std =c 8 9 - pedantic - wno - long - long - wnested - externs - wshadow - wstrict - prototypes - wundef - wno - overlength - strings - wall - wno - unused - function - wextra - wcast - align - wconditional - uninitialized - wreserved - identifier - fvisibility = hidden cflags = - g - o2 ldflags = options used to compile and link : external signer = no multiprocess = no with libs = no with wallet = yes with sqlite = yes with bdb = yes with gui / qt = no with zmq = no with test = not building test_bitcoin because fuzzing is enabled with fuzz binary = yes with bench = no with upnp = no with natpmp = no use asm = no usdt tracing = no sanitizers = debug enabled = no gprof enabled = no werror = no lto = no target os = darwin22 . <number> build os = darwin22 . <number> cc = gcc cflags = - pthread - g - o2 cppflags = - dabort_on_failed_assume - u_fortify_source - d_fortify_source = <number> - dhave_build_info - xclang - internal - isystem / usr / local / include - dmac_osx - dobjc_old_dispatch_prototypes = <number> - dprovide_fuzz_main_function cxx = g + + - std =c + + <number> cxxflags = - wstack - protector - fstack - protector - all - wall - wextra - wgnu - wformat - wformat - security - wvla - wshadow - field - wthread - safety - wloop - analysis - wredundant - decls - wunused - member - function - wdate - time - wconditional - uninitialized - woverloaded - virtual - wsuggest - override - wunreachable - code - loop - increment - wimplicit - fallthrough - wdocumentation - wno - unused - parameter - wno - self - assign - g - o2 ldflags = - lpthread - wl , - bind_at_load - wl , - fixup_chains - wl , - headerpad_max_install_names - wl , - dead_strip - wl , - dead_strip_dylibs ar = / usr / bin / ar arflags = cr . <repeated> make # # # relevant log output ar libbitcoin_util . a cxx libbitcoin_consensus_a - arith_uint256 . o cxx consensus / libbitcoin_consensus_a - merkle . o cxx consensus / libbitcoin_consensus_a - tx_check . o cxx libbitcoin_consensus_a - hash . o cxx primitives / libbitcoin_consensus_a - block . o cxx primitives / libbitcoin_consensus_a - transaction . o cxx libbitcoin_consensus_a - pubkey . o cxx script / libbitcoin_consensus_a - bitcoinconsensus . o cxx script / libbitcoin_consensus_a - interpreter . o cxx script / libbitcoin_consensus_a - script . o cxx libbitcoin_consensus_a - uint256 . o ar libbitcoin_consensus . a cxx crypto / libbitcoin_crypto_base_la - chacha_poly_aead . lo cxx crypto / libbitcoin_crypto_base_la - chacha20 . lo cxx crypto / libbitcoin_crypto_base_la - poly1305 . lo cxx crypto / libbitcoin_crypto_base_la - muhash . lo cxx crypto / libbitcoin_crypto_base_la - ripemd160 . lo cxx crypto / libbitcoin_crypto_base_la - sha1 . lo cxx crypto / libbitcoin_crypto_base_la - sha256 . lo cxx crypto / libbitcoin_crypto_base_la - sha3 . lo cxx crypto / libbitcoin_crypto_base_la - sha512 . lo cxx crypto / libbitcoin_crypto_base_la - siphash . lo cxxld crypto / libbitcoin_crypto_base . la cxx rpc / libbitcoin_cli_a - client . o ar libbitcoin_cli . a cxxld test / fuzz / fuzz undefined symbols for architecture arm64 : "" crc32c : : extendarm64 ( unsigned int , unsigned char const * , unsigned long ) "" , referenced from : crc32c : : extend ( unsigned int , unsigned char const * , unsigned long ) in libcrc32c . a ( libcrc32c_la - crc32c . o ) ld : symbol ( s ) not found for architecture arm64 clang : error : linker command failed with exit code <number> ( use - v to see invocation ) make [ <number> <sad> * * * [ test / fuzz / fuzz ] error <number> make [ <number> <sad> * * * [ all - recursive ] error <number> make [ all - recursive ] error <number> # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? <number> . x # # # operating system and version macos ventura <number> # # # machine specifications m1 macbook pro <number> "" <number> / <number> / 6 4 gb 1 tb",2
bitcoin/bitcoin,"bitcoin core v25 . <number> crashes # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour bitcoin core keeps crashing and will not stay up . i have been running a bitcoin node on a windows <number> computer for several years and had never experienced a bitcoin core crash before . i had been running bitcoin core v0 . <number> since if first came out but left the node unattended for a few weeks this summer . i restarted it earlier this week and the bitcoin core log window showed its data base was several thousand blocks out of sync . the last i saw on the bitcoin core log window was that it had some <number> blocks to go to complete the synchronization . i do not know if the synchronization finished or not , but a few hours later i noticed the bitcoin core log window gone and the bitcoin - qt . exe process disappeared from the task manager process list . i restarted bitcoin core and surprisingly the log window showed its data base was more than one thousand blocks out of sync . in other words , it lost a large number of blocks in the crash . i rebooted the windows computer and restarted bitcoin core three more times but it crashed within a few hours each time . i updated bitcoin core to v25 . <number> hoping that the latest version would fix the problem . unfortunately , i tried v25 . <number> several times and it crashes within a few hours each time . i reran bitcoin core from the command prompt window ( bitcoind . exe ) with the debug option and wrote its output to a file ( bitcoinlog . txt ) , which i am including below to help identify the problem . i would really appreciate it if anyone would could give me some guidance on how to fix this problem , so i can get my bitcoin node back up and running . # # # expected behaviour expected my bitcoin node to sync up to the bitcoin blockchain and continue to load new blocks like it has reliably done for several years now . # # # steps to reproduce restarted bitcoin core ( bitcoin - qt . exe ) after some <number> or <number> weeks offline . # # # relevant log output [ bitcoinlog . txt ] ( <url> # # # how did you obtain bitcoin core pre - built binaries # # # what version of bitcoin core are you using ? v25 . <number> # # # operating system and version windows <number> version 2 2 h2 # # # machine specifications using a windows <number> computer running intel core i7 - <number> cpu with <number> gb ram and <number> tb hdd .",2
bitcoin/bitcoin,"failure running ci locally : "" # <number> <number> error : unable to find a match : glibc - devel . x86_64 libstdc + + - devel . x86_64 glibc - devel . i686 libstdc + + - devel . i686 "" # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour i am following the guide in ` ci / readme . md ` but i can not get any of the environments to run successfully . # # # expected behaviour run ci locally # # # steps to reproduce macos : ` ` ` - - > docker - v docker version <number> . <number> , build cb74dfc - - > uname - a darwin <number> . <number> darwin kernel version <number> . <number> : thu <date> <time> pst <number> ; root : xnu - <number> . <number> . <number> ~ <number> / release_arm64_t8101 arm64 ` ` ` command : ` file_env ="". / ci / test / 0 0 _setup_env_i686_centos . sh "" . / ci / test_run_all . sh ` # # # relevant log output full log : <url> end of log : ` ` ` # <number> <number> + bash - c ' dnf - y - - allowerasing install gcc - c + + glibc - devel . x86_64 libstdc + + - devel . x86_64 glibc - devel . i686 libstdc + + - devel . i686 ccache libtool make git python3 python3 - pip which patch lbzip2 xz procps - ng dash rsync coreutils bison util - linux ' # <number> <number> extra packages for enterprise linux <number> - aarch64 <number> mb / s | <number> mb <time> # <number> <number> extra packages for enterprise linux <number> - next - <number> kb / s | <number> kb <time> # <number> <number> last metadata expiration check : <time> ago on sat <date> <time> <number> . # <number> <number> no match for argument : glibc - devel . x86_64 # <number> <number> no match for argument : libstdc + + - devel . x86_64 # <number> <number> no match for argument : glibc - devel . i686 # <number> <number> no match for argument : libstdc + + - devel . i686 # <number> <number> package python3 - <date> - <number> . el9 . aarch64 is already installed . # <number> <number> error : unable to find a match libstdc + + - devel . x86_64 glibc - devel . i686 libstdc + + - devel . i686 ` ` ` # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? master f0758d8a <phone> d9c057e7aa079ffa9e1c16 # # # operating system and version macos <number> . <number> # # # machine specifications _no response_",2
bitcoin/bitcoin,"build fails on fedora <number> - configure failed for src / secp256k1 # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour found a recent similar issue but did not help : <url> i am trying to run a build on fedora <number> and i am seeing the same error . when i run . / configure i see the following : configure : warning : bitcoin core requires this library for bdb ( legacy ) wallet support configure : warning : passing - - without - bdb will suppress this warning checking for sqlite . <repeated> yes checking whether to build wallet with support for sqlite . <repeated> yes checking whether userspace , statically defined tracing tracepoints are supported . <repeated> no checking for natpmp . h . <repeated> yes checking for initnatpmp in - lnatpmp . <repeated> yes checking for boostlib >= <number> . <number> ( <number> ) . <repeated> yes checking whether boost . process can be used . <repeated> yes checking for seccomp - bpf ( linux x86 - <number> ) . <repeated> yes checking for event . <repeated> yes checking for event_pthreads . <repeated> yes checking if evhttp_connection_get_peer expects const char * * . <repeated> no checking for zmq . <repeated> yes checking for libmultiprocess . <repeated> no checking whether to build bitcoind . <repeated> yes checking whether to build bitcoin - cli . <repeated> yes checking whether to build bitcoin - tx . <repeated> yes checking whether to build bitcoin - wallet . <repeated> yes checking whether to build bitcoin - util . <repeated> yes checking whether to build experimental bitcoin - chainstate . <repeated> no checking whether to build libraries . <repeated> yes checking if ccache should be used . <repeated> yes checking whether c compiler accepts - fdebug - prefix - map = a =b . <repeated> yes checking whether c preprocessor accepts - fmacro - prefix - map = a =b . <repeated> yes checking if wallet should be enabled . <repeated> yes checking whether to build with support for upnp . <repeated> no checking whether to build with support for nat - pmp . <repeated> yes checking whether to build with nat - pmp enabled by default . <repeated> no checking whether to build test_bitcoin . <repeated> yes checking whether to reduce exports . <repeated> no checking that generated files are newer than configure . <repeated> done configure : creating . / config . status config . status : creating libbitcoinconsensus . pc config . status : creating makefile config . status : creating src / makefile config . status : creating doc / man / makefile config . status : creating share / setup . nsi config . status : creating share / qt / info . plist config . status : creating test / config . ini config . status : creating contrib / devtools / split - debug . sh config . status : creating src / config / bitcoin - config . h config . status : src / config / bitcoin - config . h is unchanged config . status : executing depfiles commands config . status : executing libtool commands = = = configuring in src / secp256k1 ( / home / kau / code / bitcoin / bitcoin / src / secp256k1 ) configure : running / bin / sh . / configure - - disable - option - checking ' - - prefix <annoyed> usr / local ' ' - - without - miniupnpc ' ' - - disable - bench ' ' - - without - gui ' ' - - disable - shared ' ' - - with - pic ' ' - - enable - benchmark = no ' ' - - enable - module - recovery ' ' - - enable - module - schnorrsig ' - - cache - file <annoyed> dev / null - - srcdir = . configure : error : cannot find required auxiliary files : compile missing configure : error : . / configure failed for src / secp256k1 would be appreciate if you can guide if this error is because of a recent change . # # # expected behaviour configure script completes successfully . # # # steps to reproduce run the following on master - - without - miniupnpc - - disable - bench - - without - gui # # # relevant log output already pasted above # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? master <user> # # # operating system and version fedora <number> # # # machine specifications _no response_",2
bitcoin/bitcoin,"allow accepting non - standard transactions on mainnet via local rpc to mitigate mempool divergence between nodes and miners , node operators should have the ability to submit non - standard transactions to their local mempools . one of the reasons to do so is to improve the quality of fee estimates . acceptance via the p2p network has been discussed in <url> and is deemed to be problematic . this issue is for enabling non - standard transaction submission for the ` sendrawtransaction ` rpc only .",2
bitcoin/bitcoin,"dumpprivkey - this type of wallet does not support this command ( code - <number> ) # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour hello . when i try export private key for bitcoin address i got : this type of wallet does not support this command ( code - <number> ) i tried on all <number> types of bitcoin address base58 and bech32 ( legacy , segwit . <repeated> ) . i am using zip version of bitcoin core v23 . <number> downloaded from # # # expected behaviour it should display private key . bitcoin address cannot be found ? if i insert random value for bitcoin address i got this error . # # # steps to reproduce <number> . run bitcoin core v23 . <number> <number> . select help > console <number> . insert command and press enter dumpprivkey <bitcoin_address> # # # relevant log output _no response_ # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? bitcoin core v23 . <number> # # # operating system and version windows # # # machine specifications /",2
bitcoin/bitcoin,"method not found # # # issues , reports or feature requests related to the gui should be opened directly on the gui repo - [x ] i still think this issue should be opened here # # # report < img width = "" <number> "" alt = "" 企业微信截图_958a8dcb - <number> - 4 0 7 e - a01b - 1 4 eee3df3e14 "" src = "" <url> i build the node with ubuntu20 . <number> and <number> . <number> . but some api return the error as the picture . the run command "" bitcoind - datadir <annoyed> data2 / . btc / data - rpcuser = "" x <elongated> "" - rpcpassword = "" x <elongated> "" - server - txindex = <number> """,2
bitcoin/bitcoin,"compiling a bitcoin core version that accepts transactions over 1 0 0 vkb # # # please describe the feature you ' d like to see added . i couldnt manage to find any label that fits this , when people try to inscribe ordinals that are bigger than 4 0 0 kb they can not be able to because bitcoin core doesnt allow transactions over 1 0 0 kvb even though that would require the person to be a miner or pay miners for it but it still would be nice to guide us on how to compile such bitcoin core version # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like a guide on what code changes should be done to compile such version # # # describe any alternatives you have considered trying to look through the code to understand it and do it myself # # # please leave any additional context _no response_",2
bitcoin/bitcoin,"get block or transaction data via rpc api when in the ibd process # # # please describe the feature you ' d like to see added . can i get the block or transaction data via the rpc api when the bitcoin core is still in the ibd process that has not been finished ? for example , can i get the downloaded block data via calling the ` getblock ` api while the client is still in the ibd process ? # # # is your feature related to a problem , if so please describe it . _no response_ # # # describe the solution you ' d like _no response_ # # # describe any alternatives you have considered _no response_ # # # please leave any additional context _no response_",2
bitcoin/bitcoin,"bitcoin - core . macos < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > when i install bitcoin core for my mac from official site . i used data_dir to my deskop but i did not found ` . bitcoin ` folder . and i checked out ` / users / ${ user } / library / application support ` but did not get any folder here too . i am unable add to my path of mac but bitcoin - core app running properly . i am stuck now please help * * expected behavior * * < ! - - - what behavior did you expect ? - - > * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"test : p2p_message_capture . py fails with undefined sanitizer on ubuntu <number> relevant configure variable : ` - - with - sanitizers = undefined ` ` ` ` $ test / functional / p2p_message_capture . py <number> - <number> - 2 3 t <time> . 2 5 1 0 0 0 z testframework ( info ) : prng seed is : <number> <number> - <number> - 2 3 t <time> . 2 5 1 0 0 0 z testframework ( info ) : initializing test directory / tmp / bitcoin_func_test_v2mn0yad <number> - <number> - 2 3 t <time> . 6 6 6 0 0 0 z testframework ( info ) : stopping nodes traceback ( most recent call last ) : file "" test / functional / p2p_message_capture . py "" , line <number> , in <module> messagecapturetest ( ) . main ( ) file "" / home / sjors / dev / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main exit_code = self . shutdown ( ) file "" / home / sjors / dev / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in shutdown self . stop_nodes ( ) file "" / home / sjors / dev / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in stop_nodes node . stop_node ( wait = wait , wait_until_stopped = false ) file "" / home / sjors / dev / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in stop_node raise assertionerror ( "" unexpected stderr { } = { } "" . format ( stderr , expected_stderr ) ) assertionerror : unexpected stderr streams . h : <number> <time> : runtime error pointer passed as argument <number> , which is declared to never be null ! = [ node <number> ] cleaning up leftover process ` ` ` seems pretty consistent on the latest master .",2
bitcoin/bitcoin,"hidden fee ( about <percent> of sum ) while send < - - describe the issue - - > hello , my wallet is [ 3 kxxeaecid1h1h5ya2xnjsxlx8fk6ttoqj ] ( <url> - <number> transactions total . i sent <number> . 1 btc , but in blockchain i found <number> transactions : my own ( <number> . 1 btc ) + unknown fee ( <number> . 0 1 9 9 9 2 4 1 btc ) - <percent> of sum ! txid : [ 7 0 a951a496a809597b79753be7b4f05a7b13ece1d09d8d0ae643b0190ace1fba ] ( <url> next i sent <number> . 1 btc , but in blockchain i found <number> transactions : my own ( <number> . 1 btc ) + unknown fee ( <number> . 0 0 9 9 7 9 4 9 btc ) - <percent> of sum ! txid i do not understand how this could happen . in bitcoin core i see only my transactions , but blockchain show me more <number> transactions to unknown wallets ( [ bc1qg3ds029ydsum9pgldpfhmcp94sncrvhqzltp5q ] ( <url> [ 3 be8zpdguqxxbagjj5imvg7nyzzglxahhr ] ( <url> so , total balance in blockchain is less then in bitcoin core on this amount ( <number> . 0 2 9 9 7 1 8 9 btc ) bitcoin core show me <number> btc , but blockchain show me <number> . 3 5 9 9 7 2 4 7 btc ( minus two fees above ) what is my really balance ? what are these hidden payments ? tell me more about it , please . what can i do to solve this problems in future ? screens from my bitcoin core is attached below . thanks ! * * expected behavior * * < ! - - - what behavior did you expect ? - - > * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > bitcoin core version v0 . <number> ( <number> - bit ) - from official site . < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > linux blockchain <number> . <number> - <number> - generic # <number> - ubuntu smp wed <date> <time> utc <number> x86_64 x86_64 x86_64 gnu / linux < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - > ! [ balance ] ( <url> ! [ sent1 ] ( <url> ! [ sent2 ] ( <url> ! [ transactions ] ( <url> ! [ version ] ( <url>",2
bitcoin/bitcoin,"bitcoin blockchain download alternate please anyone out there having knowledge help me that * * how can i deploy a node and start transactions on btc core without downloading the complete blockchain * * , as its more than 3 5 0 gb right now . i want to know if there is any possible solution to it ? < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> pm "" src = "" <url>",2
bitcoin/bitcoin,"how to interpret the asm section of scriptsig i tried asking the question in [ this post ] ( <url> on bitcoin . stackexchange . com but got no answer . the question is this : i am trying to understand the output from ` bitcoin - cli ` . the sample transaction ' s txid is ` 0 6 1 9 5 9 f1a3360d3781a870b2d43f73f7105b194b22f3765fcb9b8f545f9c8317 ` , from block <number> . the ` asm ` section of ` scriptpubkey ` ( i . e . , ` op_dup op_hash160 2 8 dce60cf7ba4d749afce5fd9781a403d293b74a op_equalverify op_checksig ` ) is more understandable , it decodes ` hex ` and shows the assembly in a ( sort of ) human - readable format : ` ` ` "" vout "" : [ { "" value "" : <number> , "" n "" : <number> , "" scriptpubkey "" : { "" asm "" : "" op_dup op_hash160 2 8 dce60cf7ba4d749afce5fd9781a403d293b74a op_equalverify op_checksig "" , "" hex "" : "" 7 6 a91428dce60cf7ba4d749afce5fd9781a403d293b74a88ac "" , "" address "" : "" 1 4 j4eyoxaawa4shj9yceyvj7fsqywvuh9b "" , "" type "" : "" pubkeyhash "" } } , . <repeated> ] ` ` ` the question arises when it comes to the ` asm ` section of ` scriptsig ` : ` ` ` { "" txid "" : "" b66e78c919e36a6c563ceb1b29cfec26f7dec3c3fc1b3631c84056f3ae147f2f "" , "" vout "" : <number> , "" scriptsig "" : { "" asm "" : "" <phone> dceb566dec99cf195aba5d6313f1e95eb7bfc74c93a794c4bfd6dd9f4082d8a002203b495b70b917b3dffdcbe70fc6ff7de910d1697efccc14f3eea6944bda87d21c [ all ] <phone> c4d3240d818f400ab66fd4de438f2fd9174641ea76480b95cd6e883ec274a10b0691d85ac2cb87dcb9eef58b3abb8ee4bd277c8d6fea09eace2bc24a "" , "" hex "" : "" 4 8 3 0 4 5 0 2 2 1 0 0 dceb566dec99cf195aba5d6313f1e95eb7bfc74c93a794c4bfd6dd9f4082d8a002203b495b70b917b3dffdcbe70fc6ff7de910d1697efccc14f3eea6944bda87d21c01410445554717c4d3240d818f400ab66fd4de438f2fd9174641ea76480b95cd6e883ec274a10b0691d85ac2cb87dcb9eef58b3abb8ee4bd277c8d6fea09eace2bc24a "" } ` ` ` as you can see , it is ` 3 0 4 5 0 2 2 0 6 ee08c76923816e4ba287142e9f147fe9cd0f26e6bd58b9a43f2283b1c614f46022100d5de298b627407bc7d5ac0a40259cafb865c30c6a67db926c7284da96ff71abd [ all ] 0 4 0 8 4 1 9 5 8 a405ca1c05de4dcf04dfdfd6e7de5e7cb106744977e3d99eab3e59a2b5bc2441e0ad179055c14200745feb2da2d1b4485087e3a9a2a88a6531a6d6b02 ` , which is not really decoded . i checked the same transaction from [ blockstream . info ] ( <url> it ' s result is : ` op_pushbytes_72 3 0 4 5 0 2 2 0 6 ee08c76923816e4ba287142e9f147fe9cd0f26e6bd58b9a43f2283b1c614f46022100d5de298b627407bc7d5ac0a40259cafb865c30c6a67db926c7284da96ff71abd01 op_pushbytes_65 0 4 0 8 4 1 9 5 8 a405ca1c05de4dcf04dfdfd6e7de5e7cb106744977e3d99eab3e59a2b5bc2441e0ad179055c14200745feb2da2d1b4485087e3a9a2a88a6531a6d6b02 ` which looks more readable and i can understand that this ` scriptsig ` pushes two byte arrays into the bitcoin vm ' s stack . so the questions are why ` bitcoin - cli ` ' s output is like this ? <number> . how to interpret it ? especially , how to understand the ` [ all ] ` part in its ` asm ` ? ( perhaps you can just give me a rtfm link ? )",2
bitcoin/bitcoin,"sender always unknown on the laatest version . < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > sender always show unknown . is there any way to see senders in order to make our own accounting ? * * expected behavior * * to see sender too . < ! - - - what behavior did you expect ? - - > * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"sha256sums . asc - using untrusted key getting warning ` using untrusted key ! ` when running gpg verification : ` ` ` gpg - - keyserver hkp :// keyserver . ubuntu . com \ \ - - auto - key - retrieve \ \ - - trust - model always \ \ - - verify sha256sums . asc sha256sums ` ` ` output and warning : ` ` ` # <number> <number> gpg : requesting key 0 a41bdc3f4faff1c from hkp server keyserver . ubuntu . com # <number> <number> gpg : key 0 a41bdc3f4faff1c : public key "" aaron clauson ( sipsorcery ) < <email> > "" imported # <number> <number> gpg : total number processed : <number> # <number> <number> gpg : imported : <number> # <number> <number> gpg : good signature from "" aaron clauson ( sipsorcery ) < <email> > "" [ unknown ] # <number> <number> gpg : warning : using untrusted key ! # <number> <number> gpg : signature made thu <date> <time> <number> utc # <number> <number> gpg : using rsa key ed9bdf7ad6a55e232e84524257ff9bdbcc301009 # <number> <number> gpg : requesting key 5 7 ff9bdbcc301009 from hkp server keyserver . ubuntu . com # <number> <number> gpg : key 5 7 ff9bdbcc301009 : <number> duplicate signatures removed # <number> <number> gpg : key 5 7 ff9bdbcc301009 : public key "" sjors provoost < <email> > "" imported # <number> <number> gpg : total number processed : <number> # <number> <number> gpg : imported : <number> # <number> <number> gpg : good signature from "" sjors provoost < <email> > "" [ unknown ] # <number> <number> gpg : aka "" sjors provoost < <email> > "" [ unknown ] # <number> <number> gpg : warning : using untrusted key ! # <number> <number> gpg : signature made tue <date> <time> <number> utc # <number> <number> gpg : using rsa key 6 a8f9c266528e25aeb1d7731c2371d91cb716ea7 # <number> <number> gpg : issuer "" <email> "" # <number> <number> gpg : requesting key c2371d91cb716ea7 from hkp server keyserver . ubuntu . com # <number> <number> gpg : key c2371d91cb716ea7 : public key "" sebastian falbesoner ( thestack ) < <email> > "" imported # <number> <number> gpg : total number processed : <number> # <number> <number> gpg : imported : <number> # <number> <number> gpg : good signature from "" sebastian falbesoner ( thestack ) < <email> > "" [ unknown ] # <number> <number> gpg : warning : using untrusted key ! # <number> <number> gpg : signature made thu <date> <time> <number> utc # <number> <number> gpg : using rsa key 2 8 e72909f1717fe9607754f8a7beb2621678d37d # <number> <number> gpg : issuer "" <email> "" # <number> <number> gpg : key da43140f88b4e81f : public key "" <email> < <email> > "" imported # <number> <number> gpg : total number processed : <number> # <number> <number> gpg : imported : <number> # <number> <number> gpg : requesting key a7beb2621678d37d from hkp server keyserver . ubuntu . com # <number> <number> gpg : key a7beb2621678d37d : public key "" vertion < <email> > "" imported # <number> <number> gpg : total number processed : <number> # <number> <number> gpg : imported : <number> # <number> <number> gpg : good signature from "" vertion < <email> > "" [ unknown ] # <number> <number> gpg : warning untrusted key ! ` ` `",2
bitcoin/bitcoin,"flushing block file to disk failed when using external drive on macos ventura i kept getting an error regarding flushing block to disk . i have tried to use bitcoin core v0 . <number> and it does not help ; although v0 . <number> was better at loading the backup file , but i could not send any of my bitcoin out . i can not seem to load the wallet or the backup file properly . not a coder and hoping someone can help . i do not think it ' s an issue with the hard drive . any suggestion ? i am on macosx ventura <number> . debug . log file : <number> - <number> - 0 4 t <time> z using wallet / volumes / extreme ssd / bruceltd wallet . dat <number> - <number> - 0 4 t <time> z berkeleyenvironment : : open : logdir <annoyed> volumes / extreme ssd / database errorfile <annoyed> volumes / extreme ssd / db . log . . . <number> - <number> - 0 4 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 0 4 t <time> z error : serializefiledb : failed to flush file / volumes / extreme ssd / peers . 1 7 6 f db . log file : fsync : inappropriate ioctl for device panic : inappropriate ioctl for device txn_checkpoint : failed to flush the buffer cache : db_runrecovery error , run database recovery",2
bitcoin/bitcoin,"add ability to ignore ` script_verify_discourage_x ` flags when broadcasting transactions on test networks . after bugs like it would be useful to be able to reproduce these bugs in regtest . currently to do so you would need to manually edit the code and comment out the script flag , then compile your self . it would be beneficial if we could disable these flags to create more reproducible tests .",2
bitcoin/bitcoin,"initialblockdownload set to false prematurely < - - describe the issue - - > * * expected behavior * * getblockchaininfo . initialblockdownload stays true until all blocks are imported * * actual behavior * * ` getblockchaininfo . initialblockdownload ` becomes false before before all blocks are imported : ` ` ` <number> - <number> - 0 1 t <time> z updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 f01ad0649cefc26c86c24a2808109c584bdcdc0d5293 height = <number> version =0 x20f0c000 log2_work = <number> tx = <number> date = ' <number> - <number> - 3 1 t <time> z ' progress = <number> cache = <number> . 3 mib ( 2 2 6 9 6 3 1 txo ) <number> - <number> - 0 1 t <time> z leaving initialblockdownload ( latching to false ) <number> - <number> - 0 1 t <time> z updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 8 3 1 5 da1efb6937a0d06015891b0412164dc2531e3004 height = <number> version =0 x20c00000 log2_work = <number> tx = <number> date = ' <number> - <number> - 3 1 t <time> z ' progress = <number> cache = <number> . 7 mib ( 2 2 7 3 0 0 0 txo ) < snip <number> lines of updatetip > <number> - <number> - 0 1 t <time> z updatetip best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 6 5 6 d346fbac96ddcec6ee31b6abd696a241cd6994181 height = <number> version =0 x2e4a6000 log2_work = <number> tx = <number> date = ' <number> - <number> - 0 1 t <time> z ' progress = <number> cache = <number> . 6 mib ( 2 5 7 3 6 6 9 txo ) ` ` ` observe the <number> seconds it took my ( reasonably fast ) machine to import the remaining blocks . this is a problem for me since my application relies on this field to determine if syncing is complete . we can not rely on ` getblockchaininfo . verificationprogress ` either due to <url> < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > start a node that has not been synced in a while and observe the log output . * * system information * * observed on version <number> from the website . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"sqlite version vulnerability <url> sqlite version is before vulnerability [ cve - <number> - <number> ] ( <url> was fixed , details of which are described [ here ] ( <url>",2
bitcoin/bitcoin,"estimatesmartfee returns <number> sat / vb on an empty mempool for inclusion in <number> blocks < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > * * expected behavior * * < ! - - - what behavior did you expect ? - - > when the mempool is empty , i expect something really low , at least smaller than <number> sat / vb , but not <number> sat / vb * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > ` ` ` $ bitcoin - cli - testnet estimatesmartfee <number> { "" feerate "" : <number> , "" blocks "" : <number> } ` ` ` ` ` ` $ bitcoin - cli - testnet estimatesmartfee <number> { "" feerate "" : <number> , "" blocks "" } ` ` ` * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > run ` bitcoin - cli - testnet estimatesmartfee ` on the testnet when the difficulty drops to <number> ( happens every few months or so i dont know why ) * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > bitcoincore . org download v23 < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > * * any extra information that might be useful in the debugging process . * * blocks were flowing <emphasis> in like every few seconds a block got mined < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"centos compilation error - no rule to make target < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! any report , issue or feature request related to the gui should be reported at <url> - - > < ! - - describe the issue - - > i am trying to compile bitcoin v23 . <number> tag ( commit id : ` fcf6c8f ` ) . during the compilation i received this error : ` ` ` make [ <number> <sad> * * * no rule to make target ' % reldir %/ lib / univalue . cpp ' , needed by ' % reldir %/ lib / libunivalue_la - univalue . lo ' . stop . ` ` ` os <number> compiler gcc <number> . <number> < ! - - - what behavior did you expect ? - - > < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,bitcoin core ' s sync has stop for days ! [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url> os info 2 1 h2 what should i do ?,2
bitcoin/bitcoin,"sync slow syncing blockchain is sometimes very slow . i think this has become more common in the past months . here ' s a screenshot from network traffic window in the gui , see how there are long periods of absolutely no traffic . also see how it only downloaded about <number> mb during the first 3 0 minutes of syncing . it ' s also often connected to just <number> or <number> peers which is weird . [ bitcointrafficverylowandslow ] ( <url> internet is stable and i can easily download <number> mb / second . cpu load stays mostly low so lack of cpu resources does not explain this . memory usage is also not high . i have set the memory option in the settings menu to <number> gb . sometimes it starts to work faster if you close and restart the program . but as i said this has become worse in the past months . why does not it download blocks faster than this ? if the problem is that peers can not provide blocks faster , why is not it looking for more peers ? i am using the latest master from git as of today .",2
bitcoin/bitcoin,"conan build system < - - describe the issue - - > does bitcoin support building with conan ( <url> build system ? is there a way to build bitcoin and the ` depends ` library with conan ? < ! - - - what behavior did you expect ? - - > < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"is there an alternative to the - blockmaxsize option available in the client ? the - blockmaxsize option for bitcoind was removed with issue # <number> , and i need to be able to limit my block creation size , is there an alternative setting that will accomplish this or is there no control of this now beyond restricting my mempool with fees ?",2
bitcoin/bitcoin,"i can not define bitcoin . conf file in different folder as default i installed the bitcoin libraries on / mnt / volume_fra1_02 , ran it via the following setting bitcoind - datadir <annoyed> mnt / volume_fra1_02 / . bitcoin - conf <annoyed> mnt / volume_fra1_02 / . bitcoin / bitcoin . conf i get an error when i want to create a new wallet bitcoin - cli createwallet adminwallet response : no authentication cookie could be found , and rpc password is not set . see - rpcpassword and - stdinrpcpass . configuration file probably still based on "" / root / . bitcoin / bitcoin . conf "" directory . could you help ?",2
bitcoin/bitcoin,. / configure hang up i am using mac os <number> . <number> i get to this point everytime everyway i try to configure the code from source . <repeated> ` ` ` checking dependency style of g + + . <repeated> gcc3 checking whether g + + supports c + + <number> features with - std =c + + <number> . <repeated> yes checking whether std : : filesystem can be used without link library . <repeated> no checking whether std : : filesystem needs - lstdc + + fs . <repeated> no checking whether std : : filesystem needs - lc + + fs . <repeated> configure : error : in ` / bitcoin / bitcoin - cli ' : configure : error figure out how to use std : : filesystem see ` config . log ' for more details ` ` ` what can i do to remedy the situation ?,2
bitcoin/bitcoin,bitcoin - cli createwallet error : compiled without sqlite support i installed bitcoin core on ubuntu server ( i got help from : <url> the following commands work fine : bitcoin - cli - getinfo bitcoin - cli getblockchaininfo bitcoin - cli getnetworkinfo bitcoin - cli getpeerinfo i get this error when i want to create a new wallet : request : bitcoin - cli createwallet adminwallet response : error code : - <number> error message without sqlite support ( required for descriptor wallets ),2
bitcoin/bitcoin,"feature_config_args . py failure run test : ` python3 feature_config_args . py ` results in error : > > <number> - <number> - 0 1 t <time> . 5 7 4 0 0 0 z testframework ( info ) : initializing test directory / tmp / bitcoin_func_test_die2tixp > <number> - <number> - 0 1 t <time> . 3 0 6 0 0 0 z testframework ( info ) : test config args logging > <number> - <number> - 0 1 t <time> . 5 6 4 0 0 0 z testframework ( info ) : test seed peers > <number> - <number> - 0 1 t <time> . 6 9 7 0 0 0 z testframework ( error ) : assertion failed > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main > self . run_test ( ) > file "" / home / user / bitcoin / test / functional / feature_config_args . py "" , line <number> , in run_test > self . test_seed_peers ( ) > file "" / home / user / bitcoin / test / functional / feature_config_args . py "" , line <number> , in test_seed_peers > self . start_node ( <number> , extra_args =[ ' - dnsseed = <number> ' , ' - fixedseeds = <number> ' , f ' - mocktime ={ start } ' ] ) > file "" / usr / lib / python3 . <number> / contextlib . py "" , line <number> , in __exit__ > next ( self . gen ) > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in assert_debug_log > self . _raise_assertion_error ( ' expected messages "" { } "" does not partially match log :\\ n \ \ n { } \ \ n \ \ n ' . format ( str ( expected_msgs ) , print_log ) ) > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in _raise_assertion_error > raise assertionerror ( self . _node_msg ( msg ) ) > assertionerror : [ node <number> ] expected messages "" [ ' loaded <number> addresses from peers . dat ' , ' <number> addresses found from dns seeds ' , ' opencon thread start ' ] "" does not partially match log : > > > > > > > <number> - <number> - 0 1 t <time> . 7 7 6 9 6 5 z [ init ] [ init / common . cpp : <number> ] [ logpackageversion ] bitcoin core version v23 . <number> - ce3b75690d10 ( release build ) > <number> - <number> - 0 1 t <time> . 7 7 7 0 1 7 z [ init ] [ init . cpp : <number> ] [ initparameterinteraction ] initparameterinteraction : parameter interaction : - bind set - > setting - listen = <number> > <number> - <number> - 0 1 t <time> . 7 7 7 0 8 8 z [ init ] [ init . cpp : <number> ] [ appinitparameterinteraction ] validating signatures for all blocks . > <number> - <number> - 0 1 t <time> . 7 7 7 0 9 4 z [ init ] [ init . cpp : <number> ] [ appinitparameterinteraction ] setting nminimumchainwork = <number> > <number> - <number> - 0 1 t <time> . 7 7 7 1 6 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ kernel / context . cpp : <number> ] [ context ] using the ' sse4 ( 1 way ) , sse41 ( 4 way ) , avx2 ( 8 way ) ' sha256 implementation > <number> - <number> - 0 1 t <time> . 7 7 7 1 7 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ random . cpp : <number> ] [ reporthardwarerand ] using rdseed as an additional entropy source > <number> - <number> - 0 1 t <time> . 7 7 7 1 8 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ random . cpp : <number> ] [ reporthardwarerand ] using rdrand as an additional entropy source > <number> - <number> - 0 1 t <time> . 7 7 8 9 7 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init / common . cpp : <number> ] [ startlogging ] default data directory / home / user / . bitcoin > <number> - <number> - 0 1 t <time> . 7 7 8 9 8 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init / common . cpp : <number> ] [ startlogging ] using data directory / tmp / bitcoin_func_test_die2tixp / node0 / regtest > <number> - <number> - 0 1 t <time> . 7 7 8 9 9 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init / common . cpp : <number> ] [ startlogging ] config file : / tmp / bitcoin_func_test_die2tixp / node0 / bitcoin . conf > <number> - <number> - 0 1 t <time> . 7 7 9 0 1 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : regtest = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 2 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] bind = "" <number> . <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 3 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] discover = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 3 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] dnsseed = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 4 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] fallbackfee = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 5 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] fixedseeds = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 6 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] keypool = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 6 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] listenonion = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 7 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] natpmp = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 8 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] peertimeout = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 8 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] port = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 0 9 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] printtoconsole = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 0 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] rpcdoccheck = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 1 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] rpcport = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 1 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] server = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 2 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] shrinkdebugfile = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 3 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] unsafesqlitesync = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 4 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] config file arg : [ regtest ] upnp = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 4 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : datadir =""/ tmp / bitcoin_func_test_die2tixp / node0 "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 5 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : debug = "" "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 6 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : debugexclude = "" libevent "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 7 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : debugexclude = "" leveldb "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 7 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : dnsseed = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 8 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : fixedseeds = "" <number> "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 9 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : logsourcelocations = "" "" > <number> - <number> - 0 1 t <time> . 7 7 9 1 9 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : logthreadnames = "" "" > <number> - <number> - 0 1 t <time> . 7 7 9 2 0 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : logtimemicros = "" "" > <number> - <number> - 0 1 t <time> . 7 7 9 2 1 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : mocktime = "" <phone> "" > <number> - <number> - 0 1 t <time> . 7 7 9 2 2 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ util / system . cpp : <number> ] [ logargsprefix ] command - line arg : uacomment = "" testnode0 "" > <number> - <number> - 0 1 t <time> . 7 7 9 2 3 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] using at most <number> automatic connections ( <number> file descriptors available ) > <number> - <number> - 0 1 t <time> . 7 8 8 0 8 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ script / sigcache . cpp : <number> ] [ initsignaturecache ] using <number> mib out of <number> / <number> requested for signature cache , able to store <number> elements > <number> - <number> - 0 1 t <time> . 7 9 6 9 4 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ validation . cpp : <number> ] [ initscriptexecutioncache ] using <number> mib out of <number> / <number> requested for script execution cache , able to store <number> elements > <number> - <number> - 0 1 t <time> . 7 9 6 9 8 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] script verification uses <number> additional threads > <number> - <number> - 0 1 t <time> . 7 9 7 3 6 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ scheduler ] [ util / thread . cpp : <number> ] [ tracethread ] scheduler thread start > <number> - <number> - 0 1 t <time> . 8 0 6 7 7 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ inithttpallowlist ] [ http ] allowing http connections from : <number> . <number> / <number> : : <number> / <number> > <number> - <number> - 0 1 t <time> . 8 0 6 8 9 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ httpbindaddresses ] [ http ] binding rpc on address : : <number> port <number> > <number> - <number> - 0 1 t <time> . 8 0 7 1 0 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ httpbindaddresses ] [ http ] binding rpc on address <number> . <number> port <number> > <number> - <number> - 0 1 t <time> . 8 0 7 1 5 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ inithttpserver ] [ http ] initialized http server > <number> - <number> - 0 1 t <time> . 8 0 7 1 6 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ inithttpserver ] [ http ] creating work queue of depth <number> > <number> - <number> - 0 1 t <time> . 8 0 7 1 7 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ rpc / server . cpp : <number> ] [ startrpc ] [ rpc ] starting rpc > <number> - <number> - 0 1 t <time> . 8 0 7 1 9 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httprpc . cpp : <number> ] [ starthttprpc ] [ rpc ] starting http rpc server > <number> - <number> - 0 1 t <time> . 8 0 7 2 0 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httprpc . cpp : <number> ] [ initrpcauthentication ] using random cookie authentication . > <number> - <number> - 0 1 t <time> . 8 0 7 2 9 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ rpc / request . cpp : <number> ] [ generateauthcookie ] generated rpc authentication cookie / tmp / bitcoin_func_test_die2tixp / node0 / regtest / . cookie > <number> - <number> - 0 1 t <time> . 8 0 7 3 1 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ registerhttphandler ] [ http ] registering http handler for / ( exactmatch <number> ) > <number> - <number> - 0 1 t <time> . 8 0 7 3 2 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ registerhttphandler ] [ http ] registering http handler for / wallet / ( exactmatch <number> ) > <number> - <number> - 0 1 t <time> . 8 0 7 3 3 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ starthttpserver ] [ http ] starting http server > <number> - <number> - 0 1 t <time> . 8 0 7 3 4 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ httpserver . cpp : <number> ] [ starthttpserver ] [ http ] starting <number> worker threads > <number> - <number> - 0 1 t <time> . 8 0 7 5 5 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ http ] [ httpserver . cpp : <number> ] [ threadhttp ] [ http ] entering http event loop > <number> - <number> - 0 1 t <time> . 8 0 7 7 8 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ wallet / load . cpp : <number> ] [ verifywallets ] using wallet directory / tmp / bitcoin_func_test_die2tixp / node0 / regtest / wallets > <number> - <number> - 0 1 t <time> . 8 0 7 8 0 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : verifying wallet ( s ) … > <number> - <number> - 0 1 t <time> . 8 0 7 8 4 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] using / <number> prefix for ip bucketing > <number> - <number> - 0 1 t <time> . 8 0 7 8 5 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : loading p2p addresses … > <number> - <number> - 0 1 t <time> . 8 0 8 1 1 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ logging / timer . h : <number> ] [ log ] [ addrman ] checkaddrman : new <number> , tried <number> , total <number> started > <number> - <number> - 0 1 t <time> . 8 0 8 2 2 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ logging / timer . h : <number> ] [ log ] [ addrman ] checkaddrman : completed ( <number> . 0 0 ms ) > <number> - <number> - 0 1 t <time> . 8 0 8 2 4 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ addrdb . cpp : <number> ] [ loadaddrman ] loaded <number> addresses from peers . dat 0 ms > <number> - <number> - 0 1 t <time> . 8 0 8 5 8 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : loading banlist … > <number> - <number> - 0 1 t <time> . 8 0 8 6 2 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ banman . cpp : <number> ] [ loadbanlist ] [ net ] loaded <number> banned node addresses / subnets 0 ms > <number> - <number> - 0 1 t <time> . 8 0 8 6 4 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ net . cpp : <number> ] [ settrynewoutboundpeer ] [ net ] setting try another outbound peer = false > <number> - <number> - 0 1 t <time> . 8 0 8 6 5 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ net . cpp : <number> ] [ setnetworkactive ] setnetworkactive : true > <number> - <number> - 0 1 t <time> . 8 1 0 4 0 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ policy / fees . cpp : <number> ] [ read ] [ estimatefee ] reading estimates : <number> buckets counting confirms up to <number> blocks > <number> - <number> - 0 1 t <time> . 8 1 0 6 0 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ policy / fees . cpp : <number> ] [ read ] [ estimatefee ] reading estimates : <number> buckets counting confirms up to <number> blocks > <number> - <number> - 0 1 t <time> . 8 1 1 1 3 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ policy / fees . cpp : <number> ] [ read ] [ estimatefee ] reading estimates : <number> buckets counting confirms up to <number> blocks > <number> - <number> - 0 1 t <time> . 8 1 1 2 5 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] cache configuration : > <number> - <number> - 0 1 t <time> . 8 1 1 2 6 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] * using <number> mib for block index database > <number> - <number> - 0 1 t <time> . 8 1 1 2 7 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] * using <number> mib for chain state database > <number> - <number> - 0 1 t <time> . 8 1 1 2 9 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] * using <number> mib for in - memory utxo set ( plus up to <number> mib of unused mempool space ) > <number> - <number> - 0 1 t <time> . 8 1 1 3 1 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : loading block index … > <number> - <number> - 0 1 t <time> . 8 1 1 3 3 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ validation . cpp : <number> ] [ initializechainstate ] switching active chainstate to chainstate [ ibd ] @ height - <number> ( null ) > <number> - <number> - 0 1 t <time> . 8 1 1 3 6 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] opening leveldb in / tmp / bitcoin_func_test_die2tixp / node0 / regtest / blocks / index > <number> - <number> - 0 1 t <time> . 8 2 6 8 8 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] opened leveldb successfully > <number> - <number> - 0 1 t <time> . 8 2 6 9 2 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] using obfuscation key for / tmp / bitcoin_func_test_die2tixp / node0 / regtest / blocks / index : <number> > <number> - <number> - 0 1 t <time> . 8 2 7 0 9 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ node / blockstorage . cpp : <number> ] [ loadblockindexdb ] loadblockindexdb : last block file = <number> > <number> - <number> - 0 1 t <time> . 8 2 7 1 1 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ node / blockstorage . cpp : <number> ] [ loadblockindexdb ] loadblockindexdb : last block file info : cblockfileinfo ( blocks = <number> , size = <number> , heights = <number> . <repeated> <number> , time = <number> - <number> - <number> . <repeated> <number> - <number> - <number> ) > <number> - <number> - 0 1 t <time> . 8 2 7 1 1 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ node / blockstorage . cpp : <number> ] [ loadblockindexdb ] checking all blk files are present . <repeated> > <number> - <number> - 0 1 t <time> . 8 2 7 1 6 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] opening leveldb in / tmp / bitcoin_func_test_die2tixp / node0 / regtest / chainstate > <number> - <number> - 0 1 t <time> . 8 4 6 3 3 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] opened leveldb successfully > <number> - <number> - 0 1 t <time> . 8 4 6 4 4 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ dbwrapper . cpp : <number> ] [ cdbwrapper ] using obfuscation key for / tmp / bitcoin_func_test_die2tixp / node0 / regtest / chainstate : 2 2 3 bb69586684f13 > <number> - <number> - 0 1 t <time> . 8 4 6 5 4 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ validation . cpp : <number> ] [ loadchaintip ] loaded best chain : hashbestchain =0 f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206 height = <number> date = <number> - <number> - 0 2 t <time> z progress = <number> > <number> - <number> - 0 1 t <time> . 8 4 6 5 6 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : verifying blocks … > <number> - <number> - 0 1 t <time> . 8 4 6 5 8 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] block index 3 5 ms > <number> - <number> - 0 1 t <time> . 8 4 7 5 5 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] block tree size = <number> > <number> - <number> - 0 1 t <time> . 8 4 7 5 8 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ init . cpp : <number> ] [ appinitmain ] nbestheight = <number> > <number> - <number> - 0 1 t <time> . 8 4 7 6 3 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ loadblk ] [ util / thread . cpp : <number> ] [ tracethread ] loadblk thread start > <number> - <number> - 0 1 t <time> . 8 4 7 7 2 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ loadblk ] [ kernel / mempool_persist . cpp : <number> ] [ loadmempool ] imported mempool transactions from disk : <number> succeeded , <number> failed , <number> expired , <number> already there , <number> waiting for initial broadcast > <number> - <number> - 0 1 t <time> . 8 4 7 7 4 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ loadblk ] [ util / thread . cpp : <number> ] [ tracethread ] loadblk thread exit > <number> - <number> - 0 1 t <time> . 8 4 7 7 9 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ net . cpp : <number> ] [ bindlistenport ] bound to <number> . <number> . <time> <number> > <number> - <number> - 0 1 t <time> . 8 4 7 8 3 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ net . cpp : <number> ] [ bindlistenport ] bound to <number> . <number> . <time> <number> > <number> - <number> - 0 1 t <time> . 8 4 7 8 6 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ addrdb . cpp : <number> ] [ readanchors ] loaded <number> addresses from "" anchors . dat "" > <number> - <number> - 0 1 t <time> . 8 4 7 9 3 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ net . cpp : <number> ] [ start ] <number> block - relay - only anchors will be tried for connections . > <number> - <number> - 0 1 t <time> . 8 4 7 9 5 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : starting network threads … > <number> - <number> - 0 1 t <time> . 8 4 8 0 2 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ net ] [ util / thread . cpp : <number> ] [ tracethread ] net thread start > <number> - <number> - 0 1 t <time> . 8 4 8 0 9 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ addcon ] [ util / thread . cpp : <number> ] [ tracethread ] addcon thread start > <number> - <number> - 0 1 t <time> . 8 4 8 2 3 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ util / thread . cpp : <number> ] [ tracethread ] opencon thread start > <number> - <number> - 0 1 t <time> . 8 4 8 3 0 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ msghand ] [ util / thread . cpp : <number> ] [ tracethread ] msghand thread start > <number> - <number> - 0 1 t <time> . 8 4 8 3 2 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ init ] [ noui . cpp : <number> ] [ noui_initmessage ] init message : done loading > <number> - <number> - 0 1 t <time> . 8 4 8 3 9 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ util / thread . cpp : <number> ] [ tracethread ] dnsseed thread start > <number> - <number> - 0 1 t <time> . 8 4 8 4 1 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ net . cpp : <number> ] [ threaddnsaddresseed <elongated> ] loading addresses from dns seed dummyseed . invalid . > <number> - <number> - 0 1 t <time> . 8 5 7 5 3 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ addrman . cpp : <number> ] [ addsingle ] [ addrman ] added <number> . <number> . <time> <number> mapped to as0 to new [ <number> ] [ <number> ] > <number> - <number> - 0 1 t <time> . 8 5 7 5 6 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ addrman . cpp : <number> ] [ add_ ] [ addrman ] added <number> addresses ( of <number> ) from jzva5xo5xttqfkfm . internal : <number> tried , <number> new > <number> - <number> - 0 1 t <time> . 8 5 7 5 7 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ net . cpp : <number> ] [ threaddnsaddresseed <elongated> ] <number> addresses found from dns seeds > <number> - <number> - 0 1 t <time> . 8 5 7 5 8 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ dnsseed ] [ util / thread . cpp : <number> ] [ tracethread ] dnsseed thread exit > <number> - <number> - 0 1 t <time> . 9 2 1 8 0 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ http ] [ httpserver . cpp : <number> ] [ http_request_cb ] [ http ] received a post request for / from <number> . <number> . <time> <number> > <number> - <number> - 0 1 t <time> . 9 2 1 8 8 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ httpworker . <number> ] [ rpc / request . cpp : <number> ] [ parse ] [ rpc ] threadrpcserver method = getblockcount user = __cookie__ > <number> - <number> - 0 1 t <time> . 9 2 3 6 2 8 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ http ] [ httpserver . cpp : <number> ] [ http_request_cb ] [ http ] received a post request for / from <number> . <number> . <time> <number> > <number> - <number> - 0 1 t <time> . 9 2 3 8 5 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ httpworker . <number> ] [ rpc / request . cpp : <number> ] [ parse ] [ rpc ] threadrpcserver method = getmempoolinfo user = __cookie__ > <number> - <number> - 0 1 t <time> . 3 4 9 1 2 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 4 9 2 0 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ net . cpp : <number> ] [ connectnode ] [ net : debug ] trying connection <number> . <number> . <time> <number> lastseen = <number> . 9 hrs > <number> - <number> - 0 1 t <time> . 3 5 4 1 5 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ net . cpp : <number> ] [ cnode ] [ net ] added connection peer = <number> > <number> - <number> - 0 1 t <time> . 3 5 4 2 3 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ net . cpp : <number> ] [ pushmessage ] [ net ] sending version ( <number> bytes ) peer = <number> > <number> - <number> - 0 1 t <time> . 3 5 4 3 0 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ net_processing . cpp : <number> ] [ pushnodeversion ] [ net ] send version message : version <number> , blocks = <number> , txrelay = <number> , peer = <number> > <number> - <number> - 0 1 t <time> . 8 5 4 9 5 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 5 6 8 8 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 5 7 6 4 9 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 5 9 2 5 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 6 0 7 3 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 6 1 6 9 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 6 3 2 9 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 6 4 8 3 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 6 7 7 3 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 6 9 8 0 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 7 0 8 1 7 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 7 2 1 1 1 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 7 3 1 1 6 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 7 4 3 9 0 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 7 5 9 7 5 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 7 7 2 6 4 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 8 7 8 5 6 3 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > <number> - <number> - 0 1 t <time> . 3 8 0 3 0 2 z ( mocktime : <number> - <number> - 0 1 t <time> z ) [ opencon ] [ addrman . cpp : <number> ] [ select_ ] [ addrman ] selected <number> . <number> . <time> <number> from new > > > <number> - <number> - 0 1 t <time> . 7 0 9 0 0 0 z testframework ( info ) : stopping nodes > <number> - <number> - 0 1 t <time> . 8 6 2 0 0 0 z testframework ( warning ) : not cleaning up dir / tmp / bitcoin_func_test_die2tixp > <number> - <number> - 0 1 t <time> . 8 6 2 0 0 0 z testframework ( error ) : test failed . test logging available at / tmp / bitcoin_func_test_die2tixp / test_framework . log > <number> - <number> - 0 1 t <time> . 8 6 2 0 0 0 z testframework ( error ) : > <number> - <number> - 0 1 t <time> . 8 6 2 0 0 0 z testframework ( error ) : hint : call / home / user / bitcoin / test / functional / combine_logs . py ' / tmp / bitcoin_func_test_die2tixp ' to consolidate all logs > <number> - <number> - 0 1 t <time> . 8 6 3 0 0 0 z testframework ( error ) : > <number> - <number> - 0 1 t <time> . 8 6 3 0 0 0 z testframework ( error ) : if this failure happened unexpectedly or intermittently , please file a bug and provide a link or upload of the combined log . > <number> - <number> - 0 1 t <time> . 8 6 3 0 0 0 z testframework ( error ) : <url> > <number> - <number> - 0 1 t <time> . 8 6 3 0 0 0 z testframework ( error ) : logs : ` python3 / home / user / bitcoin / test / functional / combine_logs . py ' / tmp / bitcoin_func_test_die2tixp ' ` pasted here",2
bitcoin/bitcoin,"loadwalletinternal logic check in the rpc method when the rpc method "" loadwallet "" is called , it will call loadwallet ( <url> and loadwalletinternal . when the user repeatedly call the loadwallet with same wallet name . in the loadwalletinternal funcntion , although it does not call addwallet repeatedly to add to context . wallets and avoid open sqlite database repeatedly . but it will make new sqlitedatabase unique_ptr and do some series checks . is it better to use getwallet to determine if the wallet is already loaded before calling loadwalletinternal ?",2
bitcoin/bitcoin,"reproducible test failures i have been having issues performing tests on prs , so i have tried it multiple times now on master , deleting the directory and starting fresh . this is what i get each time : ` make ` > . <repeated> > . <repeated> > ar minisketch / libminisketch_clmul . a > cxxld test / test_bitcoin > collect2 : fatal error : ld terminated with signal <number> [ killed ] > compilation terminated . > make [ <number> <sad> * * * [ makefile : <number> : test / test_bitcoin ] error <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make [ <number> <sad> * * * [ makefile : <number> : all - recursive ] error <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make : * * * [ makefile : <number> : all - recursive ] error <number> ` make ` a second time , successful > . <repeated> > . <repeated> > cxx util / libbitcoinconsensus_la - strencodings . lo > cxxld libbitcoinconsensus . la > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > making all in doc / man > make [ <number> <sad> entering directory ' / home / user / bitcoin / doc / man ' > make [ <number> <sad> nothing to be done for ' all ' . > make [ <number> <sad> leaving directory ' / home / user / bitcoin / doc / man ' > make [ <number> <sad> entering directory ' / home / user / bitcoin ' > make [ <number> <sad> nothing to be done for ' all - am ' . > make [ <number> <sad> leaving directory ' / home / user / bitcoin ' user <user> : ~ / bitcoin $ ` make check ` > making check in src > make [ <number> <sad> entering directory ' / home / user / bitcoin / src ' > make [ <number> <sad> entering directory ' / home / user / bitcoin / src ' > make [ <number> <sad> entering directory ' / home / user / bitcoin ' > make [ <number> <sad> leaving directory ' / home / user / bitcoin ' > make check - tests check - local > make [ <number> <sad> entering directory ' / home / user / bitcoin / src ' > make [ <number> <sad> entering directory ' / home / user / bitcoin / src ' > pass : minisketch / test > pass : univalue / test / object > pass : univalue / test / unitester > pass : univalue / test / no_nul > make [ <number> <sad> entering directory ' / home / user / bitcoin ' > make [ <number> <sad> leaving directory ' / home / user / bitcoin ' > pass : qt / test / test_bitcoin - qt > testsuite summary for bitcoin core <number> . <number> > ' # total : <number> > ' # pass : <number> > ' # skip : <number> > ' # xfail : <number> > ' # fail : <number> > ' # xpass : <number> > ' # error : <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > running tests : addrman_tests from test / addrman_tests . cpp > / bin / bash : line <number> : test / test_bitcoin : permission denied > make [ <number> <sad> * * * [ makefile : <number> : test / addrman_tests . cpp . test ] error <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make [ <number> <sad> * * * [ makefile : <number> : check - am ] error <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make [ <number> <sad> * * * [ makefile : <number> : check - recursive ] error <number> > make [ <number> <sad> leaving directory ' / home / user / bitcoin / src ' > make : * * * [ makefile : <number> : check - recursive ] error <number> errors also . user <user> : ~ / bitcoin $ ` sudo make install ` ( no issues ) . user <user> : ~ / bitcoin / test / functional $ ` python3 test_runner . py - - extended ` > feature_block . py | ✖ failed | <number> s > feature_dbcrash . py | ✖ failed | <number> s > feature_pruning . py | ✖ failed | <number> s details for each failed test : > <number> / <number> - feature_block . py failed , duration : <number> s > stdout : > <number> - <number> - 1 6 t <time> . 9 9 6 0 0 0 z testframework ( info ) : initializing test directory / tmp / test_runner_ ₿ _ 🏃 _20220615_202813 / feature_block_235 > <number> - <number> - 1 6 t <time> . 1 5 4 0 0 0 z testframework ( info ) : reject block with invalid tx : outputmissing > <number> - <number> - 1 6 t <time> . 3 8 0 0 0 0 z testframework ( info ) : reject block with invalid tx : inputmissing > <number> - <number> - 1 6 t <time> . 5 8 7 0 0 0 z testframework ( info ) : reject block with invalid tx : badinputoutpointindex > <number> - <number> - 1 6 t <time> . 8 0 0 0 0 0 z testframework ( info ) : reject block with invalid tx : duplicateinput > <number> - <number> - 1 6 t <time> . 0 2 5 0 0 0 z testframework ( info ) : reject block with invalid tx : prevoutnullinput > <number> - <number> - 1 6 t <time> . 1 9 5 0 0 0 z testframework ( info ) : reject block with invalid tx : nonexistentinput > <number> - <number> - 1 6 t <time> . 4 1 1 0 0 0 z testframework ( info ) : reject block with invalid tx : spendtoomuch > <number> - <number> - 1 6 t <time> . 5 9 3 0 0 0 z testframework ( info ) : reject block with invalid tx : createnegative > <number> - <number> - 1 6 t <time> . 7 6 9 0 0 0 z testframework ( info ) : reject block with invalid tx : createtoolarge > <number> - <number> - 1 6 t <time> . 9 8 0 0 0 0 z testframework ( info ) : reject block with invalid tx : createsumtoolarge > <number> - <number> - 1 6 t <time> . 1 9 2 0 0 0 z testframework ( info ) : reject block with invalid tx : toomanysigops > <number> - <number> - 1 6 t <time> . 4 6 4 0 0 0 z testframework ( info ) : do not reorg to a chain of the same length > <number> - <number> - 1 6 t <time> . 5 7 1 0 0 0 z testframework ( info ) : reorg to a longer chain > <number> - <number> - 1 6 t <time> . 7 2 4 0 0 0 z testframework ( info ) : reorg back to the original chain > <number> - <number> - 1 6 t <time> . 1 0 9 0 0 0 z testframework ( info ) : reject a chain with a double spend , even if it is longer > <number> - <number> - 1 6 t <time> . 4 5 4 0 0 0 z testframework ( info ) : reject a block where the miner creates too much coinbase reward > <number> - <number> - 1 6 t <time> . 6 6 5 0 0 0 z testframework ( info ) : reject a chain where the miner creates too much coinbase reward , even if the chain is longer > <number> - <number> - 1 6 t <time> . 0 2 3 0 0 0 z testframework ( info ) : reject a chain where the miner creates too much coinbase reward , even if the chain is longer ( on a forked chain ) > <number> - <number> - 1 6 t <time> . 3 1 4 0 0 0 z testframework ( info ) : accept a block with lots of checksigs > <number> - <number> - 1 6 t <time> . 4 4 2 0 0 0 z testframework ( info ) : reject a block with too many checksigs > <number> - <number> - 1 6 t <time> . 6 9 8 0 0 0 z testframework ( info ) : reject a block with a spend from a re - org ' ed out tx > <number> - <number> - 1 6 t <time> . 9 0 8 0 0 0 z testframework ( info ) : reject a block with a spend from a re - org ' ed out tx ( on a forked chain ) > <number> - <number> - 1 6 t <time> . 2 2 7 0 0 0 z testframework ( info ) : reject a block spending an immature coinbase . > <number> - <number> - 1 6 t <time> . 4 4 0 0 0 0 z testframework ( info ) : reject a block spending an immature coinbase ( on a forked chain ) > <number> - <number> - 1 6 t <time> . 7 1 5 0 0 0 z testframework ( info ) : accept a block of weight max_block_weight > <number> - <number> - 1 6 t <time> . 9 8 3 0 0 0 z testframework ( info ) : reject a block of weight max_block_weight + <number> > <number> - <number> - 1 6 t <time> . 3 7 8 0 0 0 z testframework ( info ) : reject a block with coinbase input script size out of range > <number> - <number> - 1 6 t <time> . 1 5 0 0 0 0 z testframework ( info ) : accept a block with the max number of op_checkmultisig sigops > <number> - <number> - 1 6 t <time> . 2 7 7 0 0 0 z testframework ( info ) : reject a block with too many op_checkmultisig sigops > <number> - <number> - 1 6 t <time> . 4 4 7 0 0 0 z testframework ( info ) : accept a block with the max number of op_checkmultisigverify sigops > <number> - <number> - 1 6 t <time> . 5 0 9 0 0 0 z testframework ( info ) : reject a block with too many op_checkmultisigverify sigops > <number> - <number> - 1 6 t <time> . 7 4 1 0 0 0 z testframework ( info ) : accept a block with the max number of op_checksigverify sigops > <number> - <number> - 1 6 t <time> . 8 1 8 0 0 0 z testframework ( info ) : reject a block with too many op_checksigverify sigops > <number> - <number> - 1 6 t <time> . 0 6 9 0 0 0 z testframework ( info ) : reject a block spending transaction from a block which failed to connect > <number> - <number> - 1 6 t <time> . 4 3 4 0 0 0 z testframework ( info ) : check p2sh sigops are correctly counted > <number> - <number> - 1 6 t <time> . 3 3 0 0 0 0 z testframework ( info ) : reject a block with too many p2sh sigops > <number> - <number> - 1 6 t <time> . 6 8 7 0 0 0 z testframework ( info ) : accept a block with the max number of p2sh sigops > <number> - <number> - 1 6 t <time> . 1 6 1 0 0 0 z testframework ( info ) : build block <number> manually > <number> - <number> - 1 6 t <time> . 1 8 2 0 0 0 z testframework ( info ) : reject a block with a non - coinbase as the first tx > <number> - <number> - 1 6 t <time> . 3 9 3 0 0 0 z testframework ( info ) : reject a block with no transactions > <number> - <number> - 1 6 t <time> . 6 0 5 0 0 0 z testframework ( info ) : reject a block with invalid work > <number> - <number> - 1 6 t <time> . 8 3 5 0 0 0 z testframework ( info ) : reject a block with a timestamp > <number> hours in the future > <number> - <number> - 1 6 t <time> . 8 9 7 0 0 0 z testframework ( info ) : reject a block with invalid merkle hash > <number> - <number> - 1 6 t <time> . 0 5 9 0 0 0 z testframework ( info ) : reject a block with incorrect pow limit > <number> - <number> - 1 6 t <time> . 2 7 1 0 0 0 z testframework ( info ) : reject a block with two coinbase transactions > <number> - <number> - 1 6 t <time> . 4 7 8 0 0 0 z testframework ( info ) : reject a block with duplicate transactions > <number> - <number> - 1 6 t <time> . 8 0 0 0 0 0 z testframework ( info ) : reject a block with timestamp before mediantimepast > <number> - <number> - 1 6 t <time> . 0 2 0 0 0 0 z testframework ( info ) : accept a previously rejected future block at a later time > <number> - <number> - 1 6 t <time> . 1 5 0 0 0 0 z testframework ( info ) : reject a block with a duplicate transaction in the merkle tree ( but with a valid merkle root ) > <number> - <number> - 1 6 t <time> . 3 7 1 0 0 0 z testframework ( info ) : reject a block with two duplicate transactions in the merkle tree ( but with a valid merkle root ) > <number> - <number> - 1 6 t <time> . 7 9 7 0 0 0 z testframework ( info ) : reject a block with a transaction with prevout . n out of range > <number> - <number> - 1 6 t <time> . 0 1 1 0 0 0 z testframework ( info ) : reject a block with a transaction with outputs > inputs > <number> - <number> - 1 6 t <time> . 3 2 5 0 0 0 z testframework ( info ) : reject a block with a transaction with a duplicate hash of a previous transaction ( bip30 ) > <number> - <number> - 1 6 t <time> . 6 5 6 0 0 0 z testframework ( info ) : reject a block with a transaction with a nonfinal locktime > <number> - <number> - 1 6 t <time> . 8 6 5 0 0 0 z testframework ( info ) : reject a block with a coinbase transaction with a nonfinal locktime > <number> - <number> - 1 6 t <time> . 0 7 3 0 0 0 z testframework ( info ) : accept a valid block even if a bloated version of the block has previously been sent > <number> - <number> - 1 6 t <time> . 4 5 0 0 0 0 z testframework ( info ) : accept a block with a transaction spending an output created in the same block > <number> - <number> - 1 6 t <time> . 5 5 8 0 0 0 z testframework ( info ) : reject a block with a transaction spending an output created later in the same block > <number> - <number> - 1 6 t <time> . 7 7 3 0 0 0 z testframework ( info ) : reject a block with a transaction double spending a transaction created in the same block > <number> - <number> - 1 6 t <time> . 9 9 1 0 0 0 z testframework ( info ) : reject a block trying to claim too much subsidy in the coinbase transaction > <number> - <number> - 1 6 t <time> . 2 0 4 0 0 0 z testframework ( info ) : accept a block claiming the correct subsidy in the coinbase transaction > <number> - <number> - 1 6 t <time> . 3 1 0 0 0 0 z testframework ( info ) : reject a block containing a transaction spending from a non - existent input > <number> - <number> - 1 6 t <time> . 5 2 3 0 0 0 z testframework ( info ) : reject a block containing a duplicate transaction but with the same merkle root ( merkle tree malleability > <number> - <number> - 1 6 t <time> . 8 5 7 0 0 0 z testframework ( info ) : reject a block containing too many sigops after a large script element > <number> - <number> - 1 6 t <time> . 2 4 8 0 0 0 z testframework ( info ) : check sigops are counted correctly after an invalid script element > <number> - <number> - 1 6 t <time> . 7 3 2 0 0 0 z testframework ( info ) : test transaction resurrection during a re - org > <number> - <number> - 1 6 t <time> . 3 3 2 0 0 0 z testframework ( info ) : accept a block with invalid opcodes in dead execution paths > <number> - <number> - 1 6 t <time> . 4 5 0 0 0 0 z testframework ( info ) : test re - orging blocks with op_return in them > <number> - <number> - 1 6 t <time> . 1 7 7 0 0 0 z testframework ( info ) : test a re - org of one week ' s worth of blocks ( <number> blocks ) > > > stderr : > <number> / <number> - feature_pruning . py failed , duration : <number> s > stdout : > <number> - <number> - 1 6 t <time> . 1 9 1 0 0 0 z testframework ( info ) : initializing test directory / tmp / test_runner_ ₿ _ 🏃 _20220615_202813 / feature_pruning_244 > <number> - <number> - 1 6 t <time> . 4 6 0 0 0 0 z testframework ( info ) : warning this test requires 4 gb of disk space > <number> - <number> - 1 6 t <time> . 5 0 2 0 0 0 z testframework ( info ) : mining a big blockchain of <number> blocks > <number> - <number> - 1 6 t <time> . 4 8 3 0 0 0 z testframework ( info ) : check that we have not started pruning yet because we are below pruneafterheight > <number> - <number> - 1 6 t <time> . 6 0 5 0 0 0 z testframework ( info ) : success > <number> - <number> - 1 6 t <time> . 6 1 7 0 0 0 z testframework ( info ) : though we are already using more than 5 5 0 mib , current usage : <number> > <number> - <number> - 1 6 t <time> . 6 1 7 0 0 0 z testframework ( info ) : mining <number> more blocks should cause the first block file to be pruned > <number> - <number> - 1 6 t <time> . 5 8 8 0 0 0 z testframework ( info ) : success > <number> - <number> - 1 6 t <time> . 5 8 9 0 0 0 z testframework ( info ) : usage should be below target : <number> > <number> - <number> - 1 6 t <time> . 5 9 2 0 0 0 z testframework ( info ) : check that we will exceed disk space target if we have a very high stale block rate > <number> - <number> - 1 6 t <time> . 5 9 7 0 0 0 z testframework ( info ) : mine <number> ( stale ) blocks on node <number> , followed by <number> ( main chain ) block reorg from node <number> , for <number> rounds > <number> - <number> - 1 6 t <time> . 7 8 0 0 0 0 z testframework ( info ) : usage can be over target because of high stale rate : <number> > <number> - <number> - 1 6 t <time> . 8 4 5 0 0 0 z testframework ( info ) : check that we can survive a <number> block reorg still > <number> - <number> - 1 6 t <time> . 8 4 9 0 0 0 z testframework ( info ) : current block height : <number> > <number> - <number> - 1 6 t <time> . 8 5 0 0 0 0 z testframework ( info ) : invalidating block 4 4 af5766f16112c8b04c19385700828e5ac236f42808804330eaf46e1312f4fb at height <number> > <number> - <number> - 1 6 t <time> . 7 8 5 0 0 0 z testframework ( info ) : new best height : <number> > <number> - <number> - 1 6 t <time> . 9 0 1 0 0 0 z testframework ( info ) : generating new longer chain of <number> more blocks > <number> - <number> - 1 6 t <time> . 9 0 0 0 0 0 z testframework ( info ) : reconnect nodes > <number> - <number> - 1 6 t <time> . 2 3 1 0 0 0 z testframework ( error ) : jsonrpc error > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _get_response > http_response = self . __conn . getresponse ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in getresponse > response . begin ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in begin > version , status , reason = self . _read_status ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _read_status > line = str ( self . fp . readline ( _maxline + <number> ) , "" iso - <number> - <number> "" ) > file "" / usr / lib / python3 . <number> / socket . py "" , line <number> , in readinto > return self . _sock . recv_into ( b ) > socket . timeout : timed out > > during handling of the above exception , another exception occurred : > > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main > self . run_test ( ) > file "" / home / user / bitcoin / test / functional / feature_pruning . py "" , line <number> , in run_test > self . reorg_test ( ) # ( <number> , ) > file "" / home / user / bitcoin / test / functional / feature_pruning . py "" , line <number> , in reorg_test > self . sync_blocks ( self . nodes [ <number> : <number> ] , timeout = <number> ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in sync_blocks > best_hash = [x . getbestblockhash ( ) for x in rpc_connections ] > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in <listcomp> > best_hash = [x . getbestblockhash ( ) for x in rpc_connections ] > file "" / home / user / bitcoin / test / functional / test_framework / coverage . py "" , line <number> , in __call__ > return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ > response , status = self . _request ( ' post ' , self . __url . path , postdata . encode ( ' utf - <number> ' ) ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _request > return self . _get_response ( ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _get_response > raise jsonrpcexception ( { > test_framework . authproxy . jsonrpcexception : ' getbestblockhash ' rpc took longer than <number> seconds . consider using larger timeout for calls that take longer to return . ( - <number> ) > <number> - <number> - 1 6 t <time> . 7 8 9 0 0 0 z testframework ( info ) : stopping nodes > <number> - <number> - 1 6 t <time> . 8 0 8 0 0 0 z testframework . node0 ( error ) : unable to stop node . > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in stop_node > self . stop ( wait = wait ) > file "" / home / user / bitcoin / test / functional / test_framework / coverage . py "" , line <number> , in __call__ > return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ > response , status = self . _request ( ' post ' , self . __url . path , postdata . encode ( ' utf - <number> ' ) ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _request > self . __conn . request ( method , path , postdata , headers ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in request > self . _send_request ( method , url , body , headers , encode_chunked ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _send_request > self . putrequest ( method , url , * * skips ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in putrequest > raise cannotsendrequest ( self . __state ) > http . client . cannotsendrequest : request - sent > [ node <number> ] cleaning up leftover process > [ node <number> ] cleaning up leftover process > [ node <number> ] cleaning up leftover process > [ node <number> ] cleaning up leftover process > > > stderr : > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / feature_pruning . py "" , line <number> , in <module> > prunetest ( ) . main ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main > exit_code = self . shutdown ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in shutdown > self . stop_nodes ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in stop_nodes > node . wait_until_stopped ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in wait_until_stopped > wait_until_helper ( self . is_node_stopped , timeout = timeout , timeout_factor = self . timeout_factor ) > file "" / home / user / bitcoin / test / functional / test_framework / util . py "" , line <number> , in wait_until_helper > if predicate ( <sad> > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in is_node_stopped > assert return_code = = <number> , self . _node_msg ( > assertionerror : [ node <number> ] node returned non - zero exit code ( - <number> ) when stopping > <number> / <number> - feature_dbcrash . py failed , duration : <number> s > stdout : > <number> - <number> - 1 6 t <time> . 2 9 5 0 0 0 z testframework ( info ) : initializing test directory / tmp / test_runner_ ₿ _ 🏃 _20220615_202813 / feature_dbcrash_243 > <number> - <number> - 1 6 t <time> . 9 5 4 0 0 0 z testframework ( info ) : prepped <number> utxo entries > <number> - <number> - 1 6 t <time> . 3 9 6 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 0 7 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 9 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 8 5 1 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 2 5 1 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 2 5 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 8 2 8 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 7 9 3 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 8 1 6 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 4 9 8 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 1 6 8 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 8 4 9 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 6 2 7 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 4 7 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 7 4 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 5 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 7 4 0 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 6 7 9 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 7 6 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 7 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 8 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 7 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 1 8 3 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 0 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 2 9 1 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 2 7 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 7 5 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 7 7 8 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 5 5 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 2 7 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 1 0 6 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 4 9 5 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 1 0 3 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 4 7 1 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 0 8 3 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 4 1 6 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 3 3 9 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 6 3 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 8 5 4 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 0 3 2 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 5 9 7 0 0 0 z testframework ( error ) : unexpected exception caught during testing > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _request > return self . _get_response ( ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _get_response > http_response = self . __conn . getresponse ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in getresponse > response . begin ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in begin > version , status , reason = self . _read_status ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _read_status > raise remotedisconnected ( "" remote end closed connection without "" > http . client . remotedisconnected : remote end closed connection without response > > during handling of the above exception , another exception occurred : > > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main > self . run_test ( ) > file "" / home / user / bitcoin / test / functional / feature_dbcrash . py "" , line <number> , in run_test > self . wallet . rescan_utxos ( ) > file "" / home / user / bitcoin / test / functional / test_framework / wallet . py "" , line <number> , in rescan_utxos > res = self . _test_node . scantxoutset ( action = "" start "" , scanobjects =[ self . get_descriptor ( ) ] ) > file "" / home / user / bitcoin / test / functional / test_framework / coverage . py "" , line <number> , in __call__ > return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ > response , status = self . _request ( ' post ' , self . __url . path , postdata . encode ( ' utf - <number> ' ) ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _request > self . __conn . request ( method , path , postdata , headers ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in request > self . _send_request ( method , url , body , headers , encode_chunked ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _send_request > self . endheaders ( body , encode_chunked = encode_chunked ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in endheaders > self . _send_output ( message_body , encode_chunked = encode_chunked ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _send_output > self . send ( msg ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in send > self . connect ( ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in connect > self . sock = self . _create_connection ( > file "" / usr / lib / python3 . <number> / socket . py "" , line <number> , in create_connection > raise err > file "" / usr / lib / python3 . <number> / socket . py "" , line <number> , in create_connection > sock . connect ( sa ) > connectionrefusederror : [ errno <number> ] connection refused > <number> - <number> - 1 6 t <time> . 6 1 8 0 0 0 z testframework ( info ) : stopping nodes > <number> - <number> - 1 6 t <time> . 7 4 3 0 0 0 z testframework . node3 ( error ) : unable to stop node . > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in stop_node > self . stop ( wait = wait ) > file "" / home / user / bitcoin / test / functional / test_framework / coverage . py "" , line <number> , in __call__ > return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ > response , status = self . _request ( ' post ' , self . __url . path , postdata . encode ( ' utf - <number> ' ) ) > file "" / home / user / bitcoin / test / functional / test_framework / authproxy . py "" , line <number> , in _request > self . __conn . request ( method , path , postdata , headers ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in request > self . _send_request ( method , url , body , headers , encode_chunked ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in _send_request > self . putrequest ( method , url , * * skips ) > file "" / usr / lib / python3 . <number> / http / client . py "" , line <number> , in putrequest > raise cannotsendrequest ( self . __state ) > http . client . cannotsendrequest : request - sent > [ node <number> ] cleaning up leftover process > > > stderr : > traceback ( most recent call last ) : > file "" / home / user / bitcoin / test / functional / feature_dbcrash . py "" , line <number> , in <module> > chainstatewritecrashtest ( ) . main ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in main > exit_code = self . shutdown ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in shutdown > self . stop_nodes ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_framework . py "" , line <number> , in stop_nodes > node . wait_until_stopped ( ) > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in wait_until_stopped > wait_until_helper ( self . is_node_stopped , timeout = timeout , timeout_factor = self . timeout_factor ) > file "" / home / user / bitcoin / test / functional / test_framework / util . py "" , line <number> , in wait_until_helper > if predicate ( <sad> > file "" / home / user / bitcoin / test / functional / test_framework / test_node . py "" , line <number> , in is_node_stopped > assert return_code = = <number> , self . _node_msg ( > assertionerror : [ node <number> ] node returned non - zero exit code ( - <number> ) when stopping try failed tests individually : user <user> : ~ / bitcoin / test / functional $ ` python3 feature_block . py ` > . <repeated> > . <repeated> > <number> - <number> - 1 6 t <time> . 4 5 7 0 0 0 z testframework ( info ) : test re - orging blocks with op_return in them > <number> - <number> - 1 6 t <time> . 2 0 8 0 0 0 z testframework ( info ) : test a re - org of one week ' s worth of blocks ( <number> blocks ) > killed i did not terminate , not sure why this was killed . user <user> : ~ / bitcoin / test / functional $ ` python3 feature_dbcrash . py ` > . <repeated> > . <repeated> > <number> - <number> - 1 6 t <time> . 3 9 9 0 0 0 z testframework ( info ) : iteration <number> , generating <number> transactions [ <number> , <number> , <number> ] > <number> - <number> - 1 6 t <time> . 1 4 8 0 0 0 z testframework ( info ) : verifying utxo hash matches for all nodes > <number> - <number> - 1 6 t <time> . 6 1 1 0 0 0 z testframework ( info ) : restarted nodes : [ <number> , <number> , <number> ]; crashes on restart : <number> > <number> - <number> - 1 6 t <time> . 7 3 5 0 0 0 z testframework ( info ) : stopping nodes > <number> - <number> - 1 6 t <time> . 0 8 2 0 0 0 z testframework ( info ) : cleaning up / tmp / bitcoin_func_test_dlvz7p5v on exit > <number> - <number> - 1 6 t <time> . 0 8 4 0 0 0 z testframework ( info ) : tests successful this succeeds when tested individually user <user> : ~ / bitcoin / test / functional $ ` python3 feature_pruning . py ` > . <repeated> > . <repeated> > <number> - <number> - 1 6 t <time> . 1 2 7 0 0 0 z testframework ( info ) : done > <number> - <number> - 1 6 t <time> . 2 0 8 0 0 0 z testframework ( info ) : stopping nodes > <number> - <number> - 1 6 t <time> . 5 8 4 0 0 0 z testframework ( info ) : cleaning up / tmp / bitcoin_func_test_l2tv3kpn on exit > <number> - <number> - 1 6 t <time> . 5 8 4 0 0 0 z testframework ( info ) : tests successful this also succeeds user <user> : ~ / bitcoin / test / util $ ` python3 test_runner . py ` successful ` bitcoin - qt ` opens as expected machine details <number> up to date debian <number> standard vm coreboot <number> x86",2
bitcoin/bitcoin,"issues with compiling bitcoin using the depends folder bitcoin core version <number> < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > hello everyone . i have ran into a make host issue while doing the steps needed to compile bitcoin on my computer . below is a screenshot of the situation and the steps detailed i used to get this program working . for another reference , i followed the steps to compile bitcoin from these links <url> the screen shot below shows everything else . thank you for your time . ! [ new bitcoin issue ] ( <url> * * expected behavior * * to compile normally to finish compiling the core on my laptop < ! - - - what behavior did you expect ? - - > * * actual behavior * * a makefile error < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > i am using bitcoin core <number> < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > i am using windows <number> as my operating system and wsl for the terminal . < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"bitcoin core will not compile on windows <number> < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! any report , issue or feature request related to the gui should be reported at <url> - - > < ! - - describe the issue - - > i have ran into several issues during my compiling of bitcoin . i have tried to compile this thing but each time i try to do so , there is a new issue . for reference i am using ubuntu bash terminal to compile bitcoin . my operating system is windows <number> and i have tried everything i know to troubleshoot this issue . i have downloaded clang , mingw g + + and i will attach the a photo to explain this issue . each time i try to use the . / configure , . / autogen . sh and sudo make , there seems to be issues and this will not compile at all . any help will be greatly appreciated . thank you ! ps . i have also created a virtual environment and i have installed the necessary tools for it as well . ! [ bitcoin error ] ( <url> < ! - - - what behavior did you expect ? - - > i expected the bitcoin core to compile . < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > the version of bitcoin core is version <number> i have forked it from the bitcoin github bitcoin page . < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > i am using an asus x555y series laptop . < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > my operating system is windows <number> . < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - > [ config . log ] ( <url>",2
bitcoin/bitcoin,"walletpassphrase gives error "" must request wallet rpc through / wallet / <filename> "" when issuing an rpc for walletpassphrase , the following error is returned : _ { \ \ "" result \ \ "" : null , \ \ "" error \ \"":{ \ \ "" code \ \"": - <number> , \ \ "" message \ \"": \ \ "" wallet file not specified ( must request wallet rpc through / wallet / <filename> uri - path ) . \ \ "" } , \ \ "" id \ \"": <number> } _ documentation from here : _ <url> states sample call looks like this : _as a json - rpc call > curl - - user myusername - - data - binary ' { "" jsonrpc "" : "" <number> "" , "" id "" : "" curltest "" , "" method "" : "" walletpassphrase "" , "" params "" : [ "" my pass phrase "" , <number> ] } ' - h ' content - type <url> ( e . g . no / wallet / <filename> in the rpc ) further , my issue stems from wallet names with embedded spaces , for example "" encrypted with space "" . qt allows for the creation of wallet names with spaces ( and other characters ) , which are not valid in a uri , at least as plain text . the cli functions as expected when issued with the rpcwallet parameter , and allows for spaces because the wallet name is enclosed in double quotes . at a minimum the docs should reflect the proper usage of the rpc call , however its unclear how support for wallet names with internal spaces would be supported at all given the current design . a better design would be to add the wallet name as a parameter in the array to avoid using different uris which is inconsistent with the usage of all other rpcs in core . btc <number>",2
bitcoin/bitcoin,multiplication result may overflow i believe there is chance of overflow due to multiplication in [ this ] ( <url> line of code .,2
bitcoin/bitcoin,wallet address issue good time i have bitcoin core wallet fully update it whenever i try to do sending using the wallet the address box alwayes become red color how this issue can be resolved pls i need help thank u so much,2
bitcoin/bitcoin,"cannot sync with limited ram and swap < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! any report , issue or feature request related to the gui should be reported at <url> - - > < ! - - describe the issue - - > while syncing headers with bitcoin - qt my device runs out of memory . i have a computer with only 1 gb of ram and no swap ; while syncing headers it runs out of memory . approximately , <number> headers is where it runs out . < ! - - - what behavior did you expect ? - - > i expected to be able to sync all the headers and then start downloading / verifying the blockchain . < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > while syncing the headers my device runs out of memory . < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > i can reproduce this issue every time . simply , start the bitcoin - qt application and wait until it runs out of memory at approximately <number> headers . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > debian package <number> - <number> < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > orange pi one <url> cpu : h3 quad - core cortex - a7 h . <number> / hevc 4 k drive microsd < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - > this issue is probably caused because all the headers are stored in ram . eventually this will become a problem with common lower end computers even if they have swap when the index becomes too large . it is an important issue to consider so that cheap nodes can be run in third world countries to ensure good decentralization of the network .",2
bitcoin/bitcoin,"ubuntu jammy can not compile fuzz tests with libc + + ? steps to reproduce on a fresh ubuntu install : ` ` ` export debian_frontend = noninteractive & & apt update & & apt install curl wget htop git vim ccache - y & & git clone <url> bitcoin - core & & cd bitcoin - core & & apt install build - essential libtool autotools - dev automake pkg - config bsdmainutils python3 - zmq libevent - dev libboost - system - dev libboost - test - dev clang llvm libc + + - dev libc + + abi - dev - y & & . / autogen . sh & & . / configure cc = clang cxx = ' clang + + - stdlib = libc + + ' - - enable - fuzz - - with - sanitizers = fuzzer & & make - j $( nproc ) ` ` ` ubuntu focal : ( passes ) ubuntu jammy : ( fails ) ` ` ` checking whether the linker accepts - fsanitize = fuzzer . <repeated> no configure : error did not accept requested flags , you are missing required libraries ` ` `",2
bitcoin/bitcoin,does the txcount obtained by getwalletinfo represent the number of transaction hashes ? the length of the array obtained by listtransaction and the value obtained by txcount are different . careful observation revealed duplicate tx_ids .,2
bitcoin/bitcoin,"importprivkey - this type of wallet does not support this command when i run importprivkey it returns unsupported wallet instead of importing . code : bitcoin - cli importprivkey "" priv_key "" "" label "" false error code : - <number> error message : * * this type of wallet does not support this command * * version bitcoin - cli - version * * bitcoin core rpc client version v22 . <number> - 9 d099b02d8f0 * * node fully synced ( full node ) chain : main blocks : <number> headers : <number> verification progress : <percent> difficulty [ image ] ( <url>",2
bitcoin/bitcoin,"cannot install bitcoin v22 . <number> on ubuntu <number> with current berkeley db version ( <number> . <number> ) < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > a fresh install of bitcoin v22 . <number> is not going through on ubuntu <number> , with an underlying fresh install of berkeley db version <number> . <number> . the same error was obtained on two completely unrelated systems . * * expected behavior * * no error on installation * * actual behavior * * the following error : ` ` ` console making all in src make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin ' make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin ' cxxld bitcoind / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_verify_callback ' : repmgr_net . c <sad> . text + 0x 2 4 7 ) : undefined reference to ` x509_store_ctx_get_current_cert ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 5 2 ) : undefined reference to ` x509_store_ctx_get_error_depth ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 5 d ) : undefined reference to ` x509_store_ctx_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 8 c ) : undefined reference to ` x509_get_issuer_name ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 9 c ) : undefined reference to ` x509_name_oneline ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 be ) : undefined reference to ` x509_get_subject_name ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 ce ) : undefined reference to ` x509_name_oneline ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 f0 ) : undefined reference to ` x509_verify_cert_error_string ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_conn_info_setup . isra . <number> ' : repmgr_net . c <sad> . text + 0x 4 4 6 ) : undefined reference to ` ssl_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_set_ssl_ctx ' : repmgr_net . c <sad> . text + 0x 1 3 8 d ) : undefined reference to ` openssl_init_ssl ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 9 9 ) : undefined reference to ` openssl_init_ssl ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 a5 ) : undefined reference to ` openssl_init_crypto ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 b1 ) : undefined reference to ` tls_method ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 b9 ) : undefined reference to ` ssl_ctx_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 d9 ) : undefined reference to ` ssl_ctx_set_verify ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 f6 ) : undefined reference to ` ssl_ctx_set_cipher_list ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 4 7 b ) : undefined reference to ` ssl_ctx_use_certificate_file ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 4 ee ) : undefined reference to ` ssl_ctx_set_default_passwd_cb_userdata ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 0 2 ) : undefined reference to ` ssl_ctx_use_privatekey_file ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 1 2 ) : undefined reference to ` ssl_ctx_check_private_key ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 3 0 ) : undefined reference to ` ssl_ctx_load_verify_locations ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 4 1 ) : undefined reference to ` ssl_ctx_set_default_verify_paths ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 8 4 ) : undefined reference to ` ssl_ctx_set_verify_depth ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 6 0 0 ) : undefined reference to ` ssl_ctx_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 7 4 d ) : undefined reference to ` ssl_ctx_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_accept ' : repmgr_net . c <sad> . text + 0x 1 8 4 2 ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 5 8 ) : undefined reference to ` ssl_set_fd ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 7 4 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 7 c ) : undefined reference to ` ssl_accept ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 cf ) : undefined reference to ` ssl_is_init_finished ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 fb ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 0 5 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 c0 ) : undefined reference to ` ssl_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 c5 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_connect ' : repmgr_net . c <sad> . text + 0x 1 c53 ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c71 ) : undefined reference to ` ssl_set_fd ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c76 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c7e ) : undefined reference to ` ssl_connect ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 cd8 ) : undefined reference to ` ssl_is_init_finished ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 cfb ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 d06 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 d5c ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 e7a ) : undefined reference to ` ssl_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_shutdown ' : repmgr_net . c <sad> . text + 0x 2 0 c4 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 cc ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 f3 ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 fb ) : undefined reference to ` ssl_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 2 1 b ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_write ' : repmgr_net . c <sad> . text + 0x 2 d72 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 d80 ) : undefined reference to ` ssl_write ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 e03 ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 e0e ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_readv ' : repmgr_net . c <sad> . text + 0x 4 e2d ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e64 ) : undefined reference to ` ssl_read ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e76 ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e82 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 eb6 ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 ffa ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 5 0 4 2 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 5 1 be ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_read_possible ' : repmgr_net . c <sad> . text + 0x 5 3 2 5 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_posix . o ) : in function ` __repmgr_conn_work ' : repmgr_posix . c <sad> . text + 0x 3 8 0 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_posix . c <sad> . text + 0x3 d7 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_sel . o ) : in function ` __repmgr_read_from_site ' : repmgr_sel . c <sad> . text + 0x3 e58 ) : undefined reference to ` ssl_pending ' collect2 : error : ld returned <number> exit status make [ <number> <sad> * * * [ makefile : <number> : bitcoind ] error <number> make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> * * * [ makefile : <number> : all - recursive ] error <number> make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin / src ' make : * * * [ makefile : <number> : all - recursive ] error <number> ` ` ` * * to reproduce * * i followed a flow inspired from [ this gist ] ( <url> <number> . install all required dependencies ` ` ` bash sudo apt install build - essential libtool autotools - dev autoconf pkg - config libssl - dev libboost - all - dev libminiupnpc - dev libzmq3 - dev libqt5gui5 libqt5core5a libqt5dbus5 qttools5 - dev qttools5 - dev - tools libprotobuf - dev protobuf - compiler libqrencode - dev ` ` ` <number> . get and install current berkeley db version ` ` ` bash cd / mnt / projects / crypto wget <url> tar - xvzf db - <date> . tar . gz cd db - <date> / build_unix mkdir / mnt / projects / crypto / berkeley - db bdb_prefix <annoyed> mnt / projects / crypto / berkeley - db . <repeated> / dist / configure - - disable - shared - - enable - cxx - - with - pic - - prefix =$ bdb_prefix mkdir . <repeated> / docs / bdb - sql . <repeated> / docs / gsg_db_server sudo make install ` ` ` <number> . get the latest bitcoin stable version and install ` ` ` bash cd / mnt / projects / crypto git clone <email> : bitcoin / bitcoin . git cd bitcoin git checkout v22 . <number> . / autogen . sh mkdir / mnt / projects / crypto / bitcoin_build . / configure cppflags = "" - i ${ bdb_prefix } / include / - o2 "" ldflags = "" - l ${ bdb_prefix } / lib / "" - - with - gui - - with - incompatible - bdb - - with - zmq - - enable - zmq - - prefix <annoyed> mnt / projects / crypto / bitcoin_build ` ` ` <number> . get the error ` ` ` console making all in src make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> entering directory ' / mnt / projects / crypto / bitcoin ' make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin ' cxxld bitcoind / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_verify_callback ' : repmgr_net . c <sad> . text + 0x 2 4 7 ) : undefined reference to ` x509_store_ctx_get_current_cert ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 5 2 ) : undefined reference to ` x509_store_ctx_get_error_depth ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 5 d ) : undefined reference to ` x509_store_ctx_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 8 c ) : undefined reference to ` x509_get_issuer_name ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 9 c ) : undefined reference to ` x509_name_oneline ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 be ) : undefined reference to ` x509_get_subject_name ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 ce ) : undefined reference to ` x509_name_oneline ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 f0 ) : undefined reference to ` x509_verify_cert_error_string ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_conn_info_setup . isra . <number> ' : repmgr_net . c <sad> . text + 0x 4 4 6 ) : undefined reference to ` ssl_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_set_ssl_ctx ' : repmgr_net . c <sad> . text + 0x 1 3 8 d ) : undefined reference to ` openssl_init_ssl ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 9 9 ) : undefined reference to ` openssl_init_ssl ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 a5 ) : undefined reference to ` openssl_init_crypto ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 b1 ) : undefined reference to ` tls_method ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 b9 ) : undefined reference to ` ssl_ctx_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 d9 ) : undefined reference to ` ssl_ctx_set_verify ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 3 f6 ) : undefined reference to ` ssl_ctx_set_cipher_list ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 4 7 b ) : undefined reference to ` ssl_ctx_use_certificate_file ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 4 ee ) : undefined reference to ` ssl_ctx_set_default_passwd_cb_userdata ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 0 2 ) : undefined reference to ` ssl_ctx_use_privatekey_file ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 1 2 ) : undefined reference to ` ssl_ctx_check_private_key ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 3 0 ) : undefined reference to ` ssl_ctx_load_verify_locations ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 4 1 ) : undefined reference to ` ssl_ctx_set_default_verify_paths ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 5 8 4 ) : undefined reference to ` ssl_ctx_set_verify_depth ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 6 0 0 ) : undefined reference to ` ssl_ctx_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 7 4 d ) : undefined reference to ` ssl_ctx_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_accept ' : repmgr_net . c <sad> . text + 0x 1 8 4 2 ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 5 8 ) : undefined reference to ` ssl_set_fd ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 7 4 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 7 c ) : undefined reference to ` ssl_accept ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 cf ) : undefined reference to ` ssl_is_init_finished ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 8 fb ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 0 5 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 c0 ) : undefined reference to ` ssl_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 9 c5 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_connect ' : repmgr_net . c <sad> . text + 0x 1 c53 ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c71 ) : undefined reference to ` ssl_set_fd ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c76 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 c7e ) : undefined reference to ` ssl_connect ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 cd8 ) : undefined reference to ` ssl_is_init_finished ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 cfb ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 d06 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 d5c ) : undefined reference to ` ssl_new ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 1 e7a ) : undefined reference to ` ssl_free ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_shutdown ' : repmgr_net . c <sad> . text + 0x 2 0 c4 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 cc ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 f3 ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 0 fb ) : undefined reference to ` ssl_free ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 2 1 b ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_write ' : repmgr_net . c <sad> . text + 0x 2 d72 ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 d80 ) : undefined reference to ` ssl_write ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 e03 ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 2 e0e ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_readv ' : repmgr_net . c <sad> . text + 0x 4 e2d ) : undefined reference to ` err_clear_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e64 ) : undefined reference to ` ssl_read ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e76 ) : undefined reference to ` err_print_errors_fp ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 e82 ) : undefined reference to ` ssl_get_error ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 eb6 ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 4 ffa ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 5 0 4 2 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_net . c <sad> . text + 0x 5 1 be ) : undefined reference to ` ssl_shutdown ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_net . o ) : in function ` __repmgr_ssl_read_possible ' : repmgr_net . c <sad> . text + 0x 5 3 2 5 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_posix . o ) : in function ` __repmgr_conn_work ' : repmgr_posix . c <sad> . text + 0x 3 8 0 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : repmgr_posix . c <sad> . text + 0x3 d7 ) : undefined reference to ` ssl_pending ' / usr / bin / ld : / mnt / projects / crypto / berkeley - db / lib / / libdb_cxx . a ( repmgr_sel . o ) : in function ` __repmgr_read_from_site ' : repmgr_sel . c <sad> . text + 0x3 e58 ) : undefined reference to ` ssl_pending ' collect2 : error : ld returned <number> exit status make [ <number> <sad> * * * [ makefile : <number> : bitcoind ] error <number> make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin / src ' make [ <number> <sad> * * * [ makefile : <number> : all - recursive ] error <number> make [ <number> <sad> leaving directory ' / mnt / projects / crypto / bitcoin / src ' make : * * * [ makefile : <number> error <number> ` ` ` < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > bitcoin core v22 . <number> from github ( cf commands above ) < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > tested on both ubuntu <number> as host system ( no vm ) and ubuntu <number> as guest system ( as vm ) < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > please note this was highly reproducible as fresh install on two different ubuntu <number> systems . < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"rpc : getnetworkinfo subversion has spurious trailing <number> reported by <user> : <url> confirmed on the <number> . 0 x branch : ` ` ` bitcoin - cli getnetworkinfo { "" version "" : <number> , "" subversion "" . <repeated> ` ` ` cc <user>",2
bitcoin/bitcoin,"' event2 / buffer . h ' file not found < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! any report , issue or feature request related to the gui should be reported at <url> - - > < ! - - describe the issue - - > * what behavior did you expect ? * building bitcoincore on macos bigsur <number> mac mini m1 <number> * what was the actual behavior ( provide screenshots if the issue is gui - related ) ? * % make host = arm - apple - darwin20 fails : . <repeated> cxx test / fuzz / fuzz - http_request . o test / fuzz / http_request . cpp : <time> : fatal error : ' event2 / buffer . h ' file not found <hashtag> include </hashtag> < event2 / buffer . h > ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> error generated . make [ <number> <sad> * * * [ test / fuzz / fuzz - http_request . o ] error <number> make [ <number> <sad> * * * [ all - recursive ] error <number> make [ all - recursive ] error <number> * how reliably can you reproduce the issue , what are the steps to do so ? * % cd depends / % make % cd - % . / autogen . sh % export boost_root <annoyed> opt / homebrew / opt / boost % . / configure - - prefix =$ pwd / depends / arm - apple - darwin20 . <number> - - with - boost =$ boost_root % make host = arm - apple - darwin20 * what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? * did a ` git pull ` today <date> * what type of machine are you observing the error on ( os / cpu and disk type ) ? * macos bigsur <number> mac mini m1 <number> * gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? * not a gui related issue . < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"a bug : the wallet passphrase entered was incorrect hi ~ i am new to bitcoin , so i just start to use bitcoin core on my ubuntu server . i first created <number> wallet , one of which is called ' testwallet ' . ` ` ` $ bitcoin - cli createwallet testwallet false false - passphrase = "" test1234567 "" false false true ` ` ` however , when i try to unlock it for 6 0 s : ` ` ` $ bitcoin - cli - rpcwallet = "" testwallet "" walletpassphrase "" test1234567 "" <number> ` ` ` error returned : ` ` ` error code : - <number> error message : error : the wallet passphrase entered was incorrect . ` ` ` it ' s a little bit confusing , because another one wallet seemed to be normal . is it because i use one more wallets ? here is the environment of my machine and bitcoin - core : ` ` ` $ uname - a linux <servername> <number> . <number> - <number> - generic # <number> smp mon <date> <time> utc <number> x86_64 x86_64 x86_64 gnu / linux $ bitcoin - cli getnetworkinfo { "" version "" : <number> , "" subversion "" : "" / satoshi : <number> . <number> / "" , "" protocolversion "" : <number> , "" localservices "" : "" <number> "" , "" localservicesnames "" : [ "" network "" , "" witness "" , "" network_limited "" ] , "" localrelay "" : true , "" timeoffset "" : <number> , "" networkactive "" : true , "" connections "" : <number> , "" connections_in "" : <number> , "" connections_out "" : <number> , "" networks "" : [ { "" name "" : "" ipv4 "" , "" limited "" : false , "" reachable "" : true , "" proxy "" : "" "" , "" proxy_randomize_credentials "" : false } , { "" name "" : "" ipv6 "" , "" limited "" : false , "" reachable "" : true , "" proxy "" : "" "" , "" proxy_randomize_credentials "" : false } , { "" name "" : "" onion "" , "" limited "" : true , "" reachable "" : false , "" proxy "" : "" "" , "" proxy_randomize_credentials "" : false } , { "" name "" : "" i2p "" , "" limited "" : true , "" reachable "" : false , "" proxy "" : "" "" , "" proxy_randomize_credentials "" : false } ] , "" relayfee "" : <number> , "" incrementalfee "" : <number> , "" localaddresses "" : [ ] , "" warnings "" } ` ` ` hope somebody could help . thanks a lot",2
bitcoin/bitcoin,"in file included from util / strencodings . cpp : <number> : <number> : fatal error : charconv : no such file . hello . i have a trouble . ` make [ <number> <sad> выход из каталога « / usr / local / src / wallets / bitcoin » cxx util / libbitcoinconsensus_la - strencodings . lo cxx bitcoind - bitcoind . o in file included from util / strencodings . cpp : <number> : <number> : . / util / strencodings . h : <time> : fatal error : charconv : нет такого файла или каталога <hashtag> include </hashtag> <charconv> ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . makefile : <number> : recipe for target ' util / libbitcoinconsensus_la - strencodings . lo ' failed make [ <number> <sad> * * * [ util / libbitcoinconsensus_la - strencodings . lo ] error <number> make [ <number> <sad> * * * ожидание завершения заданий … in file included from . / netaddress . h : <number> : <number> , from . / chainparams . h : <number> , from bitcoind . cpp : <number> : . / util / strencodings . h : <time> : fatal error : charconv такого файла или каталога <hashtag> include </hashtag> <charconv> ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . ` what i have to do ? ubuntu <number> x64 nogui",2
bitcoin/bitcoin,"how to quicken the make process after modifing the codes ? hello , i am just learning the bitcoin codes of version bitcoin - <number> . so i compiled the codes for a very long time to generate the bitcoind and so on . after a period of learning , i want to modify some codes in order to verify some ideas . by the way , i run the bitcoin on my wsl of the windows10 . however , i found the makefile period is a very long time despite i only modify one sentence codes . hence , would you please tell me how to quicken the make process after modifing the bitcoin codes ? thank you very much .",2
bitcoin/bitcoin,"3 0 0 gb of blockchain in dir , prune is set to 2 0 gb i assume its just rubbish from old client versions of bitcoin core . how to identify files that are not used any more ?",2
bitcoin/bitcoin,"is listtransactions correct ? ` ` ` ubuntu : ~ / environment $ bitcoin - cli - datadir = btcdir - testnet - rpcwallet = tomo2 listtransactions "" * "" [ { "" address "" : "" tb1qdvx8rmj8e94a43lsj5jq4ja6hu2efgps50s0kk "" , "" category "" : "" receive "" , "" amount "" : <number> , "" label "" : "" user1 "" , "" vout "" : <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 ea24c66faac775cca0a1963a183d03bd4f37cc9890f166d9a75fe90 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 9 7 8 6 cbc553d528be721a34502cfb8322ab5f269633aaab9cdc8f8df1e35f732d "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" } , { "" address "" : "" tb1qdcvdnq46qxk7yxzmvwmscnzalpltaxt7k74nle "" , "" category "" : "" receive "" , "" amount "" : <number> , "" label "" : "" user2 "" , "" vout "" : <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 ea24c66faac775cca0a1963a183d03bd4f37cc9890f166d9a75fe90 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 0 1 eff63265abbcc5647b1f06402aa50fb4be5f6c1d2f4128a3bdb637c6535e7d "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" } , { "" address "" : "" tb1ql7w62elx9ucw4pj5lgw4l028hmuw80sndtntxt "" , "" category "" : "" send "" , "" amount "" : - <number> , "" vout "" : <number> , "" fee "" : - <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 ff6e1692ed86533b03d527a183e44ad3382b429075466515582a3a6a "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" e08d12afb8aebe4571a293acaa7e0b84d0baccafb1d242cb0fc1d936f65d29f0 "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" , "" abandoned "" : false } , { "" address "" : "" tb1ql7w62elx9ucw4pj5lgw4l028hmuw80sndtntxt "" , "" category "" : "" send "" , "" amount "" : - <number> , "" vout "" : <number> , "" fee "" : - <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 ff6e1692ed86533b03d527a183e44ad3382b429075466515582a3a6a "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 6 e0c2ace9900b668c9b633e7045a9c0895bb9754c6f41c70cfde628bb3562d6a "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" , "" abandoned "" : false } , { "" address "" : "" tb1ql7w62elx9ucw4pj5lgw4l028hmuw80sndtntxt "" , "" category "" : "" send "" , "" amount "" : - <number> , "" vout "" : <number> , "" fee "" : - <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 f727825f1de5b724f33a4b4aca5ab16c9bfafb508824afa4e "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 1 e674af169748c15d5017b22a93e6e524e039e409c5f2b714d146a31ba61fcc7 "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" , "" comment "" : "" cooment = move "" , "" abandoned "" : false } , { "" address "" : "" tb1ql7w62elx9ucw4pj5lgw4l028hmuw80sndtntxt "" , "" category "" : "" send "" , "" amount "" : - <number> , "" vout "" : <number> , "" fee "" : - <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 f09fea29302386f747e2ad871dad1707396aed27823566d42 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 2 2 e365f45981c8550484be576ac8c9aa62400d9320873be057ed20404cae7f5c "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" , "" comment "" : "" cooment_to = move "" , "" abandoned "" : false } ] ubuntu : ~ / environment $ bitcoin - cli - datadir = btcdir - testnet - rpcwallet = tomo2 listtransactions "" * "" <number> <number> [ { "" address "" : "" tb1qdvx8rmj8e94a43lsj5jq4ja6hu2efgps50s0kk "" , "" category "" : "" receive "" , "" amount "" : <number> , "" label "" : "" user1 "" , "" vout "" : <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 ea24c66faac775cca0a1963a183d03bd4f37cc9890f166d9a75fe90 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 9 7 8 6 cbc553d528be721a34502cfb8322ab5f269633aaab9cdc8f8df1e35f732d "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" } , { "" address "" : "" tb1qdcvdnq46qxk7yxzmvwmscnzalpltaxt7k74nle "" , "" category "" : "" receive "" , "" amount "" : <number> , "" label "" : "" user2 "" , "" vout "" : <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 0 ea24c66faac775cca0a1963a183d03bd4f37cc9890f166d9a75fe90 "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" 0 1 eff63265abbcc5647b1f06402aa50fb4be5f6c1d2f4128a3bdb637c6535e7d "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" } , { "" address "" : "" tb1ql7w62elx9ucw4pj5lgw4l028hmuw80sndtntxt "" , "" category "" : "" send "" , "" amount "" : - <number> , "" vout "" : <number> , "" fee "" : - <number> , "" confirmations "" : <number> , "" blockhash "" : "" 0 0 0 0 0 0 0 0 ff6e1692ed86533b03d527a183e44ad3382b429075466515582a3a6a "" , "" blockheight "" : <number> , "" blockindex "" : <number> , "" blocktime "" : <phone> , "" txid "" : "" e08d12afb8aebe4571a293acaa7e0b84d0baccafb1d242cb0fc1d936f65d29f0 "" , "" walletconflicts "" : [ ] , "" time "" : <phone> , "" timereceived "" : <phone> , "" bip125 - replaceable "" : "" no "" , "" abandoned "" } ] i want to skip <number> past transactions . however , it seems that the three latest transactions have been eliminated . is this the correct behavior ? ideal > bitcoin - cli listtransactions "" * "" <number> <number> list transactions <number> to <number> realty > bitcoin - cli listtransactions "" * "" <number> <number> list transactions <number> to <number> <number> - <number> - 0 2 t <time> z bitcoin core version v0 . <number> ( release build )",2
bitcoin/bitcoin,"cannot sign a transaction with p2wsh / p2sh while using signrawtransactionwithwallet < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! any report , issue or feature request related to the gui should be reported at <url> - - > < ! - - describe the issue - - > i am creating a raw transaction using the rpc command ` createrawtransaction ` when i am signing the transaction with the rpc command ` signrawtransactionwithwallet ` - i am receiving the following error : ` ` ` response code : <number> responsemessage internal server error , response : { "" result "" : null , "" error "" <sad> "" code "" : - <number> , "" message "" : "" redeemscript / witnesscript <elongated> does not match scriptpubkey "" } , "" id "" : "" <number> "" ` ` ` when i am using ` signrawtransactionwithkey ` the transaction signed ( and sent ) correctly . bitcoin version : <number> os < ! - - - what behavior did you expect ? - - > < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"rate limits for ` ping ` rpc # # # issue what are the rate limits for [ ` ping ` ] ( <url> rpc which was added in <url> if no rate limits , do we need them ? # # # what behavior did you expect ? i was expecting some errors in bitcoind or maybe peers ban my node based on the rate limits mentioned in <url> > i added a ratelimit to rpc - requested ping , so that it will limit to <number> / second per peer . the regular automated keepalive ping is immune to this . but this comment says there is no rate limit : > got rid of the idea of rate - limiting user - requested rpc pings , that simplifies a chunk of code . <url> # # # what was the actual behavior ? no errors . peers did not ban my node . i was sending <number> - <number> ` ping ` per second # # # how reliably can you reproduce the issue , what are the steps to do so ? its easy to reproduce . everyone can use different ways to send ` ping ` . i used the below steps : <number> . run ` bitcoind ` with bitcoin . conf : ` ` ` testnet = <number> server = <number> test . rpcport = <number> rpcuser = user3 rpcpassword = password3 ` ` ` <number> . use intruder in burp suite to brute force and send ` ping ` rpc with different id in each request . request : ` ` ` post / http / <number> host : <number> . <time> <number> authorization : basic dxnlcjm6cgfzc3dvcmqz content - type : text / plain content - length : <number> { "" jsonrpc "" : "" <number> "" , "" id "" : "" $ curltest $"", "" method "" : "" ping "" , "" params "" : [ ] } ` ` ` <number> . check peers with ` - netinfo ` and ` getpeerinfo ` . keep an eye on bitcoind for errors . ` ` ` $ bitcoin - cli - netinfo bitcoin core v21 . <number> - bb4790816d84 testnet - <number> / satoshi : <number> . <number> / ipv4 ipv6 onion total block in <number> <number> <number> <number> out <number> <number> <number> <number> <number> total <number> <number> <number> <number> local addresses eadtub66sw6z4rtzf753aplvy7psthggirtgowr3rsnebbpsws2vyxyd . onion port <number> score <number> ` ` ` ` getpeerinfo ` : <url> requests sent at <date> <time> gmt ( <number> - <number> = <number> ) <details> <summary> first </summary> [ image ] ( <url> </details> <details> <summary> last </summary> ! [ image ] ( <url> </details> # # # bitcoin core version bitcoin core v21 . <number> - bb4790816d84 ( had compiled yesterday for testing pr <number> ) # # # what type of machine are you observing the error on ( os / cpu and disk type ) ? linux ( pop ! _os ) as vm related question",2
bitcoin/bitcoin,"the <number> block relay connections do not seem to be providing any value in all my time running the code where <number> block relays are allocated , i have never seen them receiving any blocks - it ' s always the "" full relay "" nodes that are receiving the blocks . perhaps i am misunderstanding the purpose of these <number> connections - but so far i am seeing no benefit in them - my understanding was that they were in some way more likely to be ready to receive a block than the "" full delay "" nodes , but this is not what happens from several weeks of observation .",2
bitcoin/bitcoin,"need help understanding parts of span . h hi , in span . h you define a strange macro <url> could someone please tell me the reasoning behind this ? it does not seem to be mentionned in the comments , maybe i overlooked it . the reason i do not understand is that it , as far as i know , does not have to change anything whatsoever in terms of code generation . static initialization can happen without constexpr . ( in fact it ' s a sneaky way to introduce bugs whenever you have a global std : : map … it ' s a lot of "" fun "" but i digress ) . then we have this <url> now i am very confused . is this checking if an incomplete type is convertible to another incomplete type ? i do not know how that template could possibly do anything else . it ' s not checking if ` t * * ` is implicitly convertible to ` c * * ` , is it ? * * i do not know what that does . * * why is it only constexpr when not in debug ? why is it only doing this , rather important looking , assertion in debug ? does it become valid to violate that assertion in prod ? why ? it seems odd and bug prone . thank you for your time .",2
bitcoin/bitcoin,"exception : n5boost10filesystem16filesystem_errore i got this message when i run bitcoind exception : n5boost10filesystem16filesystem_errore boost : : filesytem : : create_directory : file exists : "" home / sats / . bitcoin "" bitcoind : chainparambase . cpp : <number> cbasechainparams & base params ( ) assertion globalchainbaseparams failed aborted ( core dumped ) any ideas whats happening ?",2
bitcoin/bitcoin,why do not we have more checkpoints ? * * is your feature request related to a problem ? please describe . * * < - - a clear and concise description of what the problem is . ex . i am always frustrated when [ . <repeated> ] - - > ` src / chainparams . cpp : <number> ` has few checkpoints for the mainnet using a table of block and blockhash . why does it end at block <number> ? * * describe the solution you ' d like * * < ! - - a clear and concise description of what you want to happen . - - > either add a comment with an explanation for given number checkpoints or add checkpoints at an agreed block interval . * * describe alternatives you have considered * * < ! - - a clear and concise description of any alternative solutions or features you have considered . - - > * * additional context * * < ! - - add any other context or screenshots about the feature request here . - - > ! [ screen shot <number> - <number> - <number> at <number> <number> <number> am ] ( <url>,2
bitcoin/bitcoin,"failed to start bitcoind using ubuntu <number> hey i keep getting this error when i run sudo systemctl start bitcoind . service job for bitcoind . service failed because the control process exited with error code . see "" systemctl status bitcoind . service "" and "" journalctl - xe "" for details . when i input sudo journalctl - xe i get this . ░ ░ the job identifier is <number> . abr <number> <time> sats - desktop bitcoind [ <number> <sad> error : error parsing command line arguments : invalid parameter - daemonwait abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : control process exited , code = exited , status = <number> / failure ░ ░ subject : unit process exited ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ an execstart = process belonging to unit bitcoind . service has exited . ░ ░ ░ ░ the process ' exit code is ' exited ' and its exit status is <number> . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : failed with result ' exit - code ' . ░ ░ subject : unit failed ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ the unit bitcoind . service has entered the ' failed ' state with result ' exit - code ' . abr <number> <time> sats - desktop systemd [ <number> <sad> failed to start bitcoin daemon . ░ ░ subject : a start job for unit bitcoind . service has failed ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ a start job for unit bitcoind . service has finished with a failure . ░ ░ ░ ░ the job identifier is <number> and the job result is failed . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : scheduled restart job , restart counter is at <number> . ░ ░ subject : automatic restarting of a unit has been scheduled ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ automatic restarting of the unit bitcoind . service has been scheduled , as the result for ░ ░ the configured restart = setting for the unit . abr <number> <time> sats - desktop systemd [ <number> <sad> stopped bitcoin daemon . ░ ░ subject : a stop job for unit bitcoind . service has finished ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ a stop job for unit bitcoind . service has finished . ░ ░ ░ ░ the job identifier is <number> and the job result is done . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : start request repeated too quickly . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : failed with result ' exit - code ' . ░ ░ subject : unit failed ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ the unit bitcoind . service has entered the ' failed ' state with result ' exit - code ' . abr <number> <time> sats - desktop systemd [ <number> <sad> failed to start bitcoin daemon . ░ ░ subject : a start job for unit bitcoind . service has failed ░ ░ defined - by : systemd ░ ░ support : <url> ░ ░ ░ ░ a start job for unit bitcoind . service has finished with a failure . ░ ░ ░ ░ the job identifier is <number> and the job result is failed . lines <number> - <number> / <number> ( end ) when i input systemctl status bitcoind . service i get this : bitcoind . service - bitcoin daemon loaded : loaded ( / etc / systemd / system / bitcoind . service ; enabled ; vendor preset : enabled ) active : failed ( result : exit - code ) since thu <number> - <number> - <number> <time> cest ; 2 1 min ago docs : <url> process : <number> execstart <annoyed> usr / local / bin / bitcoind - daemonwait - pid <annoyed> run / bitcoind / bitcoind . pid - conf <annoyed> home / sats / . bitcoin / bitcoin . conf - datadir <annoyed> home / sats / . bitcoin ( code = exited , status = <number> / failure ) abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : scheduled restart job , restart counter is at <number> . abr <number> <time> sats - desktop systemd [ <number> <sad> stopped bitcoin daemon . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : start request repeated too quickly . abr <number> <time> sats - desktop systemd [ <number> <sad> bitcoind . service : failed with result ' exit - code ' . abr <number> <time> sats - desktop systemd [ <number> ] to start bitcoin daemon . ~ some idea how can i solve the problem ? <repeated>",2
bitcoin/bitcoin,"psbt strange behavior < - - describe the issue - - > hi , i am trying to set - up a coldcard to work air - gapped with bitcoin core ( via sd card ) . latest bitcoin core <number> . <number> and latest coldcard firmware used . * * expected behavior * * < ! - - - what behavior did you expect ? - - > i create a watch - only non - descriptor wallet and i execute the importmulti command created by coldcard ( 2 x5000 addresses ) . - i do a two simple ( 1 in <number> out ) test transactions ( main - net ) all looks good . core exports the psbt file ~ <number> bytes . coldcard signs it and creates the final txn ~ <number> bytes . all good . - i believe the exported unsigned psbt should stay small , but it ' s not . * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > nothing changed ( i mean core closed and reopened few hours later , that ' s all ) . new tx arrives in . trying to do the same spend and psbt export . exported psbt file is now ~ <number> kilobytes ( ! <repeated> ) . see attached partial file . [ psbtpartial . txt ] ( <url> takes minutes while coldcard digs through all of that and finally signs the one used and produces a ~ <number> bytes txn with the one input one output . nothing fancy is going on , no multisig used , no nothing . why does core include a lot / all addresses in a psbt ? also it ' s not doing it at the beginning . * * to reproduce * * - create a watch - only non - descriptor wallet and i execute the importmulti command created by coldcard ( 2 x5000 addresses ) importdescriptors ' [ { "" range "" : [ <number> , <number> ] , "" timestamp "" : "" now "" , "" active "" : true , "" watchonly "" : true , "" desc "" : "" pkh ( [ xx / 4 4 h / 0 h / 0 h ] xpubxx / <number> /* ) <hashtag> xx </hashtag> "" , "" internal "" : false } , { "" range "" : [ <number> , <number> ] , "" timestamp "" : "" now "" , "" active "" : true , "" watchonly "" : true , "" desc "" : "" pkh ( [ xx / 4 4 h / 0 h / 0 h ] xpubxx / <number> /* ) <hashtag> xx </hashtag> "" , "" internal "" - check you received the correct addresses from core - make two receive and send ( full ) amount transaction to / from on other wallet - now you should see the huge psbt file now . i have done this from scratch with bip44 and bip84 addresses and both suffer from the same issue . * * system information * * core v0 . <number> from download amd 2 9 2 0 x win10 x64",2
bitcoin/bitcoin,"bitcoins im jahr <number> hier angelegt komme nicht mehr drauf zu < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > * * expected behavior * * < ! - - - what behavior did you expect ? - - > * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"use of testmempoolaccept is not clear . testmempoolaccept yields unexpected error . * * expected behavior * * i would expect to get no error message when using ` testmempoolaccept ` on a raw transaction that decodes just fine using ` decoderawtransaction ` or at least a hint at what went wrong . * * actual behavior * * ` ` ` getrawtransaction ea369e4d7cbb9f2a2fd58dff4d0923112a00674faa1b35bfd6424f89c94ab653 0 2 0 0 0 0 0 0 0 0 0 1 0 1 7 5 c0e156c5c2545db4cf009a756a433677cce7d845f7626559329681379998950 . <repeated> decoderawtransaction "" 0 2 0 0 0 0 0 0 0 0 0 1 0 1 7 5 c0e156c5c2545db4cf009a756a433677cce7d845 . <repeated> "" { "" txid "" : "" ea369e4d7cbb9f2a2fd58dff4d0923112a00674faa1b35bfd6424f89c94ab653 "" , . <repeated> testmempoolaccept [ "" 0 2 0 0 0 0 0 0 0 0 0 1 0 1 7 5 c0e156c5c2545db4cf009a756a433677cce7d845f . <repeated> "" ] error : error parsing json ` ` ` * * to reproduce * * probably always reproducible with confirmed transactions ? * * system information * * bitcoin core <number> . <number> tgz , <number> . <number> snap and tgz",2
bitcoin/bitcoin,"cannot generate change address on descriptor wallets . i am trying to create an unsigned transaction in a wallet that contains a <number> of <number> public key descriptor . * * expected behavior * * to be able to send half a utxo and bitcoin core to handle the change address generation * * actual behavior * * instead , i get this error on clicking "" create unsigned "" . "" transaction needs a change address , but we can not generate it . please call keypoolrefill first . "" * * to reproduce * * * to create descriptor * <number> . create a standard wallet using bitcoin core <number> . run "" getnewaddress "" to generate an address <number> . run "" dumpprivkey "" using the address previously generated to get a wif key <number> . xor the wif key with <number> "" <number> "" s to create a unique key ( not to add security or randomness ) <number> . create a new standard wallet and run "" sethdseed "" with the modified wif key <number> . dump the second wallet to get the xpriv <number> . repeat steps <number> - <number> to generate <number> xprivs <number> . using the xprivs create the <number> of <number> descriptor example below wsh ( multi ( <number> , ' xpriv1 ' /* , ' xpriv2 ' /* , ' xpriv3 ' /* , ' xpriv4 ' /* , ' xpriv5 ' /* , ' xpriv6 ' /* , ' xpriv7 ' /* ) ) remove ' from the above string . <number> . in the first wallet run "" getdescriptorinfo "" to both generate the checksum for the xpriv descriptor and the xpub descriptor itself * to reproduce error * <number> . create a descriptor wallet with disabled private keys <number> . import the public key descriptor with "" importdescriptors "" example bellow importdescriptors ' [ { "" desc "" : "" wsh ( multi ( <number> , ' xpriv1 ' /* , ' xpriv2 ' /* , ' xpriv3 ' /* , ' xpriv4 ' /* , ' xpriv5 ' /* , ' xpriv6 ' /* , ' xpriv7 ' /* ))# ' checksum ' "" , "" timestamp "" : "" now "" , "" active "" remove ' from the above string . <number> . click "" create new receiving address "" in the bitcoin core ui <number> . send a small amount to the address <number> . in the send tab enter a recipient and then attempt to send half of the original amount sent to the address . <number> . you should get an error "" transaction needs a change address , but we can not generate it . please call keypoolrefill first . "" * * system info * * i am using ubuntu <number> with bitcoin core <number> . <number> rc3",2
bitcoin/bitcoin,"can not sync signet using <number> - rc3 i downloaded bitcoin - <number> . 0 rc3 - x86_64 - linux - gnu . tar . gz from bitcoincore . org / bin / . <repeated> and ran it with no prior . bitcoin folder . output : ` ` ` $ . / bitcoind - signet <number> - <number> - 1 2 t <time> z bitcoin core version v0 . <number> . 0 rc3 ( release build ) <number> - <number> - 1 2 t <time> z signet derived magic ( message start ) : 0 a03cf40 <number> - <number> - 1 2 t <time> z assuming ancestors of block 0 0 0 0 0 0 2 a1de0f46379358c1fd09906f7ac59adf3712323ed90eb59e4c183c020 have valid signatures . <number> - <number> - 1 2 t <time> z setting nminimumchainwork = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 9 fd16269a <number> - <number> - 1 2 t <time> z using the ' sse4 ( 1 way ) , sse41 ( 4 way ) , avx2 ( 8 way ) ' sha256 implementation <number> - <number> - 1 2 t <time> z using rdseed as additional entropy source <number> - <number> - 1 2 t <time> z using rdrand as an additional entropy source <number> - <number> - 1 2 t <time> z default data directory / home / twitch / . bitcoin <number> - <number> - 1 2 t <time> z using data directory / home / twitch / . bitcoin / signet <number> - <number> - 1 2 t <time> z config file : / home / twitch / . bitcoin / bitcoin . conf ( not found , skipping ) <number> - <number> - 1 2 t <time> z command - line arg : signet = "" "" <number> - <number> - 1 2 t <time> z using at most <number> automatic connections ( <number> file descriptors available ) <number> - <number> - 1 2 t <time> z using <number> mib out of <number> / <number> requested for signature cache , able to store <number> elements <number> - <number> - 1 2 t <time> z using <number> mib out of <number> / <number> requested for script execution cache , able to store <number> elements <number> - <number> - 1 2 t <time> z script verification uses <number> additional threads <number> - <number> - 1 2 t <time> z scheduler thread start <number> - <number> - 1 2 t <time> z http : creating work queue of depth <number> <number> - <number> - 1 2 t <time> z using random cookie authentication . <number> - <number> - 1 2 t <time> z generated rpc authentication cookie / home / twitch / . bitcoin / signet / . cookie <number> - <number> - 1 2 t <time> z http : starting <number> worker threads <number> - <number> - 1 2 t <time> z using wallet directory / home / twitch / . bitcoin / signet / wallets <number> - <number> - 1 2 t <time> z init message : verifying wallet ( s ) . <repeated> <number> - <number> - 1 2 t <time> z init message : loading banlist . <repeated> <number> - <number> - 1 2 t <time> z error : deserializefiledb : failed to open file / home / twitch / . bitcoin / signet / banlist . dat <number> - <number> - 1 2 t <time> z invalid or missing banlist . dat ; recreating <number> - <number> - 1 2 t <time> z setnetworkactive : true <number> - <number> - 1 2 t <time> z using / <number> prefix for ip bucketing <number> - <number> - 1 2 t <time> z cache configuration : <number> - <number> - 1 2 t <time> z * using <number> mib for block index database <number> - <number> - 1 2 t <time> z * using <number> mib for chain state database <number> - <number> - 1 2 t <time> z * using <number> mib for in - memory utxo set ( plus up to <number> mib of unused mempool space ) <number> - <number> - 1 2 t <time> z init message : loading block index . <repeated> <number> - <number> - 1 2 t <time> z switching active chainstate to chainstate [ ibd ] @ height - <number> ( null ) <number> - <number> - 1 2 t <time> z opening leveldb in / home / twitch / . bitcoin / signet / blocks / index <number> - <number> - 1 2 t <time> z opened leveldb successfully <number> - <number> - 1 2 t <time> z using obfuscation key for / home / twitch / . bitcoin / signet / blocks / index : <number> <number> - <number> - 1 2 t <time> z loadblockindexdb : last block file = <number> <number> - <number> - 1 2 t <time> z loadblockindexdb : last block file info : cblockfileinfo ( blocks = <number> , size = <number> , heights = <number> . <repeated> <number> , time = <number> - <number> - <number> . <repeated> <number> - <number> - <number> ) <number> - <number> - 1 2 t <time> z checking all blk files are present . <repeated> <number> - <number> - 1 2 t <time> z initializing databases . <repeated> <number> - <number> - 1 2 t <time> z pre - allocating up to position 0x 1 0 0 0 0 0 0 in blk00000 . dat <number> - <number> - 1 2 t <time> z opening leveldb in / home / twitch / . bitcoin / signet / chainstate <number> - <number> - 1 2 t <time> z opened leveldb successfully <number> - <number> - 1 2 t <time> z wrote new obfuscate key for / home / twitch / . bitcoin / signet / chainstate : 1 7 0 0 0 5 1 1 1 e9356f3 <number> - <number> - 1 2 t <time> z using obfuscation key for / home / twitch / . bitcoin / signet / chainstate : 1 7 0 0 0 5 1 1 1 e9356f3 <number> - <number> - 1 2 t <time> z init message : rewinding blocks . <repeated> <number> - <number> - 1 2 t <time> z block index 3 4 ms <number> - <number> - 1 2 t <time> z loadblk thread start <number> - <number> - 1 2 t <time> z updatetip : new best = 0 0 0 0 0 0 0 8 8 1 9 8 7 3 e925422c1ff0f99f7cc9bbb232af63a077a480a3633bee1ef6 height = <number> version =0 x00000001 log2_work = <number> tx = <number> date = ' <number> - <number> - 0 1 t <time> z ' progress = <number> cache = <number> . 0 mib ( 0 txo ) <number> - <number> - 1 2 t <time> z block tree size = <number> <number> - <number> - 1 2 t <time> z nbestheight = <number> <number> - <number> - 1 2 t <time> z failed to open mempool file from disk . continuing anyway . <number> - <number> - 1 2 t <time> z loadblk thread exit <number> - <number> - 1 2 t <time> z torcontrol thread start <number> - <number> - 1 2 t <time> z bound to <happy> : <sad> <number> <number> - <number> - 1 2 t <time> z bound to <number> . <number> . <time> <number> <number> - <number> - 1 2 t <time> z bound to <number> . <number> . <time> <number> <number> - <number> - 1 2 t <time> z init message : loading p2p addresses . <repeated> <number> - <number> - 1 2 t <time> z error : deserializefiledb : failed to open file / home / twitch / . bitcoin / signet / peers . dat <number> - <number> - 1 2 t <time> z invalid or missing peers . dat ; recreating <number> - <number> - 1 2 t <time> z error : deserializefiledb : failed to open file / home / twitch / . bitcoin / signet / anchors . dat <number> - <number> - 1 2 t <time> z <number> block - relay - only anchors will be tried for connections . <number> - <number> - 1 2 t <time> z init message : starting network threads . <repeated> <number> - <number> - 1 2 t <time> z dnsseed thread start <number> - <number> - 1 2 t <time> z loading addresses from dns seed 2 a01 : 7 c8 : d0 <time> <number> : : <number> <number> - <number> - 1 2 t <time> z net thread start <number> - <number> - 1 2 t <time> z init message : done loading <number> - <number> - 1 2 t <time> z opencon thread start <number> - <number> - 1 2 t <time> z addcon thread start <number> - <number> - 1 2 t <time> z msghand thread start <number> - <number> - 1 2 t <time> z loading addresses from dns seed <number> . <phone> - <number> - 1 2 t <time> z loading addresses from dns seed ntv3mtqw5wt63red . onion : <number> <number> - <number> - 1 2 t <time> z <number> addresses found from dns seeds <number> - <number> - 1 2 t <time> z dnsseed thread exit <number> - <number> - 1 2 t <time> z new outbound peer connected : version : <number> , blocks = <number> , peer = <number> ( full - relay ) <number> - <number> - 1 2 t <time> z cannot create socket for ntv3mtqw5wt63red . onion : <number> network <number> - <number> - 1 2 t <time> z adding fixed seed nodes as dns does not seem to be available . ` ` ` if i switch back to rc2 it works fine . i expect rc3 to sync , but it does not .",2
bitcoin/bitcoin,generatetoaddress blocks do not include tx ' es from mempool i mined in offline mode > <number> test blocks . then i try to send some test coins on other wallet . coins stay unconfirmed even after <number> new blocks made by generatetoaddress command in console . * * expected behavior * * generated blocks by command generatetoaddress include tx ' es from mempool . * * actual behavior * * all new blocks made by generatetoaddress command have only one tx . it do not use mempool . * * to reproduce * * <number> . run new installation of bitcoin core in offline mode <number> . generate > <number> blocks by command in console : ` ` ` generatetoaddress <number> getnewaddress ( ) - <number> ` ` ` <number> . create new wallet and send some coins on it <number> . generate more blocks by command in console <number> getnewaddress ( ) - <number> ` ` ` * * system information * * v0 . <number> . <number>,2
bitcoin/bitcoin,"compile_commands . json generated error with ccls under vim mode < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > $ . / autogen . sh $ . / configure - without - gui $ compliedb - n make when i use vim to open the project , the are some warning information display as follow : ! [ wx20201111 - <number> ] ( <url> which are all about c + + standard library . does it have relationship to makefile which was auto generated by autopen . sh ? * * expected behavior * * no error display < ! - - - what behavior did you expect ? - - > * * actual behavior * * < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * bitcoin version 2 1 8 fe60d91a9190aa0ee561479044df368214766 < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"q : cs_main protection for referencing m_blockman / g_chainman ? while making my [ de - globalize chainstatemanager ] ( <url> changes , i noticed something that i wasn ' t sure what to make of : both the global instance of ` chainstatemanager ` ` g_chainman ` and the ` blockmanager ` member of ` g_chainman ` are protected by ` : : cs_main ` . this means that changes like the following where i am simply passing a ` blockmanager ` to a function have to add an awkward ` with_lock ` in order to appease thread safety analysis . examples [ ` d227dfe ` (# <number> ) ] ( <url> <number> . [ ` f8e91ed ` (# <number> ) ] ( <url> however , i am not entirely sure if these ` : : cs_main ` protections should be required for referencing these objects themselves , especially since ` chainstatemanager ` and ` blockmanagers ` ' s member variables / functions are already well annotated w / re locks . any insights would be useful <happy>",2
bitcoin/bitcoin,"test failure in alpine linux - ' ensure adding witness outputs with uncompressed pubkeys fails ' < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > * * expected behavior * * pass the test * * actual behavior * * failed the test * * to reproduce * * run ` make check ` : ` ` ` ' creates a new transaction with a single <number> - of - <number> multisig in a p2wsh output ' , ' creates a new transaction with a single <number> - of - <number> multisig in a p2wsh output ( output in json ) ' , ' creates a new transaction with a single <number> - of - <number> multisig in a p2wsh output , wrapped in p2sh ' , ' creates a new transaction with a single <number> - of - <number> multisig in a p2wsh output , wrapped in p2sh ( output in json ) ' , ' uncompressed pubkeys should work just fine for non - witness outputs ' , ' ensure adding witness outputs with uncompressed pubkeys fails ' ] make [ <number> <sad> * * * [ makefile : <number> : check - local ] error <number> make [ <number> <sad> leaving directory ' / home / stuart / aports / community / bitcoin / src / bitcoin - <number> . <number> / src ' make [ <number> <sad> * * * [ makefile : <number> : check - am ] error <number> make [ <number> <sad> leaving directory ' / home / stuart / aports / community / bitcoin / src / bitcoin - <number> . <number> / src ' make [ <number> <sad> * * * [ makefile : <number> : check - recursive ] error <number> make [ <number> <sad> leaving directory ' / home / stuart / aports / community / bitcoin / src / bitcoin - <number> . <number> / src ' make : * * * [ makefile : <number> : check - recursive ] error <number> > > > error : bitcoin failed ` ` ` * * system information * * - bitcoin ` <number> . <number> ` - alpine linux edge ( ` <number> . <number> ` currently ) lxc container",2
bitcoin/bitcoin,"bitcoin - cli does not include wallet functionalities when compiling dependencies # # behavior when compiling bitcoin on linux 6 4 bit ( ubuntu <number> . <number> lts ) : ` ` ` git clone <url> git checkout v0 . <number> . / autogen . sh cd depends make no_qt = <number> cd . <repeated> . / configure - - prefix =$ pwd / depends / x86_64 - pc - linux - gnu - - with - gui = no - - disable - tests - - disable - gui - tests make & & make install . / depends / x86_64 - pc - linux - gnu / bin / bitcoin - cli getnewaddress ` ` ` expected behaviour : shows ` getnewaddress ` help actual behaviour : ` ` ` error code : - <number> error message : method not found ` ` ` none of the wallet commands are available . i understand that the wallet feature should enabled by default . did i mess up a step ? <details> <summary> . / config . log </summary> ` ` ` this file contains any messages produced by compilers while running configure , to aid debugging if configure makes a mistake . it was created by bitcoin core configure <number> . <number> , which was generated by gnu autoconf <number> . invocation command line was $ . / configure - - prefix <annoyed> home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu - - with - gui = no - - disable - tests - - disable - gui - tests - - enable - wallet # # - - - - - - - - - # # # # platform . # # # # - - - - - - - - - # # hostname = nuc uname - m = x86_64 uname - r = <number> . <number> - <number> - generic uname - s = linux uname - v = # <number> - ubuntu smp fri <date> <time> utc <number> / usr / bin / uname - p = unknown / bin / uname - x = unknown / bin / arch = unknown / usr / bin / arch - k = unknown / usr / convex / getsysinfo = unknown / usr / bin / hostinfo = unknown / bin / machine = unknown / usr / bin / oslevel = unknown / bin / universe = unknown path : / home / dante / bin path : / var / lib / gems / <number> / bin path : / home / dante / . cargo / bin path : / home / dante / bin path : / home / dante / bin path : / var / lib / gems / <number> / bin path : / usr / local / sbin path : / usr / local / bin path : / usr / sbin path : / usr / bin path : / sbin path : / bin path : / usr / games path : / usr / local / games path : / snap / bin path : / sbin path : / home / dante / . gem / ruby / <number> / bin path : / sbin path : / usr / local / scripts path : / usr / local / bin path : / home / dante / go / bin path : / sbin path : / home / dante / . gem / ruby / <number> / bin path : / sbin path : / usr / local / scripts path : / usr / local / bin # # - - - - - - - - - - - # # # # core tests . # # # # - - - - - - - - - - - # # configure : <number> : loading site script / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / config . site | depends_prefix = "" ` dirname ${ ac_site_file } ` / . <repeated> "" | | cross_compiling = maybe | host_alias =x 8 6 _64 - pc - linux - gnu | ac_tool_prefix =${ host_alias } - | | if test - z $ with_boost ; then | with_boost =$ depends_prefix | fi | if test - z $ with_qt_plugindir ; then | with_qt_plugindir =$ depends_prefix / plugins | fi | if test - z $ with_qt_translationdir ; then | with_qt_translationdir =$ depends_prefix / translations | fi | if test - z $ with_qt_bindir & & test - z "" <number> "" ; then | with_qt_bindir =$ depends_prefix / native / bin | fi | | if test - z $ with_qrencode & & test - n "" "" ; then | with_qrencode = no | fi | | if test - z $ enable_wallet & & test - n "" "" ; then | enable_wallet = no | fi | | if test - z $ with_miniupnpc & & test - n "" "" ; then | with_miniupnpc = no | fi | | if test - z $ with_gui & & test - n "" <number> "" ; then | with_gui = no | fi | | if test - z $ enable_zmq & & test - n "" "" ; then | enable_zmq = no | fi | | if test xlinux = xdarwin ; then | brew = no | port = no | fi | | if test xlinux = xmingw32 ; then | if test - z $ with_qt_incdir ; then | with_qt_incdir =$ depends_prefix / include | fi | if test - z $ with_qt_libdir ; then | with_qt_libdir =$ depends_prefix / lib | fi | fi | | path =$ depends_prefix / native / bin :$ path | pkg_config = "" ` which pkg - config ` - - static "" | | # these two need to remain exported because pkg - config does not see them | # otherwise . that means they must be unexported at the end of configure . ac to | # avoid ruining the cache . sigh . | export pkg_config_path =$ depends_prefix / share / pkgconfig :$ depends_prefix / lib / pkgconfig | if test - z "" "" ; then | export pkg_config_libdir =$ depends_prefix / lib / pkgconfig | fi | | cppflags = "" - i $ depends_prefix / include / $ cppflags "" | ldflags = "" - l $ depends_prefix / lib $ ldflags "" | | if test - n "" gcc - m64 "" - a - z "" ${ cc } "" ; then | cc = "" gcc - m64 "" | fi | if test - n "" g + + - m64 "" - a - z "" ${ cxx } "" ; then | cxx = "" g + + - m64 "" | fi | pythonpath =$ depends_prefix / native / lib / python3 / dist - packages :$ pythonpath | | if test - n "" ar "" ; then | ar = ar | ac_cv_path_ac_pt_ar =${ ar } | fi | | if test - n "" ranlib "" ; then | ranlib = ranlib | ac_cv_path_ac_pt_ranlib =${ ranlib } | fi | | if test - n "" nm "" ; then | nm = nm | ac_cv_path_ac_pt_nm =${ nm } | fi | | if test - n "" "" ; then | enable_reduce_exports = no | fi | | if test - n "" - pipe - o2 "" ; then | cflags = "" - pipe - o2 $ cflags "" | fi | if test - n "" - pipe - o2 "" ; then | cxxflags = "" - pipe - o2 $ cxxflags "" | fi | if test - n "" "" ; then | cppflags = "" $ cppflags "" | fi | if test - n "" "" ; then | ldflags = "" $ ldflags "" | fi configure : <number> : checking build system type configure : <number> : result : x86_64 - pc - linux - gnu configure : <number> : checking host system type configure : <number> : result : x86_64 - pc - linux - gnu configure : <number> : checking for a bsd - compatible install configure : <number> : result : / usr / bin / install - c configure : <number> : checking whether build environment is sane configure : <number> : result : yes configure : <number> : checking for x86_64 - pc - linux - gnu - strip configure : <number> : result : no configure : <number> : checking for strip configure : <number> : found / usr / bin / strip configure : <number> : result : strip configure : <number> : checking for a thread - safe mkdir - p configure : <number> : result : / bin / mkdir - p configure : <number> : checking for gawk configure : <number> : found / usr / bin / gawk configure : <number> : result : gawk configure : <number> : checking whether make sets $( make ) configure : <number> : result : yes configure : <number> : checking whether make supports nested variables configure : <number> : result : yes configure : <number> : checking whether to enable maintainer - specific portions of makefiles configure : <number> : result : yes configure : <number> : checking whether make supports nested variables configure : <number> : result : yes configure : <number> : checking for c + + compiler version configure : <number> : g + + - m64 - - version > & <number> g + + ( ubuntu <number> . <number> - 1 0 ubuntu2 ) <number> . <number> copyright ( c ) <number> free software foundation , inc . this is free software ; see the source for copying conditions . there is no warranty ; not even for merchantability or fitness for a particular purpose . configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - v > & <number> using built - in specs . collect_gcc = g + + collect_lto_wrapper <annoyed> usr / lib / gcc / x86_64 - linux - gnu / <number> / lto - wrapper offload_target_names = nvptx - none : hsa offload_target_default = <number> target : x86_64 - linux - gnu configured with : . <repeated> / src / configure - v - - with - pkgversion = ' ubuntu <number> . <number> - 1 0 ubuntu2 ' - - with - bugurl = file :/// usr / share / doc / gcc - <number> / readme . bugs - - enable - languages =c, ada , c + + , go , brig , d , fortran , objc , obj - c + + , gm2 - - prefix <annoyed> usr - - with - gcc - major - version - only - - program - suffix = - <number> - - program - prefix =x 8 6 _64 - linux - gnu - - - enable - shared - - enable - linker - build - id - - libexecdir <annoyed> usr / lib - - without - included - gettext - - enable - threads = posix - - libdir <annoyed> usr / lib - - enable - nls - - enable - clocale = gnu - - enable - libstdcxx - debug - - enable - libstdcxx - time = yes - - with - default - libstdcxx - abi = new - - enable - gnu - unique - object - - disable - vtable - verify - - enable - plugin - - enable - default - pie - - with - system - zlib - - with - target - system - zlib = auto - - enable - objc - gc = auto - - enable - multiarch - - disable - werror - - with - arch - <number> = i686 - - with - abi = m64 - - with - multilib - list = m32 , m64 , mx32 - - enable - multilib - - with - tune = generic - - enable - offload - targets = nvptx - none , hsa - - without - cuda - driver - - enable - checking = release - - build =x 8 6 _64 - linux - gnu - - host =x 8 6 _64 - linux - gnu - - target =x 8 6 _64 - linux - gnu thread model : posix gcc version <number> . <number> ( ubuntu <number> . <number> - 1 0 ubuntu2 ) configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - v > & <number> g + + : error : unrecognized command line option ' - v ' g + + : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - qversion > & <number> g + + : error : unrecognized command line option ' - qversion ' ; did you mean ' - - version ' ? g + + : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : checking whether the c + + compiler works configure : <number> : g + + - m64 - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for c + + compiler default output file name configure : <number> : result : a . out configure : <number> : checking for suffix of executables configure : <number> : g + + - m64 - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : configure : <number> : checking whether we are cross compiling configure : <number> : g + + - m64 - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : . / conftest configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking for suffix of object files configure : <number> : g + + - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : o configure : <number> : checking whether we are using the gnu c + + compiler configure : <number> : g + + - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether g + + - m64 accepts - g configure : <number> : g + + - m64 - c - g - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether make supports the include directive configure : <number> : make - f confmf . gnu & & cat confinc . out this is the am__doit target configure : <number> : $ ? = <number> configure : <number> : result : yes ( gnu style ) configure : <number> : checking dependency style of g + + - m64 configure : <number> : result : gcc3 configure : <number> : checking whether g + + - m64 supports c + + <number> features with - std =c + + <number> configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether std : : atomic can be used without link library configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for x86_64 - pc - linux - gnu - g + + configure : <number> : result : g + + - m64 - std =c + + <number> configure : <number> : checking for objective c + + compiler version configure : <number> : g + + - m64 - std =c + + <number> - - version > & <number> g + + ( ubuntu <number> . <number> - 1 0 ubuntu2 ) <number> . <number> copyright ( c ) <number> free software foundation , inc . this is free software ; see the source for copying conditions . there is no warranty ; not even for merchantability or fitness for a particular purpose . configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - v > & <number> using built - in specs . collect_gcc = g + + collect_lto_wrapper <annoyed> usr / lib / gcc / x86_64 - linux - gnu / <number> / lto - wrapper offload_target_names = nvptx - none : hsa offload_target_default = <number> target : x86_64 - linux - gnu configured with : . <repeated> / src / configure - v - - with - pkgversion = ' ubuntu <number> . <number> - 1 0 ubuntu2 ' - - with - bugurl = file :/// usr / share / doc / gcc - <number> / readme . bugs - - enable - languages =c, ada , c + + , go , brig , d , fortran , objc , obj - c + + , gm2 - - prefix <annoyed> usr - - with - gcc - major - version - only - - program - suffix = - <number> - - program - prefix =x 8 6 _64 - linux - gnu - - - enable - shared - - enable - linker - build - id - - libexecdir <annoyed> usr / lib - - without - included - gettext - - enable - threads = posix - - libdir <annoyed> usr / lib - - enable - nls - - enable - clocale = gnu - - enable - libstdcxx - debug - - enable - libstdcxx - time = yes - - with - default - libstdcxx - abi = new - - enable - gnu - unique - object - - disable - vtable - verify - - enable - plugin - - enable - default - pie - - with - system - zlib - - with - target - system - zlib = auto - - enable - objc - gc = auto - - enable - multiarch - - disable - werror - - with - arch - <number> = i686 - - with - abi = m64 - - with - multilib - list = m32 , m64 , mx32 - - enable - multilib - - with - tune = generic - - enable - offload - targets = nvptx - none , hsa - - without - cuda - driver - - enable - checking = release - - build =x 8 6 _64 - linux - gnu - - host =x 8 6 _64 - linux - gnu - - target =x 8 6 _64 - linux - gnu thread model : posix gcc version <number> . <number> ( ubuntu <number> . <number> - 1 0 ubuntu2 ) configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - v > & <number> g + + : error : unrecognized command line option ' - v ' g + + : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - qversion > & <number> g + + : error : unrecognized command line option ' - qversion ' ; did you mean ' - - version ' ? g + + : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : checking whether we are using the gnu objective c + + compiler configure : <number> : g + + - m64 - std =c + + <number> - c - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . mm > & <number> g + + : fatal error : cannot execute ' cc1objplus ' : execvp : no such file or directory compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | | int | main ( ) | { | <hashtag> if n def </hashtag> __gnuc__ | choke me | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether g + + - m64 - std =c + + <number> accepts - g configure : <number> : g + + - m64 - std =c + + <number> - c - g - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . mm > & <number> g + + : fatal error : cannot execute ' cc1objplus ' : execvp : no such file or directory compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . mm > & <number> g + + : fatal error : cannot execute ' cc1objplus ' : execvp : no such file or directory compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - g - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . mm > & <number> g + + : fatal error : cannot execute ' cc1objplus ' : execvp : no such file or directory compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking dependency style of g + + - m64 - std =c + + <number> configure : <number> : result : gcc3 configure : <number> : checking how to print strings configure : <number> : result : printf configure : <number> : checking for x86_64 - pc - linux - gnu - gcc configure : <number> : result : gcc - m64 configure : <number> : checking for c compiler version configure : <number> : gcc - m64 - - version > & <number> gcc ( ubuntu <number> . <number> - 1 0 ubuntu2 ) <number> . <number> copyright ( c ) <number> free software foundation , inc . this is free software ; see the source for copying conditions . there is no warranty ; not even for merchantability or fitness for a particular purpose . configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - v > & <number> using built - in specs . collect_gcc = gcc collect_lto_wrapper <annoyed> usr / lib / gcc / x86_64 - linux - gnu / <number> / lto - wrapper offload_target_names = nvptx - none : hsa offload_target_default = <number> target : x86_64 - linux - gnu configured with : . <repeated> / src / configure - v - - with - pkgversion = ' ubuntu <number> . <number> - 1 0 ubuntu2 ' - - with - bugurl = file :/// usr / share / doc / gcc - <number> / readme . bugs - - enable - languages =c, ada , c + + , go , brig , d , fortran , objc , obj - c + + , gm2 - - prefix <annoyed> usr - - with - gcc - major - version - only - - program - suffix = - <number> - - program - prefix =x 8 6 _64 - linux - gnu - - - enable - shared - - enable - linker - build - id - - libexecdir <annoyed> usr / lib - - without - included - gettext - - enable - threads = posix - - libdir <annoyed> usr / lib - - enable - nls - - enable - clocale = gnu - - enable - libstdcxx - debug - - enable - libstdcxx - time = yes - - with - default - libstdcxx - abi = new - - enable - gnu - unique - object - - disable - vtable - verify - - enable - plugin - - enable - default - pie - - with - system - zlib - - with - target - system - zlib = auto - - enable - objc - gc = auto - - enable - multiarch - - disable - werror - - with - arch - <number> = i686 - - with - abi = m64 - - with - multilib - list = m32 , m64 , mx32 - - enable - multilib - - with - tune = generic - - enable - offload - targets = nvptx - none , hsa - - without - cuda - driver - - enable - checking = release - - build =x 8 6 _64 - linux - gnu - - host =x 8 6 _64 - linux - gnu - - target =x 8 6 _64 - linux - gnu thread model : posix gcc version <number> . <number> ( ubuntu <number> . <number> - 1 0 ubuntu2 ) configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - v > & <number> gcc : error : unrecognized command line option ' - v ' gcc : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - qversion > & <number> gcc : error : unrecognized command line option ' - qversion ' ; did you mean ' - - version ' ? gcc : fatal error : no input files compilation terminated . configure : <number> : $ ? = <number> configure : <number> : checking whether we are using the gnu c compiler configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether gcc - m64 accepts - g configure : <number> : gcc - m64 - c - g - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for gcc - m64 option to accept iso c89 configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : none needed configure : <number> : checking whether gcc - m64 understands - c and - o together configure : <number> : gcc - m64 - c conftest . c - o conftest2 . o configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - c conftest . c - o conftest2 . o configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking dependency style of gcc - m64 configure : <number> : result : gcc3 configure : <number> : checking for a sed that does not truncate output configure : <number> : result : / bin / sed configure : <number> : checking for grep that handles long lines and - e configure : <number> : result : / bin / grep configure : <number> : checking for egrep configure : <number> : result : / bin / grep - e configure : <number> : checking for fgrep configure : <number> : result : / bin / grep - f configure : <number> : checking for ld used by gcc - m64 configure : <number> : result : / usr / bin / ld configure : <number> : checking if the linker ( / usr / bin / ld ) is gnu ld configure : <number> : result : yes configure : <number> : checking for bsd - or ms - compatible name lister ( nm ) configure : <number> : result : nm configure : <number> : checking the name lister ( nm ) interface configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : nm "" conftest . o "" configure : <number> : output <number> b some_variable configure : <number> : result : bsd nm configure : <number> : checking whether ln - s works configure : <number> : result : yes configure : <number> : checking the maximum length of command line arguments configure : <number> : result : <number> configure : <number> : checking how to convert x86_64 - pc - linux - gnu file names to x86_64 - pc - linux - gnu format configure : <number> : result : func_convert_file_noop configure : <number> : checking how to convert x86_64 - pc - linux - gnu file names to toolchain format configure : <number> : result : func_convert_file_noop configure : <number> : checking for / usr / bin / ld option to reload object files configure : <number> : result : - r configure : <number> : checking for x86_64 - pc - linux - gnu - objdump configure : <number> : result : no configure : <number> : checking for objdump configure : <number> : found / usr / bin / objdump configure : <number> : result : objdump configure : <number> : checking how to recognize dependent libraries configure : <number> : result : pass_all configure : <number> : checking for x86_64 - pc - linux - gnu - dlltool configure : <number> : result : no configure : <number> : checking for dlltool configure : <number> : result : no configure : <number> : checking how to associate runtime and link libraries configure : <number> : result : printf %s \ \ n configure : <number> : checking for x86_64 - pc - linux - gnu - ar configure : <number> : result : ar configure : <number> : checking for archiver <user> support configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : ar cr libconftest . a <user> . lst > & <number> configure : <number> : $ ? = <number> configure : <number> : ar cr libconftest . a <user> . lst > & <number> ar : conftest . <surprise> no such file or directory configure : <number> : $ ? = <number> configure : <number> : result : @ configure : <number> : checking for x86_64 - pc - linux - gnu - strip configure : <number> : result : strip configure : <number> : checking for x86_64 - pc - linux - gnu - ranlib configure : <number> : result : ranlib configure : <number> : checking command to parse nm output from gcc - m64 object configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : nm conftest . o | sed - n - e ' s / ^ . *[ ] \ \ ( [ abcdgirstw ] [ abcdgirstw ] *\\)[ ] [ ]* \ \ ( [ _a - za - z ] [ _a - za - z0 - <number> ]* \\)$ / \ \ <number> \ \ <number> \ \ <number> / p ' | sed ' / __gnu_lto / d ' > conftest . nm configure : <number> : gcc - m64 - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . c conftstm . o > & <number> configure : <number> : $ ? = <number> configure : <number> : result : ok configure : <number> : checking for sysroot configure : <number> : result : no configure : <number> : checking for a working dd configure : <number> : result : / bin / dd configure : <number> : checking how to truncate binary pipes configure : <number> : result : / bin / dd bs = <number> count = <number> configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : checking for x86_64 - pc - linux - gnu - mt configure : <number> : result : no configure : <number> : checking for mt configure : <number> : found / bin / mt configure : <number> : result : mt configure : <number> : checking if mt is a manifest tool configure : <number> : mt ' - ? ' configure : <number> : result : no configure : <number> : checking how to run the c preprocessor configure : <number> : gcc - m64 - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c conftest . c : <time> : fatal error : ac_nonexistent . h : no such file or directory <number> | <hashtag> include </hashtag> < ac_nonexistent . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < ac_nonexistent . h > configure : <number> : result : gcc - m64 - e configure : <number> : gcc - m64 - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c conftest . c : <time> : fatal error : ac_nonexistent . h : no such file or directory <number> | <hashtag> include </hashtag> < ac_nonexistent . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < ac_nonexistent . h > configure : <number> : checking for ansi c header files configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : . / conftest configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sys / types . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sys / stat . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for stdlib . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for string . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for memory . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for strings . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for inttypes . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for stdint . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for unistd . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for dlfcn . h configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for objdir configure : <number> : result : . libs configure : <number> : checking if gcc - m64 supports - fno - rtti - fno - exceptions configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - fno - rtti - fno - exceptions conftest . c > & <number> cc1 : warning : command line option ' - fno - rtti ' is valid for c + + / d / objc + + but not for c configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking for gcc - m64 option to produce pic configure : <number> : result : - fpic - dpic configure : <number> : checking if gcc - m64 pic flag - fpic - dpic works configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - fpic - dpic - dpic conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking if gcc - m64 static flag - static works configure : <number> : result : yes configure : <number> : checking if gcc - m64 supports - c - o file . o configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - o out / conftest2 . o conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking if gcc - m64 supports - c - o file . o configure : <number> : result : yes configure : <number> : checking whether the gcc - m64 linker ( / usr / bin / ld - m elf_x86_64 ) supports shared libraries configure : <number> : result : yes configure : <number> : checking whether - lc should be explicitly linked in configure : <number> : gcc - m64 - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : gcc - m64 - shared - fpic - dpic conftest . o - v - wl , - soname - wl , conftest - o conftest <number> \ \ > \ \ & <number> \\| / bin / grep - lc \ \ > / dev / null <number> \ \ > \ \ & <number> configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking dynamic linker characteristics configure : <number> : gcc - m64 - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - rpath - wl , / foo conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : gnu / linux ld . so configure : <number> : checking how to hardcode library paths into programs configure : <number> : result : immediate configure : <number> : checking whether stripping libraries is possible configure : <number> : result : yes configure : <number> : checking if libtool supports shared libraries configure : <number> : result : yes configure : <number> : checking whether to build shared libraries configure : <number> : result : yes configure : <number> : checking whether to build static libraries configure : <number> : result : yes configure : <number> : checking how to run the c + + preprocessor configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp conftest . cpp : <time> : fatal error : ac_nonexistent . h : no such file or directory <number> | <hashtag> include </hashtag> < ac_nonexistent . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | /* end confdefs . h . */ | <hashtag> include </hashtag> < ac_nonexistent . h > configure : <number> : result : g + + - m64 - std =c + + <number> - e configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp conftest . cpp : <time> : fatal error : ac_nonexistent . h : no such file or directory <number> | <hashtag> include </hashtag> < ac_nonexistent . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | /* end confdefs . h . */ | <hashtag> include </hashtag> < ac_nonexistent . h > configure : <number> : checking for ld used by g + + - m64 - std =c + + <number> configure : <number> : result : / usr / bin / ld - m elf_x86_64 configure : <number> : checking if the linker ( / usr / bin / ld - m elf_x86_64 ) is gnu ld configure : <number> : result : yes configure : <number> : checking whether the g + + - m64 - std =c + + <number> linker ( / usr / bin / ld - m elf_x86_64 ) supports shared libraries configure : <number> : result : yes configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : checking for g + + - m64 - std =c + + <number> option to produce pic configure : <number> : result : - fpic - dpic configure : <number> : checking if g + + - m64 - std =c + + <number> pic flag - fpic - dpic works configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - fpic - dpic - dpic conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking if g + + - m64 - std =c + + <number> static flag - static works configure : <number> : result : yes configure : <number> : checking if g + + - m64 - std =c + + <number> supports - c - o file . o configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - o out / conftest2 . o conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking if g + + - m64 - std =c + + <number> supports - c - o file . o configure : <number> : result : yes configure : <number> : checking whether the g + + - m64 - std =c + + <number> linker ( / usr / bin / ld - m elf_x86_64 ) supports shared libraries configure : <number> : result : yes configure : <number> : checking dynamic linker characteristics configure : <number> : result : gnu / linux ld . so configure : <number> : checking how to hardcode library paths into programs configure : <number> : result : immediate configure : <number> : checking for x86_64 - pc - linux - gnu - ar configure : <number> : result : no configure : <number> : checking for ar configure : <number> : result : ar configure : <number> : checking for x86_64 - pc - linux - gnu - ranlib configure : <number> : result : no configure : <number> : checking for ranlib configure : <number> : result : ranlib configure : <number> : checking for x86_64 - pc - linux - gnu - strip configure : <number> : result : no configure : <number> : checking for strip configure : <number> : found / usr / bin / strip configure : <number> : result : / usr / bin / strip configure : <number> : checking for x86_64 - pc - linux - gnu - gcov configure : <number> : result : no configure : <number> : checking for gcov configure : <number> : found / usr / bin / gcov configure : <number> : result : / usr / bin / gcov configure : <number> : checking for lcov configure : <number> : result : no configure : <number> : checking for python3 . <number> configure : <number> : found / usr / bin / python3 . <number> configure : <number> : result : / usr / bin / python3 . <number> configure : <number> : checking for genhtml configure : <number> : result : no configure : <number> : checking for git configure : <number> : found / usr / bin / git configure : <number> : result : / usr / bin / git configure : <number> : checking for ccache configure : <number> : result : no configure : <number> : checking for xgettext configure : <number> : result : no configure : <number> : checking for hexdump configure : <number> : found / usr / bin / hexdump configure : <number> : result : / usr / bin / hexdump configure : <number> : checking for x86_64 - pc - linux - gnu - readelf configure : <number> : result : no configure : <number> : checking for readelf configure : <number> : found / usr / bin / readelf configure : <number> : result : / usr / bin / readelf configure : <number> : checking for x86_64 - pc - linux - gnu - c + + filt configure : <number> : result : no configure : <number> : checking for c + + filt configure : <number> : found / usr / bin / c + + filt configure : <number> : result : / usr / bin / c + + filt configure : <number> : checking for x86_64 - pc - linux - gnu - objcopy configure : <number> : result : no configure : <number> : checking for objcopy configure : <number> : found / usr / bin / objcopy configure : <number> : result : / usr / bin / objcopy configure : <number> : checking for doxygen configure : <number> : result : no configure : <number> : warning : doxygen not found configure : <number> : checking whether c + + compiler accepts - werror configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - msse4 . <number> configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - msse4 . <number> - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - msse4 . <number> configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - msse4 . <number> - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - mavx - mavx2 configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - mavx - mavx2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - msse4 - msha configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - msse4 - msha - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sse4 . <number> intrinsics configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - msse4 . <number> - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sse4 . <number> intrinsics configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - msse4 . <number> - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for avx2 intrinsics configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - mavx - mavx2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sha - ni intrinsics configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - msse4 - msha - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - march = armv8 - a + crc + crypto configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - werror - march = armv8 - a + crc + crypto - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> cc1plus : error : bad value ( ' armv8 - a + crc + crypto ' ) for ' - march = ' switch cc1plus : note : valid arguments to ' - march = ' switch are : nocona core2 nehalem corei7 westmere sandybridge corei7 - avx ivybridge core - avx - i haswell core - avx2 broadwell skylake skylake - avx512 cannonlake icelake - client icelake - server cascadelake bonnell atom silvermont slm goldmont goldmont - plus tremont knl knm x86 - <number> eden - x2 nano nano - <number> nano - <number> nano - <number> nano - x2 eden - x4 nano - x4 k8 k8 - sse3 opteron opteron - sse3 athlon64 athlon64 - sse3 athlon - fx amdfam10 barcelona bdver1 bdver2 bdver3 bdver4 znver1 znver2 btver1 btver2 native configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for arm crc32 intrinsics configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / conftest . cpp > & <number> conftest . cpp : <time> : fatal error : arm_acle . h : no such file or directory <number> | <hashtag> include </hashtag> < arm_acle . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | /* end confdefs . h . */ | | <hashtag> include </hashtag> < arm_acle . h > | <hashtag> include </hashtag> < arm_neon . h > | | int | main ( ) | { | | __crc32cb ( <number> , <number> ); __crc32ch ( <number> , <number> ); __crc32cw ( <number> , <number> ); __crc32cd ( <number> , <number> ); | vmull_p64 ( <number> , <number> ); | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for x86_64 - pc - linux - gnu - pkg - config configure : <number> : result : / usr / bin / pkg - config - - static configure : <number> : checking pkg - config is at least version <number> . <number> configure : <number> : result : yes configure : <number> : checking whether byte ordering is bigendian configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> : <number> : error : expected unqualified - id before ' not ' token <number> | not a universal capable compiler | ^ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | /* end confdefs . h . */ | <hashtag> if n def </hashtag> __apple_cc__ | not a universal capable compiler | <hashtag> end if </hashtag> | typedef int dummy ; | configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> : <number> : error : ' big ' was not declared in this scope <number> | not big endian | ^ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / types . h > | <hashtag> include </hashtag> < sys / param . h > | | int | main ( ) | { | <hashtag> if </hashtag> byte_order = big_endian | not big endian | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether gcc - m64 is clang configure : <number> : result : no configure : <number> : checking whether pthreads work with - pthread configure : <number> : gcc - m64 - o conftest - pipe - o2 - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for joinable pthread attribute configure : <number> : gcc - m64 - o conftest - pipe - o2 - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : pthread_create_joinable configure : <number> : checking whether more special flags are required for pthreads configure : <number> : result : no configure : <number> : checking for pthread_prio_inherit configure : <number> : gcc - m64 - o conftest - pipe - o2 - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . c > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for special c compiler options needed for large files configure : <number> : result : no configure : <number> : checking for _file_offset_bits value needed for large files configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking whether strerror_r is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for strerror_r configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether strerror_r returns char * configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for __attribute__ ( ( visibility ) ) configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for __attribute__ ( ( dllexport ) ) configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> conftest . cpp : <number> : <number> : warning : ' dllexport ' attribute directive ignored [ - wattributes ] <number> | __attribute__ ( ( dllexport ) ) int foo ( void ) { return <number> ; } | ^ configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking for __attribute__ ( ( dllimport ) ) configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> conftest . cpp : <number> : <number> : warning : ' dllimport ' attribute directive ignored [ - wattributes ] <number> | int foo ( void ) __attribute__ ( ( dllimport ) ); | ^ configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking for library containing clock_gettime configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : none required configure : <number> : checking whether c + + compiler accepts - fpic configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - fpic - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - fstack - reuse = none configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - fstack - reuse = none - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - wstack - protector configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - wstack - protector - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + compiler accepts - fstack - protector - all configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - fstack - protector - all - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + preprocessor accepts - d_fortify_source = <number> configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - d_fortify_source = <number> conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether c + + preprocessor accepts - u_fortify_source configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - u_fortify_source conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the linker accepts - wl , - - dynamicbase configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - - dynamicbase conftest . cpp > & <number> / usr / bin / ld : unrecognized option ' - - dynamicbase ' / usr / bin / ld : use the - - help option for usage information collect2 : error : ld returned <number> exit status configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether the linker accepts - wl , - - nxcompat configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - - nxcompat conftest . cpp > & <number> / usr / bin / ld : unrecognized option ' - - nxcompat ' / usr / bin / ld : use the - - help option for usage information collect2 : error : ld returned <number> exit status configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether the linker accepts - wl , - - high - entropy - va configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - - high - entropy - va conftest . cpp > & <number> / usr / bin / ld : unrecognized option ' - - high - entropy - va ' / usr / bin / ld : use the - - help option for usage information collect2 : error : ld returned <number> exit status configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | /* end confdefs . h . */ | | int | main ( ) | { | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether the linker accepts - wl , - z , relro configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - z , relro conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the linker accepts - wl , - z , now configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - wl , - z , now conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the linker accepts - fpie - pie configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - werror - fpie - pie conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking endian . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking endian . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for endian . h configure : <number> : result : yes configure : <number> : checking sys / endian . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : sys / endian . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / endian . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < stdio . h > | <hashtag> if def </hashtag> have_sys_types_h | # include < sys / types . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_sys_stat_h | # include < sys / stat . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> stdc_headers | # include < stdlib . h > | # include < stddef . h > | <hashtag> else </hashtag> | # ifdef have_stdlib_h | # include < stdlib . h > | # endif | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_string_h | # if ! defined stdc_headers & & defined have_memory_h | # include < memory . h > | # endif | # include < string . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_strings_h | # include < strings . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_inttypes_h | # include < inttypes . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_stdint_h | # include < stdint . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_unistd_h | # include < unistd . h > | <hashtag> end if </hashtag> | <hashtag> include </hashtag> < sys / endian . h > configure : <number> : result : no configure : <number> : checking sys / endian . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp conftest . cpp : <number> <time> : fatal error : sys / endian . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / endian . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / endian . h > configure : <number> : result : no configure : <number> : checking for sys / endian . h configure : <number> : result : no configure : <number> : checking byteswap . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking byteswap . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for byteswap . h configure : <number> : result : yes configure : <number> : checking stdio . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking stdio . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for stdio . h configure : <number> : result : yes configure : <number> : checking for stdlib . h configure : <number> : result : yes configure : <number> : checking for unistd . h configure : <number> : result : yes configure : <number> : checking for strings . h configure : <number> : result : yes configure : <number> : checking for sys / types . h configure : <number> : result : yes configure : <number> : checking for sys / stat . h configure : <number> : result : yes configure : <number> : checking sys / select . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking sys / select . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sys / select . h configure : <number> : result : yes configure : <number> : checking sys / prctl . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking sys / prctl . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sys / prctl . h configure : <number> : result : yes configure : <number> : checking sys / sysctl . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> : <number> : warning : <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" [ - wcpp ] <number> | <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" | ^ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking sys / sysctl . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> : <number> : warning : <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" [ - wcpp ] <number> | <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" | ^ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sys / sysctl . h configure : <number> : result : yes configure : <number> : checking vm / vm_param . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : vm / vm_param . h : no such file or directory <number> | <hashtag> include </hashtag> < vm / vm_param . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < stdio . h > | <hashtag> if def </hashtag> have_sys_types_h | # include < sys / types . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_sys_stat_h | # include < sys / stat . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> stdc_headers | # include < stdlib . h > | # include < stddef . h > | <hashtag> else </hashtag> | # ifdef have_stdlib_h | # include < stdlib . h > | # endif | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_string_h | # if ! defined stdc_headers & & defined have_memory_h | # include < memory . h > | # endif | # include < string . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_strings_h | # include < strings . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_inttypes_h | # include < inttypes . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_stdint_h | # include < stdint . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_unistd_h | # include < unistd . h > | <hashtag> end if </hashtag> | <hashtag> include </hashtag> < vm / vm_param . h > configure : <number> : result : no configure : <number> : checking vm / vm_param . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp conftest . cpp : <number> <time> : fatal error : vm / vm_param . h : no such file or directory <number> | <hashtag> include </hashtag> < vm / vm_param . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < vm / vm_param . h > configure : <number> : result : no configure : <number> : checking for vm / vm_param . h configure : <number> : result : no configure : <number> : checking sys / vmmeter . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : sys / vmmeter . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / vmmeter . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < stdio . h > | <hashtag> if def </hashtag> have_sys_types_h | # include < sys / types . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_sys_stat_h | # include < sys / stat . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> stdc_headers | # include < stdlib . h > | # include < stddef . h > | <hashtag> else </hashtag> | # ifdef have_stdlib_h | # include < stdlib . h > | # endif | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_string_h | # if ! defined stdc_headers & & defined have_memory_h | # include < memory . h > | # endif | # include < string . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_strings_h | # include < strings . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_inttypes_h | # include < inttypes . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_stdint_h | # include < stdint . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_unistd_h | # include < unistd . h > | <hashtag> end if </hashtag> | <hashtag> include </hashtag> < sys / vmmeter . h > configure : <number> : result : no configure : <number> : checking sys / vmmeter . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp conftest . cpp : <number> <time> : fatal error : sys / vmmeter . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / vmmeter . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / vmmeter . h > configure : <number> : result : no configure : <number> : checking for sys / vmmeter . h configure : <number> : result : no configure : <number> : checking sys / resources . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : sys / resources . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / resources . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < stdio . h > | <hashtag> if def </hashtag> have_sys_types_h | # include < sys / types . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_sys_stat_h | # include < sys / stat . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> stdc_headers | # include < stdlib . h > | # include < stddef . h > | <hashtag> else </hashtag> | # ifdef have_stdlib_h | # include < stdlib . h > | # endif | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_string_h | # if ! defined stdc_headers & & defined have_memory_h | # include < memory . h > | # endif | # include < string . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_strings_h | # include < strings . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_inttypes_h | # include < inttypes . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_stdint_h | # include < stdint . h > | <hashtag> end if </hashtag> | <hashtag> if def </hashtag> have_unistd_h | # include < unistd . h > | <hashtag> end if </hashtag> | <hashtag> include </hashtag> < sys / resources . h > configure : <number> : result : no configure : <number> : checking sys / resources . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp conftest . cpp : <number> <time> : fatal error : sys / resources . h : no such file or directory <number> | <hashtag> include </hashtag> < sys / resources . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / resources . h > configure : <number> : result : no configure : <number> : checking for sys / resources . h configure : <number> : result : no configure : <number> : checking fd_zero memcpy dependence configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : no configure : <number> : checking whether getifaddrs is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether freeifaddrs is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether strnlen is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether daemon is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether le16toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether le32toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether le64toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htole16 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htole32 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htole64 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether be16toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether be32toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether be64toh is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htobe16 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htobe32 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether htobe64 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether bswap_16 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether bswap_32 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether bswap_64 is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether __builtin_clz is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether __builtin_clzl is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether __builtin_clzll is declared configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for getmemoryinfo configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for mallopt m_arena_max configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for visibility attribute configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for thread_local support configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - pthread conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for gmtime_r configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for linux getrandom syscall configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for getentropy configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : warning : ignoring return value of ' int getentropy ( void * , size_t ) ' , declared with attribute warn_unused_result [ - wunused - result ] <number> | getentropy ( nullptr , <number> ) | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for getentropy via random . h configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : warning : ignoring return value of ' int getentropy ( void * , size_t ) ' , declared with attribute warn_unused_result [ - wunused - result ] <number> | getentropy ( nullptr , <number> ) | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for sysctl configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> : <number> : warning : <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" [ - wcpp ] <number> | <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" | ^ ~ ~ ~ ~ ~ ~ conftest . cpp : <number> : <number> : error : <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" <number> | <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" | ^ ~ ~ ~ ~ conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : warning : ' int sysctl ( int * , int , void * , size_t * , void * , size_t ) ' is deprecated [ - wdeprecated - declarations ] <number> | sysctl ( nullptr , <number> , nullptr , nullptr , nullptr , <number> ); | ^ in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> <time> : note : declared here <number> | extern int sysctl ( int * __name , int __nlen , void * __oldval , | ^ ~ ~ ~ ~ ~ conftest . cpp : <number> <time> : warning : ' int sysctl ( int * , int , void * , size_t * , void * , size_t ) ' is deprecated [ - wdeprecated - declarations ] <number> | sysctl ( nullptr , <number> , nullptr , nullptr , nullptr , <number> ); | ^ in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> <time> : note : declared here <number> | extern int sysctl ( int * __name , int __nlen , void * __oldval , | ^ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / types . h > | <hashtag> include </hashtag> < sys / sysctl . h > | int | main ( ) | { | <hashtag> if def </hashtag> __linux__ | <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" | <hashtag> end if </hashtag> | sysctl ( nullptr , <number> , nullptr , nullptr , nullptr , <number> ); | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for sysctl kern_arnd configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> : <number> : warning : <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" [ - wcpp ] <number> | <hashtag> warning </hashtag> "" the < sys / sysctl . h > header is deprecated and will be removed . "" | ^ ~ ~ ~ ~ ~ ~ conftest . cpp : <number> : <number> : error : <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" <number> | <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" | ^ ~ ~ ~ ~ conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : error : ' kern_arnd ' was not declared in this scope ; did you mean ' kern_random ' ? <number> | static int name [ <number> ] = { ctl_kern , kern_arnd } ; | ^ ~ ~ ~ ~ ~ ~ ~ ~ | kern_random conftest . cpp : <number> <time> : warning : ' int sysctl ( int * , int , void * , size_t * , void * , size_t ) ' is deprecated [ - wdeprecated - declarations ] <number> | sysctl ( name , <number> , nullptr , nullptr , nullptr , <number> ); | ^ in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> <time> : note : declared here <number> | extern int sysctl ( int * __name , int __nlen , void * __oldval , | ^ ~ ~ ~ ~ ~ conftest . cpp : <number> <time> : warning : ' int sysctl ( int * , int , void * , size_t * , void * , size_t ) ' is deprecated [ - wdeprecated - declarations ] <number> | sysctl ( name , <number> , nullptr , nullptr , nullptr , <number> ); | ^ in file included from conftest . cpp : <number> : / usr / include / x86_64 - linux - gnu / sys / sysctl . h : <number> <time> : note : declared here <number> | extern int sysctl ( int * __name , int __nlen , void * __oldval , | ^ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < sys / types . h > | <hashtag> include </hashtag> < sys / sysctl . h > | int | main ( ) | { | <hashtag> if def </hashtag> __linux__ | <hashtag> error </hashtag> "" do not use sysctl on linux , it ' s deprecated even when it works "" | <hashtag> end if </hashtag> | static int name [ <number> ] = { ctl_kern , kern_arnd } ; | sysctl ( name , <number> , nullptr , nullptr , nullptr , <number> ); | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for if type char equals int8_t configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : error : static assertion failed <number> | static_assert ( std : : is_same < int8_t , char > : : value , "" "" ); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < stdint . h > | <hashtag> include </hashtag> <type_traits> | int | main ( ) | { | static_assert ( std : : is_same < int8_t , char > : : value , "" "" ); | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for fdatasync configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for f_fullfsync configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : error : ' f_fullfsync ' was not declared in this scope <number> | fcntl ( <number> , f_fullfsync , <number> ); | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | /* end confdefs . h . */ | <hashtag> include </hashtag> < fcntl . h > | int | main ( ) | { | fcntl ( <number> , f_fullfsync , <number> ); | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for o_cloexec configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for __builtin_prefetch configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for _mm_prefetch configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for strong getauxval support in the system headers configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : arm_acle . h : no such file or directory <number> | <hashtag> include </hashtag> < arm_acle . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | /* end confdefs . h . */ | | <hashtag> include </hashtag> < arm_acle . h > | <hashtag> include </hashtag> < arm_neon . h > | <hashtag> include </hashtag> < sys / auxv . h > | | int | main ( ) | { | | getauxval ( at_hwcap ) ; | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking for weak getauxval support in the compiler configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for std : : system configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for : : _wsystem configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> <time> : error : ' : : _wsystem ' has not been declared <number> | int nerr = : : _wsystem ( "" "" ); | ^ ~ ~ ~ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | /* end confdefs . h . */ | | int | main ( ) | { | int nerr = : : _wsystem ( "" "" ); | | ; | return <number> ; | } configure : <number> : result : no configure : <number> : checking whether to build bitcoin core gui configure : <number> : result : no configure : <number> : checking for berkeley db c + + headers configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb4 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb4 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb4 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb4 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb4 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb4 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db4 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db4 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db4 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb48 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb48 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb48 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb48 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb48 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb48 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db48 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db48 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db48 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb4 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb4 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb4 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb4 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb4 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb4 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db4 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db4 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db4 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb5 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb5 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb5 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb5 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb5 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb5 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db5 / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db5 / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db5 / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb5 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb5 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb5 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb5 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb5 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb5 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db5 . <number> / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db5 . <number> / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db5 . <number> / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : bdb / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < bdb / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < bdb / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : libdb / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < libdb / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < libdb / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> conftest . cpp : <number> <time> : fatal error : db / db_cxx . h : no such file or directory <number> | <hashtag> include </hashtag> < db / db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | /* end confdefs . h . */ | | <hashtag> include </hashtag> < db / db_cxx . h > | | int | main ( ) | { | | <hashtag> if </hashtag> ! ( ( db_version_major = = <number> & & db_version_minor >= <number> ) || db_version_major > <number> ) | <hashtag> error </hashtag> "" failed to find bdb <number> + "" | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : default configure : <number> : checking for main in - ldb_cxx - <number> configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - ldb_cxx - <number> > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / miniwget . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / miniwget . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for miniupnpc / miniwget . h configure : <number> : result : yes configure : <number> : checking for upnpdiscover in - lminiupnpc configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - lminiupnpc > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / miniupnpc . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / miniupnpc . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for miniupnpc / miniupnpc . h configure : <number> : result : yes configure : <number> : checking for upnpdiscover in - lminiupnpc configure : <number> : result : yes configure : <number> : checking miniupnpc / upnpcommands . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / upnpcommands . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for miniupnpc / upnpcommands . h configure : <number> : result : yes configure : <number> : checking for upnpdiscover in - lminiupnpc configure : <number> : result : yes configure : <number> : checking miniupnpc / upnperrors . h usability configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking miniupnpc / upnperrors . h presence configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for miniupnpc / upnperrors . h configure : <number> : result : yes configure : <number> : checking for upnpdiscover in - lminiupnpc configure : <number> : result : yes configure : <number> : checking whether miniupnpc api version is supported configure : <number> : g + + - m64 - std =c + + <number> - e - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros conftest . cpp configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) includes in "" / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include "" configure : <number> : result : yes configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) lib path in "" / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib / x86_64 - linux - gnu "" configure : <number> : result : no configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) lib path in "" / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib64 "" configure : <number> : result : no configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) lib path in "" / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / libx32 "" configure : <number> : result : no configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) lib path in "" / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib "" configure : <number> : result : yes configure : <number> : checking for boostlib >= <number> . <number> ( <number> ) configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the boost : : system library is available configure : <number> : g + + - m64 - std =c + + <number> - c - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for exit in - lboost_system - mt - x64 configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - lboost_system - mt - x64 > & <number> conftest . cpp : <number> : <number> : warning : declaration of ' char exit ( ) ' conflicts with built - in declaration ' void exit ( int ) ' [ - wbuiltin - declaration - mismatch ] <number> | char exit (); | ^ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the boost : : filesystem library is available configure : <number> : g + + - m64 - std =c + + <number> - c - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for exit in - lboost_filesystem - mt - x64 configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - lboost_filesystem - mt - x64 - lboost_system - mt - x64 > & <number> conftest . cpp : <number> : <number> : warning : declaration of ' char exit ( ) ' conflicts with built - in declaration ' void exit ( int ) ' [ - wbuiltin - declaration - mismatch ] <number> | char exit (); | ^ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether the boost : : thread library is available configure : <number> : g + + - m64 - std =c + + <number> - c - pthread - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include conftest . cpp > & <number> configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for exit in - lboost_thread - mt - x64 configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - lboost_thread - mt - x64 > & <number> conftest . cpp : <number> : <number> : warning : declaration of ' char exit ( ) ' conflicts with built - in declaration ' void exit ( int ) ' [ - wbuiltin - declaration - mismatch ] <number> | char exit (); | ^ ~ ~ ~ configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for mismatched boost c + + <number> scoped enums configure : <number> : g + + - m64 - std =c + + <number> - o conftest - pipe - o2 - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros - dboost_sp_use_std_atomic - dboost_ac_use_std_atomic - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib conftest . cpp - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - lboost_system - mt - x64 - lboost_filesystem - mt - x64 - lboost_thread - mt - x64 - lpthread > & <number> conftest . cpp : in function ' int main ( <sad> conftest . cpp : <number> : <number> : error : ' choke ' was not declared in this scope <number> | choke ; | ^ ~ ~ ~ ~ configure : <number> : $ ? = <number> configure : failed program was : | /* confdefs . h */ | <hashtag> define </hashtag> package_name "" bitcoin core "" | <hashtag> define </hashtag> package_tarname "" bitcoin "" | <hashtag> define </hashtag> package_version "" <number> . <number> "" | <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" | <hashtag> define </hashtag> package_bugreport "" <url> | <hashtag> define </hashtag> package_url "" <url> | <hashtag> define </hashtag> have_cxx11 <number> | <hashtag> define </hashtag> stdc_headers <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_string_h <number> | <hashtag> define </hashtag> have_memory_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_inttypes_h <number> | <hashtag> define </hashtag> have_stdint_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_dlfcn_h <number> | <hashtag> define </hashtag> lt_objdir "" . libs / "" | <hashtag> define </hashtag> use_asm <number> | <hashtag> define </hashtag> enable_sse41 <number> | <hashtag> define </hashtag> enable_avx2 <number> | <hashtag> define </hashtag> enable_shani <number> | <hashtag> define </hashtag> have_pthread_prio_inherit <number> | <hashtag> define </hashtag> have_pthread <number> | <hashtag> define </hashtag> have_decl_strerror_r <number> | <hashtag> define </hashtag> have_strerror_r <number> | <hashtag> define </hashtag> strerror_r_char_p <number> | <hashtag> define </hashtag> have_func_attribute_visibility <number> | <hashtag> define </hashtag> have_endian_h <number> | <hashtag> define </hashtag> have_byteswap_h <number> | <hashtag> define </hashtag> have_stdio_h <number> | <hashtag> define </hashtag> have_stdlib_h <number> | <hashtag> define </hashtag> have_unistd_h <number> | <hashtag> define </hashtag> have_strings_h <number> | <hashtag> define </hashtag> have_sys_types_h <number> | <hashtag> define </hashtag> have_sys_stat_h <number> | <hashtag> define </hashtag> have_sys_select_h <number> | <hashtag> define </hashtag> have_sys_prctl_h <number> | <hashtag> define </hashtag> have_sys_sysctl_h <number> | <hashtag> define </hashtag> have_decl_getifaddrs <number> | <hashtag> define </hashtag> have_decl_freeifaddrs <number> | <hashtag> define </hashtag> have_decl_strnlen <number> | <hashtag> define </hashtag> have_decl_daemon <number> | <hashtag> define </hashtag> have_decl_le16toh <number> | <hashtag> define </hashtag> have_decl_le32toh <number> | <hashtag> define </hashtag> have_decl_le64toh <number> | <hashtag> define </hashtag> have_decl_htole16 <number> | <hashtag> define </hashtag> have_decl_htole32 <number> | <hashtag> define </hashtag> have_decl_htole64 <number> | <hashtag> define </hashtag> have_decl_be16toh <number> | <hashtag> define </hashtag> have_decl_be32toh <number> | <hashtag> define </hashtag> have_decl_be64toh <number> | <hashtag> define </hashtag> have_decl_htobe16 <number> | <hashtag> define </hashtag> have_decl_htobe32 <number> | <hashtag> define </hashtag> have_decl_htobe64 <number> | <hashtag> define </hashtag> have_decl_bswap_16 <number> | <hashtag> define </hashtag> have_decl_bswap_32 <number> | <hashtag> define </hashtag> have_decl_bswap_64 <number> | <hashtag> define </hashtag> have_decl___builtin_clz <number> | <hashtag> define </hashtag> have_decl___builtin_clzl <number> | <hashtag> define </hashtag> have_decl___builtin_clzll <number> | <hashtag> define </hashtag> have_malloc_info <number> | <hashtag> define </hashtag> have_mallopt_arena_max <number> | <hashtag> define </hashtag> have_visibility_attribute <number> | <hashtag> define </hashtag> have_thread_local <number> | <hashtag> define </hashtag> have_gmtime_r <number> | <hashtag> define </hashtag> have_sys_getrandom <number> | <hashtag> define </hashtag> have_getentropy <number> | <hashtag> define </hashtag> have_getentropy_rand <number> | <hashtag> define </hashtag> have_std__system <number> | <hashtag> define </hashtag> have_system have_std__system || have_wsystem | <hashtag> define </hashtag> have_miniupnpc_miniwget_h <number> | <hashtag> define </hashtag> have_miniupnpc_miniupnpc_h <number> | <hashtag> define </hashtag> have_miniupnpc_upnpcommands_h <number> | <hashtag> define </hashtag> have_miniupnpc_upnperrors_h <number> | <hashtag> define </hashtag> have_boost /* */ | <hashtag> define </hashtag> have_boost_system /* */ | <hashtag> define </hashtag> have_boost_filesystem /* */ | <hashtag> define </hashtag> have_boost_thread /* */ | /* end confdefs . h . */ | | <hashtag> include </hashtag> < boost / config . hpp > | <hashtag> include </hashtag> < boost / version . hpp > | <hashtag> if </hashtag> ! defined ( boost_no_scoped_enums ) & & ! defined ( boost_no_cxx11_scoped_enums ) & & boost_version < <number> | <hashtag> define </hashtag> boost_no_scoped_enums | <hashtag> define </hashtag> boost_no_cxx11_scoped_enums | <hashtag> define </hashtag> check | <hashtag> end if </hashtag> | <hashtag> include </hashtag> < boost / filesystem . hpp > | | int | main ( ) | { | | <hashtag> if </hashtag> defined ( check ) | boost : : filesystem : : copy_file ( "" foo "" , "" bar "" ); | <hashtag> else </hashtag> | choke ; | <hashtag> end if </hashtag> | | ; | return <number> ; | } configure : <number> : result : ok configure : <number> : checking for event configure : <number> : $ pkg_config - - exists - - print - errors "" libevent >= <date> "" configure : <number> : $ ? = <number> configure : <number> : $ pkg_config - - exists - - print - errors "" libevent >= <date> "" configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for event_pthreads configure : <number> : $ pkg_config - - exists - - print - errors "" libevent_pthreads >= <date> "" configure : <number> : $ ? = <number> configure : <number> : $ pkg_config - - exists - - print - errors "" libevent_pthreads >= <date> "" configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking for zmq configure : <number> : $ pkg_config - - exists - - print - errors "" libzmq >= <number> "" configure : <number> : $ ? = <number> configure : <number> : $ pkg_config - - exists - - print - errors "" libzmq >= <number> "" configure : <number> : $ ? = <number> configure : <number> : result : yes configure : <number> : checking whether to build bitcoind configure : <number> : result : yes configure : <number> : checking whether to build bitcoin - cli configure : <number> : result : yes configure : <number> : checking whether to build bitcoin - tx configure : <number> : result : yes configure : <number> : checking whether to build bitcoin - wallet configure : <number> : result : yes configure : <number> : checking whether to build libraries configure : <number> : result : yes configure : <number> : checking if ccache should be used configure : <number> : result : no configure : <number> : checking if wallet should be enabled configure : <number> : result : yes configure : <number> : checking whether to build with support for upnp configure : <number> : result : yes configure : <number> : checking whether to build with upnp enabled by default configure : <number> : result : no configure : <number> : checking whether to build test_bitcoin configure : <number> : result : no configure : <number> : checking whether to reduce exports configure : <number> : result : no configure : <number> : checking that generated files are newer than configure configure : <number> : result : done configure : <number> : creating . / config . status # # - - - - - - - - - - - - - - - - - - - - - - # # # # running config . status . # # # # - - - - - - - - - - - - - - - - - - - - - - # # this file was extended by bitcoin core config . status <number> . <number> , which was generated by gnu autoconf <number> . invocation command line was config_files = config_headers = config_links = config_commands = $ . / config . status on nuc config . status : <number> : creating libbitcoinconsensus . pc config . status : <number> : creating makefile config . status : <number> : creating src / makefile config . status : <number> : creating doc / man / makefile config . status : <number> : creating share / setup . nsi config . status : <number> : creating share / qt / info . plist config . status : <number> : creating test / config . ini config . status : <number> : creating contrib / devtools / split - debug . sh config . status : <number> : creating src / config / bitcoin - config . h config . status : <number> : src / config / bitcoin - config . h is unchanged config . status : <number> : executing depfiles commands config . status : <number> : cd src & & sed - e ' /# am - - include - marker / d ' makefile | make - f - am - - depfiles make : nothing to be done for ' am - - depfiles ' . config . status : <number> : $ ? = <number> config . status : <number> : executing libtool commands configure : <number> : = = = configuring in src / univalue ( / home / dante / src / bitcoin / src / univalue ) configure : <number> : running / bin / bash . / configure - - disable - option - checking ' - - prefix <annoyed> home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu ' ' - - with - gui = no ' ' - - disable - tests ' ' - - disable - gui - tests ' ' - - enable - wallet ' ' - - disable - shared ' ' - - with - pic ' ' - - enable - benchmark = no ' ' - - with - bignum = no ' ' - - enable - module - recovery ' ' - - disable - jni ' - - cache - file <annoyed> dev / null - - srcdir = . configure : <number> : = = = configuring in src / secp256k1 ( / home / dante / src / bitcoin / src / secp256k1 ) configure : <number> : running / bin / bash . / configure - - disable - option - checking ' - - prefix <annoyed> home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu ' ' - - with - gui = no ' ' - - disable - tests ' ' - - disable - gui - tests ' ' - - enable - wallet ' ' - - disable - shared ' ' - - with - pic ' ' - - enable - benchmark = no ' ' - - with - bignum = no ' ' - - enable - module - recovery ' ' - - disable - jni ' - - cache - file <annoyed> dev / null - - srcdir = . # # - - - - - - - - - - - - - - - - # # # # cache variables . # # # # - - - - - - - - - - - - - - - - # # ac_cv_build =x 8 6 _64 - pc - linux - gnu ac_cv_c_bigendian = no ac_cv_c_compiler_gnu = yes ac_cv_cxx_compiler_gnu = yes ac_cv_env_arflags_set = ac_cv_env_arflags_value = ac_cv_env_bdb_cflags_set = ac_cv_env_bdb_cflags_value = ac_cv_env_bdb_libs_set = ac_cv_env_bdb_libs_value = ac_cv_env_ccc_set = ac_cv_env_ccc_value = ac_cv_env_cc_set = ac_cv_env_cc_value = ac_cv_env_cflags_set = ac_cv_env_cflags_value = ac_cv_env_cppflags_set = ac_cv_env_cppflags_value = ac_cv_env_cpp_set = ac_cv_env_cpp_value = ac_cv_env_cxxcpp_set = ac_cv_env_cxxcpp_value = ac_cv_env_cxxflags_set = ac_cv_env_cxxflags_value = ac_cv_env_cxx_set = ac_cv_env_cxx_value = ac_cv_env_event_cflags_set = ac_cv_env_event_cflags_value = ac_cv_env_event_libs_set = ac_cv_env_event_libs_value = ac_cv_env_event_pthreads_cflags_set = ac_cv_env_event_pthreads_cflags_value = ac_cv_env_event_pthreads_libs_set = ac_cv_env_event_pthreads_libs_value = ac_cv_env_ldflags_set = ac_cv_env_ldflags_value = ac_cv_env_libs_set = ac_cv_env_libs_value = ac_cv_env_lt_sys_library_path_set = ac_cv_env_lt_sys_library_path_value = ac_cv_env_objcxxflags_set = ac_cv_env_objcxxflags_value = ac_cv_env_objcxx_set = ac_cv_env_objcxx_value = ac_cv_env_pkg_config_libdir_set = ac_cv_env_pkg_config_libdir_value = ac_cv_env_pkg_config_path_set = ac_cv_env_pkg_config_path_value = ac_cv_env_pkg_config_set = ac_cv_env_pkg_config_value = ac_cv_env_pythonpath_set = ac_cv_env_pythonpath_value = ac_cv_env_qr_cflags_set = ac_cv_env_qr_cflags_value = ac_cv_env_qr_libs_set = ac_cv_env_qr_libs_value = ac_cv_env_qt5_cflags_set = ac_cv_env_qt5_cflags_value = ac_cv_env_qt5_libs_set = ac_cv_env_qt5_libs_value = ac_cv_env_qtaccessibility_cflags_set = ac_cv_env_qtaccessibility_cflags_value = ac_cv_env_qtaccessibility_libs_set = ac_cv_env_qtaccessibility_libs_value = ac_cv_env_qtcgl_cflags_set = ac_cv_env_qtcgl_cflags_value = ac_cv_env_qtcgl_libs_set = ac_cv_env_qtcgl_libs_value = ac_cv_env_qtclipboard_cflags_set = ac_cv_env_qtclipboard_cflags_value = ac_cv_env_qtclipboard_libs_set = ac_cv_env_qtclipboard_libs_value = ac_cv_env_qtdevicediscovery_cflags_set = ac_cv_env_qtdevicediscovery_cflags_value = ac_cv_env_qtdevicediscovery_libs_set = ac_cv_env_qtdevicediscovery_libs_value = ac_cv_env_qteventdispatcher_cflags_set = ac_cv_env_qteventdispatcher_cflags_value = ac_cv_env_qteventdispatcher_libs_set = ac_cv_env_qteventdispatcher_libs_value = ac_cv_env_qtfb_cflags_set = ac_cv_env_qtfb_cflags_value = ac_cv_env_qtfb_libs_set = ac_cv_env_qtfb_libs_value = ac_cv_env_qtfontdatabase_cflags_set = ac_cv_env_qtfontdatabase_cflags_value = ac_cv_env_qtfontdatabase_libs_set = ac_cv_env_qtfontdatabase_libs_value = ac_cv_env_qtgraphics_cflags_set = ac_cv_env_qtgraphics_cflags_value = ac_cv_env_qtgraphics_libs_set = ac_cv_env_qtgraphics_libs_value = ac_cv_env_qtplatform_cflags_set = ac_cv_env_qtplatform_cflags_value = ac_cv_env_qtplatform_libs_set = ac_cv_env_qtplatform_libs_value = ac_cv_env_qttheme_cflags_set = ac_cv_env_qttheme_cflags_value = ac_cv_env_qttheme_libs_set = ac_cv_env_qttheme_libs_value = ac_cv_env_qtxcbqpa_cflags_set = ac_cv_env_qtxcbqpa_cflags_value = ac_cv_env_qtxcbqpa_libs_set = ac_cv_env_qtxcbqpa_libs_value = ac_cv_env_qt_dbus_cflags_set = ac_cv_env_qt_dbus_cflags_value = ac_cv_env_qt_dbus_libs_set = ac_cv_env_qt_dbus_libs_value = ac_cv_env_qt_test_cflags_set = ac_cv_env_qt_test_cflags_value = ac_cv_env_qt_test_libs_set = ac_cv_env_qt_test_libs_value = ac_cv_env_univalue_cflags_set = ac_cv_env_univalue_cflags_value = ac_cv_env_univalue_libs_set = ac_cv_env_univalue_libs_value = ac_cv_env_zmq_cflags_set = ac_cv_env_zmq_cflags_value = ac_cv_env_zmq_libs_set = ac_cv_env_zmq_libs_value = ac_cv_env_build_alias_set = ac_cv_env_build_alias_value = ac_cv_env_host_alias_set = ac_cv_env_host_alias_value = ac_cv_env_target_alias_set = ac_cv_env_target_alias_value = ac_cv_func_strerror_r = yes ac_cv_func_strerror_r_char_p = yes ac_cv_have_decl___builtin_clz = yes ac_cv_have_decl___builtin_clzl = yes ac_cv_have_decl___builtin_clzll = yes ac_cv_have_decl_be16toh = yes ac_cv_have_decl_be32toh = yes ac_cv_have_decl_be64toh = yes ac_cv_have_decl_bswap_16 = yes ac_cv_have_decl_bswap_32 = yes ac_cv_have_decl_bswap_64 = yes ac_cv_have_decl_daemon = yes ac_cv_have_decl_freeifaddrs = yes ac_cv_have_decl_getifaddrs = yes ac_cv_have_decl_htobe16 = yes ac_cv_have_decl_htobe32 = yes ac_cv_have_decl_htobe64 = yes ac_cv_have_decl_htole16 = yes ac_cv_have_decl_htole32 = yes ac_cv_have_decl_htole64 = yes ac_cv_have_decl_le16toh = yes ac_cv_have_decl_le32toh = yes ac_cv_have_decl_le64toh = yes ac_cv_have_decl_strerror_r = yes ac_cv_have_decl_strnlen = yes ac_cv_header_byteswap_h = yes ac_cv_header_dlfcn_h = yes ac_cv_header_endian_h = yes ac_cv_header_inttypes_h = yes ac_cv_header_memory_h = yes ac_cv_header_miniupnpc_miniupnpc_h = yes ac_cv_header_miniupnpc_miniwget_h = yes ac_cv_header_miniupnpc_upnpcommands_h = yes ac_cv_header_miniupnpc_upnperrors_h = yes ac_cv_header_stdc = yes ac_cv_header_stdint_h = yes ac_cv_header_stdio_h = yes ac_cv_header_stdlib_h = yes ac_cv_header_string_h = yes ac_cv_header_strings_h = yes ac_cv_header_sys_endian_h = no ac_cv_header_sys_prctl_h = yes ac_cv_header_sys_resources_h = no ac_cv_header_sys_select_h = yes ac_cv_header_sys_stat_h = yes ac_cv_header_sys_sysctl_h = yes ac_cv_header_sys_types_h = yes ac_cv_header_sys_vmmeter_h = no ac_cv_header_unistd_h = yes ac_cv_header_vm_vm_param_h = no ac_cv_host =x 8 6 _64 - pc - linux - gnu ac_cv_lib_boost_filesystem_mt_x64___exit = yes ac_cv_lib_boost_system_mt_x64___exit = yes ac_cv_lib_boost_thread_mt_x64___exit = yes ac_cv_lib_db_cxx_4_8___main = yes ac_cv_lib_miniupnpc_upnpdiscover = yes ac_cv_objcxx_compiler_gnu = no ac_cv_objext =o ac_cv_path_egrep ='/ bin / grep - e ' ac_cv_path_fgrep ='/ bin / grep - f ' ac_cv_path_git <annoyed> usr / bin / git ac_cv_path_grep <annoyed> bin / grep ac_cv_path_hexdump <annoyed> usr / bin / hexdump ac_cv_path_pkg_config ='/ usr / bin / pkg - config - - static ' ac_cv_path_python <annoyed> usr / bin / python3 . <number> ac_cv_path_sed <annoyed> bin / sed ac_cv_path_ac_pt_ar = ar ac_cv_path_ac_pt_cppfilt <annoyed> usr / bin / c + + filt ac_cv_path_ac_pt_gcov <annoyed> usr / bin / gcov ac_cv_path_ac_pt_nm = nm ac_cv_path_ac_pt_objcopy <annoyed> usr / bin / objcopy ac_cv_path_ac_pt_ranlib = ranlib ac_cv_path_ac_pt_readelf <annoyed> usr / bin / readelf ac_cv_path_ac_pt_strip <annoyed> usr / bin / strip ac_cv_path_install ='/ usr / bin / install - c ' ac_cv_path_lt_dd <annoyed> bin / dd ac_cv_path_mkdir <annoyed> bin / mkdir ac_cv_prog_ar = ar ac_cv_prog_awk = gawk ac_cv_prog_cc = ' gcc - m64 ' ac_cv_prog_cpp = ' gcc - m64 - e ' ac_cv_prog_cxxcpp = ' g + + - m64 - std =c + + <number> - e ' ac_cv_prog_objcxx = ' g + + - m64 - std =c + + <number> ' ac_cv_prog_ranlib = ranlib ac_cv_prog_strip = strip ac_cv_prog_ac_ct_manifest_tool = mt ac_cv_prog_ac_ct_objdump = objdump ac_cv_prog_ac_ct_strip = strip ac_cv_prog_cc_c89 = ac_cv_prog_cc_g = yes ac_cv_prog_cxx_g = yes ac_cv_prog_make_make_set = yes ac_cv_prog_objcxx_g = no ac_cv_search_clock_gettime = ' none required ' ac_cv_sys_file_offset_bits = no ac_cv_sys_largefile_cc = no am_cv_cc_dependencies_compiler_type = gcc3 am_cv_cxx_dependencies_compiler_type = gcc3 am_cv_objcxx_dependencies_compiler_type = gcc3 am_cv_make_support_nested_variables = yes am_cv_prog_cc_c_o = yes ax_cv_pthread_clang = no ax_cv_pthread_joinable_attr = pthread_create_joinable ax_cv_pthread_prio_inherit = yes ax_cv_pthread_special_flags = no ax_cv_boost_filesystem = yes ax_cv_boost_system = yes ax_cv_boost_thread = yes ax_cv_check_cxxcppflags___d_fortify_source_2 = yes ax_cv_check_cxxcppflags___u_fortify_source = yes ax_cv_check_cxxflags___werror = yes ax_cv_check_cxxflags___wstack_protector = yes ax_cv_check_cxxflags___fpic = yes ax_cv_check_cxxflags___fstack_protector_all = yes ax_cv_check_cxxflags___fstack_reuse_none = yes ax_cv_check_cxxflags__march_armv8_apcrcpcrypto = no ax_cv_check_cxxflags__mavx__mavx2 = yes ax_cv_check_cxxflags__msse4_1 = yes ax_cv_check_cxxflags__msse4_2 = yes ax_cv_check_cxxflags__msse4__msha = yes ax_cv_check_ldflags___wl___dynamicbase = no ax_cv_check_ldflags___wl___high_entropy_va = no ax_cv_check_ldflags___wl___nxcompat = no ax_cv_check_ldflags___wl__z_now = yes ax_cv_check_ldflags___wl__z_relro = yes ax_cv_check_ldflags__fpie__pie = yes ax_cv_cxx_compile_cxx11__std_cpp11 = yes ax_cv_have_func_attribute_dllexport = no ax_cv_have_func_attribute_dllimport = no ax_cv_have_func_attribute_visibility = yes lt_cv_ar_at_file =@ lt_cv_archive_cmds_need_lc = no lt_cv_deplibs_check_method = pass_all lt_cv_file_magic_cmd ='$ magic_cmd ' lt_cv_file_magic_test_file = lt_cv_ld_reload_flag = - r lt_cv_nm_interface = ' bsd nm ' lt_cv_objdir = . libs lt_cv_path_ld <annoyed> usr / bin / ld lt_cv_path_ldcxx ='/ usr / bin / ld - m elf_x86_64 ' lt_cv_path_nm = nm lt_cv_path_mainfest_tool = no lt_cv_prog_compiler_c_o = yes lt_cv_prog_compiler_c_o_cxx = yes lt_cv_prog_compiler_pic = ' - fpic - dpic ' lt_cv_prog_compiler_pic_cxx = ' - fpic - dpic ' lt_cv_prog_compiler_pic_works = yes lt_cv_prog_compiler_pic_works_cxx = yes lt_cv_prog_compiler_rtti_exceptions = no lt_cv_prog_compiler_static_works = yes lt_cv_prog_compiler_static_works_cxx = yes lt_cv_prog_gnu_ld = yes lt_cv_prog_gnu_ldcxx = yes lt_cv_sharedlib_from_linklib_cmd = ' printf %s \ \ n ' lt_cv_shlibpath_overrides_runpath = yes lt_cv_sys_global_symbol_pipe = ' sed - n - e ' \ \ ' ' s / ^ . *[ ] \ \ ( [ abcdgirstw ] [ abcdgirstw ] *\\)[ ] [ ]* \ \ ( [ _a - za - z ] [ _a - za - z0 - <number> ]* \\)$ / \ \ <number> \ \ <number> \ \ <number> / p ' \ \ ' ' | sed ' \ \ ' ' / __gnu_lto / d ' \ \ ' ' ' lt_cv_sys_global_symbol_to_c_name_address = ' sed - n - e ' \ \ ' ' s / ^ : \ \(.*\\) . *$ / { "" \ \ <number> "" , ( void <wink> <number> } , / p ' \ \ ' ' - e ' \ \ ' ' s / ^ [ abcdgirstw ] [ abcdgirstw ] * . * \ \(.*\\)$ / { "" \ \ <number> "" , ( void <wink> \ \ & \ \ <number> } , / p ' \ \ ' ' ' lt_cv_sys_global_symbol_to_c_name_address_lib_prefix = ' sed - n - e ' \ \ ' ' s / ^ : \ \(.*\\) . *$ / { "" \ \ <number> "" , ( void <wink> <number> } , / p ' \ \ ' ' - e ' \ \ ' ' s / ^ [ abcdgirstw ] [ abcdgirstw ] * . * \ \ ( lib . *\\) $/ { "" \ \ <number> "" , ( void <wink> \ \ & \ \ <number> } , / p ' \ \ ' ' - e ' \ \ ' ' s / ^ [ abcdgirstw ] [ abcdgirstw ] * . * \ \(.*\\)$ / { "" lib \ \ <number> "" , ( void <wink> \ \ & \ \ <number> } , / p ' \ \ ' ' ' lt_cv_sys_global_symbol_to_cdecl = ' sed - n - e ' \ \ ' ' s / ^ t . * \ \(.*\\)$ / extern int \ \ <number> (); / p ' \ \ ' ' - e ' \ \ ' ' s / ^ [ abcdgirstw ] [ abcdgirstw ] * . * \ \(.*\\)$ / extern char \ \ <number> ;/ p ' \ \ ' ' ' lt_cv_sys_global_symbol_to_import = lt_cv_sys_max_cmd_len = <number> lt_cv_to_host_file_cmd = func_convert_file_noop lt_cv_to_tool_file_cmd = func_convert_file_noop lt_cv_truncate_bin ='/ bin / dd bs = <number> count = <number> ' pkg_cv_event_cflags = - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include pkg_cv_event_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - levent ' pkg_cv_event_pthreads_cflags = ' - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include ' pkg_cv_event_pthreads_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - levent_pthreads - levent ' pkg_cv_zmq_cflags = - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include pkg_cv_zmq_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - lzmq - lpthread - lrt ' # # - - - - - - - - - - - - - - - - - # # # # output variables . # # # # - - - - - - - - - - - - - - - - - # # aclocal ='$ { shell } / home / dante / src / bitcoin / build - aux / missing aclocal - <number> ' amdepbackslash ='\\ ' amdep_false ='# ' amdep_true = ' ' amtar = ' $$ { tar - tar } ' am_backslash ='\\ ' am_default_v ='$ ( am_default_verbosity ) ' am_default_verbosity = ' <number> ' am_v ='$ ( v ) ' ar = ' ar ' arflags = ' cr ' arm_crc_cxxflags = ' ' autoconf ='$ { shell } / home / dante / src / bitcoin / build - aux / missing autoconf ' autoheader ='$ { shell } / home / dante / src / bitcoin / build - aux / missing autoheader ' automake ='$ { shell } / home / dante / src / bitcoin / build - aux / missing automake - <number> ' avx2_cxxflags = ' - mavx - mavx2 ' awk = ' gawk ' bdb_cflags = ' ' bdb_cppflags = ' ' bdb_libs = ' - ldb_cxx - <number> ' bitcoin_cli_name = ' bitcoin - cli ' bitcoin_daemon_name = ' bitcoind ' bitcoin_gui_name = ' bitcoin - qt ' bitcoin_tx_name = ' bitcoin - tx ' bitcoin_wallet_tool_name = ' bitcoin - wallet ' boost_cppflags = ' - dboost_sp_use_std_atomic - dboost_ac_use_std_atomic - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include ' boost_filesystem_lib = ' - lboost_filesystem - mt - x64 ' boost_ldflags = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib ' boost_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib - lboost_system - mt - x64 - lboost_filesystem - mt - x64 - lboost_thread - mt - x64 - lpthread ' boost_system_lib = ' - lboost_system - mt - x64 ' boost_thread_lib = ' - lboost_thread - mt - x64 - lpthread ' boost_unit_test_framework_lib = ' ' brew = ' ' build_bitcoind_false ='# ' build_bitcoind_true = ' ' build_bitcoin_cli_false ='# ' build_bitcoin_cli_true = ' ' build_bitcoin_libs_false ='# ' build_bitcoin_libs_true = ' ' build_bitcoin_tx_false ='# ' build_bitcoin_tx_true = ' ' build_bitcoin_wallet_false ='# ' build_bitcoin_wallet_true = ' ' build_darwin_false = ' ' build_darwin_true ='# ' cc = ' gcc - m64 ' ccache = ' ' ccdepmode = ' depmode = gcc3 ' cflags = ' - pipe - o2 ' client_version_build = ' <number> ' client_version_is_release = ' true ' client_version_major = ' <number> ' client_version_minor = ' <number> ' client_version_revision = ' <number> ' compat_ldflags = ' ' copyright_holders = ' the %s developers ' copyright_holders_final = ' the bitcoin core developers ' copyright_holders_substitution = ' bitcoin core ' copyright_year = ' <number> ' cpp = ' gcc - m64 - e ' cppfilt ='/ usr / bin / c + + filt ' cppflags = ' - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / include / - dhave_build_info - d__stdc_format_macros ' cxx = ' g + + - m64 - std =c + + <number> ' cxxcpp = ' g + + - m64 - std =c + + <number> - e ' cxxdepmode = ' depmode = gcc3 ' cxxflags = ' - pipe - o2 ' cygpath_w = ' echo ' debug_cppflags = ' ' debug_cxxflags = ' ' defs = ' - dhave_config_h ' depdir ='. deps ' dlltool = ' false ' doxygen = ' ' dsymutil = ' ' dumpbin = ' ' echo_c = ' ' echo_n = ' - n ' echo_t = ' ' egrep ='/ bin / grep - e ' embedded_leveldb_false ='# ' embedded_leveldb_true = ' ' embedded_univalue_false ='# ' embedded_univalue_true = ' ' enable_arm_crc_false = ' ' enable_arm_crc_true ='# ' enable_avx2_false ='# ' enable_avx2_true = ' ' enable_bench_false ='# ' enable_bench_true = ' ' enable_fuzz_false = ' ' enable_fuzz_true ='# ' enable_man_false ='# ' enable_man_true = ' ' enable_qt_false = ' ' enable_qt_tests_false = ' ' enable_qt_tests_true ='# ' enable_qt_true ='# ' enable_shani_false ='# ' enable_shani_true = ' ' enable_sse41_false ='# ' enable_sse41_true = ' ' enable_sse42_false ='# ' enable_sse42_true = ' ' enable_tests_false = ' ' enable_tests_true ='# ' enable_wallet_false ='# ' enable_wallet_true = ' ' enable_zmq_false ='# ' enable_zmq_true = ' ' error_cxxflags = ' ' event_cflags = ' - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include ' event_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - levent ' event_pthreads_cflags = ' - pthread - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include ' event_pthreads_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - levent_pthreads - levent ' exeext = ' ' extended_functional_tests = ' ' fgrep ='/ bin / grep - f ' gcov ='/ usr / bin / gcov ' genhtml = ' ' genisoimage = ' ' git ='/ usr / bin / git ' glibc_back_compat_false = ' ' glibc_back_compat_true ='# ' gprof_cxxflags = ' ' gprof_ldflags = ' ' grep ='/ bin / grep ' hardened_cppflags = ' - u_fortify_source - d_fortify_source = <number> ' hardened_cxxflags = ' - fstack - reuse = none - wstack - protector - fstack - protector - all ' hardened_ldflags = ' - wl , - z , relro - wl , - z , now - pie ' harden_false ='# ' harden_true = ' ' have_builtin_prefetch = ' <number> ' have_cxx11 = ' <number> ' have_doxygen_false = ' ' have_doxygen_true ='# ' have_fdatasync = ' <number> ' have_fullfsync = ' <number> ' have_gmtime_r = ' ' have_mm_prefetch = ' <number> ' have_o_cloexec = ' <number> ' have_strong_getauxval = ' <number> ' have_weak_getauxval = ' <number> ' hexdump ='/ usr / bin / hexdump ' imagemagick_convert = ' ' installnametool = ' ' install_data ='$ { install } - m <number> ' install_program ='$ { install } ' install_script ='$ { install } ' install_strip_program ='$ ( install_sh ) - c - s ' lcov = ' ' lcov_opts = ' ' ld ='/ usr / bin / ld - m elf_x86_64 ' ldflags = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib ' leveldb_cppflags = ' ' libleveldb = ' ' libmemenv = ' ' libobjs = ' ' libs = ' ' libtool ='$ ( shell ) $( top_builddir ) / libtool ' libtool_app_ldflags = ' ' lipo = ' ' ln_s = ' ln - s ' lrelease = ' ' ltlibobjs = ' ' lt_sys_library_path = ' ' lupdate = ' ' maint = ' ' maintainer_mode_false ='# ' maintainer_mode_true = ' ' makeinfo ='$ { shell } / home / dante / src / bitcoin / build - aux / missing makeinfo ' makensis = ' ' manifest_tool = ' : ' miniupnpc_cppflags = ' ' miniupnpc_libs = ' - lminiupnpc ' mkdir_p ='/ bin / mkdir - p ' moc = ' ' moc_defs = ' - dhave_config_h - i $( srcdir ) ' nm = ' nm ' nmedit = ' ' nowarn_cxxflags = ' ' objcopy ='/ usr / bin / objcopy ' objcxx = ' g + + - m64 - std =c + + <number> ' objcxxdepmode = ' depmode = gcc3 ' objcxxflags = ' ' objdump = ' objdump ' objext ='o ' otool64 = ' ' otool = ' ' package = ' bitcoin ' package_bugreport = ' <url> package_name = ' bitcoin core ' package_string = ' bitcoin core <number> . <number> ' package_tarname = ' bitcoin ' package_url = ' <url> package_version = ' <number> . <number> ' path_separator = ' : ' pic_flags = ' - fpic ' pie_flags = ' - fpie ' pkg_config ='/ usr / bin / pkg - config - - static ' pkg_config_libdir ='/ home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib / pkgconfig ' pkg_config_path ='/ home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / share / pkgconfig <annoyed> home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / lib / pkgconfig ' pthread_cc = ' gcc - m64 ' pthread_cflags = ' - pthread ' pthread_libs = ' ' python ='/ usr / bin / python3 . <number> ' pythonpath ='/ home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / native / lib / python3 / dist - packages : ' qr_cflags = ' ' qr_libs = ' ' qt5_cflags = ' ' qt5_libs = ' ' qtaccessibility_cflags = ' ' qtaccessibility_libs = ' ' qtcgl_cflags = ' ' qtcgl_libs = ' ' qtclipboard_cflags = ' ' qtclipboard_libs = ' ' qtdevicediscovery_cflags = ' ' qtdevicediscovery_libs = ' ' qteventdispatcher_cflags = ' ' qteventdispatcher_libs = ' ' qtfb_cflags = ' ' qtfb_libs = ' ' qtfontdatabase_cflags = ' ' qtfontdatabase_libs = ' ' qtgraphics_cflags = ' ' qtgraphics_libs = ' ' qtplatform_cflags = ' ' qtplatform_libs = ' ' qttheme_cflags = ' ' qttheme_libs = ' ' qtxcbqpa_cflags = ' ' qtxcbqpa_libs = ' ' qt_dbus_cflags = ' ' qt_dbus_includes = ' ' qt_dbus_libs = ' ' qt_includes = ' ' qt_ldflags = ' ' qt_libs = ' ' qt_pie_flags = ' ' qt_select = ' qt5 ' qt_test_cflags = ' ' qt_test_includes = ' ' qt_test_libs = ' ' qt_translation_dir ='/ home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / share / . <repeated> / translations ' ranlib = ' ranlib ' rcc = ' ' readelf ='/ usr / bin / readelf ' reldflags = ' ' rsvg_convert = ' ' sanitizer_cxxflags = ' ' sanitizer_ldflags = ' ' sed ='/ bin / sed ' set_make = ' ' shani_cxxflags = ' - msse4 - msha ' shell ='/ bin / bash ' sse41_cxxflags = ' - msse4 . <number> ' sse42_cxxflags = ' - msse4 . <number> ' strip ='/ usr / bin / strip ' target_darwin_false = ' ' target_darwin_true ='# ' target_windows_false = ' ' target_windows_true ='# ' testdefs = ' ' tiffcp = ' ' uic = ' ' univalue_cflags = ' - i $( srcdir ) / univalue / include ' univalue_libs = ' univalue / libunivalue . la ' use_asm_false ='# ' use_asm_true = ' ' use_lcov_false = ' ' use_lcov_true ='# ' use_qrcode = ' ' use_qrcode_false = ' ' use_qrcode_true ='# ' use_upnp = ' ' version = ' <number> . <number> ' warn_cxxflags = ' ' windres = ' ' words_bigendian_false = ' ' words_bigendian_true ='# ' xgettext = ' ' zmq_cflags = ' - i / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / include ' zmq_libs = ' - l / home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu / lib - lzmq - lpthread - lrt ' ac_ct_ar = ' ' ac_ct_cc = ' ' ac_ct_cxx = ' ' ac_ct_dumpbin = ' ' ac_ct_objcxx = ' ' am__exeext_false = ' ' am__exeext_true ='# ' am__fastdepcc_false ='# ' am__fastdepcc_true = ' ' am__fastdepcxx_false ='# ' am__fastdepcxx_true = ' ' am__fastdepobjcxx_false ='# ' am__fastdepobjcxx_true = ' ' am__include = ' include ' am__isrc = ' ' am__leading_dot ='. ' am__nodep = ' _no ' am__quote = ' ' am__tar = ' $$ { tar - tar } chof - "" $ $ tardir "" ' am__untar = ' $$ { tar - tar } xf - ' ax_pthread_config = ' ' bindir ='$ { exec_prefix } / bin ' build ='x 8 6 _64 - pc - linux - gnu ' build_alias = ' ' build_cpu ='x 8 6 _64 ' build_os = ' linux - gnu ' build_vendor = ' pc ' datadir ='$ { datarootdir } ' datarootdir ='$ { prefix } / share ' docdir ='$ { datarootdir } / doc / ${ package_tarname } ' dvidir ='$ { docdir } ' exec_prefix ='$ { prefix } ' host ='x 8 6 _64 - pc - linux - gnu ' host_alias ='x 8 6 _64 - pc - linux - gnu ' host_cpu ='x 8 6 _64 ' host_os = ' linux - gnu ' host_vendor = ' pc ' htmldir ='$ { docdir } ' includedir ='$ { prefix } / include ' infodir ='$ { datarootdir } / info ' install_sh ='$ { shell } / home / dante / src / bitcoin / build - aux / install - sh ' libdir ='$ { exec_prefix } / lib ' libexecdir ='$ { exec_prefix } / libexec ' localedir ='$ { datarootdir } / locale ' localstatedir ='$ { prefix } / var ' mandir ='$ { datarootdir } / man ' mkdir_p ='$ ( mkdir_p ) ' oldincludedir ='/ usr / include ' pdfdir ='$ { docdir } ' prefix ='/ home / dante / src / bitcoin / depends / x86_64 - pc - linux - gnu ' program_transform_name ='s , x,x , ' psdir ='$ { docdir } ' runstatedir ='$ { localstatedir } / run ' sbindir ='$ { exec_prefix } / sbin ' sharedstatedir ='$ { prefix } / com ' subdirs = ' src / univalue src / secp256k1 ' sysconfdir ='$ { prefix } / etc ' target_alias = ' ' # # - - - - - - - - - - - # # # # confdefs . h . # # # # - - - - - - - - - - - # # /* confdefs . h */ <hashtag> define </hashtag> package_name "" bitcoin core "" <hashtag> define </hashtag> package_tarname "" bitcoin "" <hashtag> define </hashtag> package_version "" <number> . <number> "" <hashtag> define </hashtag> package_string "" bitcoin core <number> . <number> "" <hashtag> define </hashtag> package_bugreport "" <url> <hashtag> define </hashtag> package_url "" <url> <hashtag> define </hashtag> have_cxx11 <number> <hashtag> define </hashtag> stdc_headers <number> <hashtag> define </hashtag> have_sys_types_h <number> <hashtag> define </hashtag> have_sys_stat_h <number> <hashtag> define </hashtag> have_stdlib_h <number> <hashtag> define </hashtag> have_string_h <number> <hashtag> define </hashtag> have_memory_h <number> <hashtag> define </hashtag> have_strings_h <number> <hashtag> define </hashtag> have_inttypes_h <number> <hashtag> define </hashtag> have_stdint_h <number> <hashtag> define </hashtag> have_unistd_h <number> <hashtag> define </hashtag> have_dlfcn_h <number> <hashtag> define </hashtag> lt_objdir "" . libs / "" <hashtag> define </hashtag> use_asm <number> <hashtag> define </hashtag> enable_sse41 <number> <hashtag> define </hashtag> enable_avx2 <number> <hashtag> define </hashtag> enable_shani <number> <hashtag> define </hashtag> have_pthread_prio_inherit <number> <hashtag> define </hashtag> have_pthread <number> <hashtag> define </hashtag> have_decl_strerror_r <number> <hashtag> define </hashtag> have_strerror_r <number> <hashtag> define </hashtag> strerror_r_char_p <number> <hashtag> define </hashtag> have_func_attribute_visibility <number> <hashtag> define </hashtag> have_endian_h <number> <hashtag> define </hashtag> have_byteswap_h <number> <hashtag> define </hashtag> have_stdio_h <number> <hashtag> define </hashtag> have_stdlib_h <number> <hashtag> define </hashtag> have_unistd_h <number> <hashtag> define </hashtag> have_strings_h <number> <hashtag> define </hashtag> have_sys_types_h <number> <hashtag> define </hashtag> have_sys_stat_h <number> <hashtag> define </hashtag> have_sys_select_h <number> <hashtag> define </hashtag> have_sys_prctl_h <number> <hashtag> define </hashtag> have_sys_sysctl_h <number> <hashtag> define </hashtag> have_decl_getifaddrs <number> <hashtag> define </hashtag> have_decl_freeifaddrs <number> <hashtag> define </hashtag> have_decl_strnlen <number> <hashtag> define </hashtag> have_decl_daemon <number> <hashtag> define </hashtag> have_decl_le16toh <number> <hashtag> define </hashtag> have_decl_le32toh <number> <hashtag> define </hashtag> have_decl_le64toh <number> <hashtag> define </hashtag> have_decl_htole16 <number> <hashtag> define </hashtag> have_decl_htole32 <number> <hashtag> define </hashtag> have_decl_htole64 <number> <hashtag> define </hashtag> have_decl_be16toh <number> <hashtag> define </hashtag> have_decl_be32toh <number> <hashtag> define </hashtag> have_decl_be64toh <number> <hashtag> define </hashtag> have_decl_htobe16 <number> <hashtag> define </hashtag> have_decl_htobe32 <number> <hashtag> define </hashtag> have_decl_htobe64 <number> <hashtag> define </hashtag> have_decl_bswap_16 <number> <hashtag> define </hashtag> have_decl_bswap_32 <number> <hashtag> define </hashtag> have_decl_bswap_64 <number> <hashtag> define </hashtag> have_decl___builtin_clz <number> <hashtag> define </hashtag> have_decl___builtin_clzl <number> <hashtag> define </hashtag> have_decl___builtin_clzll <number> <hashtag> define </hashtag> have_malloc_info <number> <hashtag> define </hashtag> have_mallopt_arena_max <number> <hashtag> define </hashtag> have_visibility_attribute <number> <hashtag> define </hashtag> have_thread_local <number> <hashtag> define </hashtag> have_gmtime_r <number> <hashtag> define </hashtag> have_sys_getrandom <number> <hashtag> define </hashtag> have_getentropy <number> <hashtag> define </hashtag> have_getentropy_rand <number> <hashtag> define </hashtag> have_std__system <number> <hashtag> define </hashtag> have_system have_std__system || have_wsystem <hashtag> define </hashtag> have_miniupnpc_miniwget_h <number> <hashtag> define </hashtag> have_miniupnpc_miniupnpc_h <number> <hashtag> define </hashtag> have_miniupnpc_upnpcommands_h <number> <hashtag> define </hashtag> have_miniupnpc_upnperrors_h <number> <hashtag> define </hashtag> have_boost /* */ <hashtag> define </hashtag> have_boost_system /* */ <hashtag> define </hashtag> have_boost_filesystem /* */ <hashtag> define </hashtag> have_boost_thread /* */ <hashtag> define </hashtag> enable_zmq <number> <hashtag> define </hashtag> have_consensus_lib <number> <hashtag> define </hashtag> enable_wallet <number> <hashtag> define </hashtag> use_upnp <number> <hashtag> define </hashtag> client_version_major <number> <hashtag> define </hashtag> client_version_minor <number> <hashtag> define </hashtag> client_version_revision <number> <hashtag> define </hashtag> client_version_build <number> <hashtag> define </hashtag> client_version_is_release true <hashtag> define </hashtag> copyright_year <number> <hashtag> define </hashtag> copyright_holders "" the %s developers "" <hashtag> define </hashtag> copyright_holders_substitution "" bitcoin core "" <hashtag> define </hashtag> copyright_holders_final "" the bitcoin core developers "" configure <number> ` ` ` </details>",2
bitcoin/bitcoin,freebsd <number> build broken ` . / configure - - with - gui = no - - disable - tests - - disable - wallet make = gmake ` ` cxxld bench / bench_bitcoin / usr / local / bin / ld : libbitcoin_util . a ( libbitcoin_util_a - system . o ) : in function ` setupenvironment ( <sad> / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' / usr / local / bin / ld : / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' collect2 : error : ld returned <number> exit status gmake [ <number> <sad> * * * [ makefile : <number> : bitcoin - tx ] error <number> gmake [ <number> <sad> * * * waiting for unfinished jobs . <repeated> / usr / local / bin / ld : libbitcoin_util . a ( libbitcoin_util_a - system . o ) : in function ` setupenvironment ( <sad> / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' / usr / local / bin / ld : / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' collect2 : error : ld returned <number> exit status gmake [ <number> <sad> * * * [ makefile : <number> : bitcoin - cli ] error <number> / usr / local / bin / ld : libbitcoin_util . a ( libbitcoin_util_a - system . o ) : in function ` setupenvironment ( <sad> / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' / usr / local / bin / ld : / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' collect2 : error : ld returned <number> exit status gmake [ <number> <sad> * * * [ makefile : <number> : bitcoind ] error <number> / usr / local / bin / ld : libbitcoin_util . a ( libbitcoin_util_a - system . o ) : in function ` setupenvironment ( <sad> / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' / usr / local / bin / ld : / opt / bitcoin / bitcoin - <number> . <number> / src / util / system . cpp : <number> : undefined reference to ` boost : : filesystem : : path : : imbue ( std : : locale const & ) ' collect2 : error : ld returned <number> exit status gmake [ <number> <sad> * * * [ makefile : <number> : bench / bench_bitcoin ] error <number> gmake [ <number> <sad> leaving directory ' / opt / bitcoin / bitcoin - <number> . <number> / src ' gmake [ <number> <sad> * * * [ makefile : <number> : all - recursive ] error <number> gmake [ <number> <sad> leaving directory ' / opt / bitcoin / bitcoin - <number> . <number> / src ' gmake : * * * [ makefile : <number> error <number> `,2
bitcoin/bitcoin,"how to reproduce - werror = sign - compare failures locally ? i tried gcc and clang , but those failures will not show for me . see <url> while i like the warning and error , if developers can not see them on their boxes , it makes it tedious to fix those issues up .",2
bitcoin/bitcoin,"getmediantimepast : can it be optimized with a simple index - <number> ? i was working on an blockchain indexer and explorer , and as i was trying to figure out how to calculate the returned "" mediantime "" from the "" getblockchaininfo "" , i realized that the value is not what i was expecting it to be . this was in an c # implementation of similar code to bitcoin , so i initially thought it was a bug in the . net implementation . then i had a look at the c + + implementation and to me it appears to be the same . ` ` ` static constexpr int nmediantimespan = <number> ; int64_t getmediantimepast ( ) const { int64_t pmedian [ nmediantimespan ] ; int64_t * pbegin = & pmedian [ nmediantimespan ] ; int64_t * pend = & pmedian [ nmediantimespan ] ; const cblockindex * pindex = this ; for ( int i = <number> ; i < nmediantimespan & & pindex ; i + + , pindex = pindex - > pprev ) *( - - pbegin ) = pindex - > getblocktime ( ); std : : sort ( pbegin , pend ) ; return pbegin [ ( pend - pbegin ) / <number> ]; } ` ` ` from what i can understand , the method loops backwards through the blocks from the current block , populates the block time into an array , sort it ( it ' s already sorted ) , then picks the middle one ( <number> ) and returns that . is that correctly assumed ? would not simply selecting index - <number> do the exact same thing ? of course one would need to calculate for genesis and the first <number> blocks , but that ' s easy . i was expecting this method to do something else , namely take the diff time ( not the timestamp ) between each block , then picking the middle one , which would give the average time between each block for the last <number> blocks . another quick way to calculate , would be take getblocktime from the block <number> indexes back , diff with current block and divide by <number> , should give one type of average . the current code populates an array of <number> timestamps , but it only picks the one in the middle . there is no reason to populate it with <number> timestamps , as the <number> oldest are not used for anything . it also does not use or calculate / care about any of the other blocks . so if the last <number> blocks took hour <number> minutes <number> minutes <number> minutes <number> minutes <number> minutes <number> minutes <number> minutes <number> minutes <number> minutes then getmediantimepast will return a timestamp <number> hour and <number> minutes back in time from current block . i understand median is correct name for the method ( it returns the middle of the sorted numbers ) , but is the numbers to be sorted suppose to be time between blocks ( datediff ) and not the actual timestamps ? the "" mediantime "" returned is also not the time between blocks , it is the time <number> blocks back . which is why i am wondering why not simply grab the datetime of the block <number> indexes back instead ? for for me to show the median time between blocks in the block explorer , i must take ( blocktime - mediantime / <number> ) . from what i know , i think the calculate is used for consensus / mining , and not suppose to be what i am after ( average block time ) , i can calculate that another way , but i stumbled upon this and initially thought it to be something else , and thought it could maybe be improved a bit ? is that the way it is suppose to work , or am i not able to read the code properly ? thanks",2
bitcoin/bitcoin,"running full profiler how can i run a profiler to get the timings on each function of the overall executable ? looking to find out which functions are running slow , and to look more into them .",2
bitcoin/bitcoin,"bip <number> status i am not sure where to put this issue , anyway reading bip <number> , that ' s active in the protocol , i see it has a status reported as draft . for someone like me that ' s trying to implement the protocol following the specification , it ' s already difficult to dodge the official dev documentation ` <url> with its dated ( and even wrong ) informations and reading bips it ' s not clear if a bip is included or not for sure in a protocol without checking the code . i ' d expect official bips having at least a correct status reported , but as just said , bip <number> is reported as draft that sounds confusing ( and probably wrong ) [ image ] ( <url> as a side note , what ' s the most updated reference ( not mentioning "" the code "" ) currently available ? thanks",2
bitcoin/bitcoin,"osx mojave ( <number> . <number> ) build fails compiling univalue git pull origin - go in to . / src / univalue . / autogen . sh . / configure make make clean and two different coins attempted with same results . osx <number> . <number> ( mojave ) please advise if this is incompatibility or if i am missing something <happy> gcc - v configured with : - - prefix <annoyed> applications / xcode . app / contents / developer / usr - - with - gxx - include - dir <annoyed> library / developer / commandlinetools / sdks / macosx . sdk / usr / include / c + + / <number> . <number> apple clang version <number> . <number> ( clang - <number> . <number> ) target : x86_64 - apple - darwin18 . <number> thread model : posix installeddir : / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin full output : . / autogen . sh glibtoolize : putting auxiliary files in ac_config_aux_dir , ' build - aux ' . glibtoolize : copying file ' build - aux / ltmain . sh ' glibtoolize : putting macros in ac_config_macro_dirs , ' build - aux / m4 ' . glibtoolize : copying file ' build - aux / m4 / libtool . m4 ' glibtoolize : copying file ' build - aux / m4 / ltoptions . m4 ' glibtoolize : copying file ' build - aux / m4 / ltsugar . m4 ' glibtoolize : copying file ' build - aux / m4 / ltversion . m4 ' glibtoolize : copying file ' build - aux / m4 / lt ~ obsolete . m4 ' . / configure configure . ac : <number> : installing ' build - aux / compile ' configure . ac : <number> : installing ' build - aux / config . guess ' configure . ac : <number> : installing ' build - aux / config . sub ' configure . ac : <number> : installing ' build - aux / install - sh ' configure . ac : <number> : installing ' build - aux / missing ' makefile . am : installing ' build - aux / depcomp ' parallel - tests : installing ' build - aux / test - driver ' machecking whether make supports nested variables . <repeated> yes checking for a bsd - compatible install . <repeated> / usr / bin / install - c checking whether build environment is sane . <repeated> yes kchecking for a thread - safe mkdir - p . <repeated> build - aux / install - sh - c - d checking for gawk . <repeated> no checking for mawk . <repeated> no checking for nawk . <repeated> no checking for awk . <repeated> awk checking whether make sets $( make ) . <repeated> yes echecking build system type . <repeated> x86_64 - apple - darwin18 . <number> checking host system type . <repeated> x86_64 - apple - darwin18 . <number> checking how to print strings . <repeated> printf checking whether make supports the include directive . <repeated> yes ( gnu style ) checking for gcc . <repeated> gcc checking whether the c compiler works . <repeated> yes checking for c compiler default output file name . <repeated> a . out checking for suffix of executables . <repeated> checking whether we are cross compiling . <repeated> no checking for suffix of object files . <repeated> o checking whether we are using the gnu c compiler . <repeated> yes checking whether gcc accepts - g . <repeated> yes checking for gcc option to accept iso c89 . <repeated> none needed checking whether gcc understands - c and - o together . <repeated> yes checking dependency style of gcc . <repeated> gcc3 checking for a sed that does not truncate output . <repeated> / usr / bin / sed checking for grep that handles long lines and - e . <repeated> / usr / bin / grep checking for egrep . <repeated> / usr / bin / grep - e checking for fgrep . <repeated> / usr / bin / grep - f checking for ld used by gcc . <repeated> / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld checking if the linker ( / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld ) is gnu ld . <repeated> no checking for bsd - or ms - compatible name lister ( nm ) . <repeated> / usr / local / bin / nm - b checking the name lister ( / usr / local / bin / nm - b ) interface . <repeated> bsd nm checking whether ln - s works . <repeated> yes checking the maximum length of command line arguments . <repeated> <number> checking how to convert x86_64 - apple - darwin18 . <number> file names to x86_64 - apple - darwin18 . <number> format . <repeated> func_convert_file_noop checking how to convert x86_64 - apple - darwin18 . <number> file names to toolchain format . <repeated> func_convert_file_noop checking for / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld option to reload object files . <repeated> - r checking for objdump . <repeated> objdump checking how to recognize dependent libraries . <repeated> pass_all checking for dlltool . <repeated> dlltool checking how to associate runtime and link libraries . <repeated> printf %s \ \ n checking for ar . <repeated> ar checking for archiver <user> support . <repeated> @ checking for strip . <repeated> strip checking for ranlib . <repeated> ranlib checking command to parse / usr / local / bin / nm - b output from gcc object . <repeated> ok checking for sysroot . <repeated> no checking for a working dd . <repeated> / bin / dd checking how to truncate binary pipes . <repeated> / bin / dd bs = <number> count = <number> checking for mt . <repeated> no checking if : is a manifest tool . <repeated> no checking for dsymutil . <repeated> dsymutil checking for nmedit . <repeated> nmedit checking for lipo . <repeated> lipo checking for otool . <repeated> otool checking for otool64 . <repeated> no checking for - single_module linker flag . <repeated> yes checking for - exported_symbols_list linker flag . <repeated> yes checking for - force_load linker flag . <repeated> no checking how to run the c preprocessor . <repeated> gcc - e checking for ansi c header files . <repeated> yes checking for sys / types . h . <repeated> yes checking for sys / stat . h . <repeated> yes checking for stdlib . h . <repeated> yes checking for string . h . <repeated> yes checking for memory . h . <repeated> yes checking for strings . h . <repeated> yes checking for inttypes . h . <repeated> yes checking for stdint . h . <repeated> yes checking for unistd . h . <repeated> yes checking for dlfcn . h . <repeated> yes checking for objdir . <repeated> . libs checking if gcc supports - fno - rtti - fno - exceptions . <repeated> yes checking for gcc option to produce pic . <repeated> - fno - common - dpic checking if gcc pic flag - fno - common - dpic works . <repeated> yes checking if gcc static flag - static works . <repeated> no checking if gcc supports - c - o file . o . <repeated> yes checking if gcc supports - c - o file . o . <repeated> ( cached ) yes checking whether the gcc linker ( / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld ) supports shared libraries . <repeated> yes checking dynamic linker characteristics . <repeated> darwin18 . <number> dyld checking how to hardcode library paths into programs . <repeated> immediate checking whether stripping libraries is possible . <repeated> yes checking if libtool supports shared libraries . <repeated> yes checking whether to build shared libraries . <repeated> yes checking whether to build static libraries . <repeated> yes checking for g + + . <repeated> g + + checking whether we are using the gnu c + + compiler . <repeated> yes checking whether g + + accepts - g . <repeated> yes checking dependency style of g + + . <repeated> gcc3 checking how to run the c + + preprocessor . <repeated> g + + - e checking for ld used by g + + . <repeated> / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld checking if the linker ( / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld ) is gnu ld . <repeated> no checking whether the g + + linker ( / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld ) supports shared libraries . <repeated> yes checking for g + + option to produce pic . <repeated> - fno - common - dpic checking if g + + pic flag - fno - common - dpic works . <repeated> yes checking if g + + static flag - static works . <repeated> no checking if g + + supports - c - o file . o . <repeated> yes checking if g + + supports - c - o file . o . <repeated> ( cached ) yes checking whether the g + + linker ( / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / ld ) supports shared libraries . <repeated> yes checking dynamic linker characteristics . <repeated> darwin18 . <number> dyld checking how to hardcode library paths into programs . <repeated> immediate checking that generated files are newer than configure . <repeated> done configure : creating . / config . status config . status : creating makefile config . status : creating pc / libunivalue . pc config . status : creating pc / libunivalue - uninstalled . pc config . status : creating univalue - config . h config . status : executing depfiles commands config . status : executing libtool commands make / applications / xcode . app / contents / developer / usr / bin / make all - am cxx test / object - object . o cxx lib / libunivalue_la - univalue . lo cxx lib / libunivalue_la - univalue_get . lo cxx lib / libunivalue_la - univalue_read . lo cxx lib / libunivalue_la - univalue_write . lo cxxld libunivalue . la ar : ` u ' modifier ignored since ` d ' is the default ( see ` u ' ) cxxld test / object * * ld : warning : ignoring file . / . libs / libunivalue . a , building for macos - x86_64 but attempting to link with file built for unknown - unsupported file format ( 0x 2 1 0x3 c 0x 6 1 0x 7 2 0x 6 3 0x 6 8 0x3 e 0x0 a 0x 2 f 0x 2 0 0x 2 0 0x 2 0 0x 2 0 0x 2 0 0x 2 0 0x 2 0 ) undefined symbols for architecture x86_64 : * * "" univalue : : push_backv ( std : : __1 : : vector < univalue , std : : __1 : : allocator <univalue> > const & ) "" , referenced from : univalue_array ( ) in object - object . o "" univalue : : read ( char const * , unsigned long ) "" , referenced from : univalue_typecheck ( ) in object - object . o univalue_readwrite ( ) in object - object . o "" univalue : : clear ( ) "" , referenced from : univalue_set ( ) in object - object . o univalue_array ( ) in object - object . o univalue_object ( ) in object - object . o "" univalue : : pushkv ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , univalue const & ) "" , referenced from : univalue_object ( ) in object - object . o univalue : : pushkv ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , char const <wink> in object - object . o "" univalue : : setint ( long long ) "" , referenced from : univalue_set ( ) in object - object . o univalue_object ( ) in object - object . o univalue : : univalue ( long long ) in object - object . o univalue : : univalue ( int ) in object - object . o "" univalue : : setint ( unsigned long long ) "" , referenced from : univalue_set ( ) in object - object . o univalue : : univalue ( unsigned long long ) in object - object . o "" univalue : : setstr ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & ) "" , referenced from : univalue_set ( ) in object - object . o univalue_array ( ) in object - object . o univalue : : univalue ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & ) in object - object . o univalue : : univalue ( char const <wink> in object - object . o "" univalue : : pushkvs ( univalue const & ) "" , referenced from : univalue_object ( ) in object - object . o "" univalue : : setbool ( bool ) "" , referenced from : univalue_typecheck ( ) in object - object . o univalue_set ( ) in object - object . o univalue : : univalue ( bool ) in object - object . o "" univalue : : setnull ( ) "" , referenced from : univalue_set ( ) in object - object . o "" univalue : : __pushkv ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , univalue const & ) "" , referenced from : univalue_object ( ) in object - object . o "" univalue : : setarray ( ) "" , referenced from : univalue_set ( ) in object - object . o "" univalue : : setfloat ( double ) "" , referenced from : univalue_set ( ) in object - object . o univalue : : univalue ( double ) in object - object . o "" univalue : : push_back ( univalue const & ) "" , referenced from : univalue_array ( ) in object - object . o univalue : : push_back ( char const <wink> in object - object . o "" univalue : : setnumstr ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & ) "" , referenced from : univalue_constructor ( ) in object - object . o univalue_typecheck ( ) in object - object . o univalue_set ( ) in object - object . o "" univalue : : setobject ( ) "" , referenced from : univalue_set ( ) in object - object . o univalue_object ( ) in object - object . o "" univalue : : checkobject ( std : : __1 : : map < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > , univalue : : vtype , std : : __1 : : less < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > > , std : : __1 : : allocator < std : : __1 : : pair < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const , univalue : : vtype > > > const & ) const "" , referenced from : univalue_object ( ) in object - object . o "" univalue : : write ( unsigned int , unsigned int ) const "" , referenced from : univalue_readwrite ( ) in object - object . o "" univalue : : findkey ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , unsigned long & ) const "" , referenced from : univalue_object ( ) in object - object . o "" univalue : : getkeys ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_int ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_obj ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_str ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_bool ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_real ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : getobjmap ( std : : __1 : : map < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > , univalue , std : : __1 : : less < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > > , std : : __1 : : allocator < std : : __1 : : pair < std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const , univalue > > > & ) const "" , referenced from : univalue_object ( ) in object - object . o "" univalue : : getvalues ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_array ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : get_int64 ( ) const "" , referenced from : univalue_typecheck ( ) in object - object . o "" univalue : : operator [ ] ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & ) const "" , referenced from : univalue_object ( ) in object - object . o univalue_readwrite ( ) in object - object . o "" univalue : : operator [ ] ( unsigned long ) const "" , referenced from : univalue_array ( ) in object - object . o univalue_readwrite ( ) in object - object . o ld : symbol ( s ) not found for architecture x86_64 clang : error : linker command failed with exit code <number> ( use - v to see invocation ) make [ <number> <sad> * * * [ test / object ] error <number> make [ all ] error <number>",2
bitcoin/bitcoin,"[ question ] compiling only bitcoind , not others . [ question ] hi <happy> i am working on bitcoind to write something to test locally . when i type ` ` ` make ` ` ` in the working directory ( top directory ) , the compile process ( code ) which is written in makefile is very long time because others are compiled with . so this is time - consuming in my current work . * * i ' d like to compile only bitcoind * * . can i get some guide ? thanks .",2
bitcoin/bitcoin,"unable to start bitcoind after power failure hi , i am unable to restart bitcoind after a power failure : here is the debug . log error i am getting : > bitcoin <user> : ~ $ tail - f / home / bitcoin / . bitcoin / debug . log > <number> - <number> - 1 1 t <time> z opening leveldb in / home / bitcoin / . bitcoin / chainstate > <number> - <number> - 1 1 t <time> z fatal leveldb error : corruption : current file does not end with newline > <number> - <number> - 1 1 t <time> z you can use - debug = leveldb to get more complete diagnostic messages > <number> - <number> - 1 1 t <time> z fatal leveldb error : corruption : current file does not end with newline > <number> - <number> - 1 1 t <time> z : error opening block database . > please restart with - reindex or - reindex - chainstate to recover . > <number> - <number> - 1 1 t <time> z aborted block database rebuild . exiting . > <number> - <number> - 1 1 t <time> z shutdown : in progress . <repeated> > <number> - <number> - 1 1 t <time> z scheduler thread interrupt > <number> - <number> - 1 1 t <time> z shutdown : done i have tried running bitcoind - reindex bitcoin - reindex - chainstate ` ` ` not sure what i am doing wrong ?",2
bitcoin/bitcoin,"missing entry to install bitcoin setup is not successfull on a clean ubuntu system following the install instructions <url> * * expected behavior * * i would expect that installation to complete and that my setup is able to build bitcoin * * actual behavior * * apparently ` . / configure ` gave this error : "" configure : error : libdb_cxx headers missing , bitcoin core requires this library for wallet functionality ( - - disable - wallet to disable wallet functionality ) "" . next ` make ` does not work ` ` ` make make no targets specified and no makefile found . stop . * * system information * * ` ` ` * * to reproduce * * < - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > the bitcoin version this happens is b54666c849bad258d92d6d1e45a051d36055681e * * fix * * there are many solutions specified in <url> but for me the woking one was the comment <url> from <user> ` ` ` sudo add - apt - repository ppa : bitcoin / bitcoin sudo apt - get update sudo apt - get install libdb4 . <number> - dev libdb4 . <number> + + - dev ` ` ` so i guess this could be added to the ubuntu section of <url> ?",2
bitcoin/bitcoin,"wallet & decoderawtransaction should show rbf status the ` decodepsbt ` and ` decoderawtransaction ` rpc calls return the ` sequence ` for each input , but i ' d rather not have to manually determine if that implies rbf . i mainly care about this for mempool transactions . cc <user> <user>",2
bitcoin/bitcoin,"serialization - deserialization roundtrip of cpubkey does not necessarily result in an equal object when fuzzing the serialization code for ` cpubkey ` i noticed that ` deserialize <cpubkey> ( serialize ( obj ) ) = = obj ` does not hold true for all ` cpubkey obj ` . is there any reason to why ` cpubkey ` is deviating from the other serializable classes ( for which equality is defined ) in this regard ? <happy> context template < typename t > cdatastream serialize ( const t & obj ) { cdatastream ds ( ser_network , init_proto_version ) ; ds < < obj ; return ds ; } template < typename t > t deserialize ( cdatastream ds ) { t obj ; ds > > obj ; return obj ; } ` ` `",2
bitcoin/bitcoin,"error : connect econnrefused v0 . <number> my config is below , the bitcoin core version is v0 . <number> ` ` ` txindex = <number> server = <number> rpcuser = username rpcpassword = password rpcallowip = the ip i connect rpcport = port ` ` ` but i got error ` error econnrefused `",2
bitcoin/bitcoin,"no addresses returned by getnodeaddresses rpc call # # the issue i am attempting to use the ` getnodeaddresses ` rpc call to return all addresses known by a node in a docker network . i am using the ` regtest ` environment . the rpc call returns an empty array although nodes have been added on the referent node using the ` add ` rpc call . # # expected behavior < - - - what behavior did you expect ? - - > the ` getnodeaddresses ` rpc call should return nodes added via the ` add ` rpc call . # # actual behavior < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > no addresses are returned , i . e . , an empty array is returned : ` ` ` bitcoin - cli - regtest - - datadir <annoyed> root / . bitcoin / getnodeaddresses <number> [ ] ` ` ` # # reproduction < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > setup a regtest network of <number> nodes . no mining has been done , and the ` ~ / . bitcoin ` folder contains only ` bitcoin . conf ` with the following content : ` ` ` [ regtest ] regtest = <number> server = <number> rpcport = <number> port = <number> ` ` ` connect to the referent node and run the command : ` bitcoin - cli - regtest - - datadir <annoyed> root / . bitcoin / addnode "" $p : <number> "" add ` where ` p ` is the other node address for every node the referent node wants to add . run the commands on the referent node : ` ` ` bitcoin - cli - regtest - - datadir <annoyed> root / . bitcoin / getconnectioncount <number> bitcoin - cli - regtest - - datadir <annoyed> root / . bitcoin / getpeerinfo [ { "" id "" : <number> , "" addr "" : "" <number> . <number> . <time> <number> "" , # cut out part of response { "" id "" : <number> , "" addr "" : "" <number> . <number> . <time> <number> "" , "" addrbind "" : "" <number> . <number> : <number> "" , "" services "" : "" 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 d "" , "" relaytxes "" : true , "" lastsend "" : <phone> , "" lastrecv "" : <phone> , "" bytessent "" : <number> , "" bytesrecv "" : <number> , "" conntime "" : <phone> , "" timeoffset "" : <number> , "" pingtime "" : <number> , "" minping "" : <number> , "" version "" : <number> , "" subver "" : "" / satoshi : <number> . <number> / "" , "" inbound "" : true , "" addnode "" : false , "" startingheight "" : <number> , "" banscore "" : <number> , "" synced_headers "" : - <number> , "" synced_blocks "" : - <number> , "" inflight "" : [ ] , "" whitelisted "" : false , "" minfeefilter "" : <number> , "" bytessent_per_msg "" : { "" feefilter "" : <number> , "" ping "" : <number> , "" pong "" : <number> , "" sendcmpct "" : <number> , "" sendheaders "" : <number> , "" verack "" : <number> , "" version "" : <number> } , "" bytesrecv_per_msg "" : { "" feefilter "" : <number> , "" getaddr "" : <number> , "" ping "" : <number> , "" pong "" : <number> , "" sendcmpct "" : <number> , "" sendheaders "" : <number> , "" verack "" : <number> , "" version "" : <number> } } ] bitcoin - cli - regtest - - datadir <annoyed> root / . bitcoin getnodeaddresses <number> [ ] ` ` ` # # bitcoin core version < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > ` ` ` bitcoin - cli - version bitcoin core rpc client version v0 . <number> . <number> - g2472733a24a9364e4c6233ccd04166a26a68cc65 bitcoind - version bitcoin core daemon version v0 . <number> . <number> - g2472733a24a9364e4c6233ccd04166a26a68cc65 ` ` ` installed via the repository ` ppa : bitcoin / bitcoin ` # # os < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > docker container info : ` ` ` lsb_release - a no lsb modules are available . distributor id : ubuntu description : ubuntu <number> . <number> lts release : <number> codename : bionic ` ` ` host info : ` ` ` lsb_release - a no lsb modules are available . distributor id : ubuntu description : ubuntu <number> . <number> lts release : <number> codename : bionic cat / proc / cpuinfo | grep ' name ' | uniq model name core ( tm ) i7 - 4 6 0 0 m cpu @ <number> . 9 0 ghz ` ` ` <number> tb ssd disk",2
bitcoin/bitcoin,"entity too large client error : post < public ip address removed > resulted in a <number> request entity too large response got error when using decoderawtransaction . i already set the post_max_size = 2 g and upload_max_filesize = 2 g in php . ini . in apache , i already set the limitrequestbody <number> . where do i need to change in order to pass this error ? thank you .",2
bitcoin/bitcoin,"btc <number> node why can not docker use rpc commands directly through mount ports , as in previous versions ? does bitcoin . conf need special configuration ?",2
bitcoin/bitcoin,"[ feature ] get utxos for non - wallet addresses hey team , adding a feature request here for the ability to query all utxos for a given address ( s ) regardless if it ' s in the nodes wallet or not . i understand the reasoning here of why it ' s not included out - of - the - box as it could add a significant amount of overhead to the core requirements of running the node . however , it ' s fairly concerning that the most of the articles about how to get this functionality just point to using an external api to fetch this info . tutorials of how to get this functionality on the developer side are also painful when just trying to get this fairly common feature up and running . is not part of the point of running a full node to be able to get anything you need without the reliance of a third party ? "" do not trust , verify "" i would imagine the best implementation for this would be to add a flag in the ` bitcoin . conf ` of something like ` indexaddrs = <number> ` or something like that . anyway , just putting the idea out there . thanks",2
bitcoin/bitcoin,"cannot create low fee transaction via rpc * issue <emphasis> * we are running bitcoind + lnd on a raspberry pi with several other pieces of software . because we want bitcoind to be really light weight , we set the maxmempool to be <number> mb . we do not really care about having mempool data itself , but we do care about being able to create transactions and broadcast those transactions to the network . the problem we have been running into is that when the mempool is constricted like this ( <number> mb limit ) , mempoolminfee seams to start out at <number> at boot time , but then gets larger the longer bitcoind is online . we typically create transactions at <number> sat_per_byte because we do not care about them being validated immediately . the problem is we are getting this ` [ lncli ] rpc error : code = unknown desc = - <number> min fee not met , <number> < <number> ( code <number> ) ` . this error is of course being propagated by bitcoind . we set ` mintxfee ` to ` <number> ` and even ` minrelaytxfee ` to ` <number> ` , but neither seem to stop mempoolminfee from increasing over time . is there anyway to guarantee these transactions always get inserted into the mempool and broadcasted ? i did some research and it looks like the mempool operates like that to avoid dos attacks . but , should not rpc calls bypass these ddos issues . it makes sense that rpc calls should always work . * * expected behavior * * there are two appropriate ways i think this could be handled . <number> . creating a transaction via rpc should always be accepted by the local mempool and broadcasted to peers that will accept that feerate . <number> . creating a transaction via rpc should always be broadcasted to peers that will accept that feerate . * * actual behavior * * the rpc call fails with an error message similar to this ` mempool min fee not met , <number> < <number> ` * environment <emphasis> * we compile our own docker containers . important lines from the docker file are below . we are also using lnd <number> . <number> . ` ` ` from ubuntu : <number> arg version = <number> . <number> env binary_link <url> ` ` ` * * machine info * * this specifically is happening on a raspberry pi 3 b + . but , i believe it is not machine based .",2
bitcoin/bitcoin,"bitcoin . conf assumevalid = <number> does not seem to be working < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > assumevalid = <number> does not seem to work when it is in the bitcoin . conf file but does when set as a cli argument . < ! - - - what behavior did you expect ? - - > i thought setting this in the config file would be enough to set this value . < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > when set in the config file , the log says it is still using the default hex value . it does work when set as a command line argument . ! [ screenshot from <number> - <number> - <number> <date> ] ( <url> ! [ screenshot from <number> - <number> - <number> <date> ] ( <url> ! [ screenshot from <number> - <number> - <number> <date> ] ( <url> < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > just setting the config option without the command line argument recreates the issue . i did test my config file by removing the dbcache setting and that was reflected in the logs . ! [ screenshot from <number> - <number> - <number> <date> ] ( <url> ! [ screenshot from <number> - <number> - <number> <date> ] ( <url> < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > i am using the downloaded release version <number> . < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > i am using a laptop with ubuntu and a platter hard drive . < ! - - for the gui - related issue on linux provide names and versions of a distro , a desktop environment and a graphical shell ( if relevant ) . - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"how to compile and debug the latest version of bitcoin source code with visual studio <number> or visual studio <number> hello , everyone , my question is how to compile and debug the latest version of bitcoin source code with visual studio <number> or visual studio <number>",2
bitcoin/bitcoin,"getnewaddress always returns same type < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > < ! - - - what behavior did you expect ? - - > when i run "" getnewaddress legacy "" i expect to get an address starting with a "" <number> "" . when i run "" getnewaddress p2sh - segwit "" i expect to get an address starting with a "" <number> "" . when i run "" getnewaddress bech32 "" i expect to get an address starting with "" bc1 "" . < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > i always get an address starting a "" <number> "" . another user i know always gets an address starting with a "" <number> "" . ! [ getnewaddress - screenshot ] ( <url> < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > this is reproducible both from command line running bitcoind , and also bitcoin - qt . from bitcoind , i type getnewaddress legacy or from bitcoin - qt i issue the commands from the debug window console , as shown in the screenshot above . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > running version <number> downloaded from <url> < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > i am running on macos mojave version <number> . <number> , and i always get an address starting with "" <number> "" . the other user i know runs ubuntu , and always gets an address starting with a "" <number> "" . < ! - - for the gui - related issue on linux provide names and versions of a distro , a desktop environment and a graphical shell ( if relevant ) . - - > n / / a < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",2
bitcoin/bitcoin,"transaction fee calculated is only <percent> of what it ought to be the transaction fee ; recommended ; only seems to be calculating <percent> of what it should be and this is causing transactions to hang or not get processed in a timely manner . i have spent three lots of coins from my bitcoin core wallet . in each case the fee has been set to automatically calculate . i do not believe i changed the parameter since installation and it is set to confirmation target time of <number> minutes , which , when initially writing this , was supposed to be <number> btc / kb . three transactions - lastly a transaction size of <number> bytes ( <number> kbytes ] the expected fee from two weeks ago , based on the <number> btc / kb , should be about <number> . however the fee charged is only about <percent> of this <number> btc , seemingly out by a factor of <number> . it is now confirmed but attempting to accelerate on pool . viabtc . com / tools / txaccelerator / failed because the fee was too low secondly - a similar small fee of <number> btc was applied on a previous transaction two weeks earlier . i have no idea of what the calculation btc / kb would have then been . and thirdly - a transaction another week earlier had a fee calculated of <number> btc or <number> btc / kb , i again have no idea of what the btc / kb was at that time , but this is also seemingly out be a factor of <number> or <number> . if the calculation is out , using mbtc instead of satoshis or some such thing , then i worry any sudden change could adversely impact people who have identified the bug and have manually set their btc / kb value to <number> times what it ought to be ! <repeated> any workaround setting the btc / kb value manually is really worrisome for anyone not using wallets regularly , i would worry i could pay <number> time more than i ought to !",2
bitcoin/bitcoin,"how to get onion addresses for seeds thank you <user> however , your list does not have onion addresses at all _originally posted by <user> in <url> i have no idea why . my comandline is - h dnsseed . emzy . de - n node3 . emzy . de - m emzy . emzy . de - p <number>",2
bitcoin/bitcoin,"bitcoin - cli does ' nt have "" getnewaddress "" method i have installed bitcoin core running on my local machine . i want to create a new address using bitcoin - cli , executing a command as follows then an error message was displayed . how can i solve it ? ` ` ` $ bitcoin - cli getnewaddress error code : - <number> error message not found ` ` ` the version of bitcoin - core is as follows . ` ` ` $ bitcoin - cli - version bitcoin core rpc client version v0 . <number> . <number> - 7 4 2 f7dd97 ` ` ` the machine running bitcoin core is virtual machine on virtualbox on windows7 . the guest os is ubuntu server and version is as follows . ` ` ` $ uname - a linux ubuntu <number> . <number> - <number> - generic # <number> - ubuntu smp tue <date> <time> utc <number> x86_64 x86_64 x86_64 gnu / linux ` ` `",2
bitcoin/bitcoin,"why bitcoin core is a monolith ? i do not understand why bitcoin core software was designed as a monolith and not as a microservices architecture . now the node , the gui , the cli , the wallet are all in one . for example , some people do not need the gui , others do not need the cli , or others do not need the wallet . for example , some people can install a cli , set the node , and use it remotely , or some people can be take a wallet with a remote node . so my question is , why the core team deceided a monolith architecture ? i do not find any reasonable fact for do this .",2
bitcoin/bitcoin,"transaction broadcasting failed cont : when i try to broadcast a raw hex transaction - all look nice , bitcoin - cli return a transaction id for the sended hex data , but i looking on blockchain explorer and nohing happens . a my txid not found , likely i dont have no one peers that known about it . in my debug log i not seen no one rejects for sendedded transaction , only inv \ \ getdata \ \ tx messagesd bitcoin core version : v0 . <number> . <number> - 7 2 bd4ab86 - dirty os <number> [ tx . zip ] ( <url>",2
bitcoin/bitcoin,"[ windows , mingw ] error windres - f invalid option having the hardest time figuring out this issue for some reason it seems like its trying to pass - f option to windres during the check ` <user> - f $( windres ) ` i dont see where - f is being passed to windres at all , i know i know i could just build it on linux but i am trying to bring back an old project ( easywinbuilder ) and this seems to be my last issue im really just stuck with no solution . <repeated> [ image ] ( <url> [ config . log ] ( <url>",2
bitcoin/bitcoin,"rpc : connection refused i see that there has been some configuration updates which is why i am sure i am getting a connection refused . i tried searching for an updated bitcoin . conf example but could not find anything . this is the config throwing the error rpcpassword = pass rpcport = <number> rpcthreads = <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> rpcallowip = <number> . <number> maxconnections = <number> daemon = <number> gen = <number> alertnotify = echo %s | mail - s "" bitcoin alert "" <email> blocknotify = blocknotify . sh port <number> %s",2
bitcoin/bitcoin,"question : miner_tests . cpp with new data hi bitcoin devs : smiley : <url> ` ` ` cpp { <number> , 0 xa4a3e223 } , { <number> , 0x 1 5 c32f9e } , { <number> , 0x0 3 7 5 b547 } , { <number> , 0x 7 0 0 4 a8a5 } , { <number> , 0 xce440296 } , { <number> , 0x 5 2 cfe198 } , { <number> , 0x 7 7 a72cd0 } , { <number> , 0 xbb5d6f84 } , { <number> , 0x 8 3 f30c2c } , { <number> , 0x 4 8 a73d5b } , { <number> , 0 xef7dcd01 } , { <number> , 0x 6 8 0 9 c6c4 } , { <number> , 0x0 8 8 3 ab3c } , { <number> , 0x0 8 7 bbbe2 } , { <number> , 0x 2 1 0 4 a814 } , { <number> , 0 xdffb6daa } , ` ` ` i want to do ` miner_tests ` with new data . could you tell me how to create new one ? ` blockinfo ` i would especially like to check blockchain data after segwit . this data is maybe outdated . <repeated> : thinking : if i can , i would like to reinforce the test data and contribute to this project . : smile tried a lot . but i could not get the ` extranonce ` : sob :",2
bitcoin/bitcoin,"bitcoin - cli console walletpassphrase can not unlock wallet password with some special characters will not work bitcoin - cli console walletpassphrase can not unlock wallet password with some special characters will not work < - - describe the issue - - > i am trying unlock my wallet by console using : * walletpassphrase <emphasis> * , my problem is in my password i have a lot special characters , like : * * ` ^ "" \ \ ' @ - #& %* ; _ * * i tried include * *\\ * * before the special characters and did not help . can you guys share for me what is considered be a special characters here in bitcoin cli ? as well could guys give a sample of solve this problem here ? the commnand i used to open the wallet for <number> minutes was "" ` ^ "" \ \ ' @ - #& %* ; _ "" <number> * * so my question is how to fix this problem and make my wallet understand what is special characters and what is not . thank you .",2
bitcoin/bitcoin,"can not use signrawtransaction in the new version after i update the latest version of bitcoin core ( <number> ) , i can not use "" signrawtransaction "" to signe my tx . my cmd is this ` ` ` bitcoin - cli - rpcuser = x <elongated> - rpcpassword = x <elongated> - rpcport = x <elongated> signrawtransaction \ \ 0 2 0 0 0 0 0 0 0 1 9 ea1e1eb2b6d8fca57bb99e5747f2e3446b7aac64c1147f389c025d0af155a9e <phone> ffffffff02a00f00000000000017a914bee24fd1587112a9ab41b202fc2c01e1de59b47e87a87811000000000017a91477c56ac5820aaa6c667cc4856baad24e462a46748700000000 ` ` ` the log is ` ` ` error code : - <number> error message was removed in v0 . <number> . clients should transition to using signrawtransactionwithkey and signrawtransactionwithwallet ` ` `",2
bitcoin/bitcoin,fix virtual size limit enforcement in transaction package context ( alternative ) minimal subset of <url> to replace max_package_size with max_package_weight which accounts for additional wu necessary to not exclude default chain limit transactions that would have been accepted individually . avoids sigops vbyte confusion . <number> ) pass correct vsize to chain limit evaluations in package context <number> ) stop overly - large packages that have no existing mempool ancestors ( also a bugfix by itself if someone sets non - standard chain limits ) this should fix the known issues while not blocking additional refactoring later .,0
bitcoin/bitcoin,"validation coins disappearing mid - package evaluation while we are evaluating a package , we split it into "" subpackages "" for evaluation ( currently subpackages all have size <number> except the last one ) . if a subpackage has size <number> , we may add a tx to mempool and call ` limitmempoolsize ( ) ` , which evicts transactions if the mempool gets full . we handle the case where the just - submitted transaction is evicted immediately , but we do not handle the case in which a transaction from a previous subpackage ( either just submitted or already in mempool ) is evicted . mainly , since the coins created by the evicted transaction are cached in ` m_view ` , we do not realize the utxo has disappeared until ` checkinputsfrommempoolandcache ` asserts that they exist . also , the returned ` packagemempoolacceptresult ` reports that the transaction is in mempool even though it is not anymore . fix this by not calling ` limitmempoolsize ( ) ` until the very end , and editing the results map with "" mempool full "" if things fall out . pointed out by instagibbs in <url> on top of the v3 pr .",0
bitcoin/bitcoin,"rpc : signed - integer - overflow in analyzepsbt [ "" estimated_feerate "" ] # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour crash / ub in <url> # # # expected behaviour no crash # # # steps to reproduce * compile with ubsan * ` ubsan_options = "" suppressions =$( pwd ) / test / sanitizer_suppressions / ubsan : print_stacktrace = <number> : halt_on_error = <number> : report_error_type = <number> "" . / src / qt / bitcoin - qt ` * ` analyzepsbt chnidp8backgicagaaegicagip8dabygicagicagicagicagicagicagicagicagiaaa ` # # # relevant log output ` ` ` # <number> 0x 5 5 a94d97befd in cfeerate : : getfee ( unsigned int ) const src / policy / feerate . cpp : <number> : <number> # <number> 0x 5 5 a94d4648ca in cfeerate : : getfeeperk ( ) const src / . / policy / feerate . h : <number> <time> # <number> 0x 5 5 a94d4648ca in analyzepsbt ( <sad> : $ _13 : : operator ( ) ( rpchelpman const & , jsonrpcrequest const & ) const src / rpc / rawtransaction . cpp : <number> : <number> . <repeated> summary : undefinedbehaviorsanitizer policy / feerate . cpp : <number> : <number> in ` ` ` # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? current master # # # operating system and version linux # # # machine specifications _no response_",0
bitcoin/bitcoin,"fuzz timeout in calculatetotalbumpfees the slow fuzz seed described in # <number> was just slower than expected , not an endless loop . ensuring that every anscestor is only processed once speeds up the termination of the graph traversal . fixes # <number>",0
bitcoin/bitcoin,"fuzz : improve ` coinselection ` this pr : - moves coin creation to its own function called ` createcoins ` . - add coverage for ` eligibleforspending ` - add coverage for ` addinputs ` : get result of each algorithm ( srd , knapsack and bnb ) , call ` createcoins ` and add into them . - add coverage for ` getshuffledinputvector ` and ` getinputset ` using the result of each algorithm ( srd , knapsack and bnb ) . - add coverage for ` merge ` srd with the new utxos and , if successful , try to merge with the previous srd result .",0
bitcoin/bitcoin,"bitcoin core compiled with ubuntu focal gcc - <number> immediately segfaults # # # is there an existing issue for this ? - [x ] i have searched the existing issues # # # current behaviour segfault # # # expected behaviour no segfault # # # steps to reproduce * fresh install of ubuntu focal * ` export debian_frontend = noninteractive & & apt update & & apt install curl wget htop git vim ccache - y & & git clone <url> - - depth = <number> . / bitcoin - core & & cd bitcoin - core & & apt install build - essential libtool autotools - dev automake pkg - config bsdmainutils python3 - zmq libevent - dev libboost - dev gcc - <number> g + + - <number> - y ` * ` . / autogen . sh & & . / configure cc = gcc - <number> cxx = g + + - <number> & & make - j $( nproc ) src / bitcoind ` * ` valgrind . / src / bitcoind ` # # # relevant log output ` ` ` # valgrind . / src / bitcoind = = <number> = = memcheck , a memory error detector = = <number> = = copyright ( c ) <number> - <number> , and gnu gpl ' d , by julian seward et al . = = <number> = = using valgrind - <number> . <number> and libvex ; rerun with - h for copyright info = = <number> = = command : . / src / bitcoind = = <number> = = = = <number> = = invalid read of size <number> = = <number> = = at 0x 1 fd15f : ~ vector ( stl_vector . h : <number> ) = = <number> = = by 0x 1 fd15f : std : : filesystem : : __cxx11 : : path : : ~ path ( ) ( fs_path . h : <number> ) = = <number> = = by 0x 1 fd18b : ~ _cmpt ( fs_path . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt > ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : __destroy < std : : filesystem : : __cxx11 : : path : : _cmpt *> ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt *> ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt * , std : : filesystem : : __cxx11 : : path : : _cmpt > ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : ~ vector ( stl_vector . h : <number> ) = = <number> = = by 0x 1 fd18b : std : : filesystem : : __cxx11 : : path : : ~ path ( ) ( fs_path . h : <number> ) = = <number> = = by 0x 1 c5a7c : ~ path ( fs . h : <number> ) = = <number> = = by 0x 1 c5a7c : argsmanager : : getconfigfilepath ( ) const [ clone . cold . <number> ] ( system . cpp : <number> ) = = <number> = = by 0x 6 0 5 7 e0 : argsmanager : : readconfigfiles ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & , bool ) ( system . cpp : <number> ) = = <number> = = by 0x 4 f557f : common : : initconfig ( argsmanager & , std : : function < bool ( bilingual_str const & , std : : vector < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : allocator < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > > const & ) > ) ( init . cpp : <number> ) = = <number> = = by 0x 1 cc16e : appinit ( bitcoind . cpp : <number> ) = = <number> = = by 0x 1 cc16e : main ( bitcoind . cpp : <number> ) = = <number> = = address 0x 2 b is not stack ' d , malloc ' d or ( recently ) free ' d = = <number> = = = = <number> = = = = <number> = = process terminating with default action of signal <number> ( sigsegv ) : dumping core = = <number> = = access not within mapped region at address 0x 2 b = = <number> = = at 0x 1 fd15f : ~ vector ( stl_vector . h : <number> ) = = <number> = = by 0x 1 fd15f : std : : filesystem : : __cxx11 : : path : : ~ path ( ) ( fs_path . h : <number> ) = = <number> = = by 0x 1 fd18b : ~ _cmpt ( fs_path . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt > ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : __destroy < std : : filesystem : : __cxx11 : : path : : _cmpt *> ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt *> ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : _destroy < std : : filesystem : : __cxx11 : : path : : _cmpt * , std : : filesystem : : __cxx11 : : path : : _cmpt > ( stl_construct . h : <number> ) = = <number> = = by 0x 1 fd18b : ~ vector ( stl_vector . h : <number> ) = = <number> = = by 0x 1 fd18b : std : : filesystem : : __cxx11 : : path : : ~ path ( ) ( fs_path . h : <number> ) = = <number> = = by 0x 1 c5a7c : ~ path ( fs . h : <number> ) = = <number> = = by 0x 1 c5a7c : argsmanager : : getconfigfilepath ( ) const [ clone . cold . <number> ] ( system . cpp : <number> ) = = <number> = = by 0x 6 0 5 7 e0 : argsmanager : : readconfigfiles ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & , bool ) ( system . cpp : <number> ) = = <number> = = by 0x 4 f557f : common : : initconfig ( argsmanager & , std : : function < bool ( bilingual_str const & , std : : vector < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : allocator < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > > const & ) > ) ( init . cpp : <number> ) = = <number> = = by 0x 1 cc16e : appinit ( bitcoind . cpp : <number> ) = = <number> = = by 0x 1 cc16e : main ( bitcoind . cpp : <number> ) = = <number> = = if you believe this happened as a result of a stack = = <number> = = overflow in your program ' s main thread ( unlikely but = = <number> = = possible ) , you can try to increase the size of the = = <number> = = main thread stack using the - - main - stacksize = flag . = = <number> = = the main thread stack size used in this run was <number> . = = <number> = = = = <number> = = heap summary : = = <number> = = in use at exit : <number> bytes in <number> blocks = = <number> = = total heap usage : <number> allocs , <number> frees , <number> bytes allocated = = <number> = = = = <number> = = leak summary : = = <number> = = definitely lost : <number> bytes in <number> blocks = = <number> = = indirectly lost : <number> bytes in <number> blocks = = <number> = = possibly lost : <number> bytes in <number> blocks = = <number> = = still reachable : <number> bytes in <number> blocks = = <number> = = suppressed : <number> bytes in <number> blocks = = <number> = = rerun with - - leak - check = full to see details of leaked memory = = <number> = = = = <number> = = for lists of detected and suppressed errors , rerun with : - s = = <number> = = error summary : <number> errors from <number> contexts ( suppressed from <number> ) segmentation fault ( core dumped ) # # # how did you obtain bitcoin core compiled from source # # # what version of bitcoin core are you using ? current master # # # operating system and version - # # # machine specifications _no response_",0
bitcoin/bitcoin,"issue in ` p2p_ibd_stalling . py ` under valgrind at 4 0 c6c85c05812ee8bf824b639307b1ac17a001c4 with the native_valgrind job : ` ` ` bash test <number> - <number> - 0 5 t <time> . 0 7 4 0 0 0 z testframework . node0 ( debug ) : connecting to <number> . <number> . <time> <number> outbound - full - relay node0 <number> - <number> - 0 5 t <time> . 2 6 5 7 3 1 z [ msghand ] [ net_processing . cpp : <number> ] [ sendmessages ] [ net ] requesting block 7 5 2 4 0 5 4 3 9 cea869d584044084502582bc209e4ef97e4bf3b8c2ba3958acaf606 ( <number> ) peer = <number> node0 <number> - <number> - 0 5 t <time> . 2 6 7 2 9 5 z [ msghand ] [ net . cpp : <number> ] [ pushmessage ] [ net ] sending getdata ( <number> bytes ) peer = <number> node0 <number> - <number> - 0 5 t <time> . 2 6 9 8 6 2 z [ http ] [ httpserver . cpp : <number> ] [ http_request_cb ] [ http ] received a post request for / from <number> . <number> . <time> <number> node0 <number> - <number> - 0 5 t <time> . 2 7 1 5 6 8 z [ msghand ] [ net_processing . cpp : <number> ] [ processmessage ] [ net ] received : headers ( <number> bytes ) peer = <number> node0 <number> - <number> - 0 5 t <time> . 5 8 8 0 3 2 z [ httpworker . <number> ] [ rpc / request . cpp : <number> ] [ parse ] [ rpc ] threadrpcserver method = addconnection user = __cookie__ node0 <number> - <number> - 0 5 t <time> . 7 0 9 0 6 2 z [ httpworker . <number> ] [ net . cpp : <number> ] [ connectnode ] [ net : debug ] trying connection <number> . <number> . <time> <number> lastseen = <number> . 0 hrs node0 <number> - <number> - 0 5 t <time> . 7 3 1 5 0 4 z [ httpworker . <number> ] [ net . cpp : <number> ] [ cnode ] [ net ] added connection peer = <number> test <number> - <number> - 0 5 t <time> . 0 9 7 0 0 0 z testframework . utils ( error ) : wait_until ( ) failed . predicate : ' ' ' ' test_function = lambda : self . is_connected ' ' ' test <number> - <number> - 0 5 t <time> . 0 9 7 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" / home / ubuntu / ci_scratch / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / home / ubuntu / ci_scratch / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / p2p_ibd_stalling . py "" , line <number> , in run_test peers . append ( node . add_outbound_p2p_connection ( p2pstaller ( stall_block ) , p2p_idx = id , connection_type = "" outbound - full - relay "" ) ) file "" / home / ubuntu / ci_scratch / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_node . py "" , line <number> , in add_outbound_p2p_connection p2p_conn . wait_for_connect ( ) file "" / home / ubuntu / ci_scratch / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / p2p . py "" , line <number> , in wait_for_connect wait_until_helper ( test_function , timeout = timeout , lock <tong> 2 p_lock ) file "" / home / ubuntu / ci_scratch / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / util . py "" , line <number> , in wait_until_helper raise assertionerror ( "" predicate { } not true after { } seconds "" . format ( predicate_source , timeout ) ) assertionerror : predicate ' ' ' ' test_function = lambda : self . is_connected ' ' ' not true after <number> seconds test <number> - <number> - 0 5 t <time> . 1 0 2 0 0 0 z testframework ( debug ) : closing down network thread test <number> - <number> - 0 5 t <time> . 1 2 3 0 0 0 z testframework . utils ( error ) : wait_until ( ) failed . predicate : ' ' ' ' wait_until_helper ( lambda self . network_event_loop . is_running ( ) , timeout = timeout ) ' ' ' node0 <number> - <number> - 0 5 t <time> . 1 9 8 2 0 8 z [ msghand ] [ net_processing . cpp : <number> ] [ updatepeerstateforreceivedheaders ] [ net ] protecting outbound peer = <number> from eviction node0 <number> - <number> - 0 5 t <time> . 2 0 1 8 2 0 z [ msghand ] [ net . cpp : <number> ] [ pushmessage ] [ net ] sending sendheaders ( <number> bytes ) peer = <number> ` ` `",0
bitcoin/bitcoin,"use of a wallet should not be blocked in prune mode ( "" wallet loading failed . <repeated> beyond pruned data "" ) it is impossible to load and use a wallet neither in bitcoin - qt gui nor console / rpc commands . the wallet is only about <number> weeks old and typical settings are used ( data pruned to <number> gb ) . * * expected behavior * * - a wallet is loaded ( either manually or automatically on startup ) and addresses are shown in bitcoin - qt , - successful execution of wallet rpc commands , e . g . getwalletinfo , dumpwallet , walletpassphrasechange etc . ( not different from the way it had worked and looked the previous time a user started bitcoin core ) . * * actual behavior * * [ wallet loading failed prune ] ( <url> > wallet loading failed . prune : last wallet synchronisation goes beyond pruned data . you need to - reindex ( download the whole blockchain again in case of pruned node ) also , rpc commands fail , for example getwalletinfo , dumpwallet ( except for listwallets ) . * * to reproduce * * <number> . start bitcoin - qt <number> . select file - > open wallet - > [ wallet name ] reproducible every time in prune mode . the wallet was created at the time for which data were pruned . the wallet was created in bitcoin - qt . * * system information * * official binary bitcoin core <number> . <number> , windows <number> .",0
bitcoin/bitcoin,"core stops to run with ` failed to read block ` error * * expected behavior * * while my node was running it get an internal failure that force it to stop the running ` ` ` <number> - <number> - 2 1 t <time> z syncing basic block filter index with block chain from height <phone> - <number> - 2 1 t <time> z error : serializefiledb : rename - into - place failed <number> - <number> - 2 1 t <time> z syncing basic block filter index with block chain from height <phone> - <number> - 2 1 t <time> z error : readblockfromdisk : deserialize or i / o error - readcompactsize ( <sad> size too large : iostream error at flatfilepos ( nfile = <number> , npos = <number> ) <number> - <number> - 2 1 t <time> z * * * threadsync : failed to read block 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 def83407e184fbc5f53b47effdaef7b15e2d6f6b8579d from disk <number> - <number> - 2 1 t <time> z error : a fatal internal error occurred , see debug . log for details ` ` ` the code is based on the commit ` 9 4 0 7 0 0 2 9 fb6b783833973f9fe08a3a871994492f ` * * to reproduce * * no idea how i can reproduce it , now i switch to the tagged version to see if this happens too * * system information * * ` ` ` ➜ bitcoin git <sad> master ) neofetch _ , met $ $$ $ $ gg . vincent <user> , g $ $$ $$ $$ $$ $$ $$ $ $ p . - - - - - - - - - - - , g $ $ p "" "" "" "" y $ $ . "" . os : debian gnu / linux <number> ( bullseye ) x86_64 , $ $ p ' ` $$ $ . host : optiplex <number> ' , $ $ p , ggs . ` $ $ b : kernel : <number> . <number> - <number> - amd64 ` d $ $ ' , $p "" ' . $$ $ uptime : <number> days , <number> hours , <number> mins $ $ p d $ ' , $ $ p packages : <number> ( dpkg ) $$ : $ $ . - , d $ $ ' shell : zsh <number> $$ ; y $b . _ _ , d $p ' terminal : / dev / pts / <number> y $ $ . ` . ` "" y $ $$ $ p "" ' cpu : intel i5 - 7 5 0 0 t ( <number> ) @ <number> . 3 0 0 ghz ` $ $ b "" - . __ gpu : intel hd graphics <number> ` y $ $ memory / 7 8 4 0 mib ` y $ $ . ` $ $ b . ` y $ $ b . ` "" y $b . _ ` "" "" "" ` ` ` < - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",0
bitcoin/bitcoin,"error : timeout on transient error : could not connect to the server <number> . <number> : <number> ( error code <number> - "" eof reached "" ) < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > when i am trying to use ` bitcoin - cli ` while bitcoind running it displays error message : ` ` ` error : timeout on transient error : could not connect to the server <number> . <number> : <number> ( error code <number> - "" eof reached "" ) make sure the bitcoind server is running and that you are connecting to the correct rpc port . ` ` ` < ! - - - what behavior did you expect ? - - > running command getblockchaininfo ` < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > ` bitcoind - txindex ` ` ` ` [ . <repeated> ] <number> - <number> - 1 7 t <time> z bound to <number> . <number> : <number> <number> - <number> - 1 7 t <time> z bound to <happy> : <sad> <number> <number> - <number> - 1 7 t <time> z bound to <number> . <number> : <number> [ . <repeated> ] ` ` ` < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > i am on macos ventura <number> intel i5 macbook pro . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > ` <number> - <number> - 1 7 t <time> z bitcoin core version v24 . <number> ( release build ) ` < ! - - any extra information that might be useful in the debugging process . - - > i saw a related issue few years ago and apparently this issue should to be patched .",0
bitcoin/bitcoin,"decodescript miniscript functionality tops out at <number> bytes after <url> miniscript is extracted from compatible scripts , unless the ` fillablesigningprovider : : addcscript ` check for ` max_script_element_size ` fails , which then it falls back to legacy behavior . <user>",0
bitcoin/bitcoin,"rpc getdescriptorinfo should not output undefined value of the "" descriptor "" * * expected behavior * * ` ` ` { "" descriptor "" : "" tr ( cutfblpuabapmtkwjcds4rwhuseububfkpmogrbtmqfnja3vgrle ) <hashtag> tdkpah70 </hashtag> "" , "" checksum "" : "" tdkpah70 "" , "" isrange "" : false , "" issolvable "" : true , "" hasprivatekeys "" : true } ` ` ` * * actual behavior * * { "" descriptor "" : "" tr ( * * 9 6 7 a9c6e1e189a8390e8ed870b918cc64742bebc372625071c410ac96b1aa580 ) <hashtag> x9af7q5s </hashtag> * *"", "" checksum "" : "" tdkpah70 "" , "" isrange "" : false , "" issolvable "" : true , "" hasprivatekeys "" : true } * * to reproduce * * execure the rpc command , e . g . "" tr ( cutfblpuabapmtkwjcds4rwhuseububfkpmogrbtmqfnja3vgrle ) "" ` * * system information * * bitcoin <number> . <number>",0
bitcoin/bitcoin,"wallet passphrases silently ignore everything after a null character # # problem encrypting a wallet through json rpc or qt * appears to * allow a user to include null characters in the passphrase , but silently ignores everything including and after the first null character . for instance ( on regtest ) , trying to set a passphrase of "" ` a { null character } b ` "" : ` curl - - user __cookie__ - - data - binary ' { "" jsonrpc "" : "" <number> "" , "" id "" : "" curltest "" , "" method "" : "" encryptwallet "" , "" params "" : [ "" a \ \ u0000b "" ] } ' - h ' content - type : text / plain ; ' <url> this will succeed , but allow the user to unlock with the passphrase "" ` a ` "" , instead of the expected full passphrase ( which also works ) : ` curl - - user __cookie__ - - data - binary ' { "" jsonrpc "" : "" <number> "" , "" id "" : "" curltest "" , "" method "" : "" walletpassphrase "" , "" params "" : [ "" a "" , <number> ] } ' - h ' content - type <url> i am also able to replicate it in qt on my macos machine by running ` printf ' a \ \ 0 b ' | pbcopy ` and pasting the result into the passphrase dialog . my main concern is a user thinking that they are generating , say , <number> random bytes as a passphrase , and if they are unlucky and get a zero in the first few bytes , it unexpectedly cuts their security down to almost nothing . # # root cause the reason is due to our ` securestring ` type . ` securestring ` is a ` std : : string ` specialization with a secure allocator . however , when assigned , it ' s treated like a c - string ( no explicit length and null - terminated ) . see the [ original pr ] ( <url> for more details . # # potential solutions i think there are two plausible approaches to take . the first ( and my preference ) is to allow and support null characters , and i will submit a pr that enables that ( by making ` securestring ` use the entire string ) . the second is to explicitly reject any passphrases that contain null characters . one significant complication may be that , if anyone is already <emphasis> using a passphrase with a null , then my first solution would stop their wallet from unlocking . however , it would still be unlockable just by trimming the null and any subsequent characters .",0
bitcoin/bitcoin,"every other change address is unused i am seeing <percent> of generated change addresses not being used . it ' s as if two are reserved for every one that is used . this is happening in version v24 . <number> . here ' s a sequence of commands that shows what i mean . only the odd numbered change hdkeypaths are used : ` ` ` $ / bin / rm - fr ~ / . bitcoin / regtest $ bitcoin - qt - regtest & $ bitcoin - cli - regtest createwallet <number> > / dev / null $ bitcoin - cli - regtest createwallet <number> > / dev / null $ bitcoin - cli - regtest - rpcwallet = <number> - generate <number> > / dev / null $ bitcoin - cli - regtest - rpcwallet = <number> - generate <number> > / dev / null $ to =$( bitcoin - cli - regtest - rpcwallet = <number> getnewaddress ) $ txs =$( for i in { <number> . <repeated> <number> }; do bitcoin - cli - regtest - rpcwallet = <number> sendtoaddress $ to <number> ; done ) $ addrs =$( for i in $ txs ; do bitcoin - cli - regtest getrawtransaction $ i true | jq - mc ' . vout [ ]| { value : . value , addr : . scriptpubkey . address } ' ; done | grep - v ' : <number> , ' | jq - r . addr ) $ for a in $ addrs ; do bitcoin - cli - regtest - rpcwallet = <number> getaddressinfo $ a | jq - r . hdkeypath ; done m / <number> ' / <number> ' / <number> ' / <number> / <number> m / <number> ' / <number> ' / <number> ' / <number> / <number> m / <number> ' / <number> ' / <number> ' / <number> / <number> m / <number> ' / <number> ' / <number> ' / <number> / <number> m / <number> ' / <number> ' / <number> ' / <number> / <number> ` ` ` this happens whether or not i create descriptor wallets . for example , with a descriptor wallet i get these descriptors for the change addresses : ` ` ` wpkh ( [ 7 7 9 aecf0 / <number> ' / <number> ' / <number> ' / <number> / <number> ] 0 2 9 9 1 9 0 5 2 bd5b904afbf71d90afbec0a852d1f0181914b6d07bcab028a0d1ac3e4 ) <hashtag> 9 4 rhf9lu </hashtag> wpkh ( [ 7 7 9 aecf0 / <number> ' / <number> ' / <number> ' / <number> / <number> ] 0 3 6 0 ecbba4ebaac3ab4f62f14b12a136c2cc5e8264ac1be2af1b5e4413944a2c6a ) <hashtag> lwjzrh62 </hashtag> wpkh ( [ 7 7 9 aecf0 / <number> ' / <number> ' / <number> ' / <number> / <number> ] 0 3 d3f72c2cc6085bdee5388ff3b96df7d27ac18d85b5cc8d8048e287b52777062e ) <hashtag> 5 q4fsvmd </hashtag> wpkh ( [ 7 7 9 aecf0 / <number> ' / <number> ' / <number> ' / <number> / <number> ] 0 2 b77a29387f645bb5ed27ef25144ae67a984e056e41b35fce33bb377d199845ab ) <hashtag> mc608kvz </hashtag> wpkh ( [ 7 7 9 aecf0 / <number> ' / <number> ' / <number> ' / <number> / <number> ] 0 2 4 9 8 8 fafba17c2d6f5b2b27428e2fb65ca8a7882ac0fae1422e4fdf46c95dbc61 ) <hashtag> mwxzye2v </hashtag> ` ` ` and for a legacy wallet i get these wpkh ( [ bcc40f8d / <number> ' / <number> ' / <number> ' ] 0 3 d3cc823cd01dcb23dce0ac53f8b38c3cb8b9ddbdfc789550c280d25cba432932 ) <hashtag> 3 vxlglf6 </hashtag> wpkh ( [ bcc40f8d / <number> ' / <number> ' / <number> ' ] 0 3 0 1 6 e2243892459754e125954d205136247f67580e9379ca931f2ec374b924aef ) <hashtag> kxwx63p3 </hashtag> wpkh ( [ bcc40f8d / <number> ' / <number> ' / <number> ' ] 0 2 3 6 dd8b3a6ebd26a4e1e9dc1d84bf053438c339babacd202000f44917bd452336 ) <hashtag> jaq2hgn4 </hashtag> wpkh ( [ bcc40f8d / <number> ' / <number> ' / <number> ' ] 0 3 7 8 5 8 8 4 e52fad4588d2d0e1df48b8ba691dddfb <phone> a3b46cb0202132fa ) <hashtag> 0 d786n6s </hashtag> wpkh ( [ bcc40f8d / <number> ' / <number> ' / <number> ' ] 0 3 ba88cdc9a37aa676a1152c6d7d970aa32324b99d8c6f775c4543e016161ea4d9 ) <hashtag> vdvr44ar </hashtag> ` ` `",0
bitcoin/bitcoin,. .,0
bitcoin/bitcoin,"test : format - truncation warning in dbwrapper_tests compiling master at e1bf5470f919cf212703466411968916db8ae61f on ubuntu <number> ( with depends ) : ` ` ` cpp cxx test / test_bitcoin - dbwrapper_tests . o test / dbwrapper_tests . cpp : in member function ‘ void dbwrapper_tests : : iterator_string_ordering : : test_method ( ) ’ : test / dbwrapper_tests . cpp : <number> <time> : error : ‘ % d ’ directive output may be truncated writing between <number> and <number> bytes into a region of size <number> [ - werror = format - truncation <happy> <number> | snprintf ( buf , sizeof ( buf ) , "" % d "" , x) ; | ^ ~ test / dbwrapper_tests . cpp : <number> <time> : note : directive argument in the range [ - <phone> , <number> ] <number> | snprintf ( buf , sizeof ( buf ) , "" % d "" , x) ; | ^ ~ ~ ~ in file included from / usr / include / stdio . h : <number> , from / usr / include / c + + / <number> / cstdio : <number> , from / usr / include / c + + / <number> / ext / string_conversions . h : <number> , from / usr / include / c + + / <number> / bits / basic_string . h : <number> , from / usr / include / c + + / <number> / string : <number> , from . / clientversion . h : <number> , from . / dbwrapper . h : <number> , from test / dbwrapper_tests . cpp : <number> : in function ‘ int snprintf ( char * , size_t , const char * , . <repeated> ) ’ , inlined from ‘ void dbwrapper_tests : : iterator_string_ordering : : test_method ( ) ’ at test / dbwrapper_tests . cpp : <number> <time> : / usr / include / x86_64 - linux - gnu / bits / stdio2 . h : <number> <time> : note output between <number> and <number> bytes into a destination of size <number> <number> | return __builtin___snprintf_chk ( __s , __n , __use_fortify_level - <number> , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | __glibc_objsize ( __s ) , __fmt , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | __va_arg_pack ()); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ` ` ` ( i ran with ` - - enable - werror ` , otherwise it ' s merely a warning ) cc <user>",0
bitcoin/bitcoin,"private key import result with <number> address generation . < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > private key import result with <number> address generation . * * expected behavior * * therefore , it seems a bug but also future request assumes that as bitcoin address generation has choice of version like segwit or leacy , the private key import should result within the setup . < ! - - - what behavior did you expect ? - - > * * actual behavior * * generates <number> types wallet like legacy , segwit and p2pkh < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > * * to reproduce * * < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",0
bitcoin/bitcoin,build : xproto fails to install on aarch64 - unknown - linux - musl ` ` ` bash staging xproto . <repeated> make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' making install in specs make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' making install in siaddresses make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs / siaddresses ' make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs / siaddresses ' make [ <number> <sad> nothing to be done for ' install - exec - am ' . make [ <number> <sad> nothing to be done for ' install - data - am ' . make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs / siaddresses ' make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs / siaddresses ' make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' make [ <number> <sad> nothing to be done for ' install - exec - am ' . make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / specs ' make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' make [ <number> <sad> entering directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' make [ <number> <sad> nothing to be done for ' install - exec - am ' . . / install - sh - c - d ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / lib / pkgconfig ' . / install - sh - c - d ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / include / x11 ' . / install - sh - c - d ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / include / x11 ' mkdir : can not create directory ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / include ' : file exists mkdir : can not create directory ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / include / x11 ' : file exists make [ <number> <sad> * * * [ makefile : <number> : install - nodist_xprotoheaders ] error <number> make [ <number> <sad> * * * waiting for unfinished jobs . <repeated> / usr / bin / install - c - m <number> xproto . pc ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / lib / pkgconfig ' / usr / bin / install - c - m <number> ap_keysym . h deckeysym . h hpkeysym . h keysymdef . h keysym . h sunkeysym . h xalloca . h xarch . h xatom . h xdefs . h xf86keysym . h xfuncs . h x . h xmd . h xosdefs . h xos . h xos_r . h xproto . h xprotostr . h xthreads . h xw32defs . h xwdfile . h xwindows . h xwinsock . h ' / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / bitcoin / depends / aarch64 - unknown - linux - musl / include / x11 ' make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' make [ <number> <sad> * * * [ makefile : <number> : install - am ] error <number> make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' make [ <number> <sad> * * * [ makefile : <number> : install - recursive ] error <number> make [ <number> <sad> leaving directory ' / bitcoin / depends / work / build / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 ' make : * * * [ funcs . mk : <number> : / bitcoin / depends / work / staging / aarch64 - unknown - linux - musl / xproto / <date> - 0 7 4 4 1 8 fcbf5 / . stamp_staged ] error <number> make directory ' / bitcoin / depends ' ` ` ` does not happen when building just the package .,0
bitcoin/bitcoin,"intermittent issue in wallet_backwards_compatibility ( berkeleyenvironment : : open : error - <number> opening database environment : db_runrecovery : fatal error , run database recovery ) ` ` ` $ wget <url> $ tar xvf wallet_backwards_compatibility_113 . tar . xz $ . / test / functional / combine_logs . py - c . / wallet_backwards_compatibility_113 | tail - <number> | head - <number> node2 <number> - <number> - 1 1 t <time> . 9 0 3 8 4 9 z [ httpworker . <number> ] [ wallet / bdb . cpp : <number> ] [ open ] berkeleyenvironment : : open : logdir <annoyed> root / bitcoin - core / ci / scratch / test_runner / test_runner_ ₿ _ 🏃 _20230111_000842 / wallet_backwards_compatibility_113 / node2 / regtest / wallets / w1 / database errorfile <annoyed> root / bitcoin - core / ci / scratch / test_runner / test_runner_ ₿ _ 🏃 _20230111_000842 / wallet_backwards_compatibility_113 / node2 / regtest / wallets / w1 / db . log node2 <number> - <number> - 1 1 t <time> . 9 0 6 4 0 5 z [ httpworker . <number> ] [ wallet / bdb . cpp : <number> ] [ open ] berkeleyenvironment : : open : error - <number> opening database environment : db_runrecovery : fatal error , run database recovery test <number> - <number> - 1 1 t <time> . 9 0 9 0 0 0 z testframework ( error ) : jsonrpc error traceback ( most recent call last ) : file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / wallet_backwards_compatibility . py "" , line <number> , in run_test node . loadwallet ( wallet_name ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / coverage . py "" , line <number> , in __call__ return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ raise jsonrpcexception ( response [ ' error ' ] , status ) test_framework . authproxy . jsonrpcexception file verification failed . error initializing wallet database environment "" / root / bitcoin - core / ci / scratch / test_runner / test_runner_ ₿ _ 🏃 _20230111_000842 / wallet_backwards_compatibility_113 / node2 / regtest / wallets / w1 "" this error could occur if this wallet was not shutdown cleanly and was last loaded using a build with a newer version of berkeley db . if so , please use the software that last loaded this wallet ( - <number> )",0
bitcoin/bitcoin,"p2p_permissions intermittent timeout ` ` ` sh wget <url> tar - xvf p2p_permissions . tar . xz test / functional / combine_logs . py - c . / p2p_permissions_15 / | tail - <number> | head - <number> test <number> - <number> - 0 9 t <time> . 3 8 5 0 0 0 z testframework . utils ( error ) : wait_until ( ) failed . predicate : ' ' ' ' self . wait_until ( lambda : txid in self . nodes [ <number> ] . getrawmempool ( ) ) ' ' ' test <number> - <number> - 0 9 t <time> . 3 8 6 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / p2p_permissions . py "" , line <number> , in run_test self . check_tx_relay ( ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / p2p_permissions . py "" , line <number> , in check_tx_relay self . wait_until ( lambda : txid in self . nodes [ <number> ] . getrawmempool ( ) ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in wait_until return wait_until_helper ( test_function , timeout = timeout , timeout_factor = self . options . timeout_factor ) file "" / root / bitcoin - core / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / util . py "" , line <number> , in wait_until_helper raise assertionerror ( "" predicate { } not true after { } seconds "" . format ( predicate_source , timeout ) ) assertionerror : predicate ' ' ' ' self . wait_until ( lambda in self . nodes [ <number> ] . getrawmempool ( ) ) ' ' ' not true after <number> seconds",0
bitcoin/bitcoin,"test : sqlite tests on freebsd fail with modulenotfounderror : no module named ' _sqlite3 ' it would be good if the test was skipped instead , or if the documentation explained how to install py - sqlite . ` ` ` <number> / <number> - wallet_descriptor . py - - descriptors failed , duration : <number> s stdout : stderr : traceback ( most recent call last ) : file "" / tmp / cirrus - ci - build / bitcoin - core / test / functional / wallet_descriptor . py "" , line <number> , in <module> import sqlite3 file "" / usr / local / lib / python3 . <number> / sqlite3 / __init__ . py "" , line <number> , in <module> from sqlite3 . dbapi2 import * file "" / usr / local / lib / python3 . <number> / sqlite3 / dbapi2 . py "" , line <number> , in <module> from _sqlite3 import * modulenotfounderror module named ' _sqlite3 '",0
bitcoin/bitcoin,"bitcoin core startup is interrupted if there is the setting mintxfee = <number> in bitcoin . conf * * expected behavior * * it is expected that bitcoin core is not shutdown automatically during startup . moreover , it should be possible to set transaction fee at <number> . * * actual behavior * * [ bitcoin core error mintxfee ] ( <url> * * to reproduce * * <number> ) click bitcoin - qt . exe . * * system information * * <number> ) a wallet that contained unspent funds was opened previously . <number> ) the bitcoin . conf file contains "" mintxfee = <number> "" . <number> ) bitcoin core <number> . <number> official binary for windows ( portable ) .",0
bitcoin/bitcoin,contrib / install_db4 . sh script fails on freebsd running ` contrib / install_db4 . sh ` to download and build libdb4 for legacy wallet support fails on freebsd . ` ` ` $ . / contrib / install_db4 . sh ` pwd ` [ . <repeated> ] sha256sum : option requires an argument - - c usage : sha256sum [ - pqrtx ] [ - c file ] [ - s string ] [ files . <repeated> ] $ ` ` ` this is on freebsd <number> fixed by,0
bitcoin/bitcoin,intermittent failure in ` tool_wallet . py ` <url>,0
bitcoin/bitcoin,"bitcoind . service : start request repeated too quickly . i am obtaining the following error when initializing bitcoind service : ` ` ` × bitcoind . service - bitcoin daemon loaded : loaded ( / etc / systemd / system / bitcoind . service ; enabled ; vendor preset : enabled ) active : failed ( result : exit - code ) since sat <number> - <number> - <number> <time> - <number> ; 5 min ago process : <number> execstart <annoyed> home / satoshi / snap / bitcoin - core / <number> / bin / bitcoind - daemon - pid <annoyed> run / bitcoind / bitcoind . pid - conf <annoyed> home / satoshi / sna > cpu : 2 4 ms dez <number> <time> nakamoto systemd [ <number> <sad> bitcoind . service : scheduled restart job , restart counter is at <number> . dez <number> <time> nakamoto systemd [ <number> <sad> stopped bitcoin daemon . dez <number> <time> nakamoto systemd [ <number> <sad> bitcoind . service : start request repeated too quickly . dez <number> <time> nakamoto systemd [ <number> <sad> bitcoind . service : failed with result ' exit - code ' . dez <number> <time> nakamoto systemd [ <number> <sad> failed to start bitcoin daemon . ` ` ` bitcoind . service file : ` ` ` # it is not recommended to modify this file in - place , because it will # be overwritten during package upgrades . if you want to add further # options or overwrite existing ones then use # $ systemctl edit bitcoind . service # see "" man systemd . service "" for details . # note that almost all daemon options could be specified in # / etc / bitcoin / bitcoin . conf , but keep in mind those explicitly # specified as arguments in execstart = will override those in the # config file . [ unit ] description = bitcoin daemon after = network . target [ service ] execstart <annoyed> home / satoshi / snap / bitcoin - core / <number> / bin / bitcoind - daemon \ \ - pid <annoyed> run / bitcoind / bitcoind . pid \ \ - conf <annoyed> home / satoshi / snap / bitcoin - core / common / . bitcoin / bitcoin . conf \ \ - datadir <annoyed> home / satoshi / snap / bitcoin - core / common / . bitcoin # make sure the config directory is readable by the service user permissionsstartonly = true <hashtag> exec start pre </hashtag> <annoyed> bin / chgrp bitcoin / etc / bitcoin # process management # # # # # # # # # # # # # # # # # # # # type = forking pidfile <annoyed> run / bitcoind / bitcoind . pid restart = on - failure timeoutstopsec = <number> # directory creation and permissions # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # run as bitcoin : bitcoin user = satoshi group = satoshi # / run / bitcoind runtimedirectory = bitcoind runtimedirectorymode = <number> # / etc / bitcoin configurationdirectory = bitcoin configurationdirectorymode = <number> # / var / lib / bitcoind statedirectory = bitcoind statedirectorymode = <number> # hardening measures # # # # # # # # # # # # # # # # # # # # # provide a private / tmp and / var / tmp . privatetmp = true # mount / usr , / boot / and / etc read - only for the process . protectsystem = full # deny access to / home , / root and / run / user <hashtag> protect home </hashtag> = true # disallow the process and all of its children to gain # new privileges through execve ( ) . nonewprivileges = true # use a new / dev namespace only populated with api pseudo devices # such as / dev / null , / dev / zero and / dev / random . privatedevices = true # deny the creation of writable and executable memory mappings . memorydenywriteexecute = true [ install ] wantedby = multi - user . target ` ` ` i have installed bitcoin - core via snap . bitcoin core rpc client version v24 . <number> os <number> . <number> lts x86_64",0
bitcoin/bitcoin,"bitcoind v23 . <number> : traps : b - scheduler [ <number> ] trap invalid opcode < - - describe the issue - - > issue : bitcoind process ended abruptly < ! - - - what behavior did you expect ? - - > expected behaviour : bitcoind should continue to run < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > log output : nothing unusual terminal output : ` <number> illegal instruction ( core dumped ) ` dmesg output : ` traps : b - scheduler [ <number> ] trap invalid opcode ip : 5 5 5 ad42d5cb2 sp : 7 efc897f9590 error : <number> in bitcoind [ 5 5 5 ad3c91000 + 9 b4000 ] ` < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > cannot reproduce easily , this happened first time in last <number> years running bitcoind non - stop . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > $ bitcoin - cli - - version bitcoin core rpc client version v23 . <number> official build from the website , running normally from the user folder . no systemd , no service , but in screen session , in bash loop . bash loop allows it to automatically restart , so i experienced no downtime , bitcoind has started again automatically . full command line bitcoind was running with is : ` / home / user / bitcoin / bitcoin - <number> / bin / bitcoind > / home / user / . bitcoin / logs / <number> - <number> - 1 4 _02h04m24s . log <number> > & <number> ` < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > server - style machine with ecc ram : os : debian gnu / linux <number> ( bullseye ) x86_64 cpu : amd ryzen <number> <number> ( <number> ) @ <number> . 0 0 0 ghz [ <number> ° c ] ram : 6 4 gb ram <number> mt / s multi - bit ecc hdd : btrfs raid10 mode , 4 x 4 tb hard drives . no issues with btrfs or drives , all perfectly clean < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > no gui , bitcoind running in screen session , logging to a file < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - > i could upload entire session debug . log ( <number> . 5 mib ) on request . bitcoind process was running from <date> to <date> then crashed . log file ends cleanly with no error messages , last few lines are : ` ` ` <number> - <number> - 1 3 t <time> z socks5 ( ) connect to x <elongated> . onion : <number> failed : host unreachable <number> - <number> - 1 3 t <time> z connect ( ) to <number> . <number> : <number> failed after wait : connection refused ( <number> ) <number> - <number> - 1 3 t <time> z connect ( ) to <number> . <number> : <number> failed after wait : connection refused ( <number> ) <number> - <number> - 1 3 t <time> z updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1 5 0 9 b87e82e97a74d09c703ed21ee0a91fd90b5c6656 height = <number> version =0 x20a00000 log2_work = <number> tx = <number> date = ' <number> - <number> - 1 3 t <time> z ' progress = <number> cache = <number> . 6 mib ( 2 3 2 7 4 txo ) <number> - <number> - 1 3 t <time> z connect ( ) to <number> . <number> : <number> failed after wait : connection refused ( <number> ) <number> - <number> - 1 3 t <time> z updatetip best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 4 c776b362f68b0e645a1d7366e3b50a10a7809af50fc height = <number> version =0 x23778000 log2_work = <number> tx = <number> date = ' <number> - <number> - 1 3 t <time> z ' progress = <number> cache = <number> . 6 mib ( 3 1 7 5 9 txo ) ` ` `",0
bitcoin/bitcoin,"can not bump fee after abandoning child transaction steps send transaction ( a ) to a different wallet <number> . using the change , create another transaction ( b ) to a different wallet <number> . ` abandontransaction ` b <number> . ` bumpfee ` a fails with ` transaction has descendants in the wallet `",0
bitcoin/bitcoin,"validation , bugfix : provide more info in * mempoolacceptresult this pr fixes a bug and improves the mempool accept interface to return information more predictably . bug : in package validation , we first try the transactions individually ( see doc / policy / packages . md for more explanation ) and , if they all failed for missing inputs and policy - related ( i . e . fee ) reasons , we will try package validation . otherwise , we will just "" quit early "" since , for example , if a transaction had an invalid signature , adding a child will not help make it valid . currently , when we quit early , we are not setting the ` package_state ` to be invalid , so the caller might think it succeeded . also , we are returning no results - it makes more sense to return the individual transaction failure . thanks instagibbs for catching <url> also , make the package results interface generally more useful / predictable always return the feerate at which a transaction was considered for ` checkfeerate ` in ` mempoolacceptresult : : m_effective_feerate ` when it was successful . this can replace the current ` packagemempoolacceptresult : : m_package_feerate ` , which only sometimes exists . - always provide an entry for every transaction in ` packagemempoolacceptresult : : m_tx_results ` when the error is ` pckg_tx ` .",0
bitcoin/bitcoin,"the btc module is not working with the latest magento php <number> version hello team , hope that you are doing well we are facing an issue with the btc modules not working with the latest magento php version <number> . kindly please check and let us know on this how we can use the module with php version <number> . looking forward to hearing from you soon . thank you !",0
bitcoin/bitcoin,"new bitcoin ddos technique * edit <emphasis> * issue has been closed while the bug undergoes further review . hi , i will keep this concise as the attack is neither innovative nor sophisticated . it is possible to slow and entirely halt external connections from communicating with a bitcoin - core node . i do not know whether peer connections remain open or frequently disconnect / reconnect . what happens to sync ' d nodes with active peers is yet to be seen . it ' s too early , at least for me , to know whether peers would drop a node under attack over timeouts and how bitcoind behaves with peers while tcp / <number> is under fire . so - further research required . this needs to be tested against sync ' d nodes . there is a dos here - but also a ( probably slim ) possibility that a scaled attack would only result in new nodes being unable to enter the network . it is accomplished by repeatedly sending a small request , e . g . ` ` ` x x ` ` ` with thousands of concurrent socket requests . in this screen shot i am using a handful of cheap digital ocean machines to attack <number> node . [ corn ] ( <url> as the script exhausts itself and finds trouble assigning itself open sockets the ddos becomes hit or miss in external requests . sometimes connecting , usually not connecting , and sometimes producing "" black hole "" events wherein it ' s impossible to disconnect from an external telnet request until it drops you — which is atypical behavior . the attack works the best within the first <number> seconds of the attack script being launched - and so may require further optimization . am i missing something ? because i am definitely able to stress my own nodes with this technique . it is my opinion that a botnet could probably tango down the bitcoin network - or at least slow finality considerably . i have opted for full disclosure for a variety of reasons . - kev <url>",0
bitcoin/bitcoin,"test failure in rpc_net . py see <url> ( master ) the problem here is is that calling ` disconnect_p2ps ` waits until ` self . num_test_p2p_connections ( ) = = <number> ` . ` num_test_p2p_connections ( ) ` checks the field ` subver ` in ` getpeerinfo ` to distinguish p2p nodes from full nodes . however , if we are dealing with a p2p connection that has never sent a version , the node has never received the special subversion and the wait is ineffective ( we continue even though the disconnection is not yet completed ) .",0
bitcoin/bitcoin,"breking in mac os i am trying to use bitcoin core on mac os i want to record on an external hd but i get the following error all the time <number> - <number> - 1 7 t <time> z msghand thread exit <number> - <number> - 1 7 t <time> z net thread exit <number> - <number> - 1 7 t <time> z opencon thread exit <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : serializefiledb : failed to flush file / volumes / extreme ssd / bitcoin_core / peers . <number> <number> - <number> - 1 7 t <time> z dumpanchors : flush <number> outbound block - relay - only peer addresses to anchors . dat started <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : serializefiledb : failed to flush file / volumes / extreme ssd / bitcoin_core / anchors . ceb3 <number> - <number> - 1 7 t <time> z dumpanchors : flush <number> outbound block - relay - only peer addresses to anchors . dat completed ( <number> . 0 1 s ) <number> - <number> - 1 7 t <time> z scheduler thread exit <number> - <number> - 1 7 t <time> z writing <number> unbroadcast transactions to disk . <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z failed to dump mempool : filecommit failed . continuing anyway . <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : flush : failed to commit file <number> <number> - <number> - 1 7 t <time> z * * * flushing block file to disk failed . this is likely the result of an i / o error . <number> - <number> - 1 7 t <time> z error : a fatal internal error occurred , see debug . log for details <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : flush : failed to commit file <number> <number> - <number> - 1 7 t <time> z * * * flushing undo file to disk failed . this is likely the result of an i / o error . <number> - <number> - 1 7 t <time> z error : a fatal internal error occurred , see debug . log for details <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : flush : failed to commit file <number> <number> - <number> - 1 7 t <time> z * * * flushing block file to disk failed . this is likely the result of an i / o error . <number> - <number> - 1 7 t <time> z error : a fatal internal error occurred , see debug . log for details <number> - <number> - 1 7 t <time> z filecommit : fcntl f_fullfsync failed : <number> <number> - <number> - 1 7 t <time> z error : flush : failed to commit file <number> <number> - <number> - 1 7 t <time> z * * * flushing undo file to disk failed . this is likely the result of an i / o error . <number> - <number> - 1 7 t <time> z error : a fatal internal error occurred , see debug . log for details <number> - <number> - 1 7 t <time> z shutdown",0
bitcoin/bitcoin,"macos reports error on shutdown of gui macos reports error on shutdown of gui . "" bitcoin core quit unexpectedly . <repeated> "" although this may be hardware related , memory and cpu load seem normal ( screenshots and logs attached . ) * * expected behavior * * application closes without error message . * * actual behavior * * application quits ( and restarts without error or corruption of data ) but message persists . * * to reproduce * * the error message seems to persist every time under normal conditions , ( closing the application invokes the message "" bitcoin core is shutting down , do not shut down the computer until this window has disappeared "" then opens fine on relaunch . * * system information * * bitcoin core <number> . <number> version : <number> . <number> ( <number> . <number> ) code type : x86 - <number> ( native ) parent process : launchd [ <number> ] os version <number> ( 2 2 a380 ) [ log2022 - <number> - <number> . txt ] ( <url> < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url> < img width = "" <number> "" alt = "" screenshot <number> - <number> - <number> at <number> <number> <number> "" src = "" <url>",0
bitcoin/bitcoin,"whitelisting fails for nodes added with addnode whitelisting works for subnets for outgoing and incoming connections , unless the node has been added using addnode , in which case the permissions are always "" n / a "" in the gui and the node is disconnected as if there is no whitelisting ( e . g . when historical block download is reached , or a mempool request comes in , etc ) .",0
bitcoin/bitcoin,"test : failure in interface_rest . py <url> ` ` ` bash test <number> - <number> - 0 1 t <time> . 7 7 1 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in main self . run_test ( ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ interface_rest . py "" , line <number> , in run_test self . sync_all ( ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in sync_all self . sync_mempools ( nodes ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in sync_mempools raise assertionerror ( "" mempool sync timed out after { } s:{ } "" . format ( assertionerror : mempool sync timed out after 4 8 0 s : { ' cbd1d27c4d06b4dfd479c34aef4b57d3be14dfc20ccc74f2699fde370f17d1d3 ' } set ( ) test <number> - <number> - 0 1 t <time> . 7 7 1 0 0 0 z testframework ( debug ) down network thread ` ` `",0
bitcoin/bitcoin,unable to cross compile zeromq dependency for arm64 - apple - darwin < - - describe the issue - - > trying to compile starting with dependencies fails with checking build system type . <repeated> x86_64 - pc - linux - gnu checking host system type . <repeated> invalid configuration ` arm64 - apple - darwin ' : machine ` arm64 - apple ' not recognized configure : error : / bin / bash config / config . sub arm64 - apple - darwin failed * * expected behavior * * < ! - - - what behavior did you expect ? - - > * * actual behavior * * checking for arm64 - apple - darwin - pkg - config . <repeated> no checking for pkg - config . <repeated> / usr / bin / pkg - config checking pkg - config is at least version <number> . <number> . <repeated> yes checking for xmlto . <repeated> no checking for asciidoc . <repeated> no checking build system type . <repeated> x86_64 - pc - linux - gnu checking host system type . <repeated> invalid configuration ` arm64 - apple - darwin ' : machine ` arm64 - apple ' not recognized configure : error : / bin / bash config / config . sub arm64 - apple - darwin failed make : * * * [ funcs . mk : <number> : / home / bitcoin / downloads / bitcoin - master / depends / work / build / arm64 - apple - darwin / zeromq / <number> . <number> - 0 f6f30942e0 / . / . stamp_configured ] error <number> * * to reproduce * * command host = arm64 - apple - darwin - j <number> * * system information * * master branch ubuntu <number> core i5 intel 1 6 gb ram,0
bitcoin/bitcoin,"instability in ` listunspent ` after # <number> i maintain a [ wallet ] ( <url> that uses ` bitcoin - cli ` for signing transactions , and its test system uses regtest to create a bunch of utxos for the signing tests to sign . these tests use a git - committed ` wallet . dat ` that is copied into the newly - created ` datadir ` before launching ` bitcoind ` . for years , up until bc886fcb31e1afa7bbf7b86bfd93e51da7076ccf ( part of # <number> ) , this was stable and worked well . occasionally i would need to recreate all the transactions when this project changed mining (# <number> ) or coin selection (# <number> ) but i could handle that . what i can not handle is having the generated transactions be different each time i run the exact same sequence of ` bitcoin - cli ` commands starting from the same ` wallet . dat ` . starting with bc886fcb31e1afa7bbf7b86bfd93e51da7076ccf , that ' s what happens . is this a supported use model , or is it unreasonable to expect this to be stable ? summarizing : * v0 . <number> - - stable * v0 . <number> - - stable * v22 . <number> - - stable * v23 . <number> - - stable * 2 7 2 3 5 6 0 2 4 db978c92112167f8d8e4cc62adad63d ( which is bc886fcb31e1afa7bbf7b86bfd93e51da7076ccf ^ ) - - stable * bc886fcb31e1afa7bbf7b86bfd93e51da7076ccf - - unstable * v24 . 0 rc2 - - unstable * current master ( f37bd15d472fdc7dd3d40cafaba9e8dfddd6b530 ) - - unstable < - - describe the issue - - > * * expected behavior * * the generated transactions should be identical given an identical series of ` bitcoin - cli ` calls . * * actual behavior * * the generated transactions choose different input coins from the regtest blockchain every time i run . * * to reproduce * * in my [ glacier wallet fork ] ( <url> run ` t / online_regtest_wallet . py recreate - all - tests ` ; git commit the result . run the same command again - - ` git status ` should show no changes . when it ' s unstable , dozens of files change . * * system information * * running on whonix <number> . building inside the git repo as such ( cd depends & & make - j4 no_qt = <number> ) & & . / autogen . sh & & . / configure - - prefix = ` pwd ` / depends / x86_64 - pc - linux - gnu & & make - j4 ` ` `",0
bitcoin/bitcoin,"signrawtransactionwithwallet fails with signed non - wallet inputs and breaks the existing signatures the error returned by the rpc is : ` unable to sign input , invalid stack size ( possibly missing key ) ` it seems that this rpc alters the scriptsig of all of the inputs that were already signed . the problem is very similar to <url> which is already solved . here ' s an example of the broken signatures after signing : ` ` ` diff - - - 0 _before . txt <number> - <number> - <number> <time> . <number> - <number> + + + 1 _after . txt <number> - <number> - <number> <time> . <number> - <number> @ @ - <number> + <number> @ @ { - "" txid "" : "" 7 1 0 7 a82fd2d783d32e2926bf91b5dea5908203a3d81f8bb67b267f9aeab3c16d "" , - "" hash "" : "" 7 1 0 7 a82fd2d783d32e2926bf91b5dea5908203a3d81f8bb67b267f9aeab3c16d "" , + "" txid "" : "" b4b09eebed1f427f7ae80004ca1e55688567bc8e01af7608311c7d0a31ffcf83 "" , + "" hash "" : "" 6 8 5 1 0 0 e81d34140941b3c15af9598e0d2ffdbee92b3706c82561ff2232c94cbc "" , "" version "" : <number> , - "" size "" : <number> , - "" vsize "" : <number> , - "" weight "" : <number> , + "" size "" : <number> , + "" vsize "" : <number> , + "" weight "" : <number> , "" locktime "" : <number> , "" vin "" : [ { "" txid "" : "" c2afc743b00acb7a9a9cccc9e9b2751f0ae17206a9492dc0537ec72f44c7f6e0 "" , "" vout "" : <number> , "" scriptsig "" : { - "" asm "" : "" <number> 3 0 4 4 0 2 2 0 2 0 1 b3d70770c2b325c8cac16593a4606cdcb29c094f67ec700d6aaadfdcc3bdf022057b70804b24d66664f1a82f7936773fbc3bcb15740a0e7b772ddf7bfe486b6fe [ single | anyonecanpay ] <phone> eaf58c896885fc1597a848d268ad618e621b33be77bed67bcfa6180611ac31022073f947c7d2ebd81c1d52236030dd1a5d6b178c9361786ff9d6d3ef9740be1c76 [ single | anyonecanpay ] 5 2 2 1 0 3 d28e83bfc9d2ab30f26f7b62ac4b64a932272ba124c6c4d29e901eaf8724aab7210366dc61fb53bce09777fe7c4766a55af9649cc26deb21b204ae6bf4e93413a46c2102439d6a6bc199460e4e0a6e5fdcb87162cafa5cae6ee4845d23d9dc5e840ffdb153ae "" , - "" hex "" : "" 0 0 4 7 3 0 4 4 0 2 2 0 2 0 1 b3d70770c2b325c8cac16593a4606cdcb29c094f67ec700d6aaadfdcc3bdf022057b70804b24d66664f1a82f7936773fbc3bcb15740a0e7b772ddf7bfe486b6fe83473044022034eaf58c896885fc1597a848d268ad618e621b33be77bed67bcfa6180611ac31022073f947c7d2ebd81c1d52236030dd1a5d6b178c9361786ff9d6d3ef9740be1c76834c69522103d28e83bfc9d2ab30f26f7b62ac4b64a932272ba124c6c4d29e901eaf8724aab7210366dc61fb53bce09777fe7c4766a55af9649cc26deb21b204ae6bf4e93413a46c2102439d6a6bc199460e4e0a6e5fdcb87162cafa5cae6ee4845d23d9dc5e840ffdb153ae "" + "" asm "" : "" <number> <number> 5 2 2 1 0 3 d28e83bfc9d2ab30f26f7b62ac4b64a932272ba124c6c4d29e901eaf8724aab7210366dc61fb53bce09777fe7c4766a55af9649cc26deb21b204ae6bf4e93413a46c2102439d6a6bc199460e4e0a6e5fdcb87162cafa5cae6ee4845d23d9dc5e840ffdb153ae "" , + "" hex "" : "" 0 0 0 0 4 c69522103d28e83bfc9d2ab30f26f7b62ac4b64a932272ba124c6c4d29e901eaf8724aab7210366dc61fb53bce09777fe7c4766a55af9649cc26deb21b204ae6bf4e93413a46c2102439d6a6bc199460e4e0a6e5fdcb87162cafa5cae6ee4845d23d9dc5e840ffdb153ae "" } , "" sequence "" : <number> } , @ @ - <number> + <number> @ @ "" asm "" : "" "" , "" hex "" : "" "" } , + "" txinwitness "" : [ + "" 3 0 4 4 0 2 2 0 2 dc3e510dc053dcdd29be701441337c93f6923686cc5ba4a915dbc17073dd26c02207aa8b76a447bde52b4cb1146781745826692d4484327f2a78a73818183e5f37f01 "" , + "" 0 3 7 9 d702db49e91dd63127278c06ed99ef05b43d15f2583bb28b4f0e9b49b9f50c "" + ] , "" sequence "" : <number> } ] , ` ` ` i have prepared a bash script that reproduces it ( i used hal to merge the transactions ) : ` ` ` bash #/ bin / bash <hashtag> set </hashtag> - x set - e shopt - s expand_aliases alias b - dae = "" ~ / opt / bitcoin / bin / bitcoind - datadir =$ pwd - daemon = <number> "" alias b - cli = "" ~ / opt / bitcoin / bin / bitcoin - cli - datadir =$ pwd "" declare - a privkeys declare - a pubkeys declare - a multisigs declare - a addresses declare - a redeemscripts declare - a txs declare - a scriptpubkeys declare - a vouts declare - a mergetxs <hashtag> parameters </hashtag> for the multisig addreesses addresstype = legacy multisig_n = <number> multisig_m = <number> multisig_count = <number> <hashtag> create </hashtag> the keypairs for i in $( seq <number> $(( multisig_count * multisig_m - <number> ) ) ) ; do keypair =$( hal key generate - - regtest <number> > / dev / null ) privkeys [ $ i ] =$( echo $ keypair | jq - r . wif_private_key ) pubkeys [ $ i ] =$( echo $ keypair | jq - r . public_key ) done b - cli stop & & sleep <number> || true rm - rf regtest b - dae b - cli - rpcwait createwallet "" "" echo <hashtag> create </hashtag> the multisig addresses for i in $( seq <number> $(( multisig_count - <number> ) ) ) ; do mspubkeys ='[ ' sep = for j in $( seq $(( i * multisig_m ) ) $(( i * multisig_m + multisig_m - <number> ) ) ) ; do mspubkeys + =""$ sep \ \""$ { pubkeys [ $ j ] } \ \ "" "" sep =', ' done mspubkeys + ='] ' echo b - cli createmultisig ${ multisig_n } "" ${ mspubkeys } "" $ addresstype multisigs [ $ i ] =$( b - cli - rpcwait createmultisig ${ multisig_n } "" ${ mspubkeys } "" $ addresstype ) addresses [ $ i ] =$( echo "" ${ multisigs [ $ i ] } "" | jq - r ' . address ' ) redeemscripts [ $ i ] =$( echo "" ${ multisigs [ $ i ] } "" | jq - r ' . redeemscript ' ) done echo "" pubkeys : ${ pubkeys [ @ ] } "" echo "" privkeys : ${ privkeys [ @ ] } "" <hashtag> generate </hashtag> enough blocks to unlock the reward dest =$( b - cli getnewaddress ) b - cli generatetoaddress <number> $ dest > / dev / null <number> > & <number> <hashtag> send </hashtag> <number> btc to each multisig address for i in $( seq <number> $(( multisig_count - <number> ) ) ) ; do txs [ $ i ] =$( b - cli sendtoaddress "" ${ addresses [ $ i ] } "" <number> ) scriptpubkeys [ $ i ] =$( b - cli decoderawtransaction $( b - cli getrawtransaction ${ txs [ $ i ] })| jq - r "" . vout [ ] . scriptpubkey | select ( . address = = \\""$ { addresses [ $ i ] } \ \ "" ) . hex "" ) vouts [ $ i ] =$( b - cli decoderawtransaction $( b - cli getrawtransaction ${ txs [ $ i ] })| jq - r "" . vout [ ] | select ( . scriptpubkey . address = = \\""$ { addresses [ $ i ] } \ \ "" ) . n "" ) <hashtag> echo </hashtag> ${ txs [ $ i ] } ${ scriptpubkeys [ $ i ] } b - cli generatetoaddress <number> $ dest > / dev / null <number> > & <number> done echo <hashtag> each </hashtag> multisig sends its coins to a new wallet ( destwallet ) and signs the transaction using signrawtransactionwithkey echo createwallet destwallet b - cli createwallet destwallet destaddress =$( b - cli - rpcwallet = destwallet getnewaddress ) echo unloadwallet destwallet b - cli unloadwallet destwallet for i in $( seq <number> $(( multisig_count - <number> ) ) ) ; do echo "" sending utxo ${ txs [ $ i ] } to final address $ destaddress "" tx =$( b - cli createrawtransaction "" [ { \ \ "" txid \ \"": \\""$ { txs [ $ i ] } \ \ "" , \ \ "" vout \ \"": ${ vouts [ $ i ] } } ] "" "" [ { \ \ "" $ destaddress \ \"": <number> } ] "" ) prevtxs =""[{\\ "" txid \ \"": \\""$ { txs [ $ i ] } \ \ "" , \ \ "" vout \ \"": ${ vouts [ $ i ] } , \ \ "" scriptpubkey \ \"": \\""$ { scriptpubkeys [ $ i ] } \ \ "" , \ \ "" redeemscript \ \"": \\""$ { redeemscripts [ $ i ] } \ \ "" , \ \ "" amount \ \"": <number> } ] "" <hashtag> sign </hashtag> with keys one by one for j in $( seq $(( i * multisig_m ) ) $(( i * multisig_m + multisig_n - <number> ) ) ) ; do tx =$( b - cli signrawtransactionwithkey $ tx "" [\\""$ { privkeys [ $ j ] } \ \ "" ] "" "" $ prevtxs "" "" single | anyonecanpay "" ) tx =$( echo $ tx | jq - r ' . hex ' ) done mergetxs [ $ i ] =$ tx done echo finalinputs = "" "" finaloutputs = "" "" <hashtag> create </hashtag> the transaction to provide the funds to pay for the previous transactions fee_input =$( b - cli listunspent | jq - r ' . [ <number> ] ' ) fee_input_amount =$( echo $ fee_input | jq - r ' . amount ' ) fee = <number> change_amount =$( bc - l < < < "" $ fee_input_amount - $ fee "" ) changeaddress =$( b - cli getnewaddress ) change_tx =$( b - cli createrawtransaction "" [ $ fee_input ] "" "" [ { \ \ "" $ changeaddress \ \"": $ change_amount } ] "" ) mergetxs [ $ multisig_count ] =$ change_tx sepin = "" "" sepout = "" "" <hashtag> prepare </hashtag> a json with the inputs , and another one with the outputs for i in $( seq <number> $(( multisig_count ) ) ) ; do decodedtx =$( hal tx decode ${ mergetxs [ $ i ] } ) inputno =$( echo $ decodedtx | jq - r "" . inputs | length - <number> "" ) for k in $( seq <number> $ inputno ) ; do input =$( echo $ decodedtx | jq - r "" . inputs [ $ k ] | del ( . sequence ) "" ) finalinputs + =""$ sepin $ input "" sepin ="", "" done outputno =$( echo $ decodedtx | jq - r "" . outputs | length - <number> "" ) for k in $( seq <number> $ outputno ) ; do output =$( echo $ decodedtx | jq - r "" . outputs [ $ k ] | del ( . n ) "" ) finaloutputs + =""$ sepout $ output "" sepout ="", "" done done fulltx = "" { \ \ "" version \ \"": <number> , \ \ "" locktime \ \"": <number> , \ \ "" inputs \ \"": [ $ finalinputs ] , \ \ "" outputs \ \"": [ $ finaloutputs ] } "" echo $ fulltx | jq > fulltx . txt <hashtag> create </hashtag> the transaction with all the inputs and outpts , and sign it using signrawtransactionwithwallet ( fails . <repeated> ) finaltx =$( echo $ fulltx | jq | hal tx create <number> > / dev / null ) b - cli decoderawtransaction $ finaltx > 0 _before . txt echo "" tx before signrawtransactionwithwallet : $ finaltx "" signedfinaltx =$( b - cli signrawtransactionwithwallet $ finaltx ) echo "" tx after signrawtransactionwithwallet $ signedfinaltx | jq - r ' . hex ' ) "" b - cli decoderawtransaction $( echo $ signedfinaltx | jq - r ' . hex ' ) > 1 _after . txt diff - naurwp 0 _before . txt 1 _after . txt > <number> . diff || true signerror =$( echo $ signedfinaltx | jq - r ' . errors [ <number> ] . error / / empty ' ) if [ - n "" $ signerror "" ] ; then echo "" error signing with wallet ( $ signerror ) "" exit - <number> fi senttx =$( b - cli sendrawtransaction $( echo $ signedfinaltx | jq - r ' . hex ' ) ) b - cli gettransaction $ senttx b - cli generatetoaddress <number> $ dest > / dev / null <number> > & <number> b - cli gettransaction $ senttx ` ` ` i have tested with bitcoin core <number> . please let me know if there ' s anything else i can test .",0
bitcoin/bitcoin,"intermitted failure in p2p_sendtxrcncl . py i have seen ` p2p_sendtxrcncl . py ` fail randomly multiple times by now , e . g . : <url> also noted here : <url> log : ` ` ` node0 <number> - <number> - 2 1 t <time> . 8 9 3 2 6 4 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ node \ \ txreconciliation . cpp : <number> ] [ preregisterpeer ] [ txreconciliation : debug ] pre - register peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 3 4 6 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending sendtxrcncl ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 4 5 0 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending verack ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 5 7 0 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ processmessage ] [ net ] receive version message : / python - p2p - tester : <number> . <number> /: version <number> , blocks = - <number> , us = <number> . <number> . <time> <number> , txrelay = <number> , peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 6 4 9 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ processmessage ] [ net ] received : verack ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 7 0 3 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending sendcmpct ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 8 1 1 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ node \ \ txreconciliation . cpp : <number> ] [ forgetpeer ] [ txreconciliation : debug ] forget txreconciliation state of peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 3 8 9 8 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending ping ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 1 1 1 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending getheaders ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 2 4 3 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ sendmessages ] [ net ] initial getheaders ( <number> ) to peer = <number> ( startheight : - <number> ) node0 <number> - <number> - 2 1 t <time> . 8 9 4 3 5 5 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending feefilter ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 4 8 9 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ processmessage ] [ net ] received : ping ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 5 5 4 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ pushmessage ] [ net ] sending pong ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 6 8 2 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ processmessage ] [ net ] received : wtxidrelay ( <number> bytes ) peer = <number> node0 <number> - <number> - 2 1 t <time> . 8 9 4 7 4 0 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ processmessage ] [ net ] wtxidrelay received after verack from peer = <number> ; disconnecting test <number> - <number> - 2 1 t <time> . 9 5 4 0 0 0 z testframework . p2p ( debug ) : closed connection to : <number> . <number> . <time> <number> node0 <number> - <number> - 2 1 t <time> . 9 5 4 5 8 1 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net . cpp : <number> ] [ closesocketdisconnect ] [ net ] disconnecting peer = <number> node0 <number> - <number> - 2 1 t <time> . 9 5 4 8 4 9 z [ c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ src \ \ net_processing . cpp : <number> ] [ finalizenode ] [ net ] cleared nodestate for peer = <number> test <number> - <number> - 2 1 t <time> . 9 7 0 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in main self . run_test ( ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ p2p_sendtxrcncl . py "" , line <number> , in run_test peer . send_and_ping ( msg_verack ( ) ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ p2p . py "" , line <number> , in send_and_ping self . sync_with_ping ( timeout = timeout ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ p2p . py "" , line <number> , in sync_with_ping self . wait_until ( test_function , timeout = timeout ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ p2p . py "" , line <number> , in wait_until wait_until_helper ( test_function , timeout = timeout , lock <tong> 2 p_lock , timeout_factor = self . timeout_factor ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ util . py "" , line <number> , in wait_until_helper if predicate ( ) "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ p2p . py "" , line <number> , in test_function assert self . is_connected assertionerror ` ` `",0
bitcoin/bitcoin,"bitcoin - qt crashed after completing a full blockchain rescan while investigating # <number> i tried running : $ bitcoin - cli rescanblockchain to rescan the full blockchain . it took about <number> hours to run the full rescan , and after the scan was complete bitcoin - qt crashed with this error in gdb : thread <number> "" b - http "" received signal sigpipe , broken pipe . the end of the debug . log file showed this at the time of the crash [ chris ] still rescanning . at block <number> . progress = <number> . <phone> - <number> - 1 9 t <time> z [ chris ] still rescanning . at block <number> . progress = <number> . <phone> - <number> - 1 9 t <time> z [ chris ] scanning current mempool transactions . <number> - <number> - 1 9 t <time> z [ chris ] rescan completed in 3 4 2 6 0 1 5 0 ms i posted the full backtraces in <url> before making a separate report for this crash . this was bitcoin - qt built from git tag ` v24 . 0 rc1 ` on debian linux , running on intel ( r ) core ( tm ) i7 - 8 7 5 0 h cpu @ <number> . 2 0 ghz with ssd hard drives .",0
bitcoin/bitcoin,test failure in ` feature_index_prune . py ` <url> <url> <url>,0
bitcoin/bitcoin,"` scanblocks ` rpc result includes false - positives the recently introduced ` scanblocks ` rpc uses bip157 block filters for quickly determining which blocks contain a specified set of output scripts . by nature this can lead to false - positive results , i . e . the filter set matches the block filter even though none of the contained txs spend or create utxos with the specified set of scriptpubkeys . can be reproduced on testnet with address ` tb1qcxf2gv93c26s6mqz7y6etpqdf70zmn67dualgr ` : ` ` ` $ . / src / bitcoin - cli - testnet scanblocks start ' [ "" addr ( tb1qcxf2gv93c26s6mqz7y6etpqdf70zmn67dualgr ) "" ] ' { "" from_height "" : <number> , "" to_height "" : <number> , "" relevant_blocks "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 1 bc35077dec4104e0ab1f667ae27059bd907f9a8fac55c802ae36 "" , "" 0 0 0 0 0 0 0 0 0 0 0 1 2 0 a9c50542d73248fb7c37640c252850f0cf273134ad9febaf61 "" , "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 2 f7af3835da8b6146b0bfb243b8842f09c495fa1e74d454ed "" , "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 4 c32651728193bfbe91f6789683b8d6ac6ae2d22ebd3cb5d3 "" ] , "" completed "" : true } ` ` ` looking closer at the first returned block 0 0 0 0 0 0 0 0 0 0 0 1 bc35077dec4104e0ab1f667ae27059bd907f9a8fac55c802ae36 ( <url> one can see that the contained single ( coinbase ) transaction is not related to the passed address : ` ` ` $ . / src / bitcoin - cli - testnet getblock 0 0 0 0 0 0 0 0 0 0 0 1 bc35077dec4104e0ab1f667ae27059bd907f9a8fac55c802ae36 <number> . <repeated> "" vout "" : [ { "" value "" : <number> , "" n "" : <number> , "" scriptpubkey "" : { "" asm "" : "" op_dup op_hash160 ad0050c5a24b9496e34e34410f35c735f66584ca op_equalverify op_checksig "" , "" desc "" : "" addr ( mwhhaaquafcyjjocy6qezrg6sblt6kcdab ) <hashtag> shkcfhzk </hashtag> "" , "" hex "" : "" 7 6 a914ad0050c5a24b9496e34e34410f35c735f66584ca88ac "" , "" address "" : "" mwhhaaquafcyjjocy6qezrg6sblt6kcdab "" , "" type "" } } ] , . <repeated> ` ` ` it might make sense to introduce an option that actives a second pass phase in which the found blocks are inspected in order to filter out these false - positives . i guess it is debatable if this can really be considered a bug ( in practice it should not be a problem ) , but at the very least we should mention the possibility of false - positives in the rpc help to not confuse users .",0
bitcoin/bitcoin,add lock annotation for feefilterrounder : : round ( ) ci failure from # <number> calling ` with_lock ( ) ` on a non - recursive mutex requires not holding it beforehand .,0
bitcoin/bitcoin,"bitcoind dumps core when deriveaddresses is called with index <phone> ( <number> ^ <number> - <number> ) the ` deriveaddresses ` json - rpc endpoint accepts a descriptor and an index for the derivation . currently , the rpc interface caps the allowed index derivation range in the interval [ <number> ^ <number> - <number> ] . however , calling ` deriveaddresses ` using a range that includes the index ` <phone> ` ( <number> ^ <number> - <number> ) results in a crash of bitcoind ( on an amd64 machine ) : ` ` ` bitcoin - cli deriveaddresses "" descriptor "" "" [ <phone> , <phone> ] "" [ bitcoind dumps core ] ` ` ` in the pr that addresses this pull request (# <number> ) is included a test case that causes the crash . for convenience , here ' s the contents of a core dump generated with that test : ` ` ` $ sudo coredumpctl debug bitcoind pid : <number> ( bitcoind ) uid : <number> ( muxator ) gid : <number> ( muxator ) signal : <number> ( abrt ) timestamp : thu <number> - <number> - <number> <time> cest ( 4 min 5 6 s ago ) command line : <base> / src / bitcoind - datadir <annoyed> tmp / test_runner_ ₿ _ 🏃 _20221006_174805 / rpc_deriveaddresses_crash_0 / node0 - logtimemicros - debug - debugexclude = libevent - debugexclude = leveldb - uacomment = testnode0 - logthreadnames - logsourcelocations - loglevel = trace - sandbox = log - and - abort executable : <base> / src / bitcoind [ . <repeated> ] message : process <number> ( bitcoind ) of user <number> dumped core . [ . <repeated> ] elf object binary architecture : amd x86 - <number> gnu gdb ( gdb ) fedora <number> - <number> . fc36 [ . <repeated> ] core was generated by ` <base> / src / bitcoind - datadir <annoyed> tmp / test_runner_ ₿ _ � � ' . program terminated with signal sigabrt , aborted . # <number> 0x0 0 0 0 7 f55a8ac6c4c in __pthread_kill_implementation ( ) from / lib64 / libc . so . <number> [ current thread is <number> ( thread 0x 7 f5576ffd640 ( lwp <number> ) ) ] missing separate debuginfos , use : dnf debuginfo - install glibc - <date> . fc36 . x86_64 libevent - <date> - <number> . fc36 . x86_64 libgcc - <number> . <number> - <number> . fc36 . x86_64 libstdc + + - <number> . <number> - <number> . fc36 . x86_64 sqlite - libs - <number> . <number> - <number> . fc36 . x86_64 zlib - <date> - <number> . fc36 . x86_64 ( gdb ) bt # <number> 0x0 0 0 0 7 f55a8ac6c4c in __pthread_kill_implementation ( ) from / lib64 / libc . so . <number> # <number> 0x0 0 0 0 7 f55a8a769c6 in raise ( ) from / lib64 / libc . so . <number> # <number> 0x0 0 0 0 7 f55a8a607f4 in abort ( ) from / lib64 / libc . so . <number> # <number> 0x0 0 0 0 7 f55a8c40c11 in __addvsi3 ( ) from / lib64 / libgcc_s . so . <number> # <number> 0x <phone> a84d73 in operator ( ) ( __closure =0 x7f5576ffbd50 , self = . <repeated> , request = . <repeated> ) at rpc / output_script . cpp : <number> # <number> 0x <phone> a87ec2 in std : : __invoke_impl < univalue , deriveaddresses ( <sad> : < lambda ( const rpchelpman & , const jsonrpcrequest & ) > & , const rpchelpman & , const jsonrpcrequest & > ( std : : __invoke_other , struct { . <repeated> } & ) ( __f = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> a8773f in std : : __invoke_r < univalue , deriveaddresses ( <sad> : < lambda ( const rpchelpman & , const jsonrpcrequest & ) > & , const rpchelpman & , const jsonrpcrequest & > ( struct { . <repeated> } & ) ( __fn = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> a86ec3 in std : : _function_handler < univalue ( const rpchelpman & , const jsonrpcrequest & ) , deriveaddresses ( <sad> : < lambda ( const rpchelpman & , const jsonrpcrequest & ) > <sad> : _m_invoke ( const std : : _any_data & , const rpchelpman & , const jsonrpcrequest & ) ( __functor = . <repeated> , __args # <number> = . <repeated> , __args # <number> = . <repeated> ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 6 0 b4641 in std : : function < univalue ( rpchelpman const & , jsonrpcrequest const & )>: : operator ( ) ( rpchelpman const & , jsonrpcrequest const & ) const ( this =0 x7f5576ffbd50 , __args # <number> = . <repeated> , __args # <number> = . <repeated> ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 6 0 a93b8 in rpchelpman : : handlerequest ( this =0 x7f5576ffbd30 , request = . <repeated> ) at rpc / util . cpp : <number> # <number> 0x0 0 0 0 5 6 3 9 3 5 9 cc1a2 in crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> <sad> : operator ( ) ( jsonrpcrequest const & , univalue & , bool ) const ( __closure =0 x5639368fbb00 < registeroutputscriptrpccommands ( crpctable & <sad> : commands + <number> > , request = . <repeated> , result = . <repeated> ) at . / rpc / server . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 5 9 e0be3 in std : : __invoke_impl < bool , crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> } & , jsonrpcrequest const & , univalue & , bool > ( std : : __invoke_other , crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> } & , jsonrpcrequest const & , univalue & , bool & & ) ( __f = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 5 9 da9ba in std : : __invoke_r < bool , crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> } & , jsonrpcrequest const & , univalue & , bool > ( crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> } & , jsonrpcrequest const & , univalue & , bool & & ) ( __fn = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 5 9 d4916 in std : : _function_handler < bool ( jsonrpcrequest const & , univalue & , bool ) , crpccommand : : crpccommand ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , rpchelpman (* )()): : { lambda ( jsonrpcrequest const & , univalue & , bool ) # <number> }>: : _m_invoke ( std : : _any_data const & , jsonrpcrequest const & , univalue & , bool & & ) ( __functor = . <repeated> , __args # <number> = . <repeated> , __args # <number> = . <repeated> , __args # <number> = <user> : true ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x0 0 0 0 5 6 3 9 3 5 9 3 0 2 c6 in std : : function < bool ( jsonrpcrequest const & , univalue & , bool ) <sad> : operator ( ) ( jsonrpcrequest const & , univalue & , bool ) const ( this =0 x5639368fbb00 < registeroutputscriptrpccommands ( crpctable & <sad> : commands + <number> > , __args # <number> = . <repeated> , __args # <number> = . <repeated> , __args # <number> = true ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x <phone> b0afd5 in executecommand ( command = . <repeated> , request = . <repeated> , result = . <repeated> , last_handler = true ) at rpc / server . cpp : <number> # <number> 0x <phone> b0abae in executecommands ( commands = std : : vector of length <number> , capacity <number> = { . <repeated> } , request = . <repeated> , result = . <repeated> ) at rpc / server . cpp : <number> # <number> 0x <phone> b0ad52 in crpctable : : execute ( this =0 x5639368fc4c0 <tablerpc> , request = . <repeated> ) at rpc / server . cpp : <number> # <number> 0x <phone> cbe3f5 in httpreq_jsonrpc ( context = std : : any containing node : : nodecontext * = { . <repeated> } , req =0 x7f556c002da0 ) at httprpc . cpp : <number> # <number> 0x <phone> cc0548 in operator ( ) ( __closure =0 x7f556c001980 , req =0 x7f556c002da0 ) at httprpc . cpp : <number> # <number> 0x <phone> cc1c3a in std : : __invoke_impl < bool , starthttprpc ( const std : : any & <sad> : < lambda ( httprequest * , const std : : string & ) > & , httprequest * , const std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & > ( std : : __invoke_other , struct { . <repeated> } & ) ( __f = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> cc1a78 in std : : __invoke_r < bool , starthttprpc ( const std : : any & <sad> : < lambda ( httprequest * , const std : : string & ) > & , httprequest * , const std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & > ( struct { . <repeated> } & ) ( __fn = . <repeated> ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> cc182e in std : : _function_handler < bool ( httprequest * , const std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & ) , starthttprpc ( const std : : any & <sad> : < lambda ( httprequest * , const std : : string & ) > <sad> : _m_invoke ( const std : : _any_data & , httprequest * & & , const std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > & ) ( __functor = . <repeated> , __args # <number> = <user> : 0x 7 f556c002da0 , __args # <number> = "" "" ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x <phone> cd1c0c in std : : function < bool ( httprequest * , std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const & )>: : operator ( ) ( httprequest * , std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const & ) const ( this =0 x7f556c003fb0 , __args # <number> =0 x7f556c002da0 , __args # <number> = "" "" ) at / usr / include / c + + / <number> / bits / std_function . h : <number> # <number> 0x <phone> cd0fc2 in httpworkitem : : operator ( ) ( this =0 x7f556c003f80 ) at httpserver . cpp : <number> # <number> 0x <phone> cd326d in workqueue <httpclosure> : : run ( this =0 x563938765d10 ) at httpserver . cpp : <number> # <number> 0x <phone> ccbf1d in httpworkqueuerun ( queue =0 x563938765d10 , worker_num = <number> ) at httpserver . cpp : <number> # <number> 0x <phone> cdfd51 in std : : __invoke_impl < void , void (* ) ( workqueue <httpclosure> * , int ) , workqueue <httpclosure> * , int > ( __f = <user> : 0x 5 6 3 9 3 5 ccbeb2 < httpworkqueuerun ( workqueue <httpclosure> * , int ) > ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> cdfa <elongated> in std : : __invoke < void (* ) ( workqueue <httpclosure> * , int ) , workqueue <httpclosure> * , int > ( __fn = <user> : 0x 5 6 3 9 3 5 ccbeb2 < httpworkqueuerun ( workqueue <httpclosure> * , int ) > ) at / usr / include / c + + / <number> / bits / invoke . h : <number> # <number> 0x <phone> cdf8a6 in std : : thread : : _invoker < std : : tuple < void (* ) ( workqueue <httpclosure> * , int ) , workqueue <httpclosure> * , int > <sad> : _m_invoke < 0 ul , 1 ul , 2 ul > ( this =0 x56393877e7e8 ) at / usr / include / c + + / <number> / bits / std_thread . h : <number> # <number> 0x <phone> cdf7db in std : : thread : : _invoker < std : : tuple < void (* ) ( workqueue <httpclosure> * , int ) , workqueue <httpclosure> * , int > <sad> : operator ( ) ( this =0 x56393877e7e8 ) at / usr / include / c + + / <number> / bits / std_thread . h : <number> # <number> 0x <phone> cdf60f in std : : thread : : _state_impl < std : : thread : : _invoker < std : : tuple < void (* ) ( workqueue <httpclosure> * , int ) , workqueue <httpclosure> * , int > > <sad> : _m_run ( this =0 x56393877e7e0 ) at / usr / include / c + + / <number> / bits / std_thread . h : <number> # <number> 0x0 0 0 0 7 f55a8e15b03 in execute_native_thread_routine ( ) from / lib64 / libstdc + + . so . <number> # <number> 0x0 0 0 0 7 f55a8ac4e2d in start_thread ( ) from / lib64 / libc . so . <number> # <number> 0x0 0 0 0 7 f55a8b4a1b0 in clone3 ( ) from / lib64 / libc . so . <number> ` ` ` the affected code seems to be in frame <number> , and corresponds to the following code ( gdb ) frame <number> # <number> 0x <phone> a84d73 in operator ( ) ( __closure =0 x7f5576ffbd50 , self = . <repeated> , request = . <repeated> ) at rpc / output_script . cpp : <number> <number> for ( int i = range_begin ; i <= range_end ; + + i ) { ` ` ` i think the reason is that , while ` range_begin ` and ` range_end ` are ` uint64_t ` , ` i ` is an ` int ` , which on many platforms means ` int32_t ` . when ` i ` is assigned ` <number> ^ <number> - <number> ` and is then incremented , it wraps back and causes the crash .",0
bitcoin/bitcoin,"wallet / spend . cpp assertion error < - - this issue tracker is only for technical issues related to bitcoin core . general bitcoin questions and / or support requests are best directed to the bitcoin stackexchange at <url> for reporting security issues , please read instructions at <url> if the node is "" stuck "" during sync or giving "" block checksum mismatch "" errors , please ensure your hardware is stable by running memtest and observe cpu temperature with a load - test tool such as linpack before creating an issue ! - - > < ! - - describe the issue - - > bitcoind fails to determine a fee rate which ends up crashing < ! - - - what behavior did you expect ? - - > rpc method ` sentoaddress ` should work as intended and properly calculate the fee . < ! - - - what was the actual behavior ( provide screenshots if the issue is gui - related ) ? - - > it throws and error and exits bitcoind ` ` ` <number> - <number> - 0 3 t <time> z fest <elongated> : <number> > <percent> decay <number> : feerate : <number> from ( <number> - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) fail : ( - <number> - - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) <number> - <number> - 0 3 t <time> z fest <elongated> : <number> > <percent> decay <number> : feerate : <number> from ( <number> - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) fail : ( - <number> - - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) <number> - <number> - 0 3 t <time> z fest <elongated> : <number> > <percent> decay <number> : feerate : <number> from ( <number> - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) fail : ( - <number> - - <number> ) <percent> <number> / ( <number> <number> mem <number> out ) bitcoind : wallet / spend . cpp : <number> : bool cwallet : : createtransactioninternal ( const std : : vector <crecipient> & , ctransactionref & , camount & , int & , bilingual_str & , const ccoincontrol & , feecalculation & , bool ) ` coin_selection_params . m_subtract_fee_outputs || fee_needed <= change_and_fee - change_amount ' failed . aborted ( core dumped ) ` ` ` < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > can not reproduce , the error is seemingly random . < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > <number> < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > windows <number> wsl linux ubuntu <number> ryzen <number> 5 9 5 0 x <number> gb ram < ! - - gui - related issue ? what is your operating system and its version ? if linux , what is your desktop environment and graphical shell ? - - > < ! - - any extra information that might be useful in the debugging process . - - > < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - >",0
bitcoin/bitcoin,"` bitcoind ` and ` bitcoin - qt ` crashes while creating psbt ` bitcoind ` and ` bitcoin - qt ` crashes when creating a psbt with ` walletcreatefundedpsbt ` * * expected behavior * * ` bitcoind ` and ` bitcoin - qt ` should not crash and users should not see assertion errors on mainnet . failing gracefully with some error message would be better . although i was not able to reproduce this issue on fedora so ideally this should be fixed . * * actual behavior * * error in bitcoind : ` ` ` assertion failed : selected_effective_value >= target , file wallet / coinselection . cpp , line <number> ` ` ` error in bitcoin - qt : [ image ] ( <url> * * to reproduce * * <number> . create bitcoin . conf ` ` ` signet = <number> txindex = <number> signet . rpcport = <number> rpcuser = user rpcpassword = pass fallbackfee = <number> ` ` ` <number> . add descriptor in a wallet using bitcoin - qt console : ` ` ` importmulti ' [ { "" desc "" : "" sh ( multi ( <number> , [ aa130fed ] 0 2 de62833c851509cf13c0c63e0dc3ef7b74f7d86b17e9090fab68c09830cb2f2e , [ 0 0 8 9 0 d8d ] 0 3 cf11a2bc1371dad3691bce59e37d85b12aae6b030b533d4e7c5aedeb47a5ec0e ) ) <hashtag> j8w6g6zg </hashtag> "" , "" timestamp "" : "" now "" } ] ' ` ` ` <number> . create psbt using bitcoin - qt , click on different options in the error message and you will notice it creates psbt with no change by clicking ' ignore ' <number> times else crashes with others : ` ` ` walletcreatefundedpsbt "" [ { \ \ "" txid \ \"": \ \ "" e911734bf711009dd6057c88bc598df2d31c18d1cc6bd6c790a704b519d77e68 \ \ "" , \ \ "" vout \ \"": <number> } ] "" "" [ { \ \ "" tb1q9fxp2p0sm93jdlul8v3svzn7x69zcd9ljsgans \ \"": \ \ "" <number> \ \ "" } ] "" ` ` ` <number> . close bitcoin - qt and open powershell or command prompt to create psbt bitcoind . exe ` ` ` ` ` ` ` bitcoin - cli . exe - rpcwallet = m1 walletcreatefundedpsbt "" [ { \ \ "" txid \ \"": \ \ "" e911734bf711009dd6057c88bc598df2d31c18d1cc6bd6c790a704b519d77e68 \ \ "" , \ \ "" vout \ \"": <number> } ] "" "" [ { \ \ "" tb1q9fxp2p0sm93jdlul8v3svzn7x69zcd9ljsgans \ \"": \ \ "" <number> \ \ "" } ] "" ` ` ` * * system information * * windows <number> bitcoin core master branch",0
bitcoin/bitcoin,"ci versions of git can not handle merges it appears git versions older than <number> can not handle some simple merges , and many of our ci tasks use such old versions . the simplest fix is probably to just use github ' s generated merge branch rather than trying to merge in each ci instance , but i have not tested it .",0
bitcoin/bitcoin,ci error : commit : failed to commit latest coinstatsindex state this error happened in a push in <url> i pushed the code again ` git commit - - amend ` and did not happen . it seems to be intermittent . <url> ` ` ` <number> - <number> - 1 5 t <time> . 4 8 1 3 6 6 z ( mocktime : <number> - <number> - 3 1 t <time> z ) [ coinstatsindex ] [ util / system . h : <number> ] [ error ] error : commit : failed to commit latest coinstatsindex state . <repeated> summary : threadsanitizer race on vptr ( ctor / dtor vs virtual call ) src / index / base . cpp : <number> : <number> in baseindex : : ~ baseindex ( ) ` ` `,0
bitcoin/bitcoin,test in wallet_resendwallettransactions . py <url>,0
bitcoin/bitcoin,i2p destinations should be signature_type = <number> <url>,0
bitcoin/bitcoin,"do not assume signature grinding for external signers since # <number> our fee calculation assumes that ecsda signatures are a maximum of <number> bytes , rather than <number> . however , not all hardware wallets implement r - value grinding . this can cause us to underpay fees when using an external signer . that ' s especially problematic when the user picks <number> sat / vbyte as the fee , because it will not even get into our own mempool . the easiest solution would be to modify ` externalsignerscriptpubkeyman ` to assume <number> bytes for ecsda signatures . slightly more advanced would be to expand hwi to keep track of which devices and firmware versions ( if any ) support r - value grinding . we ' d then have to store that in the wallet in some new field . personally i ' d rather work on better taproot support , which thanks to schnorr avoids this issue altogether .",0
bitcoin/bitcoin,"unknown descriptor in wallet crashes attempting to load a wallet with an unknown descriptor causes a fatal error : ` ` ` <number> - <number> - 0 6 t <time> z init message : loading wallet … <number> - <number> - 0 6 t <time> z [ descriptor ] invalid descriptor : can only have tr at top level : iostream error <number> - <number> - 0 6 t <time> z [ descriptor ] invalid descriptor : can only have tr at top level : iostream error <number> - <number> - 0 6 t <time> z [ descriptor ] setting spkman to active : id = 5 0 e6032b4d1d62c020ebb6a4d0c8fafc6f01ea80f4d2a41c1a256086611636e9 , type = legacy , internal = false <number> - <number> - 0 6 t <time> z [ descriptor ] setting spkman to active : id = 7 f709abbde61e8c253798375692bf41202e625ed74a65f9474d464be34fb2870 , type = p2sh - segwit , internal = false <number> - <number> - 0 6 t <time> z [ descriptor ] setting spkman to active : id = 2 dc6c34099e0ca1235fc990a51042967ecbc8c8ac57062f964739cba2f451be8 , type = bech32 , internal = false <number> - <number> - 0 6 t <time> z [ descriptor ] setting spkman to active : id = 8 d3c63ea1ac9936e789713dc8bae857d328ff029ab9392e67c716752e9b11b0c , type = bech32m , internal = false <number> - <number> - 0 6 t <time> z [ descriptor ] releasing wallet <number> - <number> - 0 6 t <time> z * * * * * * * * * * * * * * * * * * * * * * * * exception : st12out_of_range map : : at bitcoin in runaway exception * * * * * * * * * * * * * * * * * * * * * * * * exception : st12out_of_range map : : at bitcoin in runaway exception bitcoin - qt : . / checkqueue . h : <number> : ccheckqueue <cscriptcheck> : : ~ ccheckqueue ( ) [ t = cscriptcheck ] : assertion ` m_worker_threads . empty ( ) ' failed . aborted ` ` ` > a fatal error occurred . bitcoin core can no longer continue safely and will quit . to reproduce , a quick hack - - - a / src / script / descriptor . cpp + + + b / src / script / descriptor . cpp @ @ - <number> + <number> @ @ std : : unique_ptr <descriptorimpl> parsescript ( uint32_t & key_exp_index , span < const error = "" can only have addr ( ) at top level "" ; return nullptr ; } - if ( ctx = = parsescriptcontext : : top & & func ( "" tr "" , expr ) ) { + if ( ctx = = parsescriptcontext : : top & & func ( "" trx "" , expr ) ) { auto arg = expr ( expr ) ; auto internal_key = parsepubkey ( key_exp_index , arg , parsescriptcontext : : p2tr , out , error ) ; if ( internal_key ) { ` ` `",0
bitcoin/bitcoin,fs : ` _overlapped ` missing initializers ` ` ` bash fs . cpp : in member function ‘ bool fsbridge : : filelock : : trylock ( ) ’ : fs . cpp : <number> <time> : error : missing initializer for member ‘ _overlapped : : internalhigh ’ [ - werror = missing - field - initializers ] <number> | _overlapped overlapped = { <number> }; | ^ fs . cpp : <number> <time> : error : missing initializer for member ‘ _overlapped : : <anonymous> ’ [ - werror = missing - field - initializers ] fs . cpp : <number> <time> : error initializer for member ‘ _overlapped : : hevent ’ [ - werror = missing - field - initializers ] ` ` ` <url>,0
bitcoin/bitcoin,"segmentation fault when compiling with libfuzzer and lto (x 8 6 _64 ) steps to reproduce : * install fresh ubuntu jammy operating system * ` export debian_frontend = noninteractive & & apt update & & apt install curl wget htop git vim ccache - y & & git clone <url> bitcoin - core & & cd bitcoin - core & & apt install build - essential libtool autotools - dev automake pkg - config bsdmainutils python3 - zmq libevent - dev libboost - dev libsqlite3 - dev libdb + + - dev clang llvm libc + + - dev libc + + abi - dev - y & & . / autogen . sh & & . / configure cc = ' clang - flto ' cxx = ' clang + + - flto ' - - enable - fuzz - - with - sanitizers = fuzzer & & make - j $( nproc ) ` output : ` ` ` ar minisketch / libminisketch . a ar libbitcoin_wallet . a cxxld test / fuzz / fuzz clang : error : unable to execute command fault ( core dumped ) ` ` ` the same does not happen when compiling "" normally "" ( dropping ` - - with - sanitizers = fuzzer ` ) .",0
bitcoin/bitcoin,"intermittent failure in ` wallet_groups . py ` <url> : ` ` ` file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ wallet_groups . py "" , line <number> , in run_test txid6 = self . nodes [ <number> ] . sendtoaddress ( self . nodes [ <number> ] . getnewaddress ( ) , <number> ) ( . <repeated> ) assertionerror <number> ] expected messages "" [ ' fee non - grouped = <number> , grouped = <number> , using grouped ' ] "" does not partially match log",0
bitcoin/bitcoin,"checkblockindex stalls for extremely long time stale chain tips can block the rpc and all other responses from the peer for very long time ( more than <number> minutes , sometimes hours , or undefinetely ) . during that time i can see that my local node no longer replies to any other peer ( e . g . to bitnode . io core ) . and in process viewers , i can see that there ' s a single thread blocking all other threads in a tight loop using <percent> cpu in a tight loop . i have seen that this is occurs in any db indexer ( notably the chainstate indexer during ibd , or the txindexer , or the coinstat db indexer ) , notably in some critical steps where new blocks are added to the chain ( once every about <number> minutes ) . this seems to happen when a new block contains a transaction validating a better chain , and invalidating other blocks from dead branches . in that case , the ongoin indexer will fail with unchecked conditions in its internal iterator , apparently because it expect the next block to be present and locks it incorrectly , and is not able to release it and retry if that next valid block has been replaced by a better block in the chain . ` ` ` <number> - <number> - 2 1 t <time> . 8 7 5 8 3 4 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr =[ <number> : a880 : cad : d0 : : d9e : f001 ] : <number> ( block - relay - only ) <number> - <number> - 2 1 t <time> . 0 1 2 0 2 2 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 5 4 1 4 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 3 0 3 6 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 3 9 5 4 8 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 7 3 8 5 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 1 3 5 8 6 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 2 2 2 6 3 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 7 4 7 9 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 9 5 2 9 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 3 9 0 5 3 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 4 1 5 1 0 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 1 0 5 1 5 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 1 6 1 8 5 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 5 5 8 0 4 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 4 6 0 2 5 6 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 5 8 f0830f8c7cf41c06740b2004b4186e439b0b989062 height = <number> version =0 x2281c000 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 7 mib ( 2 7 1 9 2 txo ) <number> - <number> - 2 1 t <time> . 2 4 7 4 8 7 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 5 6 8 8 3 5 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 d0b2bbb26ce7bb0eef932eed58853ebe6525ca71c85a height = <number> version =0 x20000004 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 9 mib ( 2 8 9 6 4 txo ) <number> - <number> - 2 1 t <time> . 7 8 5 6 3 0 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 0 1 0 8 5 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 2 7 3 0 7 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 1 2 8 7 1 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 0 2 2 0 6 3 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 4 1 0 9 3 4 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 8 f79590f23c8bb78c3d4d22d1cd4bab3a0e94fc7e6be height = <number> version =0 x2ab96000 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 1 mib ( 3 0 9 6 6 txo ) <number> - <number> - 2 1 t <time> . 0 3 3 9 2 9 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 9 8 5 2 6 9 z [ mapport . cpp : <number> ] [ processupnp ] upnp port mapping successful . <number> - <number> - 2 1 t <time> . 0 0 5 2 6 8 z [ mapport . cpp : <number> ] [ processupnp ] upnp port mapping successful . ` ` ` finally later it may eventually detect that and detect the inconsistency , by eviting some peers . ` ` ` <number> - <number> - 2 1 t <time> . 2 3 4 2 6 8 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 2 3 6 2 8 6 z [ net_processing . cpp : <number> ] [ checkforstaletipandevictpeers ] potential stale tip detected , will try using extra outbound peer ( last tip update : <number> seconds ago ) <number> - <number> - 2 1 t <time> . 8 7 7 2 6 8 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr = <number> . <number> : <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 4 3 6 2 7 0 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr = <number> . <number> : <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 6 0 9 3 2 0 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr =[ 2 a <time> <number> : 6 c : <number> : <number> : <number> : <number> : <number> <sad> <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 5 4 6 4 3 6 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr = <number> . <number> <time> <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 1 4 6 3 1 0 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 cb8a63d4c0f24bf0e1c3e6bedbf0436ebab43a3ee690 height = <number> version =0 x20800000 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 5 mib ( 4 1 8 6 1 txo ) <number> - <number> - 2 1 t <time> . 0 6 5 3 2 6 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 4 8 6 7 4 2 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 e1bb77ce90948b3050df8ee2143d1913fd583ce3c765 height = <number> version =0 x20c00000 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 6 mib ( 4 8 5 8 8 txo ) <number> - <number> - 2 1 t <time> . 4 8 8 1 8 8 z [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 7 1 f39891841e7128e85f45f386ddde82f65c12dd5773 height = <number> version =0 x20400000 log2_work = <number> tx = <number> date = ' <number> - <number> - 2 1 t <time> z ' progress = <number> cache = <number> . 8 mib ( 5 0 1 8 0 txo ) <number> - <number> - 2 1 t <time> . 3 6 2 1 7 0 z [ index / base . cpp : <number> ] [ threadsync ] syncing coinstatsindex with block chain from height <phone> - <number> - 2 1 t <time> . 3 6 4 1 7 0 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr = <number> . <number> : <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 7 1 8 5 5 9 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr =[ <number> : 7 a8 <time> <number> : 2 b <time> a <time> 7 a : fac : <number> <sad> <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 3 0 2 9 3 6 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr = <number> . <number> : <number> ( outbound - full - relay ) <number> - <number> - 2 1 t <time> . 0 3 9 3 9 0 z [ net_processing . cpp : <number> ] [ processmessage ] new outbound peer connected : version : <number> , blocks = <number> , peer = <number> , peeraddr =[ 2 4 0 b : <number> : <number> : 9 d00 : e65f : 1 ff : fe10 : d767 ] : <number> ( outbound - full - relay ) ` ` ` and here again we see the deadlock occuring for long time in the coindb indexer ( note the difference of time between the last too events ) . and in fact nothing else occured : no resposes to other peers , the coindb indexer was stalled , the rpc service stopped to respond even locally . it seems then that there are malicious nodes on the p2p network sending bad blocks that "" partially pass "" the basic security , but can cause bitcoin core to be blocked and become unresponsive . it can no longer respond even to a basic rpc such as "" bitcoin - getinfo "" , or "" bitcoin - netinfo "" it will not even repond to a "" bitcoin - cli stop "" : some threads are stopped , but not all , and the indexer thread is still running in its tight loop ; if you are in that situation , the only way to recover is to kill the process entirely , and you will get the related index to become invalid as it was not committed , it will have to be reconstructed from zero . most often , you have no way to recover ( given that the rpc service is not responsive , and even the ctrl + c breaker , or a simple kill - hangup "" will not work . you have to do an instant kill ( kill - <number> on linux , or manual kill in the process viewer on windows : nothing else will be synced to disk ) . if the index was the ibd index , the chainstate can only be recovered by check level <number> ( works often , but not necessarily always , but never with check level <number> ) . if it does not work , then you have to rebuild the chainstate entirely . but for any other index ( such as the "" txindex "" or "" coinstats "" index , there ' s no check at all with recovery at startup , so the only way to recover is to entirely delete the content of that index directory and rebuild it from zero ( and this will take many hours , and may fail again in the middle , due to bad blocks sent by malicious peers and that may have still not been replaced by valid blocks in the main chain ) . apparently these bad blocks seem to come from unapproved forks ( possibly implementing segwit with too large block sizes ? ) , which are not correctly detected and cause the indexer to fail or run into some infinite loops , or some bad signatures using unsupported / snon - standard bytecodes , taking extremely long time to validate or just discard rapidly as invalid . such events with bad blocks coming from malicious peers tend to be frequent now . unfortunately it ' s not easy to track which nodes caused that bug as they may not even be connected since long : they have sent their garbage and have disconnected themselves as soon as done , just to crash specific versions of bitcoin core not detecting them . ( note : this is once again occuring with the current unmodified release v23 . <number> from your official site ; it affects both linux and windows versions ) . are there blocks that are extremely long to validate or index ( taking much more than just about <number> minutes normally needed here to process every logged group of about <number> blocks , such as more than <number> minutes in the 1 st break above , but still not completed after several hours at end of the logs above ) ? is not it one of the reasons why segwit was asked for , with blocks grouping many signatures , for example those generated by very large miner pools and needing tons of transactions to validate all their micropayments to many participants ? - - - - see also bug <url> ( may be related with the same race issue when there ' s a new chaintip updated while the indexer is running and iterating from the older chain view ) if this is the cause , the bug is manifested only when you run it on a machine with enough cores to allow more threads to run concurrently . the crashes / hangs i experiment are all runing with <number> cores activates ; your test environment may just have <number> or <number> cores configured , most probably not enough to experiment the race issue between concurrent threads , and not experienced by people running it on cheap pc or notebooks , or in cheap / free vms for azure and amazon ( if you get just <number> or <number> non dedicated cores ) . note also that synchronization issues in code may even more likely to cause bugs on cpus running multiple threads per core , or using more complex parallelism ( e . g . new intel processors with power - efficient and performance cpus , with more complex caching layers , or needing special kernel - initiated firmware mitigations like retpoline against attacks based on latency response time on caches and with speculative execution models ; those possibile attacks and firmware or os mitigations exist on many processors , and their effect on execution latency may vary , depending on the number of active concurrent threads ; if you have low i / o latency on disk storage , this will also increase the likelyhood of race issues caused by improper synchronization . but in all these cases this greatly impacts the leveldb indexers , not properly iterating over the main chain of blocks . ) if you want to experiment it faster , you may want to test it on a machine with many more cpu cores , like newest amd threadripper ( <number> concurrent threads ) . and then try running multiple indexers running in parallel ( the chainstate indexer in ibd , the coinsdbindexer , the txindex , and the coinsdbindex : one of them will crash / hand , or report "" data corruption "" , but not caused by any hardware issue , but incorrect synchronization in bitcoin core ' s code ) . as well it seems that all bugs like <url> ( reported quite often , but always diagnosed in replies "" incorrectly as a "" hardware issue "" , just supposed but never really verified ) are also related to the same synchronization issues between any indexer thread and the thread handling any changes of the tips on the chainstate ( notably if there ' s a change between concurrent forks for new blocks after establishing a consensus . they more likely occurs at end of a trading day , when more blocks are announced for compensating transactions made by major traders ( it ' s then more frequent to get new blocks emitted more rapidly than just once every <number> minutes during the rest of the day . - - - - subsidiary question : is there way to have a knonw list of trustable peers , and a blacklist for bad peers sending fake / corrupted data ? is not bitcoin missing some additional checks of the structure of these blocks ( not just the validation of the individual attempted transactions , but basically its assumed format , includnig for complex multipart signature schemes , or peers attempting to force the use of a hard fork with a disappoved bip proposal , or attemping to use it too early before a valid date and proper nogociation , if they run some specific subchains ) ? one remote peer i remarked was always present when i saw crashes / hangs was this one type net mping ping send recv txn blk hb addrp addrl age id address version out full ipv6 <number> <number> <number> <number> <number> <number> <number> <number> [ <number> <time> d <time> <number> : 2 b2b : : <sad> <number> <number> / satoshi : <number> . <number> ( bitcore - sl ) / uasf - segwit : <number> ( bip148 ) / i have seen it several times sending groups of <number> or <number> unconfirmed blocks with large transactions in a row . it is likely a non - upgraded miner node . ( according to whois , is is hosted by ovh in france , and likely on a fiber access ; it uses the most antique version of satoshi i seen for a "" full node "" , but with local patches trying to enforce bip148 but with wrong version announcements ) . do i need to list it in my banlist ?",0
bitcoin/bitcoin,docfix parameter for ` walletprocesspsbt ` i think the documentation is incorrect since # <number> . see <url>,0
bitcoin/bitcoin,"undefinedbehaviorsanitizer : stack - overflow in miniscript ( descriptor_parse ) to reproduce : ` ` ` wget <url> fuzz = descriptor_parse . / src / test / fuzz / fuzz . / crash - 2 f09727aed5aca089c341208564876bc9c096ebf . bin . not . txt - rss_limit_mb = <number> ` ` ` ` ` ` = = <number> = = error : undefinedbehaviorsanitizer : stack - overflow on address 0x 7 ffcf4e35ff8 ( pc 0x 5 5 a9a0f40e0c bp 0x 7 ffcf4e36010 sp 0x 7 ffcf4e36000 t119584 ) # <number> 0x 5 5 a9a0f40e0c in void __gnu_cxx : : new_allocator < miniscript : : node < unsigned int > <sad> : destroy < miniscript : : node < unsigned int > const > ( miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / ext / new_allocator . h : <number> # <number> 0x 5 5 a9a0f3c96b in void std : : allocator_traits < std : : allocator < miniscript : : node < unsigned int > >>: : destroy < miniscript : : node < unsigned int > const > ( std : : allocator < miniscript : : node < unsigned int > > & , miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / alloc_traits . h : <number> : <number> # <number> 0x 5 5 a9a0f3c96b in std : : _sp_counted_ptr_inplace < miniscript : : node < unsigned int > const , std : : allocator < miniscript : : node < unsigned int > > , ( __gnu_cxx : : _lock_policy ) <number> <sad> : _m_dispose ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : _sp_counted_base < ( __gnu_cxx : : _lock_policy ) <number> <sad> : _m_release ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : __shared_count < ( __gnu_cxx : : _lock_policy ) <number> <sad> : ~ __shared_count ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in std : : __shared_ptr < miniscript : : node < unsigned int > const , ( __gnu_cxx : : _lock_policy ) <number> <sad> : ~ __shared_ptr ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const > > ( std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy_aux <false> : : __destroy < std : : shared_ptr < miniscript : : node < unsigned int > const >*>( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const >*>( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const > > ( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : allocator < std : : shared_ptr < miniscript : : node < unsigned int > const > > & ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : vector < std : : shared_ptr < miniscript : : node < unsigned int > const > , std : : allocator < std : : shared_ptr < miniscript : : node < unsigned int > const > >>: : ~ vector ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_vector . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in miniscript : : node < unsigned int > : : ~ node ( ) src / . / script / miniscript . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void __gnu_cxx : : new_allocator < miniscript : : node < unsigned int > <sad> : destroy < miniscript : : node < unsigned int > const > ( miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / ext / new_allocator . h : <number> <time> # <number> 0x 5 5 a9a0f3c96b in void std : : allocator_traits < std : : allocator < miniscript : : node < unsigned int > >>: : destroy < miniscript : : node < unsigned int > const > ( std : : allocator < miniscript : : node < unsigned int > > & , miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / alloc_traits . h : <number> : <number> # <number> 0x 5 5 a9a0f3c96b in std : : _sp_counted_ptr_inplace < miniscript : : node < unsigned int > const , std : : allocator < miniscript : : node < unsigned int > > , ( __gnu_cxx : : _lock_policy ) <number> <sad> : _m_dispose ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> : <number> . <repeated> . <repeated> . <repeated> + / <number> / bits / stl_vector . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in miniscript : : node < unsigned int > : : ~ node ( ) src / . / script / miniscript . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void __gnu_cxx : : new_allocator < miniscript : : node < unsigned int > <sad> : destroy < miniscript : : node < unsigned int > const > ( miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / ext / new_allocator . h : <number> <time> # <number> 0x 5 5 a9a0f3c96b in void std : : allocator_traits < std : : allocator < miniscript : : node < unsigned int > >>: : destroy < miniscript : : node < unsigned int > const > ( std : : allocator < miniscript : : node < unsigned int > > & , miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / alloc_traits . h : <number> : <number> # <number> 0x 5 5 a9a0f3c96b in std : : _sp_counted_ptr_inplace < miniscript : : node < unsigned int > const , std : : allocator < miniscript : : node < unsigned int > > , ( __gnu_cxx : : _lock_policy ) <number> <sad> : _m_dispose ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : _sp_counted_base < ( __gnu_cxx : : _lock_policy ) <number> <sad> : _m_release ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : __shared_count < ( __gnu_cxx : : _lock_policy ) <number> <sad> : ~ __shared_count ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in std : : __shared_ptr < miniscript : : node < unsigned int > const , ( __gnu_cxx : : _lock_policy ) <number> <sad> : ~ __shared_ptr ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / shared_ptr_base . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const > > ( std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy_aux <false> : : __destroy < std : : shared_ptr < miniscript : : node < unsigned int > const >*>( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const >*>( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const >*) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in void std : : _destroy < std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const > > ( std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : shared_ptr < miniscript : : node < unsigned int > const > * , std : : allocator < std : : shared_ptr < miniscript : : node < unsigned int > const > > & ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_construct . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in std : : vector < std : : shared_ptr < miniscript : : node < unsigned int > const > , std : : allocator < std : : shared_ptr < miniscript : : node < unsigned int > const > >>: : ~ vector ( ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / stl_vector . h : <number> : <number> # <number> 0x 5 5 a9a0f40eec in miniscript : : node < unsigned int > : : ~ node ( ) src / . / script / miniscript . h : <number> <time> # <number> 0x 5 5 a9a0f40eec in void __gnu_cxx : : new_allocator < miniscript : : node < unsigned int > <sad> : destroy < miniscript : : node < unsigned int > const > ( miniscript : : node < unsigned int > const <wink> / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / ext / new_allocator . h : <number> <time> summary : undefinedbehaviorsanitizer / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / ext / new_allocator . h : <number> in void __gnu_cxx : : new_allocator < miniscript : : node < unsigned int > <sad> : destroy < miniscript : : node < unsigned int > const > ( miniscript : : node < unsigned int > const <wink> ` ` `",0
bitcoin/bitcoin,"test : failure in interface_usdt_validation . py <url> ` ` ` bash test <number> - <number> - 0 1 t <time> . 1 0 4 0 0 0 z testframework ( info ) : handle_blockconnected ( <sad> connectedblock ( hash = 2 f97f65fda63aba081bf2867f7f99657c39436306c4aed3c899264b8bde7c5ab height = <number> , transactions = <number> , inputs = <number> , sigops = <number> , duration = <number> ) test <number> - <number> - 0 1 t <time> . 1 0 4 0 0 0 z testframework ( info ) : handle_blockconnected ( <sad> connectedblock ( hash = 5 4 d4bd414e5de85cee4dc5f1e7bcf4e358cf6e7ce556647debe2e9d007459ec7 height = <number> , transactions = <number> , inputs = <number> , sigops = <number> , duration = <number> ) test <number> - <number> - 0 1 t <time> . 1 4 8 0 0 0 z testframework ( info ) : check that we traced <number> blocks test <number> - <number> - 0 1 t <time> . 1 4 9 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / interface_usdt_validation . py "" , line <number> , in run_test assert_equal ( blocks_expected , blocks_checked ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - pc - linux - gnu / test / functional / test_framework / util . py "" , line <number> , in assert_equal raise assertionerror ( "" not ( %s ) "" % "" = = "" . join ( str ( arg ) for arg in ( thing1 , thing2 ) + args ) ) assertionerror = = <number> ) ` ` `",0
bitcoin/bitcoin,"psbt . h : <number> <time> : runtime error : unsigned integer overflow : <number> - <number> cannot be represented in type ' unsigned long ' steps to reproduce : * build with ` integer ` sanitizer * ` ubsan_options = "" suppressions =$( pwd ) / test / sanitizer_suppressions / ubsan : print_stacktrace = <number> : halt_on_error = <number> : report_error_type = <number> "" . / src / qt / bitcoin - qt ` * enter into the rpc debug console "" chnidp8bakoro2mdawmda5ggcaaa / / / / cqataad + / / / 1 aaaaaaaaaaaaaaaqaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaajaaaaaaaaaaaaaaaaaaaaaaaaad + / / / 1 zm9ybmv3nwx1y2vmellmegaaaaaaaaaaaaaaaaaaaamkawmdawmdawmdawmacvmba3fkaaaaaaaaaaaabaalaaaaaaaaacewdq0zdq0ndq0ndq0ncweaah9 / f39 / fwmaaabno6p / / / ka <elongated> = = "" `",0
bitcoin/bitcoin,"bitcoin core <number> installer incompatible with windows mandatory aslr < - - describe the issue - - > try downloading the current version <number> of bitcoin core for windows from the official site : the installer ( exe ) is corrupted ( will not run , corrupted exe format ) only the zip file is correct . workaround can use the <number> installer , then download the <number> zip and extract the executables from there to overwrite the existing exe files ( qt . exe in the main program files \ \ bitcoin folder , other executables in the zip go to the "" daemon "" subfolder ) so the exe package was incorrectly built , or was incorrectly transfered / truncated on the download site on bitcoin . org .",0
bitcoin/bitcoin,"psbt . h : <number> <time> : runtime error : unsigned integer overflow : <number> - <number> cannot be represented in type ' unsigned long ' this needs a code change or suppression added : ` ` ` $ ubsan_options = "" suppressions =$( pwd ) / scratch / fuzz_gen / code / test / sanitizer_suppressions / ubsan : print_stacktrace = <number> : halt_on_error = <number> : report_error_type = <number> "" fuzz = partially_signed_transaction_deserialize / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz / tmp / crash - e4a4fe6f63596cd582f208eea9be69b716f61165 info : running with entropic power schedule ( 0 xff , <number> ) . info : seed : <phone> info : loaded <number> modules ( <number> inline <number> - bit counters ) : <number> [ 0x 5 5 5 8 8 7 da9f40 , 0x 5 5 5 8 8 7 df8935 ) , info : loaded <number> pc tables ( <number> pcs ) : <number> [ 0x 5 5 5 8 8 7 df8938 , 0 x5558882e2888 ) , / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz : running <number> inputs <number> time ( s ) each . running : / tmp / crash - e4a4fe6f63596cd582f208eea9be69b716f61165 psbt . h : <number> <time> : runtime error : unsigned integer overflow : <number> - <number> cannot be represented in type ' unsigned long ' # <number> 0x 5 5 5 8 8 5 2 7 1 5 9 8 in void psbtoutput : : unserialize <cdatastream> ( cdatastream & ) src / . / psbt . h : <number> <time> # <number> 0x 5 5 5 8 8 5 2 3 3 a4e in void unserialize < cdatastream , psbtoutput & > ( cdatastream & , psbtoutput & ) src / . / serialize . h : <number> : <number> # <number> 0x 5 5 5 8 8 5 2 3 3 a4e in cdatastream & cdatastream : : operator > > < psbtoutput & > ( psbtoutput & ) src / . / streams . h : <number> : <number> # <number> 0x 5 5 5 8 8 5 2 3 0 2 4 f in void partiallysignedtransaction : : unserialize <cdatastream> ( cdatastream & ) src / . / psbt . h : <number> <time> # <number> 0x 5 5 5 8 8 5 2 2 edee in void unserialize < cdatastream , partiallysignedtransaction & > ( cdatastream & , partiallysignedtransaction & ) src / . / serialize . h : <number> : <number> # <number> 0x 5 5 5 8 8 5 2 2 edee in cdatastream & cdatastream : : operator > > < partiallysignedtransaction & > ( partiallysignedtransaction & ) src / . / streams . h : <number> : <number> # <number> 0x <phone> f5 in void ( anonymous namespace ) : : deserializefromfuzzinginput <partiallysignedtransaction> ( span < unsigned char const > , partiallysignedtransaction & , std : : optional <int> , int ) src / . / src / test / fuzz / deserialize . cpp : <number> <time> # <number> 0x <phone> f5 in partially_signed_transaction_deserialize_fuzz_target ( span < unsigned char const > ) src / . / src / test / fuzz / deserialize . cpp : <number> : <number> # <number> 0x 5 5 5 8 8 5 1 0 6 6 8 2 in std : : _function_handler < void ( span < unsigned char const > ) , void (* ) ( span < unsigned char const > )>: : _m_invoke ( std : : _any_data const & , span < unsigned char const > & & ) / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / std_function . h : <number> : <number> # <number> 0x 5 5 5 8 8 5 4 9 ae5a in std : : function < void ( span < unsigned char const > )>: : operator ( ) ( span < unsigned char const > ) const / usr / bin / . <repeated> / lib / gcc / x86_64 - linux - gnu / <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> / bits / std_function . h : <number> <time> # <number> 0x 5 5 5 8 8 5 4 9 aad6 in llvmfuzzertestoneinput src / . / src / test / fuzz / fuzz . cpp : <number> : <number> # <number> 0x 5 5 5 8 8 5 0 2 9 3 7 2 in fuzzer : : fuzzer : : executecallback ( unsigned char const * , unsigned long ) ( / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz + 0x 1 4 1 a372 ) ( buildid : 8 e23fc37575bb16be5b418c47853b5da4e548abb ) # <number> 0x <phone> d0 in fuzzer : : runonetest ( fuzzer : : fuzzer * , char const * , unsigned long ) ( / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz + 0x 1 4 0 4 8 d0 ) ( buildid : 8 e23fc37575bb16be5b418c47853b5da4e548abb ) # <number> 0x 5 5 5 8 8 5 0 1 9 5 8 7 in fuzzer : : fuzzerdriver ( int * , char * * * , int (* ) ( unsigned char const * , unsigned long ) ) ( / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz + 0x 1 4 0 a587 ) ( buildid : 8 e23fc37575bb16be5b418c47853b5da4e548abb ) # <number> 0x 5 5 5 8 8 5 0 4 2 3 4 2 in main ( / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz + 0x 1 4 3 3 3 4 2 ) ( buildid : 8 e23fc37575bb16be5b418c47853b5da4e548abb ) # <number> 0x 7 fa7994a3082 in __libc_start_main / build / glibc - sziz7b / glibc - <number> / csu / . <repeated> / csu / libc - start . c : <number> <time> # <number> 0x 5 5 5 8 8 5 0 0 e1cd in _start ( / root / fuzz_dir / scratch / fuzz_gen / code / src / test / fuzz / fuzz + 0x 1 3 ff1cd ) ( buildid : 8 e23fc37575bb16be5b418c47853b5da4e548abb ) summary : undefinedbehaviorsanitizer psbt . h : <number> <time> in ` ` ` ` ` ` $ base64 / tmp / crash - e4a4fe6f63596cd582f208eea9be69b716f61165 rpckghbzynt / aqa9aaaaaaf6aagdewea + 8 qaap9glcalb / itycf / bwabar4aaaaawaaaaaaaghbz ygebtetaixpsmaeaaan8 / wabkwah / acoccgaaf <elongated> / aaaapweaaaaaab8aaaaawqaa / waaaf8ai4j ycquaaf8aaia / wgbaaf8aaajyaaaaceh / aajqfkygag <elongated> / aacseaassbkwah <elongated> / acoccgaaf <elongated> / a <elongated> apweaaaaaab8aaaaaaag / aaraafkaap8aaah / acocqaaiqag / a <elongated> / wgbaaf8aaajwgaaaaf8aaaw awtkaab8aaabqa8ab / waajassab <elongated> / wajgniaaaabfwaaad8baaaaaag / aeg <elongated> / abkaap8 ah <elongated> / aaccgaah <elongated> / aanqfkygag <elongated> / aaaauapaaf8aakhaaeraaf8aaaacfz / aaeraaf8jgkaamga aax8aaaa / aqaaaaabvwaaaaaaab8aaabzaad / ab <elongated> / wajgniji4ab / waagaag <elongated> / a <elongated> / wgab / wa adabzmoabvwaafadwah <elongated> / aajiqabkwah / aaaaan8 / wabkwah / acoccgaaf <elongated> / aaaapweaaaaaab8 aawqaa <elongated> / waaaf8ai4jycyuaaf8aaia / wgbaaf8aaajyaaaaaf8aacnawtkaab8aaajiqabkwer aaf8ai4jyaaaaax8aaaa / aqaaaaabvwaaaaaaab8aaabzaad / ab <elongated> / wajgniji4ab / waagaacea <elongated> bvwaap8iaqah / aacciaah <elongated> / aamafkygag <elongated> / aaaauapaaf8aakhapwaaad8aaqaaaag / afk <elongated> aap8aaah / acoccgklgah / aacap8iaqah / aaccgaah <elongated> / aajqfk <elongated> / aacciaah <elongated> / aamafkygag <elongated> / aaaauapaaf8aaaacqeraaf8ai4jyaaaaax8aaaa / aqaabvwaabbvwazaad <elongated> / ab <elongated> / wa aaniaab <elongated> / waadubzmoabvwaafadwah <elongated> / aajiqabkwah / aaaaan8 / wabkwah / acoccgaaf <elongated> / a <elongated> apweaaaaaab8aaaaaaag / aawqaa <elongated> / waaaf8ai4jycyuaaf8aaiaaaaabvwaaaaaaaaaaaaaaaaa aad / caah / aamafkygag <elongated> / aaaauapaaf8aakhaaeraaf8aaaacfz / aaeraaf8ai4jyaaaaax8aaaa / aqaabvwaabzaad <elongated> / ab <elongated> / wajgniji4aa / waaaf8ai4jycquaaf8aaia / wgbaaf8aaajyaaa aaf8aacnqgtkaab8aaajiqabkweraaf8ai4jyaaaaax8aaaa / aqaaaaabvwaaaaaaab8acsaawqa a / waaaf8ai4jaaahaab8aad / caeab / waancaab <elongated> / waadabzmoabvwaafadwah <elongated> / aakbkwah <elongated> / acoccgaaf <elongated> / aaaapweaaaaaab8aaaaaaaaaqb8agqaa / waaaf8e2an / wcaaqeeav0aahaka / kv yi4a / wteaad / yj4iiiiiiq4aaaaaaaa =",0
bitcoin/bitcoin,"intermittent failure in feature_startupnotify . py on the master branch : <url> ` ` ` <number> / <number> - feature_startupnotify . py failed , duration : <number> s stdout : <number> - <number> - 1 9 t <time> . 3 6 6 0 0 0 z testframework ( info ) : initializing test directory c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ test_runner_ ₿ _ 🏃 _20220719_080912 \ \ feature_startupnotify_71 <number> - <number> - 1 9 t <time> . 9 6 2 0 0 0 z testframework ( info ) : test - startupnotify command is run when node starts <number> - <number> - 1 9 t <time> . 0 1 9 0 0 0 z testframework ( info ) : test - startupnotify is executed once <number> - <number> - 1 9 t <time> . 0 1 9 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in main self . run_test ( ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ feature_startupnotify . py "" , line <number> , in run_test assert_equal ( file_content . count ( file_name ) , <number> ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ util . py "" , line <number> , in assert_equal raise assertionerror ( "" not ( %s ) "" % "" = = "" . join ( str ( arg ) for arg in ( thing1 , thing2 ) + args ) ) assertionerror = = <number> ) ` ` ` did not find an existing open issue but looks to be the same win64 ci failure reported in <url>",0
bitcoin/bitcoin,"verifychain <number> <number> aborts with assertion "" hashprevblock = = view . getbestblock ( ) "" failed providing a low number of blocks works , however , it just crashed for me with <number> blocks . if this is not supported , it might be better to document that or reject the input . commit 6 9 1 a08718beff31d1b821b192609ea3bfdb24d41 output : ` ` ` bitcoin - qt : validation . cpp : <number> : bool cchainstate : : connectblock ( const cblock & , blockvalidationstate & , cblockindex * , ccoinsviewcache & , bool ) ` hashprevblock = = view . getbestblock ( ) ' failed .",0
bitcoin/bitcoin,"gui for readlink buffer overflow and handle gracefully if readlink returns the size of the buffer , an overflow may have ( safely ) occurred . pass a buffer size of max_path + <number> ( the size of the actual buffer ) to detect this scenario .",0
bitcoin/bitcoin,"confusing filtering by block hash behaviour in ` listsinceblock ` the reproduction speaks for itself : ` ` ` diff diff - - git a / test / functional / wallet_listsinceblock . py b / test / functional / wallet_listsinceblock . py index fc06565983 . <repeated> e06fbf120a <number> - - - a / test / functional / wallet_listsinceblock . py + + + b / test / functional / wallet_listsinceblock . py @ @ - <number> + <number> @ @ class listsinceblocktest ( bitcointestframework ) : self . connect_nodes ( <number> , <number> ) self . generate ( self . nodes [ <number> ] , coinbase_maturity + <number> ) - self . test_no_blockhash ( ) - self . test_invalid_blockhash ( ) - self . test_reorg ( ) - self . test_double_spend ( ) - self . test_double_send ( ) - self . double_spends_filtered ( ) - self . test_targetconfirmations ( ) + # self . test_no_blockhash ( ) + # self . test_invalid_blockhash ( ) + # self . test_reorg ( ) + # self . test_double_spend ( ) + # self . test_double_send ( ) + # self . double_spends_filtered ( ) + # self . test_targetconfirmations ( ) + self . test_consistent_with_gettransaction ( ) def test_no_blockhash ( self ) : self . log . info ( "" test no blockhash "" ) @ @ - <number> + <number> @ @ class listsinceblocktest ( bitcointestframework ) : assert_equal ( original_found , false ) assert_equal ( double_found , false ) + def test_consistent_with_gettransaction ( self ) : + "" "" "" test the filtering in listtransactions is consistent with gettransaction ' s + output . + + the block hash parameter gives , according to the documentation , "" the block hash + to list transactions since "" . test that if we have a transaction confirmed at a + certain block , listing the coins since this block will output this transaction . + "" "" "" + txid = self . nodes [ <number> ] . sendtoaddress ( self . nodes [ <number> ] . getnewaddress ( ) , <number> ) + self . generate ( self . nodes [ <number> ] , <number> ) + tx = self . nodes [ <number> ] . gettransaction ( txid ) + coins = self . nodes [ <number> ] . listsinceblock ( tx [ "" blockhash "" ] ) [ "" transactions "" ] + assert txid in ( c [ "" txid "" ] for c in coins ) + if __name__ = = ' __main__ ' ` ` ` is it intended that filtering transaction "" since "" block n only output transactions confirmed from block n + <number> ?",0
bitcoin/bitcoin,"qa : intermittent failure in ` wallet_encryption . py - - descriptors ` <url> <details> <summary> log excerpt </summary> <p> ` ` ` <number> / <number> - wallet_encryption . py - - descriptors failed , duration : <number> s stdout : <number> - <number> - 2 7 t <time> . 0 9 5 0 0 0 z testframework ( info ) : initializing test directory c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ test_runner_ ₿ _ 🏃 _20220627_071202 \ \ wallet_encryption_65 <number> - <number> - 2 7 t <time> . 8 2 0 0 0 0 z testframework ( info ) : check a timeout less than the limit <number> - <number> - 2 7 t <time> . 0 0 7 0 0 0 z testframework ( error ) : assertion failed traceback ( most recent call last ) : file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ test_framework . py "" , line <number> , in main self . run_test ( ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ wallet_encryption . py "" , line <number> , in run_test assert_greater_than ( expected_time_with_buffer , actual_time ) file "" c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ test_framework \ \ util . py "" , line <number> , in assert_greater_than raise assertionerror ( "" %s <= %s "" % ( str ( thing1 ) , str ( thing2 ) ) ) assertionerror : <phone> . <number> <= <phone> <number> - <number> - 2 7 t <time> . 0 7 0 0 0 0 z testframework ( info ) : stopping nodes <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( warning ) : not cleaning up dir c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ test_runner_ ₿ _ 🏃 _20220627_071202 \ \ wallet_encryption_65 <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : test failed . test logging available at c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ test_runner_ ₿ _ 🏃 _20220627_071202 \ \ wallet_encryption_65 / test_framework . log <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : hint : call c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ cirrus - ci - build \ \ test \ \ functional \ \ combine_logs . py ' c :\\ users \ \ containeradministrator \ \ appdata \ \ local \ \ temp \ \ test_runner_ ₿ _ 🏃 _20220627_071202 \ \ wallet_encryption_65 ' to consolidate all logs <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : if this failure happened unexpectedly or intermittently , please file a bug and provide a link or upload of the combined log . <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) : <url> <number> - <number> - 2 7 t <time> . 2 1 1 0 0 0 z testframework ( error ) </p> </details>",0
bitcoin/bitcoin,"threadsanitizer : data race on vptr ( ctor / dtor vs virtual call ) in baseindex <url> ` ` ` bash warning : threadsanitizer : data race on vptr ( ctor / dtor vs virtual call ) ( pid = <number> ) write of size <number> at 0x 7 ffe0efae9f8 by main thread : # <number> baseindex : : ~ baseindex ( ) src / index / base . cpp : <number> : <number> ( test_bitcoin + 0 xcc6b69 ) # <number> coinstatsindex : : ~ coinstatsindex ( ) src / . / index / coinstatsindex . h : <number> : <number> ( test_bitcoin + 0x3 b9b21 ) # <number> coinstatsindex_tests : : coinstatsindex_initial_sync : : test_method ( ) src / test / coinstatsindex_tests . cpp : <number> : <number> ( test_bitcoin + 0x3 b9b21 ) # <number> coinstatsindex_tests : : coinstatsindex_initial_sync_invoker ( ) src / test / coinstatsindex_tests . cpp : <number> : <number> ( test_bitcoin + 0x3 b814b ) # <number> boost : : detail : : function : : void_function_invoker0 < void (* ) ( ) , void > : : invoke ( boost : : detail : : function : : function_buffer & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 bbf1d ) # <number> boost : : function0 <void> : : operator ( ) ( ) const / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : detail : : forward : : operator ( ) ( ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : detail : : function : : function_obj_invoker0 < boost : : detail : : forward , int > : : invoke ( boost : : detail : : function : : function_buffer & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : function0 <int> : : operator ( ) ( ) const / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> int boost : : detail : : do_invoke < boost : : shared_ptr < boost : : detail : : translator_holder_base > , boost : : function < int ( ) > > ( boost : : shared_ptr < boost : : detail : : translator_holder_base > const & , boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> boost : : execution_monitor : : catch_signals ( boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> boost : : execution_monitor : : execute ( boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae8c0 ) # <number> boost : : execution_monitor : : vexecute ( boost : : function < void ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> : <number> ( test_bitcoin + 0x 1 aa21b ) # <number> boost : : unit_test : : unit_test_monitor_t : : execute_and_translate ( boost : : function < void ( ) > const & , unsigned long ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_monitor . ipp : <number> : <number> ( test_bitcoin + 0x 1 aa21b ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 ddb63 ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 de1d8 ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 de1d8 ) # <number> boost : : unit_test : : framework : : run ( unsigned long , bool ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 a8e66 ) # <number> boost : : unit_test : : unit_test_main ( boost : : unit_test : : test_suite * (* ) ( int , char * <wink> , int , char * <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_main . ipp : <number> : <number> ( test_bitcoin + 0x 1 c19c6 ) # <number> main / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_main . ipp : <number> <time> ( test_bitcoin + 0x 1 c1ff6 ) previous read of size <number> at 0x 7 ffe0efae9f8 by thread t1 ( mutexes : write m603 ) : # <number> baseindex : : setbestblockindex ( cblockindex const <wink> : : $ _1 : : operator ( ) ( ) const src / index / base . cpp : <number> : <number> ( test_bitcoin + 0 xcc74e6 ) # <number> baseindex : : setbestblockindex ( cblockindex const <wink> src / index / base . cpp : <number> : <number> ( test_bitcoin + 0 xcc74e6 ) # <number> baseindex : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> src / index / base . cpp : <number> : <number> ( test_bitcoin + 0 xcc9759 ) # <number> cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _8 : : operator ( ) ( ) const : : ' lambda ' ( cvalidationinterface & <sad> : operator ( ) ( cvalidationinterface & ) const src / validationinterface . cpp : <number> : <number> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> void mainsignalsimpl : : iterate < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _8 : : operator ( ) ( ) const : : ' lambda ' ( cvalidationinterface & ) > ( cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _8 : : operator ( ) ( ) const : : ' lambda ' ( cvalidationinterface & ) & & ) src / validationinterface . cpp : <number> <time> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _8 : : operator ( ) ( ) const src / validationinterface . cpp : <number> <time> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 : : operator ( ) ( ) const src / validationinterface . cpp : <number> : <number> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> decltype ( static_cast < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 & > ( fp ) ( ) ) std : : __1 : : __invoke < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 & > ( cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / type_traits : <number> : <number> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> void std : : __1 : : __invoke_void_return_wrapper < void , true > : : __call < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 & > ( cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / invoke . h : <number> : <number> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> std : : __1 : : __function : : __alloc_func < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 , std : : __1 : : allocator < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> std : : __1 : : __function : : __func < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 , std : : __1 : : allocator < cmainsignals : : blockconnected ( std : : __1 : : shared_ptr < cblock const > const & , cblockindex const <wink> : : $ _9 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 2 2 3 a4 ) # <number> std : : __1 : : __function : : __value_func < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b6b71 ) # <number> std : : __1 : : function < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b6b71 ) # <number> singlethreadedschedulerclient : : processqueue ( ) src / scheduler . cpp : <number> : <number> ( test_bitcoin + 0x 1 0 b6b71 ) # <number> singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 : : operator ( ) ( ) const src / scheduler . cpp : <number> <time> ( test_bitcoin + 0x 1 0 b8875 ) # <number> decltype ( static_cast < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 & > ( fp ) ( ) ) std : : __1 : : __invoke < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 & > ( singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / type_traits : <number> : <number> ( test_bitcoin + 0x 1 0 b8875 ) # <number> void std : : __1 : : __invoke_void_return_wrapper < void , true > : : __call < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 & > ( singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / invoke . h : <number> : <number> ( test_bitcoin + 0x 1 0 b8875 ) # <number> std : : __1 : : __function : : __alloc_func < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 , std : : __1 : : allocator < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b8875 ) # <number> std : : __1 : : __function : : __func < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 , std : : __1 : : allocator < singlethreadedschedulerclient : : maybescheduleprocessqueue ( <sad> : $ _1 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b8875 ) # <number> std : : __1 : : __function : : __value_func < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b5b5c ) # <number> std : : __1 : : function < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 0 b5b5c ) # <number> cscheduler : : servicequeue ( ) src / scheduler . cpp : <number> <time> ( test_bitcoin + 0x 1 0 b5b5c ) # <number> chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 : : operator ( ) ( ) const src / test / util / setup_common . cpp : <number> <time> <number> ( test_bitcoin + 0 xa4e7b8 ) # <number> decltype ( static_cast < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & > ( fp ) ( ) ) std : : __1 : : __invoke < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & > ( chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / type_traits : <number> : <number> ( test_bitcoin + 0 xa4e7b8 ) # <number> void std : : __1 : : __invoke_void_return_wrapper < void , true > : : __call < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & > ( chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / invoke . h : <number> : <number> ( test_bitcoin + 0 xa4e7b8 ) # <number> std : : __1 : : __function : : __alloc_func < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 , std : : __1 : : allocator < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0 xa4e7b8 ) # <number> std : : __1 : : __function : : __func < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 , std : : __1 : : allocator < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > , void ()>: : operator ( ) ( ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0 xa4e7b8 ) # <number> std : : __1 : : __function : : __value_func < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 1 5 7 6 0 f ) # <number> std : : __1 : : function < void ()>: : operator ( ) ( ) const / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __functional / function . h : <number> <time> ( test_bitcoin + 0x 1 1 5 7 6 0 f ) # <number> util : : tracethread ( char const * , std : : __1 : : function < void ()>) src / util / thread . cpp : <number> : <number> ( test_bitcoin + 0x 1 1 5 7 6 0 f ) # <number> decltype ( static_cast < void (*> ( fp ) ( static_cast < char const *>( fp0 ) , static_cast < chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > ( fp0 ) ) ) std : : __1 : : __invoke < void (* ) ( char const * , std : : __1 : : function < void ()>), char const * , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > ( void (* & & ) ( char const * , std : : __1 : : function < void ()>), char const * & & , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / type_traits : <number> : <number> ( test_bitcoin + 0 xa4e3b1 ) # <number> void std : : __1 : : __thread_execute < std : : __1 : : unique_ptr < std : : __1 : : __thread_struct , std : : __1 : : default_delete < std : : __1 : : __thread_struct > > , void (* ) ( char const * , std : : __1 : : function < void ()>), char const * , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 , 2 ul , 3 ul > ( std : : __1 : : tuple < std : : __1 : : unique_ptr < std : : __1 : : __thread_struct , std : : __1 : : default_delete < std : : __1 : : __thread_struct > > , void (* ) ( char const * , std : : __1 : : function < void ()>), char const * , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > & , std : : __1 : : __tuple_indices < 2 ul , 3 ul > ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / thread : <number> : <number> ( test_bitcoin + 0 xa4e3b1 ) # <number> void * std : : __1 : : __thread_proxy < std : : __1 : : tuple < std : : __1 : : unique_ptr < std : : __1 : : __thread_struct , std : : __1 : : default_delete < std : : __1 : : __thread_struct > > , void (* ) ( char const * , std : : __1 : : function < void ()>), char const * , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 > > ( void <wink> / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / thread : <number> : <number> ( test_bitcoin + 0 xa4e3b1 ) location is stack of main thread . location is global ' ? <repeated> ' at 0x 7 ffe0ef91000 ( [ stack ] + 0x0 0 0 0 0 0 0 1 d9f8 ) mutex m603 ( 0x 5 5 8 df2c934a0 ) created at : # <number> pthread_mutex_init <null> ( test_bitcoin + 0x 1 1 cf6f ) # <number> std : : __1 : : recursive_mutex : : recursive_mutex ( ) <null> ( libc + + . so . <number> + 0x 4 9 fb3 ) # <number> __libc_start_main <null> ( libc . so . <number> + 0x 2 9 eba ) thread t1 ' b - scheduler ' ( tid = <number> , running ) created by main thread at : # <number> pthread_create <null> ( test_bitcoin + 0x 1 1 b7fd ) # <number> std : : __1 : : __libcpp_thread_create ( unsigned long * , void * (* ) ( void <wink> , void <wink> / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / __threading_support : <number> <time> ( test_bitcoin + 0 xa47a76 ) # <number> std : : __1 : : thread : : thread < void ( & ) ( char const * , std : : __1 : : function < void ()>), char const ( & ) [ <number> ] , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 , void > ( void ( & ) ( char const * , std : : __1 : : function < void ()>), char const ( & ) [ <number> ] , chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & <sad> : $ _0 & & ) / usr / lib / llvm - <number> / bin / . <repeated> / include / c + + / v1 / thread : <number> <time> ( test_bitcoin + 0 xa47a76 ) # <number> chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) src / test / util / setup_common . cpp : <number> <time> ( test_bitcoin + 0 xa47a76 ) # <number> testingsetup : : testingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) src / test / util / setup_common . cpp : <number> : <number> ( test_bitcoin + 0 xa47ed9 ) # <number> testchain100setup : : testchain100setup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) src / test / util / setup_common . cpp : <number> : <number> ( test_bitcoin + 0 xa48be3 ) # <number> coinstatsindex_tests : : coinstatsindex_initial_sync : : coinstatsindex_initial_sync ( ) src / test / coinstatsindex_tests . cpp : <number> : <number> ( test_bitcoin + 0x3 b7c8b ) # <number> coinstatsindex_tests : : coinstatsindex_initial_sync_invoker ( ) src / test / coinstatsindex_tests . cpp : <number> : <number> ( test_bitcoin + 0x3 b7c8b ) # <number> boost : : detail : : function : : void_function_invoker0 < void (* ) ( ) , void > : : invoke ( boost : : detail : : function : : function_buffer & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 bbf1d ) # <number> boost : : function0 <void> : : operator ( ) ( ) const / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : detail : : forward : : operator ( ) ( ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : detail : : function : : function_obj_invoker0 < boost : : detail : : forward , int > : : invoke ( boost : : detail : : function : : function_buffer & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 2 2 0 8 7 7 ) # <number> boost : : function0 <int> : : operator ( ) ( ) const / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / function / function_template . hpp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> int boost : : detail : : do_invoke < boost : : shared_ptr < boost : : detail : : translator_holder_base > , boost : : function < int ( ) > > ( boost : : shared_ptr < boost : : detail : : translator_holder_base > const & , boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> boost : : execution_monitor : : catch_signals ( boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae59e ) # <number> boost : : execution_monitor : : execute ( boost : : function < int ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> <time> ( test_bitcoin + 0x 1 ae8c0 ) # <number> boost : : execution_monitor : : vexecute ( boost : : function < void ( ) > const & ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / execution_monitor . ipp : <number> : <number> ( test_bitcoin + 0x 1 aa21b ) # <number> boost : : unit_test : : unit_test_monitor_t : : execute_and_translate ( boost : : function < void ( ) > const & , unsigned long ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_monitor . ipp : <number> : <number> ( test_bitcoin + 0x 1 aa21b ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 ddb63 ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 de1d8 ) # <number> boost : : unit_test : : framework : : state : : execute_test_tree ( unsigned long , unsigned long , boost : : unit_test : : framework : : state : : random_generator_helper const <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 de1d8 ) # <number> boost : : unit_test : : framework : : run ( unsigned long , bool ) / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / framework . ipp : <number> <time> ( test_bitcoin + 0x 1 a8e66 ) # <number> boost : : unit_test : : unit_test_main ( boost : : unit_test : : test_suite * (* ) ( int , char * <wink> , int , char * <wink> / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_main . ipp : <number> : <number> ( test_bitcoin + 0x 1 c19c6 ) # <number> main / tmp / cirrus - ci - build / depends / x86_64 - pc - linux - gnu / include / boost / test / impl / unit_test_main . ipp : <number> <time> ( test_bitcoin + 0x 1 c1ff6 ) summary : threadsanitizer : data race on vptr ( ctor / dtor vs virtual call ) src / index / base . cpp : <number> : <number> in baseindex : : ~ baseindex ( ) = = = = = = = = = = = = = = = = = = exit status ` ` `",0
bitcoin/bitcoin,". / wallet / bdb . h : <time> : fatal error : db_cxx . h : no such file or directory i am having build error with current master ( 1 c7ef0abd11f35a27cc860ceb7e075b78f53cecf ) , with gentoo linux : ` ` ` $ make clean $ . / autogen . sh $ . / configure $ make . <repeated> cxx qt / libbitcoinqt_a - rpcconsole . o in file included from qt / rpcconsole . cpp : <number> : . / wallet / bdb . h : <time> : fatal error : db_cxx . h such file or directory <number> | <hashtag> include </hashtag> < db_cxx . h > | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated . $ locate db_cxx . h / usr / include / db4 . <number> / db_cxx . h / usr / include / db5 . <number> / db_cxx . h ` ` ` reverting 4 6 a890960e4b07e5aec479aa8e07e9c34ce68aee (# <number> ) fixes the problem .",0
bitcoin/bitcoin,nlocktime bug in bitcoin core v23 recreating the bug report <url> because the previous one closed by a party who does not know nothing about bitcoin apparently . * * block height * * for nlocktime does not work and this poses a * * serious risk for smart contracts * * . i have added block height to nlocktime and got converted to timestamp which corresponds to <number> / <number> / <number> <time> found this bug when verifying a raw transaction .,0
bitcoin/bitcoin,"depends bdb - werror = format - security "" format not a string literal and no format arguments "" running ` make - c depends ` on nixos with gcc <number> . <number> results in ` ` ` c + + libtool : compile : gcc - c - i . - i . <repeated> / dist / . / . <repeated> - i / home / russ / work / bitcoin / depends / x86_64 - pc - linux - gnu / include - d_gnu_source - d_reentrant - pipe - o2 - wno - error = implicit - function - declaration . <repeated> / dist / . / . <repeated> / txn / txn . c - fpic - dpic - o txn . o . <repeated> / dist / . / . <repeated> / txn / txn . c : in function ‘ __txn_begin ’ : . <repeated> / dist / . / . <repeated> / txn / txn . c : <number> : <number> : error : format not a string literal and no format arguments [ - werror = format - security ] <number> | __db_errx ( env , txnalloc ) ; | ^ ~ ~ ~ ~ ~ ~ ~ ~ . <repeated> / dist / . / . <repeated> / txn / txn . c : in function ‘ __txn_compensate_begin ’ : . <repeated> / dist / . / . <repeated> / txn / txn . c : <number> : <number> : error : format not a string literal and no format arguments [ - werror = format - security ] <number> | __db_errx ( env , txnalloc ) ; | ^ ~ ~ ~ ~ ~ ~ ~ ~ cc1 : some warnings being treated as errors make [ <number> <sad> * * * [ makefile : <number> : txn . o ] error <number> make [ <number> <sad> leaving directory ' / home / russ / work / bitcoin / depends / work / build / x86_64 - pc - linux - gnu / bdb / <date> - 1 2 0 5 7 0 3 c56a / build_unix ' make : * * * [ funcs . mk : <number> : / home / russ / work / bitcoin / depends / work / build / x86_64 - pc - linux - gnu / bdb / <date> - 1 2 0 5 7 0 3 c56a / build_unix / . stamp_built ] error <number> make directory ' / home / russ / work / bitcoin / depends ' ` ` ` i could work around it with ` ` ` diff diff - - git a / depends / packages / bdb . mk b / depends / packages / bdb . mk index dc536fd3991 . <repeated> b69276cb154 <number> - - - a / depends / packages / bdb . mk + + + b / depends / packages / bdb . mk @ @ - <number> + <number> @ @ $( package ) _config_opts_freebsd = - - with - pic $( package ) _config_opts_netbsd = - - with - pic $( package ) _config_opts_openbsd = - - with - pic $( package ) _config_opts_android = - - with - pic - $( package ) _cflags + = - wno - error = implicit - function - declaration + $( package ) _cflags + = - wno - error = implicit - function - declaration - wno - error = format - security $( package ) _cxxflags + = - std =c + + <number> $( package ) _cppflags_mingw32 = - dunicode - d_unicode endef ` ` ` but i do not know if this is the right fix or if there possibly is a real bug in the code causing the warning .",0
bitcoin/bitcoin,"starting with an unsupported wallet configured leads to a segfault ( master only ? ) ` ` ` # <number> __ubsan : : ubsanondeadlysignal ( int , void * , void <wink> ( ) at / var / tmp / portage / sys - libs / compiler - rt - sanitizers - <number> . <number> / work / compiler - rt / lib / ubsan / ubsan_signals_standalone . cpp : <number> # <number> < signal handler called > # <number> std : : __detail : : _list_node_base : : _m_unhook ( this =0 x13dca9a70 ) at / var / tmp / portage / sys - devel / gcc - <number> . <number> / work / gcc - <number> . <number> / libstdc + + - v3 / src / c + + <number> / list . cc : <number> # <number> 0x0 0 0 0 0 0 0 1 1 f25c9c0 in std : : __cxx11 : : list < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > , std : : allocator < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > > <sad> : _m_erase ( std : : _list_iterator < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > > ) ( this =0 x7ffef8013e98 , __position = . <repeated> ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / stl_list . h : <number> # <number> 0x0 0 0 0 0 0 0 1 1 f241cc4 in std : : __cxx11 : : list < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > , std : : allocator < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > > <sad> : erase ( std : : _list_const_iterator < std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > > ) > > ) ( this =0 x7ffef8013e98 , __position = . <repeated> ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / list . tcc : <number> # <number> wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 : : operator ( ) ( ) const ( this =< optimized out > ) at wallet / wallet . cpp : <number> # <number> std : : __invoke_impl < void , wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 & > ( std : : __invoke_other , wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 & ) ( __f = . <repeated> ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / invoke . h : <number> # <number> std : : __invoke_r < void , wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 & > ( wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 & ) ( __fn = . <repeated> ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / invoke . h : <number> # <number> std : : _function_handler < void ( ) , wallet : : handleloadwallet ( wallet : : walletcontext & , std : : function < void ( std : : unique_ptr < interfaces : : wallet , std : : default_delete < interfaces : : wallet > >)>): : $ _1 > : : _m_invoke ( std : : _any_data const & ) ( __functor = . <repeated> ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / std_function . h : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e8fb1f8 in std : : function < void ()>: : operator ( ) ( ) const ( this =< optimized out > ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / std_function . h : <number> # <number> 0x0 0 0 0 0 0 0 1 1 f08d6c0 in interfaces : : ( anonymous namespace ) : : cleanuphandler : : ~ cleanuphandler ( this =0 x7ffef801edd0 ) at interfaces / handler . cpp : <number> # <number> 0x0 0 0 0 0 0 0 1 1 f08d7fc in interfaces : : ( anonymous namespace ) : : cleanuphandler : : ~ cleanuphandler ( this =0 x7ffef801edd0 ) at interfaces / handler . cpp : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e7fcd34 in std : : default_delete < interfaces : : handler > : : operator ( ) ( __ptr =0 x7ffef801edd0 , this =< optimized out > ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / unique_ptr . h : <number> # <number> std : : unique_ptr < interfaces : : handler , std : : default_delete < interfaces : : handler > <sad> : ~ unique_ptr ( this =< optimized out > ) at / usr / lib / gcc / powerpc64le - unknown - linux - gnu / <number> . <number> / include / g + + - v11 / bits / unique_ptr . h : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e90e038 in splashscreen : : ~ splashscreen ( this =0 x13dc49f00 ) at qt / splashscreen . cpp : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e90e6ec in splashscreen : : ~ splashscreen ( this =0 x13dc49f00 ) at qt / splashscreen . cpp : <number> # <number> 0x0 0 0 0 7 fffa4ff35f4 in qdeleteineventhandler ( o= < optimized out > ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qobject . cpp : <number> # <number> 0x0 0 0 0 7 fffa4ff8550 in qobject : : event ( this =< optimized out > , e =< optimized out > ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qobject . cpp : <number> # <number> 0x0 0 0 0 7 fffa45bd41c in qwidget : : event ( this =0 x13dc49f00 , event =0 x7fff98023f40 ) at / var / tmp / portage / dev - qt / qtwidgets - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / widgets / kernel / qwidget . cpp : <number> # <number> 0x0 0 0 0 7 fffa455da70 in qapplicationprivate : : notify_helper ( this =< optimized out > , receiver =0 x13dc49f00 , e =0 x7fff98023f40 ) at / var / tmp / portage / dev - qt / qtwidgets - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / widgets / kernel / qapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa4569374 in qapplication : : notify ( this =0 x7ffff9f207a0 , receiver =0 x13dc49f00 , e =0 x7fff98023f40 ) at / var / tmp / portage / dev - qt / qtwidgets - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / widgets / kernel / qapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa4faec80 in qcoreapplication : : notifyinternal2 ( receiver =0 x13dc49f00 , event =0 x7fff98023f40 ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qcoreapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa4faf00c in qcoreapplication : : sendevent ( receiver =< optimized out > , event =< optimized out > ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qcoreapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa4fb3ee4 in qcoreapplicationprivate : : sendpostedevents ( receiver =0 x0 , event_type = <number> , data =0 x13dad8900 ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qcoreapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa4fb4244 in qcoreapplication : : sendpostedevents ( receiver =0 x0 , event_type =< optimized out > ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qcoreapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa5037290 in posteventsourcedispatch ( s= 0x 1 3 dbb9ce0 ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qeventdispatcher_glib . cpp : <number> # <number> 0x0 0 0 0 7 fffa22e9794 in g_main_dispatch ( context = context <user> =0 x7fff98019230 ) at . <repeated> / glib - <number> . <number> / glib / gmain . c : <number> # <number> 0x0 0 0 0 7 fffa22ee930 in g_main_context_dispatch ( context =0 x7fff98019230 ) at . <repeated> / glib - <number> . <number> / glib / gmain . c : <number> # <number> 0x0 0 0 0 7 fffa22eeb58 in g_main_context_iterate ( context = context <user> =0 x7fff98019230 , block = block <user> = <number> , dispatch = dispatch <user> = <number> , self =< optimized out > ) at . <repeated> / glib - <number> . <number> / glib / gmain . c : <number> # <number> 0x0 0 0 0 7 fffa22eec60 in g_main_context_iteration ( context =0 x7fff98019230 , may_block =< optimized out > ) at . <repeated> / glib - <number> . <number> / glib / gmain . c : <number> # <number> 0x0 0 0 0 7 fffa5036e7c in qeventdispatcherglib : : processevents ( this =0 x13dbbcc70 , flags = . <repeated> ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qeventdispatcher_glib . cpp : <number> # <number> 0x0 0 0 0 7 fff9ef44e64 in qxcbglibeventdispatcher : : processevents ( this =< optimized out > , flags = . <repeated> ) at / var / tmp / portage / dev - qt / qtgui - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / plugins / platforms / xcb / qxcbeventdispatcher . cpp : <number> # <number> 0x0 0 0 0 7 fffa4fabcc8 in qeventloop : : processevents ( this =< optimized out > , flags = . <repeated> ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / corelib / kernel / qeventloop . cpp : <number> # <number> 0x0 0 0 0 7 fffa4fac488 in qeventloop : : exec ( this =0 x7ffff9f204c8 , flags = . <repeated> ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / include / qtcore / . <repeated> / . <repeated> / src / corelib / global / qflags . h : <number> # <number> 0x0 0 0 0 7 fffa4fb9588 in qcoreapplication : : exec ( ) at / var / tmp / portage / dev - qt / qtcore - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / include / qtcore / . <repeated> / . <repeated> / src / corelib / global / qflags . h : <number> # <number> 0x0 0 0 0 7 fffa3d5704c in qguiapplication : : exec ( ) at / var / tmp / portage / dev - qt / qtgui - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / gui / kernel / qguiapplication . cpp : <number> # <number> 0x0 0 0 0 7 fffa455d98c in qapplication : : exec ( ) at / var / tmp / portage / dev - qt / qtwidgets - <number> . <number> / work / qtbase - everywhere - src - <number> . <number> / src / widgets / kernel / qapplication . cpp : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e7b4354 in guimain ( argc =< optimized out > , argv =< optimized out > ) at qt / bitcoin . cpp : <number> # <number> 0x0 0 0 0 0 0 0 1 1 e7ac070 in main ( argc =< optimized out > , argv =< optimized out > ) at qt / main . cpp : <number> ` ` `",0
bitcoin/bitcoin,"rpc : createmultisig adds incorrect warning for address type p2sh - segwit the ` createmultisig ` rpc adds a spurious warning message when creating for address type p2sh - segwit . ` ` ` createmultisig <number> ' [ "" 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 "" , "" 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 "" ] ' ' p2sh - segwit ' { "" address "" : "" 3 qfzqy7wqrbgeub7e5vvlggeeauvz1bbg9 "" , "" redeemscript "" : "" 5 2 2 1 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798210279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f8179852ae "" , "" descriptor "" : "" sh ( wsh ( multi ( <number> , 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 , 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 ) ) ) <hashtag> m8ww0c9m </hashtag> "" , "" warnings "" : [ "" unable to make chosen address type , please ensure no uncompressed public keys are present . "" ] } ` ` ` i think this traces back to <url> in the code , the comparison at <url> fails since ` output_type ` is ` outputtype : : p2sh_segwit ` and <url> returns ` outputtype : : legacy ` * * expected behavior * * no warning message . for example : ` ` ` createmultisig <number> ' [ "" 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 "" , "" 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798 "" ] ' ' p2sh - segwit ' { "" address "" : "" 3 qfzqy7wqrbgeub7e5vvlggeeauvz1bbg9 "" , "" redeemscript "" : "" 5 2 2 1 0 2 7 9 be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798210279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f8179852ae "" , "" descriptor "" } ` ` `",0
bitcoin/bitcoin,"guix build w / o substitutes failing at libgit2 check this is an upstream guix issue ( [ <number> ] ( <url> but they seem to be planning to resolve it with a version bump , and either way we have versions pinned i think . ` ` ` building / gnu / store / myai2xf4f0z2nr331mx2y4xr4jv1lwid - libgit2 - <number> . <number> . drv . <repeated> | ' unpack ' phaseild - log <number> <number> ` / gnu / store / xa8yg9nvawhfvk6hv8lhif9z5pybzan5 - libgit2 - <number> . <number> - checkout / tests / resources / status / staged_delete_modified_file ' - > ` . / tests / resources / status / staged_delete_modified_file ' <percent> [# # # # # # # # # # # # # # # # # # # # # # # # ] ild - log <number> <number> / tmp / guix - build - libgit2 - <number> . <number> . drv - <number> / source / src / regexp . c : <number> : <number> : warning : ? error ? may be used uninitialized in this function [ - wmaybe - uninitialized ] <number> | if ( error < <number> ) | ^ <percent> [# # # # # # # # # # # # # # # # # # # # # # # # ] ild - log <number> <number> [ <percent> ] building c object src / cmakefiles / git2internal . dir / repository . c . o | ' check ' phasenote : keeping build directory ` / tmp / guix - build - libgit2 - <number> . <number> . drv - <number> ' builder for ` / gnu / store / myai2xf4f0z2nr331mx2y4xr4jv1lwid - libgit2 - <number> . <number> . drv ' failed with exit code <number> build of / gnu / store / myai2xf4f0z2nr331mx2y4xr4jv1lwid - libgit2 - <number> . <number> . drv failed ` ` ` ` ` ` refs : : revparse . <repeated> f <number> ) failure : refs : : revparse : : date [ / tmp / guix - build - libgit2 - <number> . <number> . drv - <number> / source / tests / refs / revparse . c : <number> ] function call succeeded no error , expected non - zero return ` ` `",0
bitcoin/bitcoin,"bitcoin core wallet - > hd key generation disabled the bitcoin core wallet window has icons in the lower right corner . one of those icons is the letters "" hd "" . with the v . <number> version when passing the mouse cursor over this icon , a message appeared which said "" hd key generation is enabled "" . however , after downloading the latest version of bitcoin core , <number> , the message that appears to me is the opposite : "" hd key generation is disabled "" . ( 1 st screenshot ) . the wallet had no funds and , in that sense , i have not lost anything . but i am concerned that after upgrading bitcoin core to the latest version , the hd key generation has changed . i am pretty new to this and do not understand much . i have asked , read and looked for some solution but no one has been able to tell me what happens , whether it is normal or not , what the solution is . <repeated> etc . also note that , when using the "" getwalletinfo "" command in the bitcoin core console , appeared this ( 2 nd screenshot ) and that i do not know if it refers to the same ( hd key generation ) or not . also tried to upgrade the bitcoin core wallet with the "" upgradewallet "" comand . ( 3 rd screenshot ) i got the latest version of bitcoin core from and i got this url from the bitcoin core project profile on twitter . my os is windows <number> pro . and my cpu is a intel ( r ) core ( tm ) i5 - <number> cpu @ <number> . 2 0 ghz <number> ghz . for me this is first time doing this . <repeated> so sorry for what might be wrong , and sorry for my english ( i do not speak it well enough ) [ captura de pantalla <number> - <number> - <number> <number> ] ( <url> ! [ captura de pantalla <number> - <number> - <number> <number> ] ( <url> ! [ upgrade wallet ] ( <url>",0
bitcoin/bitcoin,automatic prune on tails os running bitcoin core full node on tails os new version <number> automatically prunes when started . i do not want to prune and have therefore set prune = <number> in bitcoin . conf . but that will not stop <number> from pruning . have to manually un - tick the tick box at startup . this was never the case in <number> so wondering if prune = <number> is disabled even though its stated in options,0
bitcoin/bitcoin,bug . / configure / install after the latest update can not be installed in the . / configure section with the following problem . / configure . . . checking dependency style of g + + . <repeated> gcc3 checking whether g + + supports c + + <number> features with - std =c + + <number> . <repeated> yes checking whether std : : atomic can be used without link library . <repeated> yes checking whether std : : filesystem can be used without link library . <repeated> no checking whether std : : filesystem needs - lstdc + + fs . <repeated> no checking whether std : : filesystem needs - lc + + fs . <repeated> configure : error : in ` / home / bi tcoin ' : configure : error figure out how to use std : : filesystem see ` config . log ' for more details,0
bitcoin/bitcoin,"wallet_taproot . py fails with "" insufficient funds ( - <number> ) "" ` ` ` test <number> - <number> - 2 9 t <time> . 4 0 9 0 0 0 z testframework ( error ) : jsonrpc error traceback ( most recent call last ) : file "" / private / var / folders / xx / vl5f934s6k927z1vyyl9cxth0000gn / t / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - apple - darwin / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / private / var / folders / xx / vl5f934s6k927z1vyyl9cxth0000gn / t / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - apple - darwin / test / functional / wallet_taproot . py "" , line <number> , in run_test psbt = self . psbt_online . walletcreatefundedpsbt ( [ ] , [ { self . boring . getnewaddress ( <sad> self . psbt_online . getbalance ( ) } ] , none , { "" subtractfeefromoutputs "" : [ <number> ] } ) [ ' psbt ' ] file "" / private / var / folders / xx / vl5f934s6k927z1vyyl9cxth0000gn / t / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - apple - darwin / test / functional / test_framework / coverage . py "" , line <number> , in __call__ return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) file "" / private / var / folders / xx / vl5f934s6k927z1vyyl9cxth0000gn / t / cirrus - ci - build / ci / scratch / build / bitcoin - x86_64 - apple - darwin / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ raise jsonrpcexception ( response [ ' error ' ] , status ) test_framework . authproxy . jsonrpcexception funds ( - <number> ) ` ` ` <url>",0
bitcoin/bitcoin,"wallet chainstateflushed notifications while attaching chain fixes # <number> when a rescan is performed during ` cwallet : : attachchain ( ) ` ( e . g . when loading an old wallet ) but this is interrupted by a shutdown signal , the wallet will currently stop the rescan , receive a ` chainstateflushed ` signal , set the saved best block to the tip and shut down . at next startup , the rescan is not continued or repeated because of this . but some blocks have never been scanned by the wallet , which could lead to an incorrect balance . fix this by ignoring ` chainstateflushed ` notifications until the chain is attached . since ` cwallet : : chainstateflushed ` is being manually called by ` attachchain ( ) ` anyway after finishing with the rescan , it is not a problem if intermediate notifications are ignored . manual rescans started / aborted by the ` rescanblockchain ` / ` abortrescan ` rpcs are not affected by this . i did not choose alternative ways of fixing this issue that would delay the validationinterface registration or change anything else about the handling of ` blockconnected ` signals for the reasons mentioned in [ this existing comment ] ( <url>",0
bitcoin/bitcoin,"- onlynet = onion plus - onlynet = ipv4 / etc , without - onion =, in spite of - torcontrol = specified , incorrectly kills startup in v22 , using the options : - onlynet = onion - onlynet = ipv4 - onlynet = ipv6 , combined with the options - torcontrol = ( etc ) functioned and bitcoind correctly pulled the onion socks address via a probe to tor . in v23 , that combination of options no longer functions and - onion = is required . * * expected * * using - onlynet = onion ( and ipv4 and ipv6 ) if - torcontrol = is specified should still work . * * actual behavior * * bitcoind refuses to start up due to commitid e53a8505db preventing it from doing so in init . cpp . * * to reproduce * * run as per above . * * system information * * sources built from fcf6c8f4eb217763545ede1766831a6b93f583bd ( tag v23 . <number> as of this writing i . <repeated> think ? ) error message in debug . log is : error : outbound connections restricted to tor ( - onlynet = onion ) but the proxy for reaching the tor network is not provided ( no - proxy = and no - onion = given ) or it is explicitly forbidden ( - onion = <number> ) shutdown : in progress . <repeated> scheduler thread exit shutdown",0
bitcoin/bitcoin,"rpc : calls to bitcoin rpc are truncated . # rpc : calls to bitcoin rpc are truncated . i am having a problem making batching calls to bitcoin rpc it is returning a truncated json , my suspicions is that there is some max - size for the response of requests , if that is true it should return a ` message error ` . bitcoin core version : ` <number> ` to reproduce the problem : ` ` ` bash curl - x post <url> - d ' [ { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 b3f09b8c8543f319154284e73974313edf8cc0fadefb "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 fd813bc5abf8406a428fdc2d998ae1fcda34c618282b "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 6 f516b8d0c5810f6d22d5172093fd4b9ccdb5a5b51c3 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 7 7 3 3 0 a2cb375a6b4098e79f6c5c5de147ec7007a05e6 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 6 b356851724eafeb9e402d8ca3d28876c811e64822f "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 b1e3f9ed6b431dd31e4e2c6715aa0a0adb4a358bc12 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 e0074bc06cf8f802c1f78de66782d9555688e3fe8b2f "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 9 0 e84241724e4cbd313f6a50dd9c6e7fa8cb0a186e8f "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 6 2 f6e349459fe90a1aeddc9c24cae12377146db32e54 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 c9812479c7c1e3b352e3347830cae56cef62713ac070 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 d5ca16560698cf9452715fad427a1b8fe371d23794c6 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 1 8 0 8 2 7 9 c53523533773e56593d8dc383c01764ac64 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 f582f11911747441be90bb63337079d39b7a844f4e4 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 1 7 b46d0da7ff6865b79248510656997108a00db1855 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 8 bea85c817bf7ebb3128126e5c6ed50d514c81356835 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 9 6 fc11d94c524ae490382c8ed5ea383e8c8a5fd6998c "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 ea4bc617c404920643c75b94ffc090304b19a9d51391 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 8 c80926de91a3b6b7b8572acf09ca80b945789fe55b7 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" : [ "" 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 9 7 1 0 e78d84f7e3e35e61b0e254b957d58b7c325bf2d2 "" , <number> ] } , { "" jsonrpc "" : "" <number> "" , "" method "" : "" getblock "" , "" params "" <number> ] } ] ' - h ' content - type : application / json ' - - silent ` ` `",0
bitcoin/bitcoin,"unresponsive cli issue report # <number> reminds me that i have been seeing a similar issue since several months on master built with - - enable - debug in my development environment , where the cli becomes unresponsive and eventually returns : ` ` ` error : timeout on transient error not connect to the server <number> . <number> : <number> ( error code <number> - "" timeout reached "" ) make sure the bitcoind server is running and that you are connecting to the correct rpc port . ` ` ` once the cli becomes unresponsive , it seems to happen with any cli command . this did not occur for me with v21 and i am not sure about v22 , as iirc it began sometime last fall ( <number> ) for me . until now i have been discounting it as something related to my local environment and working around it by restarting , since i restart very often for review and testing . debian testing , currently debian <date> - <number> ( <number> - <number> - <number> ) x86_64 gnu / linux . i am going to try to narrow down the issue more . is anyone else running into this ?",0
bitcoin/bitcoin,"build : do not modify ` common . init . vcxproj ` directly when building with msvc , and using a non - default toolset , the following command ` ` ` > python build_msvc \ \ msvc - autogen . py - toolset v143 ` ` ` actually modifies the source tree : ` ` ` diff > git diff warning will be replaced by crlf in build_msvc / common . init . vcxproj . the file will have its original line endings in your working directory diff - - git a / build_msvc / common . init . vcxproj b / build_msvc / common . init . vcxproj index 0 cbe2effd . <repeated> 4 4 b7efff3 <number> - - - a / build_msvc / common . init . vcxproj + + + b / build_msvc / common . init . vcxproj @ @ - <number> + <number> @ @ < propertygroup condition = "" ' $( configuration ) ' = = ' release ' "" label = "" configuration "" > <linkincremental> false </linkincremental> <usedebuglibraries> false </usedebuglibraries> - <platformtoolset> v142 </platformtoolset> + <platformtoolset> v143 </platformtoolset> <characterset> unicode </characterset> <generatemanifest> no </generatemanifest> <outdir> $( solutiondir ) $( platform ) \\$ ( configuration ) \\$ ( projectname ) \ \ </outdir> @ @ - <number> + <number> @ @ < propertygroup condition = "" ' $( configuration ) ' = = ' debug ' "" label = "" configuration "" > <linkincremental> true </linkincremental> <usedebuglibraries> true </usedebuglibraries> - <platformtoolset> v142 </platformtoolset> + <platformtoolset> v143 </platformtoolset> <characterset> unicode </characterset> <outdir> $( solutiondir ) $( platform ) \\$ ( configuration ) \\$ ( projectname ) \ \ </outdir> <intdir> $( platform ) \\$ ( configuration ) \\$ ( projectname ) \ \ </intdir> ` ` ` this pr fixes this bug .",0
bitcoin/bitcoin,"race in generatetoaddress ? test failure at <url> seems to indicate ` generatetoaddress ` has a path that does not set the height in coinbase correctly . rpc call : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 2 4 4 8 6 z [ httpworker . <number> ] [ rpc / request . cpp : <number> ] [ parse ] threadrpcserver method = generatetoaddress user = __cookie__ ` ` ` a new block immediately arrives over the network via the ` msghand ` thread : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 2 4 6 5 8 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , rpc / mining . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 2 6 4 5 2 z [ msghand ] [ validationinterface . cpp : <number> ] [ newpowvalidblock ] newpowvalidblock : block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b node1 <number> - <number> - 3 1 t <time> . 0 2 8 9 0 7 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , rpc / mining . cpp : <number> completed ( 4 2 2 5 μs ) ` ` ` then ` msghand ` gets held up in the middle of ` activatebestchain ` : ` ` ` [ . <repeated> short lock contention . <repeated> ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 0 8 2 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , validation . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 2 9 1 8 8 z [ httpworker . <number> ] [ node / miner . cpp : <number> ] [ createnewblock ] createnewblock ( <sad> block weight : <number> txs : <number> fees : <number> sigops <number> node1 <number> - <number> - 3 1 t <time> . 0 2 9 3 3 9 z [ httpworker . <number> ] [ validation . cpp : <number> ] [ connectblock ] - sanity checks : <number> . 0 1 ms [ <number> . 0 0 s ( <number> . 0 0 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 4 1 1 z [ httpworker . <number> ] [ validation . cpp : <number> ] [ connectblock ] - fork checks : <number> . 0 8 ms [ <number> . 0 1 s ( <number> . 0 8 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 4 4 8 z [ httpworker . <number> ] [ validation . cpp : <number> ] [ connectblock ] - connect <number> transactions : <number> . 0 4 ms ( <number> . 0 3 5 ms / tx , <number> . 0 0 0 ms / txin ) [ <number> . 0 1 s ( <number> . 0 5 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 4 8 6 z [ httpworker . <number> ] [ validation . cpp : <number> ] [ connectblock ] - verify <number> txins : <number> . 0 7 ms ( <number> . 0 0 0 ms / txin ) [ <number> . 0 1 s ( <number> . 1 0 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 5 2 2 z [ httpworker . <number> ] [ node / miner . cpp : <number> ] [ createnewblock ] createnewblock ( ) packages : <number> . 1 5 ms ( <number> packages , <number> updated descendants ) , validity : <number> . 4 0 ms ( total <number> . 5 5 ms ) node1 <number> - <number> - 3 1 t <time> . 0 2 9 5 5 4 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , rpc / mining . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 2 9 5 8 6 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , validation . cpp : <number> completed ( 4 4 6 μs ) ` ` ` at this point ` msghand ` has the ball again , and ` httpworker . <number> ` is waiting before calling ` incrementextranonce ` which is where the height in coinbase is set . ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 2 9 6 7 8 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - load block from disk : <number> . 0 0 ms [ <number> . 0 0 s ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 7 4 3 z [ msghand ] [ validation . cpp : <number> ] [ connectblock ] - sanity checks : <number> . 0 0 ms [ <number> . 0 0 s ( <number> . 0 0 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 8 2 3 z [ msghand ] [ validation . cpp : <number> ] [ connectblock ] - fork checks : <number> . 0 8 ms [ <number> . 0 1 s ( <number> . 0 8 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 8 7 7 z [ msghand ] [ validation . cpp : <number> ] [ connectblock ] - connect <number> transactions : <number> . 0 5 ms ( <number> . 0 4 9 ms / tx , <number> . 0 0 0 ms / txin ) [ <number> . 0 1 s ( <number> . 0 5 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 2 9 9 1 9 z [ msghand ] [ validation . cpp : <number> ] [ connectblock ] - verify <number> txins : <number> . 0 9 ms ( <number> . 0 0 0 ms / txin ) [ <number> . 0 1 s ( <number> . 1 0 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 0 0 5 4 z [ msghand ] [ validation . cpp : <number> ] [ connectblock ] - index writing : <number> . 1 4 ms [ <number> . 0 1 s ( <number> . 0 8 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 0 1 0 1 z [ msghand ] [ validationinterface . cpp : <number> ] [ blockchecked ] blockchecked : block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b state = valid node1 <number> - <number> - 3 1 t <time> . 0 3 0 1 7 3 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - connect total : <number> . 5 0 ms [ <number> . 0 5 s ( <number> . 3 7 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 0 2 0 8 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - flush : <number> . 0 4 ms [ <number> . 0 0 s ( <number> . 0 3 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 0 2 6 8 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - writing chainstate : <number> . 0 6 ms [ <number> . 0 1 s ( <number> . 0 4 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 1 7 0 2 z [ msghand ] [ policy / fees . cpp : <number> ] [ processblock ] blockpolicy estimates updated by <number> of <number> block txs , since last block <number> of <number> tracked , mempool map size <number> , max target <number> from current node1 <number> - <number> - 3 1 t <time> . 0 3 1 8 2 1 z [ msghand ] [ validation . cpp : <number> ] [ updatetiplog ] updatetip : new best = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b height = <number> version =0 x30000000 log2_work = <number> tx = <number> date = ' <number> - <number> - 3 1 t <time> z ' progress = <number> cache = <number> . 0 mib ( 1 1 2 txo ) node1 <number> - <number> - 3 1 t <time> . 0 3 1 8 5 1 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - connect postprocess : <number> . 5 8 ms [ <number> . 1 6 s ( <number> . 1 4 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 1 8 7 7 z [ msghand ] [ validation . cpp : <number> ] [ connecttip ] - connect block : <number> . 1 8 ms [ <number> . 2 2 s ( <number> . 5 8 ms / blk ) ] node1 <number> - <number> - 3 1 t <time> . 0 3 1 9 1 4 z [ msghand ] [ txmempool . cpp : <number> ] [ check ] checking mempool with <number> transactions and <number> inputs node1 <number> - <number> - 3 1 t <time> . 0 3 2 0 2 1 z [ msghand ] [ validationinterface . cpp : <number> ] [ blockconnected ] enqueuing blockconnected : block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b block height = <number> node1 <number> - <number> - 3 1 t <time> . 0 3 2 0 8 9 z [ msghand ] [ validationinterface . cpp : <number> ] [ updatedblocktip ] enqueuing updatedblocktip : new block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b fork block hash = 7 b0602dccb1e34108167e32238ec6011d29eae6da3f9e4430165bc0ecd5283d7 ( in ibd = false ) ` ` ` now the tip has been updated . looks like ` msghand ` releases the lock , but manages to reacquire it before any other thread : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 3 2 1 3 4 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention newtaskmutex , scheduler . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 3 2 1 6 7 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention newtaskmutex , scheduler . cpp : <number> completed ( 3 μs ) ` ` ` the ` scheduler ` thread seems to get the lock next : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 3 2 1 9 4 z [ scheduler ] [ validationinterface . cpp : <number> ] [ operator ( ) ] blockconnected : block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b block height = <number> node1 <number> - <number> - 3 1 t <time> . 0 3 2 2 6 1 z [ scheduler ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , net_processing . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 3 4 6 8 1 z [ scheduler ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , net_processing . cpp : <number> completed ( 2 3 9 8 μs ) ` ` ` and finally ` httpworker . <number> ` gets it , incrementing the extra nonce , and presumably updating to the new block . ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 3 4 7 2 5 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , rpc / mining . cpp : <number> completed ( 5 1 3 9 μs ) ` ` ` other threads do their thing : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 3 4 7 6 0 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention : : cs_main , validation . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 3 4 8 2 6 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention : : cs_main , validation . cpp : <number> completed ( 3 2 μs ) node1 <number> - <number> - 3 1 t <time> . 0 3 4 8 5 5 z [ scheduler ] [ validationinterface . cpp : <number> ] [ operator ( ) ] updatedblocktip : new block hash = 7 b7df2920d9870cb286408956ea95f6d307bb87dfd446bce18e4c35f7fe3651b fork block hash = 7 b0602dccb1e34108167e32238ec6011d29eae6da3f9e4430165bc0ecd5283d7 ( in ibd = false ) ` ` ` then ` httpworker . <number> ` is calling ` processnewblock ` and everything falls apart : ` ` ` node1 <number> - <number> - 3 1 t <time> . 0 3 4 8 8 7 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , validation . cpp : <number> started node1 <number> - <number> - 3 1 t <time> . 0 3 4 9 7 0 z [ httpworker . <number> ] [ logging / timer . h : <number> ] [ log ] enter : lock contention cs_main , validation . cpp : <number> completed ( 5 8 μs ) node1 <number> - <number> - 3 1 t <time> . 0 3 5 0 2 5 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter : lock contention : : cs_main , validation . cpp : <number> started test <number> - <number> - 3 1 t <time> . 0 3 6 0 0 0 z testframework ( error ) : jsonrpc error traceback ( most recent call last ) : file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / wallet_listreceivedby . py "" , line <number> , in run_test self . generatetoaddress ( self . nodes [ <number> ] , coinbase_maturity + <number> , address2 , sync_fun = self . no_op ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in generatetoaddress blocks = generator . generatetoaddress ( * args , invalid_call = false , * * kwargs ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / test_node . py "" , line <number> , in generatetoaddress return self . __getattr__ ( ' generatetoaddress ' ) ( * args , * * kwargs ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / coverage . py "" , line <number> , in __call__ return_val = self . auth_service_proxy_instance . __call__ ( * args , * * kwargs ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / authproxy . py "" , line <number> , in __call__ raise jsonrpcexception ( response [ ' error ' ] , status ) test_framework . authproxy . jsonrpcexception : processnewblock , block not accepted ( - <number> ) node1 <number> - <number> - 3 1 t <time> . 0 3 6 3 1 1 z [ httpworker . <number> ] [ util / system . h : <number> ] [ error ] error : acceptblock : bad - cb - height , block height mismatch in coinbase node1 <number> - <number> - 3 1 t <time> . 0 3 6 3 4 5 z [ httpworker . <number> ] [ validationinterface . cpp : <number> ] [ blockchecked ] blockchecked : block hash = 2 f267e107b619688b165b81e220b7012a48537ec711c044b19a9527ae7615421 state = bad - cb - height , block height mismatch in coinbase node1 <number> - <number> - 3 1 t <time> . 0 3 6 3 8 1 z [ httpworker . <number> ] [ util / system . h : <number> ] [ error ] error : processnewblock : acceptblock failed ( bad - cb - height , block height mismatch in coinbase ) node1 <number> - <number> - 3 1 t <time> . 0 3 6 4 2 5 z [ msghand ] [ logging / timer . h : <number> ] [ log ] enter contention : : cs_main , validation . cpp : <number> completed ( 1 3 6 4 μs ) ` ` `",0
bitcoin/bitcoin,"wallet wallet loading notification for encrypted wallets fixes bitcoin - core / gui # <number> . ` cwallet : : create ( ) ` notifies about wallet loading too early , that results the notification goes before ` descriptorscriptpubkeyman ` s were created and added to an encrypted wallet . and ` interfaces : : wallet : : taprootenabled ( ) ` in <url> erroneously returns ` false ` for just created encrypted descriptor wallets .",0
bitcoin/bitcoin,"bench : ` mempoolcheck ` actually runs with ` check_ratio = <number> ` for ` mempoolcheck ` , it seems that the intention was to have a mempool with ` check_ratio = <number> ` ( see ` - checkmempool = <number> ` <sad> <url> however in the subsequent line in the above snippet , a ` ctxmempool ` gets constructed with no arguments , which means that ` check_ratio ` will default to <number> : <url> manually specifying ` check_ratio ` to <number> ( according to the original intention ) and running the ` mempoolcheck ` benchmark results in an assertion error : ` ` ` diff - - git a / src / bench / mempool_stress . cpp b / src / bench / mempool_stress . cpp index afa4618e1b . <repeated> 3 2 cdb70539 <number> - - - a / src / bench / mempool_stress . cpp + + + b / src / bench / mempool_stress . cpp @ @ - <number> + <number> @ @ static void mempoolcheck ( benchmark : : bench & bench ) const int childtxs = bench . complexityn ( ) > <number> ? static_cast <int> ( bench . complexityn ( ) ) : <number> ; const std : : vector <ctransactionref> ordered_coins = createorderedcoins ( det_rand , childtxs , /* min_ancestors */ <number> ); const auto testing_setup = makenologfilecontext < const testingsetup > ( cbasechainparams : : main , { "" - checkmempool = <number> "" }); - ctxmempool pool ; + ctxmempool pool { nullptr , <number> }; lock2 ( cs_main , pool . cs ) ; const ccoinsviewcache & coins_tip = testing_setup . get ( ) - > m_node . chainman - > activechainstate ( ) . coinstip ( ); for ( auto & tx : ordered_coins ) addtx ( tx , pool ) ; ` ` ` ` ` ` sh $ . / src / bench / bench_bitcoin - filter = ' mempoolcheck ' . <repeated> bench_bitcoin : txmempool . cpp : <number> : void ctxmempool : : check ( const ccoinsviewcache & , int64_t ) const ` mempoolduplicate . havecoin ( txin . prevout ) ' failed . ` ` ` aside from this problem , we should also probably just re - use the ` ctxmempool ` in ` testingsetup ` . ping <user>",0
bitcoin/bitcoin,guix : linker warning for darwin builds ` ` ` cxxld libbitcoinconsensus . la ld : warning slash removed from - install_name ( / / lib / libbitcoinconsensus . <number> . dylib ) ` ` `,0
bitcoin/bitcoin,"feature_init intermittent issue : unicodedecodeerror : ' utf - <number> ' codec can not decode bytes in position <number> - <number> : unexpected end of data presumably because a unicode character was only written partially . ` ` ` test <number> - <number> - 1 5 t <time> . 8 1 7 0 0 0 z testframework ( error ) : unexpected exception caught during testing traceback ( most recent call last ) : file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / test_framework . py "" , line <number> , in main self . run_test ( ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / feature_init . py "" , line <number> , in run_test node . start ( extra_args =[ ' - txindex = <number> ' , ' - blockfilterindex = <number> ' , ' - coinstatsindex = <number> ' ] ) file "" / usr / lib / python3 . <number> / contextlib . py "" , line <number> , in __exit__ next ( self . gen ) file "" / tmp / cirrus - ci - build / ci / scratch / build / bitcoin - i686 - pc - linux - gnu / test / functional / test_framework / test_node . py "" , line <number> , in wait_for_debug_log log = dl . read ( ) file "" / usr / lib / python3 . <number> / codecs . py "" , line <number> , in decode ( result , consumed ) = self . _buffer_decode ( data , self . errors , final ) unicodedecodeerror : ' utf - <number> ' codec can not decode bytes in position <number> - <number> end of data ` ` ` <url>",0
bitcoin/bitcoin,"use designated initializers designated initializers are supported since gcc <number> ( our minimum required is <number> ) and clang <number> ( our minimum required is <number> ) . they work out of the box with c + + <number> , and only msvc requires the c + + <number> flag to be set . i do not expect any of our msvc users will run into issues due to this . see also <url>",0
bitcoin/bitcoin,"bitcoind repeatedly crashing at ` updatetip ` with no error message < - - describe the issue - - > * * expected behavior * * bitcoind either does not crash , or provides some error information in ` debug . log ` about the crash . * * actual behavior * * about <number> seconds after starting ` bitcoind ` , it reliably crashes right after : ` ` ` <number> - <number> - 0 5 t <time> z updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 5 faae44cda1e306fdfb468c30870a3731bbdadf24c5f 5 f height = <number> version =0 x20000000 log2_work = <number> tx = <number> date = ' <number> - <number> - 1 5 t <time> z ' progre ss = <number> cache = <number> . 1 mib ( 8 6 7 txo ) <number> - <number> - 0 5 t <time> z updatetip : new best = 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 8 7 a137cfe12343e83667c5ab290f2c6ee75fe323a1c8 ec height = <number> version =0 x20000000 log2_work = <number> tx = <number> date = ' <number> - <number> - 1 5 t <time> z ' progre ss = <number> cache = <number> . 2 mib ( 9 0 6 1 txo ) ` ` ` i am currently stuck on block ` <number> ` but that ' s after trying to run ` - reindex ` when this was happening more up towards the high 6 0 0 0 0 0 s , so i am not sure if it ' s anything particular with this block or not . * * to reproduce * * this happens every time consistently for me . * * system information * * < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > ` ` ` bitcoind - - version bitcoin core version v22 . <number> ` ` ` originally tried with building my own , then tried again with the pre - build binary from bitcoincore . org to make sure i did not do something wrong , same issue . < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > ` ` ` > lsb_release - a no lsb modules are available . distributor id : ubuntu description : ubuntu <number> . <number> lts release : <number> codename : focal ` ` ` ` ` ` > lscpu architecture : x86_64 cpu op - mode ( s ) : <number> - bit , <number> - bit cpu ( s ) : <number> vendor id : genuineintel cpu family : <number> model : <number> model name : intel ( r ) core ( tm ) i3 - <number> cpu @ <number> . 9 0 ghz . <repeated> ` ` ` ` ` ` > lsmem lsmem range size state removable block 0x0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 - 0x0 0 0 0 0 0 0 0 8 fffffff <number> . 3 g online no <number> - <number> 0x0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 - 0x0 0 0 0 0 0 0 4 6 fffffff <number> . 8 g online no <number> - <number> memory block size : 1 2 8 m total online memory : 1 6 g total offline memory : 0 b ` ` ` ` ` ` > sudo smartctl - - all / dev / sda = = = start of information section = = = model family : western digital red lu wwn device id : <number> 0 0 1 4 ee 2 6 4 8 8 ca44 firmware version : <number> . 0 0 a82 user capacity : <number> , <number> , <number> bytes [ <number> tb ] sector sizes : <number> bytes logical , <number> bytes physical rotation rate : <number> rpm ata version is : acs - <number> ( minor revision not indicated ) sata version is : sata <number> , <number> gb / s ( current : <number> gb / s ) smart support is : available - device has smart capability . smart support is : enabled = = = start of read smart data section = = = smart overall - health self - assessment test result : passed . <repeated> ` ` ` i put my ` debug . log ` s over in this gist , one default logging and one with ` debug = <number> `",0
bitcoin/bitcoin,"homebrew boost <number> breaks external signer configure test on macos <number> . <number> against the master branch . # <number> does not fix it . ` ` ` brew info boost boost : stable <number> . <number> ( bottled ) , head . <repeated> . / configure - - enable - external - signer . <repeated> checking for boostlib >= <number> . <number> ( <number> ) . <repeated> yes checking whether boost . process can be used . <repeated> no configure : error : external signing is not supported for this boost version ` ` ` it does work with boost <number> ( there was no <number> release in home - brew ) : ` ` ` brew install boost <user> . <number> . <repeated> ldflags = "" - l / usr / local / opt / boost <user> . <number> / lib "" cppflags = "" - i / usr / local / opt / boost <user> . <number> / include "" . / configure - - enable - external - signer . <repeated> checking for boostlib >= <number> . <number> ( <number> ) . <repeated> yes checking whether boost . process can be used . <repeated> yes . <repeated> options used to compile and link signer = yes ` ` ` there ' s nothing in the release notes about boost . process for either [ <number> ] ( <url> or [ <number> ] ( <url> the homebrew [ formula changes ] ( <url> are nothing but a trivial version and hash bump .",0
bitcoin/bitcoin,"gui : "" registershutdownblockreason : successfully registered : dogecoin core did not yet exit safely . <repeated> "" and error : accepttomemorypool : non - final < - - this is a dogecoincore qt not sync . - - > < ! - - sorry for my english - - > * * to reproduce file log debug * * < ! - - - error : accepttomemorypool - - > * * system information * * < ! - - what version ofdogecoin core versione v1 . <number> . <number> - bb4b082 ( <number> - bit ) - - > < ! - - what type of machine are you observing the error on ( os / cpu win <number> / i5 and disk type ssd ) - - > [ debug . log ] ( <url>",0
bitcoin/bitcoin,"segmentation fault . "" b - httpworker . <number> "" < - - describe the issue - - > after calling ` importaddress ` trough http rpc , i get segmentation fault at the end of sync ( tracked by progress . * * expected behavior * * should complete importaddress < ! - - - what behavior did you expect ? - - > should complete importaddress * * actual behavior * * segmentation fault * * to reproduce * * just try to call ` importaddress ` trought wallet rpc in bitcoin testnet . < ! - - - how reliably can you reproduce the issue , what are the steps to do so ? - - > <number> / <number> times * * system information * * ubuntu < ! - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > <number> . <number> < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > digital ocean ubuntu instance < ! - - any extra information that might be useful in the debugging process . - - > gdb < ! - - - this is normally the contents of a ` debug . log ` or ` config . log ` file . raw text or a link to a pastebin type site are preferred . - - > heres log right before crash : ` ` ` <number> - <number> - 1 9 t <time> z sending verack ( <number> bytes ) peer = <number> <number> - <number> - 1 9 t <time> z receive version message : / satoshi : <number> . <number> /: version <number> , blocks = <number> , us = <number> . <number> . <time> <number> , peer = <number> <number> - <number> - 1 9 t <time> z socket closed for peer = <number> <number> - <number> - 1 9 t <time> z disconnecting peer = <number> <number> - <number> - 1 9 t <time> z received ( <number> bytes ) peer = <number> <number> - <number> - 1 9 t <time> z sending sendheaders ( <number> bytes ) peer = <number> thread <number> "" b - httpworker . <number> "" received signal sigsegv , segmentation fault . [ switching to thread 0x 7 fffcffff700 ( lwp <number> ) ] ` ` `",0
bitcoin/bitcoin,"test - only ub in fuzz tests this line : ` ` ` c + + connmantestmsg & connman = * static_cast < connmantestmsg *>( g_setup - > m_node . connman . get ( )); ` ` ` in src / test / fuzz / process_message . cpp : <number> , is constructing a reference to a connmantestmsg , which actually refers to an object of type connman . even though connmantestmsg inherits from connman , and adds no fields , i am pretty sure this is undefined behavior . it is not detected by the sanitizer because they are not polymorphic types for which runtime type information is tracked , but if you make ` connman : : ~ connman ( ) ` ` virtual ` , it does get detected : ` ` ` test / fuzz / util . cpp : <number> <time> : runtime error : member call on address 0x 6 1 9 0 0 0 0 3 4 3 8 0 which does not point to an object of type ' connmantestmsg ' 0x 6 1 9 0 0 0 0 3 4 3 8 0 : note is of type ' cconnman ' <number> <number> <number> <number> <number> 8 e a0 <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> <number> ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ vptr for ' cconnman ' ` ` ` i do not know how to quickly solve this myself , as i am unfamiliar with this part of the code , so i am opening an issue to discuss it .",0
bitcoin/bitcoin,"build : compilation error in ` tinyformat . h ` on manjaro linux here ' s the output from ` make ` : ` ` ` . / wallet / wallet . h : <number> : <number> : required from ‘ void wallet : : cwallet : : walletlogprintf ( std : : string , params . <repeated> ) const [ with params = { std : : optional <int> }; std : : string = std : : __cxx11 : : basic_string <char> ] ’ wallet / wallet . cpp : <number> <time> : required from here . / tinyformat . h : <number> : <number> : error : no match for ‘ operator < < ’ ( operand types are ‘ std : : ostringstream ’ { aka ‘ std : : __cxx11 : : basic_ostringstream <char> ’ } and ‘ const std : : optional <int> ’ ) <number> | tmp < < value ; | ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ . / tinyformat . h : <number> : <number> : note : candidate : ‘ operator < < ( int , int ) ’ ( built - in ) ` ` ` . <repeated> ` ` ` . / wallet / wallet . h : <number> : <number> : required from ‘ void wallet : : cwallet : : walletlogprintf ( std : : string , params . <repeated> ) const [ with params = { std : : optional <int> }; std : : string = std : : __cxx11 : : basic_string <char> ] ’ wallet / wallet . cpp : <number> <time> : required from here / usr / include / c + + / <number> . <number> / ostream : <number> : <number> : error : no type named ‘ type ’ in ‘ struct std : : enable_if < false , std : : basic_ostream <char> & > ’ in file included from / usr / include / c + + / <number> . <number> / bits / fs_path . h : <number> , from / usr / include / c + + / <number> . <number> / filesystem : <number> , from . / fs . h : <number> , from . / wallet / wallet . h : <number> , from wallet / wallet . cpp : <number> : . / tinyformat . h : in instantiation of ‘ void tinyformat : : detail : : formattruncated ( std : : ostream & , const t & , int ) [ with t = std : : optional <int> ; std : : ostream = std : : basic_ostream <char> ] ’ : . / tinyformat . h : <number> <time> : required from ‘ void tinyformat : : formatvalue ( std : : ostream & , const char * , const char * , int , const t & ) [ with t = std : : optional <int> ; std : : ostream = std : : basic_ostream <char> ] ’ . / tinyformat . h : <number> <time> : required from ‘ static void tinyformat : : detail : : formatarg : : formatimpl ( std : : ostream & , const char * , const char * , int , const void <wink> [ with t = std : : optional <int> ; std : : ostream = std : : basic_ostream <char> ] ’ . / tinyformat . h : <number> <time> : required from ‘ tinyformat : : detail : : formatarg : : formatarg ( const t & ) [ with t = std : : optional <int> ] ’ . / tinyformat . h : <number> <time> : required from ‘ tinyformat : : detail : : formatlistn <n> : : formatlistn ( const args & . <repeated> ) [ with args = { std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : optional <int> }; int n = <number> ] ’ . / tinyformat . h : <number> <time> : required from ‘ tinyformat : : detail : : formatlistn < sizeof . <repeated> ( args ) > tinyformat : : makeformatlist ( const args & . <repeated> ) [ with args = { std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : optional <int> } ] ’ . / tinyformat . h : <number> <time> : required from ‘ void tinyformat : : format ( std : : ostream & , const char * , const args & . <repeated> ) [ with args = { std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : optional <int> }; std : : ostream = std : : basic_ostream <char> ] ’ . / tinyformat . h : <number> <time> : required from ‘ std : : string tinyformat : : format ( const char * , const args & . <repeated> ) [ with args = { std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , std : : optional <int> }; std : : string = std : : __cxx11 : : basic_string <char> ] ’ ` ` ` same with ` clang ` : ` ` ` / usr / bin / . <repeated> / lib64 / gcc / x86_64 - pc - linux - gnu / <number> . <number> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / include / c + + / <number> . <number> / ostream : <number> : <number> : note : candidate function not viable : no known conversion from ' const std : : optional <int> ' to ' std : : basic_ostream <char> : : __streambuf_type * ' ( aka ' basic_streambuf < char , std : : char_traits <char> > *') for 1 st argument operator < < ( __streambuf_type * __sb ) ; ^ in file included from wallet / wallet . cpp : <number> : in file included from . / wallet / wallet . h : <number> : in file included from . / fs . h : <number> : . / tinyformat . h : <number> : <number> : error operands to binary expression ( ' std : : ostringstream ' ( aka ' basic_ostringstream <char> ' ) and ' const std : : optional <int> ' ) tmp < < value ; ~ ~ ~ ^ ~ ~ ~ ~ ~ ` ` `",0
bitcoin/bitcoin,"segmentation fault in v21 and v22 releases os : ubuntu <number> . releases : bitcoin core v21 and v22 . the problem have a lot of hd wallets , with the first transaction in dic <number> , encrypted , not checked for <number> years . it starts reescaning and segmentation fault error . tried with bitcoin - qt , bitcoind , reindex the whole blockchain , and still segmentation fault . after loading the gui , load the wallet , again it closes with segmentation fault v21 and v22 downloaded and checked for signatures and hashes . the same wallets , if checked after mid - <number> - <number> ( a year or more ) work . the machine is a dell inspiron with 1 6 gb of ram ( dbcache = <number> ) . tried a lot of other things ( - rescan , etc ) that did not work . all other wallets work fine , no problems , but these ones fail with newer versions of bitcoin core . run bitcoin - wallet info , the wallets are ok , ( not corrupted or anything ) . if i do the reindex while the wallet is loaded , it crashes the moment the reindex gets to dic <number> - <date> . then i reestart without the wallet loaded and it finishes reindexing ok . in the log says it loads the wallet ok ( 5 s <happy> , starts reescaning and then nothing else ( it crashes ) . it crashes <number> or <number> seconds after reescaning ( really fast ) both bitcoin - qt and bitcoind .",0
bitcoin/bitcoin,"` gestdescriptorinfo ` ignores hardened paths in origin info for computing the checksum ( therefore returns invalid checksums ) let us say i have two xpubs , derived at ` m / <number> ' / <number> ' / <number> ' / <number> ` from a master xpub . i ' d like to create a descriptor describing a multisig between those <number> keys . i create a descriptor wallet ` ` ` $ bcreg1 - named createwallet wallet_name = repro disable_private_keys = true descriptors = true { "" name "" : "" repro "" , "" warning "" : "" "" } ` ` ` since i will not compute the checksum by hand i use the hack of getting it from ` getdescriptorinfo ` ` ` ` $ bcreg1 getdescriptorinfo "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) "" { "" descriptor "" : "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) <hashtag> hs0kuwwv </hashtag> "" , "" checksum "" : "" hs0kuwwv "" , "" isrange "" : true , "" issolvable "" : true , "" hasprivatekeys "" : false } ` ` ` now i use the output of ` getdescriptorinfo ` to import the descriptor and . <repeated> ` ` ` $ bcreg1 - rpcwallet = repro importdescriptors ' [ { "" desc "" : "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) <hashtag> hs0kuwwv </hashtag> "" , "" timestamp "" : "" now "" } ] ' [ { "" success "" : false , "" error "" : { "" code "" : - <number> , "" message "" : "" provided checksum ' hs0kuwwv ' does not match computed checksum ' rgsu3znf ' "" } } ] ` ` ` hmm . turns out the checksum ` getdescriptorinfo ` gave me is invalid ( both in the ` "" descriptor "" ` and ` "" checksum "" ` fields ) . i remember some discussions around canonicalization and the ` h ` vs ` ' ` notation for hardened paths , so i try by using ` h ` instead . ` ` ` $ bcreg1 getdescriptorinfo "" wsh ( multi ( <number> , [ 0 a0a0a0a / 1 h / 2 h / 3 h / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / 1 h / 2 h / 3 h / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) "" { "" descriptor "" : "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> ' / <number> ' / <number> ' / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) <hashtag> hs0kuwwv </hashtag> "" , "" checksum "" : "" 6 nuh6ujc "" , "" isrange "" : true , "" issolvable "" : true , "" hasprivatekeys "" : false } ` ` ` it outputs the same invalid descriptor in the ` "" descriptor "" ` field . fortunately the checksum in ` "" checksum "" ` is actually valid , so i can use that in ` importdescriptors ` . maybe worth mentioning , before trying on latest master i had an additional issue with ` <number> ` where ` getdescriptorinfo ` would literally remove the hardened paths from the origin info : ` ` ` $ bc getdescriptorinfo ' wsh ( sortedmulti ( <number> , [ f4d84203 / <number> ' / <number> ' / <number> ' ] xpub6bqglddlraesfoaxcu4vplrljwnvadtce1jfaocxupbqpyu6tdua6nzou17rryqagxpjwh7elempm6w8d6c4p5sif8rwvgvwnaskyzkn14s / <number> /* , [ ff620a62 / <number> ' / <number> ' / <number> ' ] xpub6c7cpjjsmrm6hquqx2dedqn38vf2csdec5bmqbszpga4ep64v211hyscsstkfknelchieg2wkmckrkrdhrujeacbrszhjj7np4e68rpvipe / <number> /* ) ) ' { "" descriptor "" : "" wsh ( sortedmulti ( <number> , [ f4d84203 / <number> / <number> / <number> ] xpub6bqglddlraesfoaxcu4vplrljwnvadtce1jfaocxupbqpyu6tdua6nzou17rryqagxpjwh7elempm6w8d6c4p5sif8rwvgvwnaskyzkn14s / <number> /* , [ ff620a62 / <number> / <number> / <number> ] xpub6c7cpjjsmrm6hquqx2dedqn38vf2csdec5bmqbszpga4ep64v211hyscsstkfknelchieg2wkmckrkrdhrujeacbrszhjj7np4e68rpvipe / <number> /* ) ) <hashtag> kvvtf4uz </hashtag> "" , "" checksum "" : "" kvvtf4uz "" , "" isrange "" : true , "" issolvable "" : true , "" hasprivatekeys "" : false } ` ` ` so maybe the bad checksum above was the checksum without the hardened paths ? ` ` ` $ bcreg1 getdescriptorinfo "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> / <number> / <number> / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> / <number> / <number> / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) "" { "" descriptor "" : "" wsh ( multi ( <number> , [ 0 a0a0a0a / <number> / <number> / <number> / <number> ] tpubd6nzvbkryhz4wku5uccpshsnkyoypsh9lor6kg7j6ytgcjcrpzktplpt4ntwlovirjiwphguph1v1ukkatjofcrpbczwmn6kxeptuzkg6mm / * , [ b <elongated> / <number> / <number> / <number> / <number> ] tpubd6nzvbkryhz4xm62aaub6ditmqnnbqnkqsbddsihwx2q2kaqyapscizhyzceacrsvr5qszkuf53pj6zqcrzffdj6hakakpdjafzt8h1zzdz / *)) <hashtag> rgsu3znf </hashtag> "" , "" checksum "" : "" rgsu3znf "" , "" isrange "" : true , "" issolvable "" : true , "" hasprivatekeys "" } ` ` ` bingo",0
bitcoin/bitcoin,"segfault when compiled with depends debug = <number> and libc + + see also <url> and <url> steps to reproduce on a fresh install of ubuntu focal : ` ` ` export debian_frontend = noninteractive & & apt update & & apt install curl wget htop git vim ccache - y & & git clone <url> bitcoin - core & & cd bitcoin - core & & apt install build - essential libtool autotools - dev automake pkg - config bsdmainutils python3 - zmq make automake cmake curl clang llvm libc + + - dev libc + + abi - dev g + + - multilib libtool binutils - gold bsdmainutils pkg - config python3 patch bison - y & & ( cd depends & & make debug = <number> no_qt = <number> no_wallet = <number> no_zmq = <number> no_upnp = <number> no_natpmp = <number> - j $( nproc ) ) & & . / autogen . sh & & config_site =""$ pwd / depends / x86_64 - pc - linux - gnu / share / config . site "" . / configure cc = ' clang ' cxx = ' clang + + - stdlib = libc + + ' - - enable - fuzz - - with - sanitizers = fuzzer & & make - j $( nproc ) ` ` ` ` ` ` $ fuzz = tx_pool . / src / test / fuzz / fuzz undefinedbehaviorsanitizer : deadlysignal = = <number> = = error : undefinedbehaviorsanitizer : segv on unknown address 0x0 0 0 0 0 0 0 0 0 0 0 8 ( pc 0x 7 f5d814ccb47 bp 0x 7 fff699e4ef0 sp 0x 7 fff699e4ea0 t52419 ) = = <number> = = the signal is caused by a read memory access . = = <number> = = hint : address points to the zero page . # <number> 0x 7 f5d814ccb47 in std : : __1 : : __libcpp_db : : swap ( void * , void <wink> ( / lib / x86_64 - linux - gnu / libc + + . so . <number> + 0x 4 3 b47 ) # <number> 0x 5 6 4 aea7e1377 in std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> <sad> : basic_string ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > & & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 7 7 3 7 7 ) # <number> 0x 5 6 4 aea80cf94 in std : : __1 : : __fs : : filesystem : : path : : path ( std : : __1 : : __fs : : filesystem : : path & & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 a2f94 ) # <number> 0x 5 6 4 aea80bd04 in fs : : path : : path ( std : : __1 : : __fs : : filesystem : : path ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 a1d04 ) # <number> 0x 5 6 4 aeb1f46ad in fs : : absolute ( fs : : path const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0 xf8a6ad ) # <number> 0x 5 6 4 aeb1f4c8b in argsmanager : : getdatadir ( bool ) const ( / bitcoin - core / src / test / fuzz / fuzz + 0 xf8ac8b ) # <number> 0x 5 6 4 aea80bbfa in argsmanager : : getdatadirnet ( ) const ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 a1bfa ) # <number> 0x 5 6 4 aeb1fa2bb in abspathforconfigval ( fs : : path const & , bool ) ( / bitcoin - core / src / test / fuzz / fuzz + 0 xf902bb ) # <number> 0x 5 6 4 aeb0d5074 in init : : setloggingoptions ( argsmanager const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0 xe6b074 ) # <number> 0x 5 6 4 aeaaddd15 in initlogging ( argsmanager const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 8 7 3 d15 ) # <number> 0x 5 6 4 aeaa06412 in basictestingsetup : : basictestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 7 9 c412 ) # <number> 0x 5 6 4 aeaa073e2 in chaintestingsetup : : chaintestingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 7 9 d3e2 ) # <number> 0x 5 6 4 aeaa0877f in testingsetup : : testingsetup ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 7 9 e77f ) # <number> 0x 5 6 4 aea85508d in std : : __1 : : __unique_if < testingsetup const > : : __unique_single std : : __1 : : make_unique < testingsetup const , std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & > ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 eb08d ) # <number> 0x 5 6 4 aea852a22 in std : : __1 : : unique_ptr < testingsetup const , std : : __1 : : default_delete < testingsetup const > > makenologfilecontext < testingsetup const > ( std : : __1 : : basic_string < char , std : : __1 : : char_traits <char> , std : : __1 : : allocator <char> > const & , std : : __1 : : vector < char const * , std : : __1 : : allocator < char const *> > const & ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 e8a22 ) # <number> 0x 5 6 4 aea9c5817 in ( anonymous namespace ) : : initialize_tx_pool ( ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 7 5 b817 ) # <number> 0x 5 6 4 aea7c8d20 in decltype ( std : : __1 : : forward < void (* & ) ( ) > ( fp ) ( ) ) std : : __1 : : __invoke < void (* & ) ( ) > ( void (* & ) ( ) ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 5 ed20 ) # <number> 0x 5 6 4 aea7c8c6d in void std : : __1 : : __invoke_void_return_wrapper <void> : : __call < void (* & ) ( ) > ( void (* & ) ( ) ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 5 ec6d ) # <number> 0x 5 6 4 aea7c8c0d in std : : __1 : : __function : : __alloc_func < void (* ) ( ) , std : : __1 : : allocator < void (* ) ( ) > , void ()>: : operator ( ) ( ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 5 ec0d ) # <number> 0x 5 6 4 aea7c7049 in std : : __1 : : __function : : __func < void (* ) ( ) , std : : __1 : : allocator < void (* ) ( ) > , void ()>: : operator ( ) ( ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 5 d049 ) # <number> 0x 5 6 4 aeaab965c in std : : __1 : : __function : : __value_func < void ()>: : operator ( ) ( ) const ( / bitcoin - core / src / test / fuzz / fuzz + 0x 8 4 f65c ) # <number> 0x 5 6 4 aeaab95e5 in std : : __1 : : function < void ()>: : operator ( ) ( ) const ( / bitcoin - core / src / test / fuzz / fuzz + 0x 8 4 f5e5 ) # <number> 0x 5 6 4 aeb25ac9d in initialize ( ) ( / bitcoin - core / src / test / fuzz / fuzz + 0 xff0c9d ) # <number> 0x 5 6 4 aeb25b71f in llvmfuzzerinitialize ( / bitcoin - core / src / test / fuzz / fuzz + 0 xff171f ) # <number> 0x 5 6 4 aea743437 in fuzzer : : fuzzerdriver ( int * , char * * * , int (* ) ( unsigned char const * , unsigned long ) ) ( / bitcoin - core / src / test / fuzz / fuzz + 0x 4 d9437 ) # <number> 0x 5 6 4 aea76df22 in main ( / bitcoin - core / src / test / fuzz / fuzz + 0x 5 0 3 f22 ) # <number> 0x 7 f5d810e60b2 in __libc_start_main ( / lib / x86_64 - linux - gnu / libc . so . <number> + 0x 2 7 0 b2 ) # <number> 0x 5 6 4 aea719e7d in _start ( / bitcoin - core / src / test / fuzz / fuzz + 0x 4 afe7d ) undefinedbehaviorsanitizer can not provide additional info . summary : undefinedbehaviorsanitizer ( / lib / x86_64 - linux - gnu / libc + + . so . <number> + 0x 4 3 b47 ) in std : : __1 : : __libcpp_db : : swap ( void * , void <wink> = = <number> = = aborting",0
bitcoin/bitcoin,"bitcoind and bitcoin - cli does not supports symbolic link anymore i just pulled latest master at <url> and i cannot run bitcoind or bitcoin - cli without providing a full path for data directory using ` - datadir ` option . it was working fine couple of days ago ( while running <url> i suspect it may be related to latest switch to the standard std : : filesystem library <url> * * expected behavior * * i use a symbolic link to point my / home / me / . bitcoin to an external hard drive . when running ` bitcoind ` from command line , or executing ` bitcoin - cli ` commands , without using the ` - datadir ` option , it should not terminate or crash unexpectedly . those commands should be able to follow the symbolic link and be able to read / write into the final folder . * * actual behavior * * ` bitcoind ` and ` bitcoin - cli ` are not able to follow the symbolic link . ` bitcoin - qt ` runs fine , even when running it from command line . see below commands examples . ` ` ` # create a symbolic link $ ~ / btc - things / bitcoin $ ln - s / media / data / bitcoin / home / me / . bitcoin # try to run bitcoind - crashes $ ~ / btc - things / bitcoin $ . / bitcoind * * * * * * * * * * * * * * * * * * * * * * * * exception : nst10filesystem7__cxx1116filesystem_errore filesystem error : cannot create directories : not a directory [ / home / me / . bitcoin ] bitcoin in appinit ( ) bitcoind : chainparamsbase . cpp : <number> : const cbasechainparams & baseparams ( <sad> assertion ` globalchainbaseparams ' failed . aborted ( core dumped ) # run bitcoin - qt , it works fine , the node runs $ ~ / btc - things / bitcoin $ . / bitcoin - qt qt5ct : using qt5ct plugin # try to run a bitcoin - cli command to interact with the running node , does not work $ ~ / btc - things / bitcoin $ . / bitcoin - cli getblockhash <number> * * * * * * * * * * * * * * * * * * * * * * * * exception : nst10filesystem7__cxx1116filesystem_errore filesystem error : cannot create directories a directory [ / home / me / . bitcoin ] bitcoin in appinitrpc ( ) # try the same previous command but give an absolute path for - datadir , works fine $ ~ / btc - things / bitcoin $ . / bitcoin - cli - datadir <annoyed> media / data / bitcoin getblockhash <number> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 b27f9ee7ba33d6f048f684aaeb0eea4befd80f1701126 ` ` ` * * to reproduce * * steps are indicated previously . * * system information * * < - - what version of bitcoin core are you using , where did you get it ( website , self - compiled , etc ) ? - - > running ` bitcoin core version v22 . <number> - 3 ace3a17c9bc ` . self compiled from ` master ` branch . < ! - - what type of machine are you observing the error on ( os / cpu and disk type ) ? - - > i am using linux mint with the following kernel ` linux <number> . <number> - <number> - generic x86_64 ` . i am using an additional hard drive to store the full blockchain ( ssd ) . < ! - - any extra information that might be useful in the debugging process . - - > i had no prior issues before pulling latest changes . i was previously running at <url>",0
bitcoin/bitcoin,- reindex - chainstate with - prune hangs steps to reproduce create a datadir that is pruned * start with ` - prune = <number> - reindex - chainstate ` * hangs with idle cpu ( ` - reindex ` works ),0
opencv/opencv,"fixed opencl fp16 fallback in einsum layer fixes # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"error loading onnx model while reading it from opencv dnn module # # # system information ubuntu <number> python <number> python <number> . <number> opset version = <number> # # # detailed description hello i have converted the raf - db - model_best . pth model into onnx for facial emotion recognition i am able to read the model using onnxruntime and able to generate inference , but when read it using cv2 . dnn . readnetfromonnx , it ' s throwing me error > opencv ( <number> . <number> ) [ / io / opencv / modules / dnn / src / onnx / onnx_importer . cpp : <number> ] ( <url> error : ( - <number> : unspecified error ) in function ' handlenode ' node [ [ <email> ] ( mailto : <email> )]: ( onnx_node / attn2 / transpose_1 ) parse error : opencv ( <number> . <number> ) [ / io / opencv / modules / dnn / src / layers / permute_layer . cpp : <number> ] ( <url> error failed ) ( int ) _numaxes = = inputs [ <number> ] . size ( ) in function ' getmemoryshapes ' can someone explain me how to solve this error ? # # # steps to reproduce using cv2 . readnetfromonnx ( "" model_path "" ) # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"dnn : wrong internal results in ` expand_neg_batch ` in test_onnx_layers . expand # # # system information lastest opencv and lastest onnxruntime ( <number> . <number> , released just now ) # # # detailed description the model ` expand_neg_batch . onnx ` is as follows ( note that i manually added output "" <number> "" . ) , [ image ] ( <url> for output "" <number> "" , onnxruntime produces value [ <number> , <number> , <number> , <number> ]; dnn produces value [ <number> , - <number> , - <number> , - <number> ] . my own calculation is the same as onnxruntime ' s . model is attached here : [ expand_neg_batch . new . onnx . zip ] ( <url> - - - <user> * * should we run every single onnx model in ` test_onnx_importer . cpp ` with onnxruntime and verify with our input and output data to see whether the model is correctly built and dnn can produce correct results ? * * # # # steps to reproduce <number> . download the model and run inference to get outputs with onnxruntime ` ` ` python import numpy as np import onnxruntime as ort import cv2 as cv x = np . random . randn ( <number> , <number> , <number> ) . astype ( np . float32 ) def ort_inference ( model_path ) : net = ort . inferencesession ( model_path , providers =[ ' cpuexecutionprovider ' ] ) out = net . run ( [ "" <number> "" ] , { } ) [ <number> ] print ( "" ort result ="", out ) def ocv_inference ( model_path ) : net = cv . dnn . readnet ( model_path ) net . setinput ( x ) out = net . forward ( "" <number> "" ) print ( "" ocv result ="", out ) ort_inference ( "" . / expand_neg_batch . new . onnx "" ) # forget this , small 3 d tensor is parsed incorrectly because of opencv python binding <hashtag> ocv inference </hashtag> ( "" . / expand_neg_batch . new . onnx "" ) ` ` ` <number> . clone my expand refactor pr <url> add the following code to print ` shape ` in ` onnx_importer . cpp : : parseexpand ` ( about line <number> <sad> ` ` ` cpp for ( int i = <number> ; i < mat_input_shape . total ( ); + + i ) { std : : cout < < *( mat_input_shape . ptr <int> ( ) + i ) < < "" "" ; / / cv_check ( i , *( mat_input_shape . ptr <int> ( ) + i ) >= <number> , "" dnn / onnximporter - expand shape dimension "" ); } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fix memory leak and handle realloc failure in the previous code , there was a memory leak issue where the previously allocated memory was not freed upon a failed realloc operation . this commit addresses the problem by releasing the old memory before setting the pointer to null in case of a realloc failure . this ensures that memory is properly managed and avoids potential memory leaks . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name .",0
opencv/opencv,"risc - v compilation in rvv scalable mode tested with clang <number> and gcc <number> ( risc - v rvv scalable variant ) . also on x86_64 ( default ubuntu <number> gcc ) . ` ` ` force_builders = custom build_image : custom = riscv - clang - rvv xbuild_image : custom = riscv - clang - rvv - <number> xbuild_image : custom = riscv - gcc - rvv - <number> test_modules : custom = core , imgproc , dnn buildworker : custom = linux - <number> test_timeout : custom = <number> build_contrib : custom = off ` ` `",0
opencv/opencv,"build with openvino in debug # # # pull request readiness checklist resolves <url> see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix type cast in drawdetectedmarkers , drawdetectedcornerscharuco , drawdetecteddiamonds - drawdetectedmarkers ( ) , drawdetectedcornerscharuco ( ) , drawdetecteddiamonds ( ) uses only ` point2f ` ( ` cv_32fc2 ` ) for detected corners . but this corners are casted to ` int ` later in the code . - drawdetectedmarkers ( ) , drawdetecteddiamonds ( ) has a requirement on cv_32fc2 for corners . this requirement is lost in drawdetectedcornerscharuco ( ) . strict input data requirements have been removed . added only the requirement to have <number> channels . the input corners are then casted to cv_32sc2 . added tests # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"sigill in ` merge ` # # # system information opencv version : <number> . <number> operating system / platform : ubuntu <number> . <number> lts compiler & compiler version : g + + ( ubuntu <number> . <number> - 1 ubuntu1 ~ <number> ) <number> . <number> i also reproduced error on cv <number> . <number> on windows but the minimal example has been tested on ubuntu with cv4 . <number> . # # # detailed description using ` cv : : merge ` with null matrix is causing sigill . i am fully expecting a crash or any error but not a sigill as this leads to not executing the final stacktrace in our application . # # # steps to reproduce main . cpp : ` ` ` c + + <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> < opencv2 / opencv . hpp > int main ( int argc , char * argv [ ] ) { cv : : mat planes [ <number> ]; cv : : mat mm ; std : : cout < < "" hello there before "" < < std : : endl ; cv : : merge ( planes , <number> , mm ) ; std : : cout < < "" hello there after "" < < std : : endl ; return <number> ; } ` ` ` compile with g + + - o your_program_name main . cpp ` pkg - config opencv4 - - cflags - - libs ` ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"revert pr <number> as it forces skipping tests reverts <url> current test skipping issue : ` ` ` . / bin / opencv_test_dnn - - gtest_filter = test_caffe_layers * . <repeated> [ ok ] test_caffe_layers . deconvolution / <number> ( <number> ms ) [ run ] test_caffe_layers . innerproduct / <number> , where getparam ( ) = ngraph / cpu [ ok ] test_caffe_layers . innerproduct / <number> ( <number> ms ) [ run ] test_caffe_layers . innerproduct / <number> , where getparam ( ) = ocv / ocl [ ok ] test_caffe_layers . innerproduct / <number> ( <number> ms ) [ run ] test_caffe_layers . innerproduct / <number> , where getparam ( ) = ocv / ocl_fp16 [ skip ] test with tag ' dnn_skip_ocl_fp16 ' is skipped ( ' dnn_skip_ocl_fp16 ' is in skip list ) [ ok ] test_caffe_layers . innerproduct / <number> ( <number> ms ) [ run ] test_caffe_layers . innerproduct / <number> , where getparam ( ) = ocv / cpu [ skip ] [ ok ] test_caffe_layers . innerproduct / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ngraph / cpu [ skip ] [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / ocl [ skip ] [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / ocl_fp16 [ skip ] [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / cpu [ skip ] [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) ` ` ` all further tests are skipped . more focused test invocation works well . / bin / opencv_test_dnn - - gtest_filter = test_caffe_layers . pooling_max * . <repeated> [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ngraph / cpu [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / ocl [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / ocl_fp16 [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ run ] test_caffe_layers . pooling_max / <number> , where getparam ( ) = ocv / cpu [ ok ] test_caffe_layers . pooling_max / <number> ( <number> ms ) [ - - - - - - - - - - ] <number> tests from test_caffe_layers ( <number> ms total ) [ - - - - - - - - - - ] global test environment tear - down [== = = = = = = ==] <number> tests from <number> test case ran . ( <number> ms total ) [ passed ] <number> tests . ` ` ` # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"core : arm64 works with round to nearest , ties to even . fix <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"[ rfc ] rounding rule for cv : : divide ( ) # # # system information opencv version : <number> . x branch operating system / platform : ubuntu <number> ( raspi4 , arm64 ) compiler & compiler version : gcc <number> . <number> # # # detailed description this issue is about ` cv : : divide ( ) ` . related with <url> rounding rule for it is not describled at [ cv : : divide ( ) ] ( <url> in arm64 , it works with [ round to nearest , ties away from zero ] ( <url> in x86 - <number> , it works with [ round to nearest , ties to even ] ( <url> ( possibly , the behavior may change according to the rounding mode specification of the floating point unit . ) q . their results are different . which are these behaviours correct / better ? if arm64 behaviours should be fixed . <repeated> - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - for arm64 , i think it can fix following patch . however ` v_round ( ) ` function seems to used many times . i feel the risk of breaking backwards compatibility . i would appreciate it if you could comment on this issue . before : <url> after ` ` ` diff diff - - git a / modules / core / include / opencv2 / core / hal / intrin_neon . hpp b / modules / core / include / opencv2 / core / hal / intrin_neon . hpp index 6 f8973231b . <repeated> 1 4 eb180819 <number> - - - a / modules / core / include / opencv2 / core / hal / intrin_neon . hpp + + + b / modules / core / include / opencv2 / core / hal / intrin_neon . hpp @ @ - <number> + <number> @ @ inline v_int32x4 v_trunc ( const v_float32x4 & a ) inline v_int32x4 v_round ( const v_float64x2 & a ) { static const int32x2_t zero = vdup_n_s32 ( <number> ); - return v_int32x4 ( vcombine_s32 ( vmovn_s64 ( vcvtaq_s64_f64 ( a . val ) ) , zero ) ); + return v_int32x4 ( vcombine_s32 ( vmovn_s64 ( vcvtnq_s64_f64 ( a . val ) ) , zero ) ); } inline v_int32x4 v_round ( const v_float64x2 & a , const v_float64x2 & b ) { - return v_int32x4 ( vcombine_s32 ( vmovn_s64 ( vcvtaq_s64_f64 ( a . val ) ) , vmovn_s64 ( vcvtaq_s64_f64 ( b . val ) ))); + return v_int32x4 ( vcombine_s32 ( vmovn_s64 ( vcvtnq_s64_f64 ( a . val ) ) , vmovn_s64 ( vcvtnq_s64_f64 ( b . val ) ))); } ` ` ` # # # steps to reproduce ` ` ` cpp <hashtag> include </hashtag> < opencv2 / core . hpp > <hashtag> include </hashtag> <iostream> int main ( void ) { cv : : mat src1 = ( cv : : mat_ <uchar> ( <number> ) < < <number> , <number> , <number> , <number> , <number> , <number> ); std : : cout < < src1 < < std : : endl ; cv : : mat dst ; cv : : divide ( src1 , <number> , dst ); std : : cout < < dst < < std : : endl ; return <number> ; } ` ` ` ` ` ` bash [x 8 6 - <number> ] kmtr <user> - vmware - virtual - platform : ~ / work / studyt <money> . / a . out [ <number> , <number> , <number> ; <number> , <number> , <number> ; <number> , <number> , <number> ] [ <number> , <number> , <number> ; <number> , <number> , <number> ; <number> , <number> , <number> ] [ arm64 ] kmtr <user> : ~ / work / build4 - main / study $ . / a . out [ <number> , <number> , <number> ; <number> , <number> , <number> ; <number> , <number> , <number> ] [ <number> , <number> , <number> ; <number> , <number> , <number> ; <number> , <number> , <number> ] ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fixed bug with the last <number> bytes in mjpeg encoder resolves # <number> resolves # <number> enabled parallel processing , that was temporary disabled in # <number> because of the issue # <number>",0
opencv/opencv,"the cmake cann libopsproto . so library compiled with opencv4 . <number> could not be found # # # system information my device cpu : kunpeng <number> + npu : ascend310 ， aarch64 platform # # # detailed description ascend - cann - toolkit download address . the download version is <number> . <number> , opencv compiler options reference : <url> # # # steps to reproduce trial version opencv version <number> . <number> and cann_backend_221010 seem to have been tried by <number> . download toolkit : [ <url> ascend - cann - toolkit installation command : ` . / ascend - cann - toolkit_6 . <number> . 1 _linux - aarch64 . run - - install - - install - for - all ` first step ： ` source / usr / local / ascend / ascend - toolkit / set_env . sh ` second step : cmake - d with_cann = on - d build_opencv_gapi = off - d cmake_install_prefix = install . <repeated> error message ： - - cann : updated cann_install_dir from ascend_toolkit_home <annoyed> usr / local / ascend / ascend - toolkit / latest - - cann : libascendcl . so is found at / usr / local / ascend / ascend - toolkit / latest / acllib / lib64 / libascendcl . so - - cann : libgraph . so is found at / usr / local / ascend / ascend - toolkit / latest / compiler / lib64 / libgraph . so - - cann : libge_compiler . so is found at / usr / local / ascend / ascend - toolkit / latest / compiler / lib64 / libge_compiler . so - - cann libopsproto . so . turning off have_cann # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"onnx dnn inference produce wrong result compare to <number> . <number> # # # system information opencv version : <number> . <number> operating system / platform : windows10 compiler & compiler version : visual studio <number> for cpp and python version : <date> opencv python version : <number> . <number> for python # # # detailed description i had a model in python using the pytorch platform and converted it into onnx using ` ` ` torch . export . onnx ` ` ` and i wanted to infer it in cpp . after hours , i only got weird results compared to the torch version . i ran it ` ` ` onnxruntime ` ` ` and the results were the same as the pytorch model . then i inferred it in python and got the result exactly the same as cpp and very different from ` ` ` onnxruntime ` ` ` and pytorch . here are the pictures of the results in onnxruntime / pytorch vs . opencv dnn : onnxruntime / pytorch [ image ] ( <url> opencv dnn : ! [ image ] ( <url> i do not know what exactly going on but there is no problem in using opencv <number> . <number> # # # steps to reproduce ! [ raw ] ( <url> [ wide . onnx ] ( <url> this is the image for testing in python for onnxruntime and opencv , and you can access the model file with the link above . to load the image : ` ` ` c = cv2 . imread ( "" raw . jpg "" ) c = np . transpose ( c , ( <number> , <number> ) ) x = np . expand_dims ( c , axis = <number> ) . astype ( np . float32 ) ` ` ` then infer it with onnxruntime : ` ` ` import onnxruntime as ort import numpy as np ort_sess = ort . inferencesession ( ' wide . onnx ' ) outputs = ort_sess . run ( none , { ' input ' : x}) res = outputs [ <number> ] [ <number> , :, <happy> plt . imshow ( cv2 . normalize ( res , none , <number> , <number> , cv2 . norm_minmax ) , cmap = ' gray ' ) ` ` ` to reproduce the result in opencv , set the image as input of the net model = cv2 . dnn . readnetfromonnx ( "" wide . onnx "" ) model . setinput ( x ) output_blob = model . forward ( model . getunconnectedoutlayersnames ( ) ) plt . imshow ( cv2 . normalize ( output_blob [ <number> ] [ <number> , :, <happy> , none , <number> , <number> , cv2 . norm_minmax ) , cmap = ' gray ' ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"why opencv cuda morphology slower than my simple cu kernel ? # # # system information c + + user opencv version : <number> . <number> win10 msvc2019 gpu nvidia3060ti cuda <number> # # # detailed description img size 3k*2k <censored> , element size : <number> * <number> _use opencv cpu spend 4 ms ， my cpu is amd r5 5 5 0 0 _ opencvcuda spend 3 8 0 ms this is my cuda code spend 2 8 ms cuda npp spend 5 4 ms # # # steps to reproduce img size 3k*2k <censored> , element size : <number> * <number> _use opencv cpu spend 4 ms ， my cpu is amd r5 5 5 0 0 _ _although there are three main functions blow , i have placed them in one project to ensure consistency in the environment . i comment out two main functions when using them . _ this is opencvcuda morphology code : spend 3 8 0 ms - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ` ` ` <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> < opencv2 / cudaarithm . hpp > <hashtag> include </hashtag> < opencv2 / cudaimgproc . hpp > <hashtag> include </hashtag> < opencv2 / cudafilters . hpp > <hashtag> include </hashtag> "" cuda_runtime . h "" <hashtag> include </hashtag> "" device_launch_parameters . h "" <hashtag> if </hashtag> <number> int main ( ) { cv : : mat inputimage = cv : : imread ( "" c :\\\\ users \ \ \ \ <number> \ \ \ \ desktop \ \ \ \ test . jpg "" ); cv : : cvtcolor ( inputimage , inputimage , cv : : color_bgr2gray ) ; cv : : cuda : : gpumat src ; src . upload ( inputimage ) ; cv : : cuda : : printshortcudadeviceinfo ( cv : : cuda : : getdevice ( )); int an = <number> ; cv : : mat element = cv : : getstructuringelement ( cv : : morph_rect , cv : : size ( an * <number> + <number> , an * <number> + <number> ) , cv : : point ( an , an ) ); cv : : ptr < cv : : cuda : : filter > erodefilter = cv : : cuda : : createmorphologyfilter ( cv : : morph_erode , src . type ( ) , element ) ; cv : : cuda : : gpumat gpuoutputimage ; cudaevent_t start , stop ; cudaeventcreate ( & start ) ; cudaeventcreate ( & stop ) ; cudaeventrecord ( start ) ; / / auto starttime = std : : chrono : : high_resolution_clock : : now ( ); / / double start = cv : : gettickcount ( ); erodefilter - > apply ( src , gpuoutputimage ) ; / / double fps = cv : : gettickfrequency ( ) / ( cv : : gettickcount ( ) - start ) ; / / std : : cout < < "" fps : "" < < fps < < std : : endl ; / / auto endtime = std : : chrono : : high_resolution_clock : : now ( ); / / auto duration = std : : chrono : : duration_cast < std : : chrono : : microseconds > ( endtime - starttime ) . count ( ); / / std : : cout < < "" "" < < duration / <number> < < "" ms . "" < < std : : endl ; cudaeventrecord ( stop ) ; cudaeventsynchronize ( stop ) ; float milliseconds = <number> ; cudaeventelapsedtime ( & milliseconds , start , stop ) ; std : : cout < < "" gpu runtime : "" < < milliseconds < < "" ms "" < < std : : endl ; cv : : mat outputimage ; gpuoutputimage . download ( outputimage ) ; cv : : imshow ( "" input image "" , inputimage ) ; cv : : imshow ( "" eroded image "" , outputimage ) ; cv : : waitkey ( <number> ); return <number> ; } ` ` ` - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # # # * * this is my cuda code * * # # # * * spend 2 8 ms * * ` ` ` <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> "" cuda_runtime . h "" <hashtag> include </hashtag> "" device_launch_parameters . h "" <hashtag> include </hashtag> <algorithm> __global__ void erodekernel_junk ( const uchar * input , uchar * output , int rows , int cols , int kernelsize , int kernelsize2 ) { int x = blockidx . x * blockdim . x + threadidx . x; int y = blockidx . y * blockdim . y + threadidx . y ; if (x < cols & & y < rows ) { const int halfsize = kernelsize / <number> ; const int halfsize2 = kernelsize2 / <number> ; uchar minval = <number> ; for ( int i = - halfsize2 ; i <= halfsize2 ; + + i ) { for ( int j = - halfsize ; j <= halfsize ; + + j ) { int px = x + j ; int py = y + i ; if ( px >= <number> & & px < cols & & py >= <number> & & py < rows ) { uchar val = input [ py * cols + px ] ; if ( val < minval ) { minval = val ; } } } } output [ y * cols + x] = minval ; } } int main ( ) { cv : : mat inputimage = cv : : imread ( "" c <annoyed> users / <number> / desktop / test . jpg "" ); cv : : cvtcolor ( inputimage , inputimage , cv : : color_bgr2gray ) ; int w = <number> ; int h = <number> ; cv : : mat inputimage2 ; cv : : mat element = cv : : getstructuringelement ( cv : : morph_rect , cv : : size ( w , h ) ); auto starttime = std : : chrono : : steady_clock : : now ( ); cv : : erode ( inputimage , inputimage2 , element ) ; auto endtime = std : : chrono : : steady_clock : : now ( ); auto duration = std : : chrono : : duration_cast < std : : chrono : : milliseconds > ( endtime - starttime ) . count ( ); std : : cout < < "" 程序运行时间 : "" < < duration < < "" 毫秒 "" < < std : : endl < < std : : endl < < std : : endl ; int numgpus ; cudagetdevicecount ( & numgpus ) ; if ( numgpus <= <number> ) { std : : cerr < < "" no cuda - capable devices found . "" < < std : : endl ; return <number> ; } int deviceid = <number> ; cudadeviceprop deviceprop ; cudagetdeviceproperties ( & deviceprop , deviceid ) ; int inrows = inputimage . rows ; int incols = inputimage . cols ; uchar * d_inputimage , * d_outputimage , * d_tempimage ; size_t inputsize = inrows * incols * sizeof ( uchar ) ; size_t outputsize = ( inrows ) * ( incols ) * sizeof ( uchar ) ; cudamalloc ( & d_inputimage , inputsize ) ; cudamalloc ( & d_outputimage , outputsize ) ; cudamemcpy ( d_inputimage , inputimage . data , inputsize , cudamemcpyhosttodevice ) ; dim3 block ( <number> , <number> ); dim3 grid ( ( incols + block . x - <number> ) / block . x , ( inrows + block . y - <number> ) / block . y ) ; cudaevent_t start , stop ; cudaeventcreate ( & start ) ; cudaeventcreate ( & stop ) ; cudaeventrecord ( start ) ; erodekernel_junk < < < grid , block > > > ( d_inputimage , d_outputimage , inrows , incols , w , h ) ; cudaeventrecord ( stop ) ; cudaeventsynchronize ( stop ) ; { float milliseconds = <number> ; cudaeventelapsedtime ( & milliseconds , start , stop ) ; std : : cout < < "" gpu runtime : "" < < milliseconds < < "" ms "" < < std : : endl ; } cv : : mat outputimage ( inrows , incols , cv_8uc1 ) ; cudamemcpy ( outputimage . data , d_outputimage , outputsize , cudamemcpydevicetohost ) ; cudafree ( d_inputimage ) ; cudafree ( d_outputimage ) ; cv : : namedwindow ( "" eroded "" , cv : : window_keepratio ) ; cv : : imshow ( "" eroded "" , outputimage ) ; cv : : waitkey ( <number> ); return <number> ; } ` ` ` - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - this is cuda npp code ` ` ` <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> < nppi_morphological_operations . h > int main ( ) { cv : : mat inputimage = cv : : imread ( "" c <annoyed> users / <number> / desktop / test . jpg "" ); cv : : cvtcolor ( inputimage , inputimage , cv : : color_bgr2gray ) ; if ( inputimage . empty ( ) ) { std : : cout < < "" failed to load image . "" < < std : : endl ; return - <number> ; } npp8u * pu8srcdata_dev = null ; cudamalloc ( ( void * <wink> & pu8srcdata_dev , inputimage . cols * inputimage . rows * sizeof ( npp8u ) ); cudamemcpy ( pu8srcdata_dev , inputimage . data , inputimage . cols * inputimage . rows * sizeof ( npp8u ) , cudamemcpyhosttodevice ) ; cv : : mat element = cv : : getstructuringelement ( cv : : morph_rect , cv : : size ( <number> , <number> )); npp8u * pu8tmpl_dev = null ; cudamalloc ( ( void * <wink> & pu8tmpl_dev , element . cols * element . rows * sizeof ( npp8u ) ); cudamemcpy ( pu8tmpl_dev , element . data , element . cols * element . rows * sizeof ( npp8u ) , cudamemcpyhosttodevice ) ; nppisize tmplsize = { element . cols , element . rows }; nppipoint anchor = { element . cols > > <number> , element . rows > > <number> }; npp8u * pu8dstdata_dev = null ; cudamalloc ( ( void * <wink> & pu8dstdata_dev , inputimage . cols * inputimage . rows * sizeof ( npp8u ) ); nppisize osizeroi = { inputimage . cols , inputimage . rows }; cudaevent_t start , stop ; cudaeventcreate ( & start ) ; cudaeventcreate ( & stop ) ; nppstatus status ; cudaeventrecord ( start ) ; status = nppierode_8u_c1r ( pu8srcdata_dev , inputimage . cols , pu8dstdata_dev , inputimage . cols , osizeroi , pu8tmpl_dev , tmplsize , anchor ) ; cudaeventrecord ( stop ) ; cudaeventsynchronize ( stop ) ; { float milliseconds = <number> ; cudaeventelapsedtime ( & milliseconds , start , stop ) ; std : : cout < < "" npp gpu runtime < < milliseconds < < "" ms "" < < std : : endl ; } cv : : mat outputimage ( inputimage . rows , inputimage . cols , cv_8uc1 ) ; cudamemcpy ( outputimage . data , pu8dstdata_dev , inputimage . rows * inputimage . cols , cudamemcpydevicetohost ) ; cv : : namedwindow ( "" dilated image "" , cv : : window_keepratio ) ; cv : : imshow ( "" dilated image "" , outputimage ) ; cv : : waitkey ( <number> ); return <number> ; } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"use string instead of path to fix # <number> resolves # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,add missing include backport of # <number>,0
opencv/opencv,"dnn : fix the issue in layer_fuse fix issue relates # <number> handling when the ` operation ` of the ` elemtwise layer ` is missing . ` nextdata - > params . get <string> ( "" operation "" ) ` will fail when the ` operation ` is missing . this patch adds default value for such case . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"why minmaxidx restored reduced ipp calls # # # system information / / example for c + + user opencv version : <number> or later operating system / platform : centos <number> compiler & compiler version <number> . <number> # # # detailed description cv : : minmaxidx unable to use ipp acceleration / modules / core / src / minmax . cpp <hashtag> un def </hashtag> have_ipp <hashtag> un def </hashtag> cv_ipp_run_fast <hashtag> define </hashtag> cv_ipp_run_fast ( f , . <repeated> ) <hashtag> un def </hashtag> cv_ipp_run <hashtag> define </hashtag> cv_ipp_run ( c , f , . <repeated> ) <hashtag> define </hashtag> ipp_disable_minmaxidx_many_rows <number> / / see core_minmaxidx . rows_overflow test after restored reduced ipp calls , cv : : minmaxidx need more time to run , why unable to use ipp acceleration ? # # # steps to reproduce run cv : : minmaxidx and statistical time # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"mac m1 ld : symbol ( s ) not found for architecture arm64 . build java opencv for opencv - <number> . <number> # # # system information ` ` ` ( base ) guoquanhao <user> - pro build % clang - - version apple clang version <number> . <number> ( clang - <number> . <number> . <number> ) target : arm64 - apple - darwin22 . <number> thread model : posix installeddir : / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin ( base ) guoquanhao <user> - pro build % java - - version java <number> . <number> <number> - <number> - <number> lts java ( tm ) se runtime environment ( build <number> . <number> + <number> - lts - <number> ) java hotspot ( tm ) <number> - bit server vm ( build <number> . <number> + <number> - lts - <number> , mixed mode , sharing ) ( base ) guoquanhao <user> - pro build % ` ` ` < img width = "" <number> "" alt = "" image "" src = "" <url> < img width = "" <number> "" alt = "" image "" src = "" <url> using following command ` ` ` cmake \ \ - dcmake_system_processor = arm64 \ \ - dcmake_osx_architectures = arm64 \ \ - dwith_openjpeg = off \ \ - dwith_ipp = off \ \ - dcmake_build_type = release \ \ - dcmake_install_prefix <annoyed> usr / local / opencv \ \ - djava_include_path =$ java_home / include \ \ - dbuild_opencv_python2 = off \ \ - dbuild_opencv_java = on \ \ - dinstall_python_examples = off \ \ - dinstall_c_examples = off \ \ - dbuild_zlib = off \ \ - dopencv_enable_nonfree = on \ \ - dopencv_extra_modules_path <annoyed> users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules \ \ - dbuild_examples = on . <repeated> ` ` ` # # # detailed description ` ` ` est / test_niblack_threshold . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / onevpl / data_provider_dispatcher . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / onevpl / cfg_param_device_selector . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / onevpl / device_selector_interface . cpp . o [ <percent> ] building cxx object modules / xobjdetect / tools / waldboost_detector / cmakefiles / opencv_waldboost_detector . dir / waldboost_detector . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_radon_transform . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_ximgproc_structured_edge_detection [ <percent> ] built target example_ximgproc_structured_edge_detection [ <percent> ] building cxx object modules / aruco / cmakefiles / opencv_test_aruco . dir / test / test_aruco_tutorial . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_ridge_detection_filter . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamer_pipeline_facade . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_ximgproc_thinning [ <percent> ] built target example_ximgproc_thinning [ <percent> ] building cxx object modules / aruco / cmakefiles / opencv_perf_aruco . dir / perf / perf_main . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / . <repeated> / . <repeated> / bin / opencv_waldboost_detector [ <percent> ] built target opencv_waldboost_detector [ <percent> ] building cxx object modules / aruco / cmakefiles / opencv_test_aruco . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_rolling_guidance_filter . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_run_length_morphology . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_scansegment . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamerpipeline . cpp . o [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_calibrate_camera . dir / samples / calibrate_camera . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_perf_aruco [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_slic . cpp . o [ <percent> ] built target opencv_perf_aruco [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_calibrate_camera_charuco . dir / samples / calibrate_camera_charuco . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_aruco [ <percent> ] built target opencv_test_aruco [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamersource . cpp . o in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / calibrate_camera . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_create_board . dir / samples / create_board . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamer_buffer_utils . cpp . o in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / calibrate_camera_charuco . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_calibrate_camera [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_create_board_charuco . dir / samples / create_board_charuco . cpp . o [ <percent> ] built target example_aruco_calibrate_camera in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / create_board . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_create_diamond . dir / samples / create_diamond . cpp . o <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_calibrate_camera_charuco [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamer_media_adapter . cpp . o <number> warning generated . [ <percent> ] built target example_aruco_calibrate_camera_charuco [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_create_board [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_sparse_match_interpolator . cpp . o [ <percent> ] built target example_aruco_create_board [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / streaming / gstreamer / gstreamerenv . cpp . o [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_create_marker . dir / samples / create_marker . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_gapi . dir / src / utils / itt . cpp . o in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / create_board_charuco . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_structured_edge_detection . cpp . o <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_create_board_charuco [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_create_diamond [ <percent> ] built target example_aruco_create_board_charuco in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / create_marker . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_thinning . cpp . o [ <percent> ] built target example_aruco_create_diamond [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_detect_board . dir / samples / detect_board . cpp . o [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_detect_board_charuco . dir / samples / detect_board_charuco . cpp . o [ <percent> ] building cxx object modules / ximgproc / cmakefiles / opencv_test_ximgproc . dir / test / test_weighted_median_filter . cpp . o <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_create_marker [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_gapi . dylib [ <percent> ] built target example_aruco_create_marker [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_detect_diamonds . dir / samples / detect_diamonds . cpp . o [ <percent> ] built target opencv_gapi [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_detect_markers . dir / samples / detect_markers . cpp . o in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / detect_board . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / detect_board_charuco . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / detect_diamonds . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ <number> warning generated . <number> warning generated . in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / detect_markers . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_detect_board_charuco [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_detect_board [ <percent> ] building cxx object modules / aruco / cmakefiles / example_aruco_tutorial_charuco_create_detect . dir / samples / tutorial_charuco_create_detect . cpp . o [ <percent> ] built target example_aruco_detect_board [ <percent> ] built target example_aruco_detect_board_charuco [ <percent> ] building cxx object modules / bgsegm / cmakefiles / example_bgsegm_bgfg . dir / samples / bgfg . cpp . o [ <percent> ] building cxx object modules / bgsegm / cmakefiles / opencv_test_bgsegm . dir / test / test_backgroundsubtractor_gbh . cpp . o [ <percent> ] building cxx object modules / dpm / cmakefiles / example_dpm_cascade_detect_camera . dir / samples / cascade_detect_camera . cpp . o <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_detect_markers <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_detect_diamonds [ <percent> ] built target example_aruco_detect_markers [ <percent> ] building cxx object modules / bgsegm / cmakefiles / opencv_test_bgsegm . dir / test / test_backgroundsubtractor_lsbp . cpp . o [ <percent> ] built target example_aruco_detect_diamonds [ <percent> ] building cxx object modules / bgsegm / cmakefiles / opencv_test_bgsegm . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / dpm / cmakefiles / example_dpm_cascade_detect_sequence . dir / samples / cascade_detect_sequence . cpp . o in file included from / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / tutorial_charuco_create_detect . cpp : <number> : / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / aruco / samples / aruco_samples_utility . hpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] sprintf ( buf , "" flags : %s %s %s %s "" , ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_ximgproc [ <percent> ] built target opencv_test_ximgproc [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_bif . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_bgsegm_bgfg <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_aruco_tutorial_charuco_create_detect [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_dpm_cascade_detect_camera [ <percent> ] built target example_bgsegm_bgfg [ <percent> ] built target example_aruco_tutorial_charuco_create_detect [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facemark_demo_aam . dir / samples / facemark_demo_aam . cpp . o [ <percent> ] built target example_dpm_cascade_detect_camera [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facemark_demo_lbf . dir / samples / facemark_demo_lbf . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_face_align . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facemark_lbf_fitting . dir / samples / facemark_lbf_fitting . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_facemark . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_dpm_cascade_detect_sequence [ <percent> ] built target example_dpm_cascade_detect_sequence [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_demo . dir / samples / facerec_demo . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_bgsegm [ <percent> ] built target opencv_test_bgsegm / users / guoquanhao / opencv / opencv_contrib - <number> . <number> / modules / face / samples / facemark_lbf_fitting . cpp : <number> : <number> : warning : ' sprintf ' is deprecated : this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . [ - wdeprecated - declarations ] [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_facemark_aam . cpp . o sprintf ( buff , "" faces : % i % <number> . 2 f fps , fit : % <number> . 0 f ms "" , nfaces , fps , fittime * <number> ); ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / stdio . h : <number> : <number> : note : ' sprintf ' has been explicitly marked deprecated here __deprecated_msg ( "" this function is provided for compatibility reasons only . due to security concerns inherent in the design of sprintf ( <number> ) , it is highly recommended that you use snprintf ( <number> ) instead . "" ) ^ / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx13 . <number> . sdk / usr / include / sys / cdefs . h : <number> <time> : note : expanded from macro ' __deprecated_msg ' <hashtag> define </hashtag> __deprecated_msg ( _msg ) __attribute__ ( ( __deprecated__ ( _msg ) ) ) ^ [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facemark_demo_lbf [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_facemark_lbf . cpp . o [ <percent> ] built target example_face_facemark_demo_lbf [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_loadsave . cpp . o <number> warning generated . [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facemark_lbf_fitting [ <percent> ] built target example_face_facemark_lbf_fitting [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_eigenfaces . dir / samples / facerec_eigenfaces . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facemark_demo_aam [ <percent> ] built target example_face_facemark_demo_aam [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_mace . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_fisherfaces . dir / samples / facerec_fisherfaces . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / opencv_test_face . dir / test / test_main . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_demo [ <percent> ] built target example_face_facerec_demo [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_lbph . dir / samples / facerec_lbph . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_save_load . dir / samples / facerec_save_load . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / example_face_facerec_video . dir / samples / facerec_video . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_eigenfaces [ <percent> ] building cxx object modules / face / cmakefiles / example_face_mace_webcam . dir / samples / mace_webcam . cpp . o [ <percent> ] built target example_face_facerec_eigenfaces [ <percent> ] building cxx object modules / face / cmakefiles / example_face_sampledetectlandmarks . dir / samples / sampledetectlandmarks . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_fisherfaces [ <percent> ] built target example_face_facerec_fisherfaces [ <percent> ] building cxx object modules / face / cmakefiles / example_face_sampledetectlandmarksvideo . dir / samples / sampledetectlandmarksvideo . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_face [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_lbph [ <percent> ] building cxx object modules / face / cmakefiles / example_face_sample_face_swapping . dir / samples / sample_face_swapping . cpp . o [ <percent> ] built target opencv_test_face [ <percent> ] built target example_face_facerec_lbph [ <percent> ] building cxx object modules / face / cmakefiles / example_face_sample_train_landmark_detector . dir / samples / sample_train_landmark_detector . cpp . o [ <percent> ] building cxx object modules / face / cmakefiles / example_face_sample_train_landmark_detector2 . dir / samples / sample_train_landmark_detector2 . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_save_load [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_mace_webcam [ <percent> ] built target example_face_facerec_save_load [ <percent> ] built target example_face_mace_webcam [ <percent> ] building cxx object modules / face / cmakefiles / example_face_samplewriteconfigfile . dir / samples / samplewriteconfigfile . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_sampledetectlandmarks [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_facerec_video [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_compoundkernel_tests . cpp . o [ <percent> ] built target example_face_facerec_video [ <percent> ] built target example_face_sampledetectlandmarks [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_core_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_imgproc_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_sampledetectlandmarksvideo [ <percent> ] built target example_face_sampledetectlandmarksvideo [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / common / gapi_core_perf_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_sample_train_landmark_detector2 [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_sample_train_landmark_detector [ <percent> ] built target example_face_sample_train_landmark_detector2 [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_api_example . dir / samples / api_example . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_sample_face_swapping [ <percent> ] built target example_face_sample_train_landmark_detector [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_draw_example . dir / samples / draw_example . cpp . o [ <percent> ] built target example_face_sample_face_swapping [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_face_samplewriteconfigfile [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_face_detection_mtcnn . dir / samples / face_detection_mtcnn . cpp . o [ <percent> ] built target example_face_samplewriteconfigfile [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_gaze_estimation . dir / samples / gaze_estimation . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_api_example [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_draw_example [ <percent> ] built target example_gapi_api_example [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_operators_tests . cpp . o [ <percent> ] built target example_gapi_draw_example [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / common / gapi_imgproc_perf_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_gaze_estimation [ <percent> ] built target example_gapi_gaze_estimation [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_infer_ie_onnx_hybrid . dir / samples / infer_ie_onnx_hybrid . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_render_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / common / gapi_render_perf_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_face_detection_mtcnn [ <percent> ] built target example_gapi_face_detection_mtcnn [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_infer_single_roi . dir / samples / infer_single_roi . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_infer_ie_onnx_hybrid [ <percent> ] built target example_gapi_infer_ie_onnx_hybrid [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_stereo_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / common / gapi_video_perf_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_infer_single_roi [ <percent> ] built target example_gapi_infer_single_roi [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / common / gapi_video_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_core_tests_cpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_infer_ssd_onnx . dir / samples / infer_ssd_onnx . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_core_tests_fluid . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_infer_ssd_onnx [ <percent> ] built target example_gapi_infer_ssd_onnx [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_imgproc_tests_cpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_oak_basic_infer . dir / samples / oak_basic_infer . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_imgproc_tests_fluid . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / cpu / gapi_core_perf_tests_cpu . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_oak_basic_infer [ <percent> ] built target example_gapi_oak_basic_infer [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_oak_copy . dir / samples / oak_copy . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_oak_copy [ <percent> ] built target example_gapi_oak_copy [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_oak_rgb_camera_encoding . dir / samples / oak_rgb_camera_encoding . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_ocv_stateful_kernel_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / cpu / gapi_core_perf_tests_fluid . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_oak_rgb_camera_encoding [ <percent> ] built target example_gapi_oak_rgb_camera_encoding [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_oak_small_hetero_pipeline . dir / samples / oak_small_hetero_pipeline . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_oak_small_hetero_pipeline [ <percent> ] built target example_gapi_oak_small_hetero_pipeline [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / cpu / gapi_imgproc_perf_tests_cpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_onevpl_infer_with_advanced_device_selection . dir / samples / onevpl_infer_with_advanced_device_selection . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / cpu / gapi_imgproc_perf_tests_fluid . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_onevpl_source_to_bgr_conv . dir / samples / onevpl_source_to_bgr_conv . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / cpu / gapi_video_perf_tests_cpu . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_onevpl_source_to_bgr_conv [ <percent> ] built target example_gapi_onevpl_source_to_bgr_conv [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_pipeline_modeling_tool . dir / samples / pipeline_modeling_tool . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_onevpl_infer_with_advanced_device_selection [ <percent> ] built target example_gapi_onevpl_infer_with_advanced_device_selection [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_privacy_masking_camera . dir / samples / privacy_masking_camera . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_operators_tests_cpu . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_privacy_masking_camera [ <percent> ] built target example_gapi_privacy_masking_camera [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / gpu / gapi_core_perf_tests_gpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_semantic_segmentation . dir / samples / semantic_segmentation . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_pipeline_modeling_tool [ <percent> ] built target example_gapi_pipeline_modeling_tool [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_slides_blur_gapi . dir / samples / slides_blur_gapi . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / gpu / gapi_imgproc_perf_tests_gpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / internal / gapi_compiler_perf_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_slides_blur_gapi [ <percent> ] built target example_gapi_slides_blur_gapi [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_operators_tests_fluid . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_slides_sobel_cv . dir / samples / slides_sobel_cv . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_semantic_segmentation [ <percent> ] built target example_gapi_semantic_segmentation [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_stereo_tests_cpu . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_slides_sobel_cv [ <percent> ] built target example_gapi_slides_sobel_cv [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_slides_sobel_gapi . dir / samples / slides_sobel_gapi . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / cpu / gapi_video_tests_cpu . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_slides_sobel_gapi [ <percent> ] built target example_gapi_slides_sobel_gapi [ <percent> ] building cxx object modules / gapi / cmakefiles / example_gapi_text_detection . dir / samples / text_detection . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / executor / gtbbexecutor_internal_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_array_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / perf_bench . cpp . o [ <percent> ] processing opencl kernels ( optflow ) [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / deepflow . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / perf_main . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / interfaces . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / conditioning . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / motempl . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / render / gapi_render_perf_tests_ocv . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_async_test . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_gapi_text_detection [ <percent> ] built target example_gapi_text_detection [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / fundamental . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / pcaflow . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / io . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / rlof / geo_interpolation . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_basic_hetero_tests . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / numeric . cpp . o [ <percent> ] processing opencl kernels ( stitching ) [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / autocalib . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_perf_gapi . dir / perf / streaming / gapi_streaming_source_perf_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_compile_args_tests . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / rlof / rlof_localflow . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / projection . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / blenders . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_test_tracking . dir / test / test_aukf . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / reconstruct . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / camera . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / rlofflow . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_perf_gapi [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_perf_tracking . dir / perf / perf_main . cpp . o [ <percent> ] built target opencv_perf_gapi [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_desc_tests . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / exposure_compensate . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_test_tracking . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_fluid_parallel_rois_test . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / robust . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_perf_tracking . dir / perf / perf_trackers . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / simpleflow . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_test_tracking . dir / test / test_trackerparametersio . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / example_tracking_benchmark . dir / samples / benchmark . cpp . o [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / simple_pipeline . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_test_tracking . dir / test / test_trackers . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_tracking_benchmark [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / sparse_matching_gpc . cpp . o [ <percent> ] built target example_tracking_benchmark [ <percent> ] building cxx object modules / sfm / cmakefiles / opencv_sfm . dir / src / triangulation . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_perf_tracking [ <percent> ] built target opencv_perf_tracking [ <percent> ] building cxx object modules / tracking / cmakefiles / example_tracking_csrt . dir / samples / csrt . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / matchers . cpp . o [ <percent> ] building cxx object modules / tracking / cmakefiles / opencv_test_tracking . dir / test / test_ukf . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_tracking_csrt [ <percent> ] built target example_tracking_csrt [ <percent> ] building cxx object modules / tracking / cmakefiles / example_tracking_goturntracker . dir / samples / goturntracker . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / sparsetodenseflow . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_sfm . dylib undefined symbols for architecture arm64 : "" google : : initvlog3__ ( int * * , int * , char const * , int ) "" , referenced from : libmv : : estimatefundamentalfromcorrespondences ( eigen : : matrix < double , - <number> , - <number> , <number> , - <number> , - <number> > const & , eigen : : matrix < double , - <number> , - <number> , <number> , - <number> , - <number> > const & , libmv : : estimatefundamentaloptions const & , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> >*) in libmultiview . a ( fundamental . cc . o ) libmv : : euclideanreprojectionerror ( libmv : : tracks const & , libmv : : euclideanreconstruction const & , libmv : : cameraintrinsics const & ) in libsimple_pipeline . a ( pipeline . cc . o ) libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : eightpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> >>: : model libmv : : estimate < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : eightpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > , libmv : : mlescorer < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : eightpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > > > ( libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : eightpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > const & , libmv : : mlescorer < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : eightpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > > const & , libmv : : vector < int , eigen : : aligned_allocator <int> >* , double * , double ) in libmultiview . a ( robust_fundamental . cc . o ) libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : sevenpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> >>: : model libmv : : estimate < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : sevenpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > , libmv : : mlescorer < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : sevenpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > > > ( libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : sevenpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > const & , libmv : : mlescorer < libmv : : two_view : : kernel : : kernel < libmv : : two_view : : kernel : : normalizedsolver < libmv : : fundamental : : kernel : : sevenpointsolver , libmv : : unnormalizert > , libmv : : fundamental : : kernel : : sampsonerror , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> > > > const & , libmv : : vector < int , eigen : : aligned_allocator <int> >* , double * , double ) in libmultiview . a ( robust_fundamental . cc . o ) libmv : : euclideanresect ( libmv : : vector < libmv : : marker , eigen : : aligned_allocator < libmv : : marker > > const & , libmv : : euclideanreconstruction * , bool ) in libsimple_pipeline . a ( resect . cc . o ) libmv : : euclideanintersect ( libmv : : vector < libmv : : marker , eigen : : aligned_allocator < libmv : : marker > > const & , libmv : : euclideanreconstruction <wink> in libsimple_pipeline . a ( intersect . cc . o ) libmv : : estimatehomography2dfromcorrespondences ( eigen : : matrix < double , - <number> , - <number> , <number> , - <number> , - <number> > const & , eigen : : matrix < double , - <number> , - <number> , <number> , - <number> , - <number> > const & , libmv : : estimatehomographyoptions const & , eigen : : matrix < double , <number> , <number> , <number> , <number> , <number> >*) in libmultiview . a ( homography . cc . o ) . <repeated> ld : symbol ( s ) not found for architecture arm64 clang : error : linker command failed with exit code <number> ( use - v to see invocation ) make [ <number> <sad> * * * [ lib / libopencv_sfm . <number> . <number> . dylib ] error <number> make [ <number> <sad> * * * [ modules / sfm / cmakefiles / opencv_sfm . dir / all ] error <number> make [ <number> <sad> * * * waiting for unfinished jobs . <repeated> [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_fluid_resize_test . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / motion_estimators . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_fluid_roi_test . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / example_tracking_goturntracker [ <percent> ] built target example_tracking_goturntracker [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / src / tvl1flow . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / seam_finders . cpp . o [ <percent> ] building cxx object modules / optflow / cmakefiles / opencv_optflow . dir / opencl_kernels_optflow . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_tracking [ <percent> ] built target opencv_test_tracking [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / stitcher . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_fluid_test . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / timelapsers . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_optflow . dylib [ <percent> ] built target opencv_optflow [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_fluid_test_kernels . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_frame_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_gcompiled_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_gcomputation_tests . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / util . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / warpers . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_gpu_test . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / warpers_cuda . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_graph_meta_tests . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / opencl_kernels_stitching . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_kernel_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_opaque_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_plaidml_pipelines . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_planar_test . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_stitching . dylib [ <percent> ] built target opencv_stitching [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_sample_pipelines . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_scalar_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_smoke_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_transform_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_typed_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gapi_util_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gpu / gapi_core_tests_gpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gpu / gapi_imgproc_tests_gpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / gpu / gapi_operators_tests_gpu . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / infer / gapi_infer_ie_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / infer / gapi_infer_onnx_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / infer / gapi_infer_ov_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / infer / gapi_infer_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_backend_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_dynamic_graph . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_executor_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_garg_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_gmetaarg_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_gmodel_builder_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_island_fusion_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_island_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_pattern_matching_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_perform_substitution_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_proto_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_recompilation_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_int_vectorref_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / internal / gapi_transactions_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / oak / gapi_tests_oak . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / own / conc_queue_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / own / gapi_types_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / own / last_written_value_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / own / mat_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / own / scalar_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / render / ftp_render_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / render / gapi_render_tests_ocv . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / rmat / rmat_integration_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / rmat / rmat_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / rmat / rmat_view_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / s11n / gapi_s11n_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / s11n / gapi_sample_pipelines_s11n . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_gstreamer_pipeline_facade_int_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_gstreamersource_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_sync_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_utils_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_vpl_core_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_vpl_data_provider . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_vpl_device_selector . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / streaming / gapi_streaming_vpp_preproc_test . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / util / any_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / util / optional_tests . cpp . o [ <percent> ] building cxx object modules / gapi / cmakefiles / opencv_test_gapi . dir / test / util / variant_tests . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_gapi [ <percent> ] built target opencv_test_gapi make [ all ] error <number> ` ` ` # # # steps to reproduce as above # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"modify the outputvideoformat after changing the output format in msmf backend after changing the output format , need to modify the outputvideoformat , otherwise the outputvideoformat is always cv_cap_mode_bgr , and an error will occur when converting the format in retrievevideoframe ( ) , and will always enter "" case cv_cap_mode_bgr : "" process . # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix refinedetectedmarkers fixes # <number> to refine corners of aruco markers we need to call matchimagepoints ( ) from base class board . the method matchimagepoints ( ) implemented in pimpl and we need to create temp board object . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,fixed bug when msmf webcamera does not start when build with videoio_plugin_all fixed # <number> and # <number>,0
opencv/opencv,"refinedetectedmarkers fails if cameramatrix is given as a param # # # system information opencv python version : <number> . <number> operating system / platform : windows <number> python version : <date> # # # detailed description ` refinedetectedmarkers ` method of ` arucodetector ` fails if optional ` cameramatrix ` param is given . when running following line , ` ` ` corners , ids , rejected , _ = aruco_detector . refinedetectedmarkers ( image = grayscale_image , board = charucoboard , detectedcorners = aruco_corners , detectedids = aruco_ids , rejectedcorners = aruco_rejected , cameramatrix = camera_matrix , distcoeffs = dist_coeffs , ) ` ` ` an assertion error is raised : ` ` ` corners , ids , rejected , _ = aruco_detector . refinedetectedmarkers ( cv2 . error : opencv ( <number> . <number> ) d :\\ bld \ \ libopencv_1690022693676 \ \ work \ \ modules \ \ objdetect \ \ src \ \ aruco \ \ aruco_board . cpp : <number> : error : ( - <number> : assertion failed ) ( int ) detectedcharucovecmat [ i ] . total ( ) * detectedcharucovecmat [ i ] . channels ( ) = = <number> in function ' cv : : aruco : : charucoboardimpl : : matchimagepoints ' ` ` ` no error is raised if no ` cameramatrix ` is specified . # # # steps to reproduce ` ` ` python import imageio . v3 as iio import numpy as np import cv2 grayscale_image = iio . imread ( r "" c :\\ users \ \ jnicks \ \ documents \ \ repositories \ \ camera - characterization \ \ gray_scale_image . png "" ) camera_matrix = np . array ( [ [ <number> . 0 8 4 4 4 6 3 8 e03 , <number> . 0 0 0 0 0 0 0 0 e00 , <number> . 9 9 7 0 3 9 4 5 e02 ] , [ <number> . 0 0 0 0 0 0 0 0 e00 , <number> . 0 8 5 4 5 7 4 9 e03 , <number> . 2 4 9 7 8 5 0 9 e02 ] , [ <number> . 0 0 0 0 0 0 0 0 e00 , <number> . 0 0 0 0 0 0 0 0 e00 , <number> . 0 0 0 0 0 0 0 0 e00 ] , ] ) dist_coeffs = np . array ( [ <number> , <number> , <number> , <number> ] ) charucoboard = cv2 . aruco . charucoboard ( ( <number> , <number> ) , <number> , <number> , cv2 . aruco . getpredefineddictionary ( cv2 . aruco . dict_4x4_50 ) , ) charucoboard . setlegacypattern ( true ) aruco_detector = cv2 . aruco . arucodetector ( dictionary = charucoboard . getdictionary ( ) , detectorparams = cv2 . aruco . detectorparameters ( ) ) aruco_corners , aruco_ids , aruco_rejected = aruco_detector . detectmarkers ( image = grayscale_image ) # following line works # corners , ids , rejected , _ = aruco_detector . refinedetectedmarkers ( # image = grayscale_image , # board = charucoboard , # detectedcorners = aruco_corners , # detectedids = aruco_ids , # rejectedcorners = aruco_rejected , # distcoeffs = dist_coeffs , # ) # following line fails corners , ids , rejected , _ = aruco_detector . refinedetectedmarkers ( image = grayscale_image , board = charucoboard , detectedcorners = aruco_corners , detectedids = aruco_ids , rejectedcorners = aruco_rejected , cameramatrix = camera_matrix , distcoeffs = dist_coeffs , ) ` ` ` following image is used as input # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fix v_pack_store alignment issue on windows <number> - bit . nightly build issue : <url> the issue introduced in # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"python sample code tst_scene_render . py not compatible with modern numpy when using one image or no image # # # system information opencv python version : <number> . <number> operating system / platform : macos <number> . <number> python version : <number> . <number> , numpy <number> # # # detailed description samples . python / tst_scene_render . py is outdated and needs minor changes . <number> ) np . int is deprecated since [ numpy <number> release ] ( <url> when getting next frame with no foreground image provided to the class , np . int is used to update a rectangle and gives error messages ( numpy > = <number> ) . in opencv / samples / python / tst_scene_render . py ` ` ` class testscenerender ( <sad> . <repeated> def getnextframe ( self ) : if self . foreground is not none : . <repeated> else : self . currentrect = . <repeated> + np . int ( . <repeated> ) ` ` ` <hashtag> error </hashtag> message <number> : ` ` ` traceback ( most recent call last ) : file "" / opencv / samples / python / tst_scene_render . py "" , line <number> , in <module> main ( ) file "" / opencv / samples / python / tst_scene_render . py "" , line <number> , in main img = render . getnextframe ( ) file "" / opencv / samples / python / tst_scene_render . py "" , line <number> , in getnextframe self . currentrect = self . initialrect + np . int ( <number> * cos ( self . time*self <censored> . speed ) + <number> * sin ( self . time*self <censored> . speed ) ) file "" / usr / local / lib / python3 . <number> / site - packages / numpy / __init__ . py "" , line <number> , in __getattr__ raise attributeerror ( __former_attrs__ [ attr ] ) <url> did you mean : ' inf ' ? ` ` ` change np . int to int or np . int_ seems to be ok . <number> ) np . zeros takes take int or tuple of ints for shape parameter [ ( numpy <number> ) ] ( <url> when initializing the class with no background image , two ints are given for shape parameter of np . zeros and gives error messages ( tested with numpy <number> ) in opencv / samples / python / tst_scene_render . py ` ` ` class testscenerender ( <sad> . <repeated> def __init__ ( . <repeated> bgimg . <repeated> <sad> if bgimg is not none <seallips> . <repeated> else : self . scenebg = np . zeros ( defaultsize , defaultsize , np . uint8 ) ` ` ` <hashtag> numpy </hashtag> reference ` ` ` numpy . zeros ( shape , dtype = float , . <repeated> ) parameters : shape : int or tuple of ints ` ` ` <hashtag> error </hashtag> message <number> : ` ` ` traceback ( most recent call last ) : file "" / opencv / samples / python / tst_scene_render . py "" , line <number> , in <module> main ( ) file "" / opencv / samples / python / tst_scene_render . py "" , line <number> , in main render = testscenerender ( fgimg = fgr ) file "" opencv / samples / python / tst_scene_render . py "" , line <number> , in __init__ self . scenebg = np . zeros ( defaultsize , defaultsize , np . uint8 ) typeerror : cannot interpret ' <number> ' as a data type ` ` ` # # # steps to reproduce import testscenerender and prepare images ( working directory ` ` ` import cv2 as cv from tst_scene_render import testscenerender backgr = cv . imread ( cv . samples . findfile ( ' digits . png ' ) ) fgr = cv . imread ( cv . samples . findfile ( ' board . jpg ' ) ) ` ` ` test with both foreground and background images ( works fine ) ` ` ` render = testscenerender ( bgimg = backgr , fgimg = fgr ) img = render . getnextframe ( ) cv . imshow ( ' img ' , img ) cv . waitkey ( <number> ) cv . destroyallwindows ( ) ` ` ` test with passing none to foreground image ( <hashtag> error </hashtag> message <number> ) ` ` ` render = testscenerender ( bgimg = backgr , fgimg = none ) img = render . getnextframe ( ) ` ` ` test with passing none to # # # background image ( <hashtag> error </hashtag> message <number> ) ` ` ` render = testscenerender ( bgimg = none , fgimg = fgr ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"solved bug in reduce layer # <number> # # # pull request readiness checklist resolves <url> see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"invalid memory access fix for onnx split layer parser # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work <url> - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"port stereorectify grid fix # <number> review and merge after <url> address # <number> in <number> . x manual port of <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"videowriter resulting size one off error # # # system information opencv python version : <number> . <number> operating system / platform : ubuntu <number> python version : <date> # # # detailed description saving an image of shape ` <number> , <number> , <number> ` gives shape ` <number> , <number> , <number> ` ( <number> vs <number> ) . # # # steps to reproduce ` ` ` python import numpy as np import cv2 writer = cv2 . videowriter ( ' res % 0 2 d . png ' , cv2 . videowriter_fourcc ( * ' mjpg ' ) , <number> , ( <number> , <number> ) ) writer . write ( np . zeros ( ( <number> , <number> , <number> ) , np . uint8 ) ) writer . release ( ) print ( cv2 . imread ( ' res01 . png ' ) . shape ) ` ` ` the reproducer also prints ` opencv : ffmpeg 0x 4 7 5 0 4 a4d / ' mjpg ' is not supported with codec id <number> and format ' image2 / image2 sequence ' ` after line ` writer = cv2 . videowriter ( ' res % 0 2 d . png ' , cv2 . videowriter_fourcc ( * ' mjpg ' ) , <number> , ( <number> , <number> ) ) ` also reproducible for c + + ( tested commit 1 7 9 4 cdc03c9505bb46f33a5cde5e210c1c7f65a4 ) . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"exception in onnx : : parsesplit # # # system information ` ` ` general configuration for opencv <number> . <number> - dev = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> - <number> - g0323761ea6 extra modules : location ( extra ) : c <annoyed> lib / opencv_contrib / modules version control ( extra ) : <number> . <number> - <number> - gdaaf6451 platform : timestamp : <number> - <number> - 2 9 t <time> z host : windows <number> . <number> amd64 cmake : <number> . <number> cmake generator : visual studio <number> <number> cmake build tool : c <annoyed> program files / microsoft visual studio / <number> / community / msbuild / current / bin / amd64 / msbuild . exe msvc : <number> configuration : debug release cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe ( ver <number> . <number> ) c + + flags ( release ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / md / o2 / ob2 / dndebug c + + flags ( debug ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / mdd / zi / ob0 / od / rtc1 c compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe c flags ( release ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / md / o2 / ob2 / dndebug c flags ( debug ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / mdd / zi / ob0 / od / rtc1 linker flags ( release ) : / machine <kiss> 6 4 / incremental : no linker flags ( debug ) : / machine <kiss> 6 4 / debug / incremental ccache : no precompiled headers : yes extra dependencies : cudart_static . lib nppc . lib nppial . lib nppicc . lib nppidei . lib nppif . lib nppig . lib nppim . lib nppist . lib nppisu . lib nppitc . lib npps . lib cublas . lib cudnn . lib cufft . lib - libpath <sad> <annoyed> program files / nvidia gpu computing toolkit / cuda / v12 . <number> / lib / x64 3 rdparty dependencies : opencv modules : to be built : alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto disabled : world disabled by dependency : - unavailable : cvv freetype hdf julia matlab ovis python2 applications : tests perf_tests examples apps documentation : doxygen python javadoc non - free algorithms : yes windows rt support : no gui : win32ui win32 ui : yes opengl support : yes ( opengl32 glu32 ) vtk support : yes ( ver <number> . <number> ) media i / <surprise> zlib : optimized c <annoyed> install / zlib / lib / zlib . lib debug c <annoyed> install / zlib / lib / zlibd . lib ( ver <date> ) jpeg : build - libjpeg - turbo ( ver <number> . <number> - <number> ) simd support request : yes simd support : no webp : build ( ver encoder : 0x0 2 0 f ) png : optimized c <annoyed> install / libpng / lib / libpng16 . lib debug c <annoyed> install / libpng / lib / libpng16d . lib ( ver <date> ) tiff : build ( ver <number> - <number> . <number> ) jpeg <number> : build ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes ( prebuilt binaries ) avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : yes ( <number> . <number> ) gstreamer : no directshow : yes media foundation : yes dxva : yes parallel framework : tbb ( ver <number> interface <number> ) other third - party libraries : intel ipp : <number> [ <number> . <number> ] at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / icv intel ipp iw : sources ( <number> . <number> ) at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / iw lapack : no openvino : yes ( <number> . <number> ) eigen : yes ( ver . <repeated> ) custom hal : no protobuf : build ( <number> . <number> ) flatbuffers : builtin / 3 rdparty ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas ) nvidia gpu arch : <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( nvd3d11 ) include path : c <annoyed> lib / opencv / 3 rdparty / include / opencl / <number> link libraries : dynamic load python <number> : interpreter : c <annoyed> program files / python310 / python . exe ( ver <date> ) libraries : optimized c <annoyed> program files / python310 / libs / python310 . lib debug c <annoyed> program files / python310 / libs / python310_d . lib ( ver <date> ) numpy : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / numpy / core / include ( ver <number> . <number> ) install path : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / cv2 / python - <number> python ( for build ) : c <annoyed> program files / python310 / python . exe java : ant : c <annoyed> apache - ant - <date> / bin / ant . bat ( ver <date> ) java : no jni : c <annoyed> program files / java / jdk - <number> / include c <annoyed> program files / java / jdk - <number> / include / win32 c <annoyed> program files / java / jdk - <number> / include java wrappers : yes ( ant ) java tests : yes install to : c <annoyed> install / opencv - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ` ` ` # # # detailed description model is [ here ] ( <url> thanks to <user> i can read this model in python or in c + + in release but not in debug . output is ` ` ` reading sam_vit_b . fixed . nopost . sim . onnx [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : populatenet dnn / onnx : loading onnx v8 model produced by ' pytorch ' : <number> . <number> . number of nodes = <number> , initializers = <number> , inputs = <number> , outputs = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : parseoperatorset dnn / onnx : onnx opset version = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node / add ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ div ] <sad> onnx_node ! / div ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sub ] <sad> onnx_node ! / sub ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ matmul ] <sad> onnx_node ! / matmul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sin ] <sad> onnx_node ! / sin ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cos ] <sad> onnx_node ! / cos ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ concat ] <sad> onnx_node ! / concat ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ unsqueeze ] <sad> onnx_node ! / unsqueeze ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ expand ] <sad> onnx_node ! / expand ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ not ] <sad> onnx_node ! / not ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_3 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_4 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal_3 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast_3 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_5 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_3 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal_4 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast_4 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_6 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_4 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ equal ] <sad> onnx_node ! / equal_5 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / cast_5 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_7 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_5 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / reducemean ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sub ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / sub ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ pow ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / pow ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / reducemean_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / add ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sqrt ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / sqrt ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ div ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / div ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / add_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ gelu ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / mul_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / reducemean ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sub ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / sub ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ pow ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / pow ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / reducemean_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / add ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sqrt ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / sqrt ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ div ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / div ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / add_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ gelu ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / mul_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / mask_downscaling / mask_downscaling . <number> / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_8 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sub ] <sad> onnx_node ! / sub_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / mul_9 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / add_6 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ concat ] <sad> onnx_node ! / concat_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ split ] <sad> onnx_node ! / split_1 ) from domain = ' ai . onnx ' ` ` ` exception is ` ` ` program : c :\\ install \ \ opencv \ \x 6 4 \ \ vc17 \ \ bin \ \ opencv_dnn480d . dll file : c :\\ program files \ \ microsoft visual studio \ \ <number> \ \ community \ \ vc \ \ tools \ \ msvc \ \ <number> . <number> \ \ include \ \ vector line : <number> expression : vector subscript out of range ` ` ` it ' s here <url> slicepoint size is <number> and hence there is no adress for slicepoints [ <number> ] and stack strace is ` ` ` opencv_dnn480d . dll ! std : : vector < int , std : : allocator <int> <sad> : operator [ ] ( const unsigned __int64 _pos ) line <number> at c :\\ program files \ \ microsoft visual studio \ \ <number> \ \ community \ \ vc \ \ tools \ \ msvc \ \ <number> . <number> \ \ include \ \ vector ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : parsesplit ( cv : : dnn : : dnn4_v20230620 : : layerparams & layerparams , const opencv_onnx : : nodeproto & node_proto ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode ( const opencv_onnx : : nodeproto & node_proto ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : populatenet ( ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : onnximporter ( cv : : dnn : : dnn4_v20230620 : : net & net , const char * onnxfile ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : detail : : readnet < cv : : dnn : : dnn4_v20230620 : : onnximporter , char const *>( const char * & & <args_0> ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ dnn_common . hpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : detail : : readnetdiagnostic < cv : : dnn : : dnn4_v20230620 : : onnximporter , char const *>( const char * & & <args_0> ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ dnn_common . hpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : readnetfromonnx ( const std : : string & onnxfile ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : readnet ( const std : : string & _model , const std : : string & _config , const std : : string & _framework ) line <number> at c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ dnn_read . cpp ( <number> ) ` ` ` # # # steps to reproduce in c + + and in debug netmask = readnet ( "" sam_vit_b . fixed . nopost . sim . onnx "" ); # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"doesnt use cuda for specific layer # # # system information opencv version : <number> . <number> ( current <number> . x branch ) windows compiler : msvc <number> running a onnx file ( same as here ) i noticed very high cpu usage ( gpu also gets used but its bottlenecked by the cpu ) so we went and profiled with intel vtune turns out that most of the time is spent on opt_avc2 : : fastgemm1t is this op only available for cpu ? why does not the full model run on gpu [ image ] ( <url> cuda runtime is enabled i can run many other models without issue on gpu # # # detailed description ! [ image ] ( <url> # # # steps to reproduce very basic just use net - > forward with the model linked in the other issue ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"subtracting a tuple from an image gives ` error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' ` # # # system information ` ` ` opencv python version : <number> . <number> operating system : windows <number> 2 2 h2 python version : <date> ` ` ` # # # detailed description from what i understand of the [ docs ] ( <url> ` cv2 . subtract ( ) ` should be able to subtract a tuple from an array if it has the same width as the array has channels : > difference between an array and a scalar , when src2 is constructed from scalar or has the same number of elements as src1 . channels ( <sad> however , loading an image with ` imread ` and attempting to subtract a tuple gives ` ` ` > > > cv2 . subtract ( mat , ( <number> , <number> , <number> ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' ` ` ` the same error occurs when trying to use a tuple of floats ` ` ` > > > cv2 . subtract ( mat , ( <number> , <number> , <number> ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' ` ` ` and numpy arrays of various types ` ` ` > > > cv2 . subtract ( mat , np . uint8 ( [ <number> , <number> , <number> ] ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' > > > cv2 . subtract ( mat , np . array ( [ <number> , <number> , <number> ] ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' > > > cv2 . subtract ( mat , np . float64 ( [ <number> , <number> , <number> ] ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error : ( - <number> : assertion failed ) type2 = = cv_64f & & ( sz2 . height = = <number> || sz2 . height = = <number> ) in function ' cv : : arithm_op ' ` ` ` subtracting a number works , ` ` ` > > > cv2 . subtract ( mat , <number> ) array ( [ [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , . <repeated> , ` ` ` as does subtracting the tuple using ` numpy ` ( but that will not work in my case because ` numpy ` does not saturate the subtraction like opencv does ) . ` ` ` > > > mat - ( <number> , <number> , <number> ) array ( [ [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , . <repeated> , ` ` ` the error message is also correct if i remove a channel from the tuple : ` ` ` > > > cv2 . subtract ( mat , ( <number> , <number> ) ) traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ core \ \ src \ \ arithm . cpp : <number> : error of input arguments do not match ) the operation is neither ' array op array ' ( where arrays have the same size and the same number of channels ) , nor ' array op scalar ' , nor ' scalar op array ' in function ' cv : : arithm_op ' ` ` ` # # # steps to reproduce ` ` ` py import cv2 mat = cv2 . imread ( "" c :\\\\[ . <repeated> ] . png "" ) print ( cv2 . subtract ( mat , ( <number> , <number> , <number> ) ) ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"test_tensorflow_layers . tf2_prelu fails with openvino cpu backend # # # system information os : linux x64 openvino : <number> . <number> ( official docker image ) ci link : <url> # # # detailed description ` ` ` [ run ] test_tensorflow_layers . tf2_prelu / <number> , where getparam ( ) = ngraph / cpu unknown file : failure c + + exception with description "" opencv ( <number> . <number> - dev ) / home / openvino / opencv / modules / dnn / src / ie_ngraph . cpp : <number> : error : ( - <number> : unspecified error ) in function ' initplugin ' > failed to initialize inference engine backend ( device = cpu ) : check ' false ' failed at src / inference / src / core . cpp : <number> : > check ' partialshape : : broadcast_merge_into ( tmppshape , inshape , : : ngraph : : op : : autobroadcasttype : : numpy ) ' failed at src / common / snippets / src / op / subgraph . cpp : <number> : > while validating node ' snippetsopset : : subgraph statefulpartitionedcall / statefulpartitionedcall / sequential / p_re_lu / add ( parameter_11102076 [ <number> ] : f32 [ <number> , <number> ] , parameter_11102077 [ <number> ] : f32 [ <number> ] ) - > ( f32 [ <number> , <number> ] ) ' with friendly_name ' statefulpartitionedcall / statefulpartitionedcall / sequential / p_re_lu / add ' failed to create broadcastable shapes in snippets canonicalization > > "" thrown in the test body . [ failed ] test_tensorflow_layers . tf2_prelu / <number> , where getparam ( ) = ngraph / cpu ( <number> ms ) ` ` ` # # # steps to reproduce . / bin / opencv_test_dnn - - gtest_filter = test_tensorflow_layers . tf2_prelu * # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fix compilation error on windows arm , use vaddq_f32 instead of + = # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"cv . cuda . gpumat . convertto failed to convert data types # # # system information / / example for python user opencv python version : <number> . <number> operating system / platform : ubuntu <number> python version : <date> # # # detailed description ` ` ` image = frame . convertto ( cv2 . cv_32f ) ` ` ` ` ` ` cv2 . error : opencv ( <number> . <number> ) / data / work / opencv - cuda - <number> . <number> / opencv - <number> . <number> / modules / core / src / matrix_wrap . cpp : <number> : error function / feature is not implemented ) getgpumat is available only for cuda : : gpumat and cuda : : hostmem in function ' getgpumat ' ` ` ` i use ` cv2 . cudacodec . createvideoreader ( ) ` read the video streaming , mat type for ` < cv2 . cuda . gpumat 0x 7 f826b934a50 > ; ` the data type is ` 8 uc4 ` and i tried to convert the data type to float32 but got an error # # # steps to reproduce ` ` ` cap = cv2 . cudacodec . createvideoreader ( file_path ) ret , frame = cap . nextframe ( ) image = frame . convertto ( cv2 . cv_32f ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fix stereorectify image boundaries . this should have been fixed with <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"feat : add cuda_stream and cuda_gpumat to simple types mapping this patch fixes usage of ` cuda : : stream ` in function arguments . affected modules : ` cudacodec ` namespace cuda ` ] ( <url> in public ` cudacodec . hpp ` header can be removed after merge of the patch . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix : preserve numpy writeable flag in output arguments example error output : ` ` ` opencv ( <number> . <number> - dev ) : - <number> : error : ( - <number> : bad argument ) in function ' rectangle ' > overload resolution failed - img marked as output argument , but provided numpy array marked as readonly > - expected ptr < cv : : umat > for argument ' img ' > - img marked as output argument , but provided numpy array marked as readonly > - expected ptr < cv : : umat > for argument ' img ' ` ` ` resolves opencv / opencv - python # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix python typing stubs generation for cuda modules resolves # <number> resolves # <number> resolves opencv / opencv - python # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"test_caffe_nets . fasterrcnn_vgg16 fails with enable_fast_math option # # # system information opencv : current <number> . x ( <number> . <number> - dev ) platform : ubuntu <number> , gcc <number> hardware : core i5 2 5 0 0 k ( no avx2 and no avx512 ) # # # detailed description net inference accuracy issue is raised . # # # steps to reproduce ` ` ` cmake - denable_fast_math = <number> . <repeated> / opencv make - j4 . / bin / opencv_test_dnn - - gtest_filter = "" test_caffe_nets . fasterrcnn_vgg16 / * "" ` ` ` log : ` ` ` [ run ] test_caffe_nets . fasterrcnn_vgg16 / <number> , where getparam ( ) = ocv / cpu unmatched prediction : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] highest iou : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure value of : matched actual : false expected : true model name : vgg16_faster_rcnn_final . caffemodel unmatched prediction : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] highest iou : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure value of : matched actual : false expected : true model name : vgg16_faster_rcnn_final . caffemodel unmatched prediction : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] highest iou : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure value of : matched actual : false expected : true model name : vgg16_faster_rcnn_final . caffemodel unmatched reference : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] iou diff : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure expected : ( refscores [ i ] ) <= ( confthreshold ) , actual : <number> vs <number> model name : vgg16_faster_rcnn_final . caffemodel unmatched reference : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] iou diff : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure expected : ( refscores [ i ] ) <= ( confthreshold ) , actual : <number> vs <number> model name : vgg16_faster_rcnn_final . caffemodel unmatched reference : class <number> score <number> box [ <number> x <number> from ( <number> , <number> ) ] iou diff : <number> / home / alexander / projects / opencv / opencv - master / modules / dnn / test / test_common . impl . hpp : <number> : failure expected : ( refscores [ i ] ) <= ( confthreshold ) , actual : <number> vs <number> model name [ failed ] test_caffe_nets . fasterrcnn_vgg16 / <number> , where getparam ( ) = ocv / cpu ( <number> ms ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"[ tflite ] pack layer and other fixes for ssd from keras # # # pull request readiness checklist resolves <url> * * merge with extra * * see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"it is not possible to redirect some error printed directly to cerr # # # system information problem in source code at any platform <url> and on other places # # # detailed description the code uses direct print to std : : cerr ( e . g . <url> then it is not possible to redirect via ` cv : : redirecterror ` . # # # steps to reproduce see code <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"opencv build form source # # # system information opencv version : <number> . <number> operating system / platform : windows10 compiler & compiler version : cmake + vs2022 + x64 [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url> # # # detailed description ` ` ` build started . <repeated> <number> > - - - - - - build started : project : opencv_world , configuration : release x64 - - - - - - <number> > normalize_bbox_layer . cpp <number> > region_layer . cpp <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> error c2666 : ' operator >': <number> overloads have similar conversions ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> message : could be ' bool cv : : operator > ( const cv : : matconstiterator & , const cv : : matconstiterator & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator > ( double , const cv : : mat & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator > ( const cv : : mat & , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator > ( const cv : : mat & , const cv : : mat & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> message : or ' bool operator > ( const __half & , const __half & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> message : or ' bool operator > ( const __half2 & , const __half2 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> message : or ' bool operator > ( const __nv_bfloat16 & , const __nv_bfloat16 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> message : or ' bool operator > ( const __nv_bfloat162 & , const __nv_bfloat162 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( float , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( signed char , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( unsigned char , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( char , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( short , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( unsigned short , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( int , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( unsigned int , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( long , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( unsigned long , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( __int64 , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( unsigned __int64 , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : or ' built - in c + + operator > ( bool , int ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : while trying to match the argument list ' ( t , int ) ' <number> > with <number> > [ <number> > t = half <number> > ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / region . hpp ( <number> <sad> message : while compiling class template member function ' void cv : : dnn : : cuda4dnn : : regionop <half> : : forward ( const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , cv : : dnn : : cuda4dnn : : csl : : workspace & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / op_cuda . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : dnn : : cuda4dnn : : regionop <half> ' being compiled ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ region_layer . cpp ( <number> <sad> message : see reference to function template instantiation ' cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendnode > cv : : dnn : : make_cuda_node < cv : : dnn : : cuda4dnn : : regionop , cv : : dnn : : cuda4dnn : : csl : : stream , _ty & , cv : : dnn : : cuda4dnn : : regionconfiguration <float> & > ( int , cv : : dnn : : cuda4dnn : : csl : : stream & & , _ty & , cv : : dnn : : cuda4dnn : : regionconfiguration <float> & ) ' being compiled <number> > with <number> > [ <number> > _ty = cv : : mat <number> > ] <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> error c2666 : ' operator ! = ' : <number> overloads have similar conversions ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / persistence . hpp ( <number> <sad> message : could be ' bool cv : : operator ! =( const cv : : filenodeiterator & , const cv : : filenodeiterator & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : sparsematconstiterator & , const cv : : sparsematconstiterator & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : matconstiterator & , const cv : : matconstiterator & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator ! =( double , const cv : : mat & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator ! =( const cv : : mat & , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' cv : : matexpr cv : : operator ! =( const cv : : mat & , const cv : : mat & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : umatdata : : memoryflag & , const int & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : _inputarray : : kindflag & , const int & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : accessflag & , const int & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> message : or ' bool cv : : operator ! =( const cv : : range & , const cv : : range & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_int64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int64x2 & , const cv : : hal_baseline : : v_int64x2 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_uint64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint64x2 & , const cv : : hal_baseline : : v_uint64x2 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_float64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_float64x2 & , const cv : : hal_baseline : : v_float64x2 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_float32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_float32x4 & , const cv : : hal_baseline : : v_float32x4 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_int32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int32x4 & , const cv : : hal_baseline : : v_int32x4 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_uint32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint32x4 & , const cv : : hal_baseline : : v_uint32x4 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_int16x8 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int16x8 & , const cv : : hal_baseline : : v_int16x8 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_uint16x8 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint16x8 & , const cv : : hal_baseline : : v_uint16x8 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_int8x16 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int8x16 & , const cv : : hal_baseline : : v_int8x16 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> message : or ' cv : : hal_baseline : : v_uint8x16 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint8x16 & , const cv : : hal_baseline : : v_uint8x16 & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> message : or ' bool operator ! =( const __half & , const __half & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> message : or ' bool operator ! =( const __half2 & , const __half2 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> message : or ' bool operator ! =( const __nv_bfloat16 & , const __nv_bfloat16 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> message : or ' bool operator ! =( const __nv_bfloat162 & , const __nv_bfloat162 & ) ' [ found using argument - dependent lookup ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( float , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( signed char , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( unsigned char , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( char , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( short , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( unsigned short , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( int , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( unsigned int , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( long , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( unsigned long , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( __int64 , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( unsigned __int64 , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : or ' built - in c + + operator ! =( bool , double ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : while trying to match the argument list ' ( t , double ) ' <number> > with <number> > [ <number> > t = half <number> > ] ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> message : while compiling class template member function ' void cv : : dnn : : cuda4dnn : : normalizeop <half> : : forward ( const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , cv : : dnn : : cuda4dnn : : csl : : workspace & ) ' ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / op_cuda . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : dnn : : cuda4dnn : : normalizeop <half> ' being compiled ( compiling source file d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ) <number> > d :\\ cpproject <elongated> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ( <number> <sad> message : see reference to function template instantiation ' cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendnode > cv : : dnn : : make_cuda_node < cv : : dnn : : cuda4dnn : : normalizeop , cv : : dnn : : cuda4dnn : : csl : : stream , const cv : : mat & , cv : : dnn : : cuda4dnn : : normalizeconfiguration <float> & > ( int , cv : : dnn : : cuda4dnn : : csl : : stream & & , const cv : : mat & , cv : : dnn : : cuda4dnn : : normalizeconfiguration <float> & ) ' being compiled <number> > done building project "" opencv_world . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_waldboost_detector , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_visualisation , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_version_win32 , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_version , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_model_diagnostics , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_interactive - calibration , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_img_hash , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_annotation , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_xphoto , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_ximgproc , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_xfeatures2d , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_wechat_qrcode , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_videostab , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_videoio , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_video , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_tracking , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_text , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_img_hash . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_model_diagnostics . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_annotation . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_version_win32 . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_interactive - calibration . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_superres , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_structured_light , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_stitching , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_img_hash , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_waldboost_detector . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_visualisation . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_stereo , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_version . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_shape , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_saliency , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_rgbd , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_reg , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_rapid , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_quality , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_structured_light . vcxproj "" - - failed . <number> > done building project "" opencv_test_wechat_qrcode . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_photo , configuration : release x64 - - - - - - <number> > done building project "" opencv_test_text . vcxproj "" - - failed . <number> > done building project "" opencv_test_superres . vcxproj "" - - failed . <number> > done building project "" opencv_test_videostab . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_xfeatures2d . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_phase_unwrapping , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_stereo . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_optflow , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_ximgproc . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_shape . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_objdetect , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_saliency . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_reg . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_xphoto . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_rapid . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_stitching . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_rgbd . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_video . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_ml , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_mcc , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_quality . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_tracking . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_line_descriptor , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_videoio . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_photo . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_intensity_transform , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_phase_unwrapping . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_optflow . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_imgproc , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_imgcodecs , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_highgui , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_objdetect . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_gapi , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_fuzzy , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_img_hash . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_flann , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_features2d , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_face , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_dnn_superres , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_dnn , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_cudawarping , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_mcc . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_ml . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_line_descriptor . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudastereo , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_intensity_transform . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudaoptflow , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_cudaobjdetect , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_highgui . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_fuzzy . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_imgcodecs . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudalegacy , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_cudaimgproc , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_flann . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_face . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_dnn_superres . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudawarping . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_dnn . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudafilters , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_cudafeatures2d , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_features2d . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudacodec , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_cudabgsegm , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudastereo . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudaoptflow . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_cudaarithm , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudaobjdetect . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_core , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_calib3d , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_test_bioinspired , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_imgproc . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_bgsegm , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudaimgproc . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_test_aruco , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_xphoto , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_ximgproc , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudafilters . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_gapi . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudafeatures2d . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_xfeatures2d , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudalegacy . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_wechat_qrcode , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudabgsegm . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudacodec . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_videoio , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_video , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_tracking , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_cudaarithm . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_superres , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_bgsegm . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_stitching , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_stereo , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_bioinspired . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_rgbd , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_xphoto . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_reg , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_aruco . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_calib3d . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_ximgproc . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_wechat_qrcode . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_videoio . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_photo , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_optflow , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_xfeatures2d . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_tracking . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_objdetect , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_line_descriptor , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_video . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_superres . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_imgproc , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_stereo . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_stitching . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_rgbd . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_imgcodecs , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_gapi , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_features2d , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_reg . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_dnn_superres , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_dnn , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_optflow . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_photo . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudawarping , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_cudastereo , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_test_core . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudaoptflow , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_cudaobjdetect , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_line_descriptor . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_objdetect . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudalegacy , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_imgcodecs . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_gapi . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudaimgproc , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_dnn_superres . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_features2d . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_dnn . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudafilters , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_cudafeatures2d , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudawarping . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudastereo . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudacodec , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudaobjdetect . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudabgsegm , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_imgproc . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_cudaarithm , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_core , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudalegacy . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_perf_calib3d , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_bioinspired , configuration : release x64 - - - - - - <number> > - - - - - - build started : project : opencv_perf_aruco , configuration : release x64 - - - - - - <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudaimgproc . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudafilters . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudafeatures2d . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudacodec . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudabgsegm . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_cudaarithm . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_aruco . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_bioinspired . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_calib3d . vcxproj "" - - failed . <number> > link : fatal error lnk1181 : cannot open input file ' . <repeated> \ \ . <repeated> \ \ lib \ \ release \ \ opencv_world480 . lib ' <number> > done building project "" opencv_perf_core . vcxproj "" - - failed . = = = = = = = = = = build succeeded , <number> failed , <number> up - to - date , <number> skipped = = = = = = = = = = = = = = = = = = = = build started at <time> and took <number> seconds = = = = = = = = = = ` ` ` # # # steps to reproduce ! [ image ] ( <url> ! [ image ] ( <url> ! [ image ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"c + + version of opencv <number> . <number> returns wrong matrix size for onnx model # # # system information opencv <number> . <number> operating system : centos compiler : gcc11 opencv - python - rolling - <number> . <number> windows <number> # # # detailed description i am loading the same model with opencv python ok , but get the wrong matrix size for the inference result in the compiled c + + version . netron confirms the output nodes to be <number> and <number> with sizes 1 x8400x5 and 1 x8400x4 . the python version of opencv does return this matrix size , while the c + + version returns [ <number> x <number> ] depth <number> channels ( ) <number> type ( ) <number> . the compile machine is offline and i provided the correct version of ade by hand . other dnn models load and work fine . find the model here : <url> # # # steps to reproduce compile opencv <number> offline using the following configuartion : ` ` ` cmake3 - dcmake_build_type = debug - dcmake_install_prefix <annoyed> build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - default - debug - install verbose = <number> - dopencv_extra_modules_path = ~ / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / - dcpu_baseline = avx2 - denable_omit_frame_pointer = off - dbuild_tests = off - dbuild_perf_tests = off - dbuild_examples = off - dbuild_opencv_apps = off ~ / local / frameworks / opencv / <number> . 0 _trial / source / lib - - ocv_init_download : opencv source tree is not fetched as git repository . 3 rdparty resources will be downloaded from github . com by default . - - detected processor : x86_64 python <number> . <number> - - looking for ccache - not found cleaning internal cached variable : zlib_library cleaning internal cached variable : zlib_include_dir - - could not find zlib ( missing : zlib_library zlib_include_dir ) ( required is at least version "" <number> . <number> "" ) cleaning internal cached variable : jpeg_library cleaning internal cached variable : jpeg_include_dir - - could not find jpeg ( missing : jpeg_library jpeg_include_dir ) - - libjpeg - turbo : version = <number> . <number> , build = opencv - <number> . <number> - libjpeg - turbo - debug cleaning internal cached variable : tiff_library cleaning internal cached variable : tiff_include_dir - - could not find tiff ( missing : tiff_library tiff_include_dir ) cleaning internal cached variable : webp_library cleaning internal cached variable : webp_include_dir - - could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) . openjpeg will be built from sources - - openjpeg : version = <number> . <number> , build = opencv - <number> . <number> - openjp2 - <number> . <number> - debug - - openjpeg libraries will be built from sources : libopenjp2 ( version "" <number> . <number> "" ) cleaning internal cached variable : png_library cleaning internal cached variable : png_include_dir - - could not find png ( missing : png_library png_png_include_dir ) - - ippicv : downloading ippicv_2021 . 8 _lnx_intel64_20230330_general . tgz from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at cmake / opencvdownload . cmake : <number> ( message ) : ippicv : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : 3 rdparty / ippicv / ippicv . cmake : <number> ( ocv_download ) cmake / opencvfindipp . cmake : <number> ( download_ippicv ) cmake / opencvfindlibsperf . cmake : <number> ( include ) cmakelists . txt : <number> ( include ) - - could not find openblas include . turning openblas_found off - - could not find openblas lib . turning openblas_found off - - could not find atlas ( missing : atlas_cblas_include_dir atlas_clapack_include_dir atlas_cblas_library atlas_blas_library atlas_lapack_library ) - - could not find blas ( missing : blas_libraries ) - - lapack requires blas - - a library with lapack api not found . please specify library location . - - could not find java ( missing : java_jar_executable java_javac_executable java_javah_executable java_javadoc_executable ) ( found version "" <number> . 0 _372 "" ) - - could not find jni ( missing : java_include_path java_include_path2 java_awt_include_path ) - - vtk is not found . please set - dvtk_dir in cmake to vtk build directory , or to vtk install subdirectory with vtkconfig . cmake file - - checking for module ' gtk + - <number> ' - - no package ' gtk + - <number> ' found - - checking for module ' gtk + - <number> ' - - no package ' gtk + - <number> ' found - - checking for module ' gthread - <number> >= <number> ' - - no package ' gthread - <number> ' found - - checking for modules ' libavcodec ; libavformat ; libavutil ; libswscale ' - - no package ' libavcodec ' found - - no package ' libavformat ' found - - no package ' libavutil ' found - - no package ' libswscale ' found - - ffmpeg is disabled . required libraries : libavcodec ; libavformat ; libavutil ; libswscale . missing libraries : libavcodec ; libavformat ; libavutil ; libswscale - - checking for module ' gstreamer - base - <number> ' - - no package ' gstreamer - base - <number> ' found - - checking for module ' gstreamer - app - <number> ' - - no package ' gstreamer - app - <number> ' found - - checking for module ' gstreamer - riff - <number> ' - - no package ' gstreamer - riff - <number> ' found - - checking for module ' gstreamer - pbutils - <number> ' - - no package ' gstreamer - pbutils - <number> ' found - - checking for module ' gstreamer - video - <number> ' - - no package ' gstreamer - video - <number> ' found - - checking for module ' gstreamer - audio - <number> ' - - no package ' gstreamer - audio - <number> ' found - - checking for module ' libdc1394 - <number> ' - - no package ' libdc1394 - <number> ' found - - module opencv_alphamat disabled because the following dependencies are not found : eigen - - checking for module ' freetype2 ' - - no package ' freetype2 ' found - - checking for module ' harfbuzz ' - - no package ' harfbuzz ' found - - freetype2 : no - - harfbuzz : no - - could not find hdf5 ( missing : hdf5_libraries hdf5_include_dirs ) ( found version "" "" ) - - julia not found . not compiling julia bindings . - - module opencv_ovis disabled because ogre3d was not found - - no preference for use of exported gflags cmake configuration set , and no hints for include / library directories provided . defaulting to preferring an installed / exported gflags cmake configuration if available . - - failed to find installed gflags cmake configuration , searching for gflags build directories exported with cmake . - - failed to find gflags - failed to find an installed / exported cmake configuration for gflags , will perform search for installed gflags components . - - failed to find gflags - could not find gflags include directory , set gflags_include_dir to directory containing gflags / gflags . h - - failed to find glog - could not find glog include directory , set glog_include_dir to directory containing glog / logging . h - - module opencv_sfm disabled because the following dependencies are not found : eigen glog / gflags - - checking for module ' tesseract ' - - no package ' tesseract ' found - - tesseract : no - - allocator metrics storage type : ' long long ' - - excluding from source files list : modules / imgproc / src / imgwarp . lasx . cpp - - excluding from source files list : modules / imgproc / src / resize . lasx . cpp - - registering hook ' init_module_sources_opencv_dnn ' : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / modules / dnn / cmake / hooks / init_module_sources_opencv_dnn . cmake - - opencv_dnn : filter out cuda4dnn source code - - excluding from source files list : <build> / modules / dnn / layers / layers_common . rvv . cpp - - excluding from source files list : <build> / modules / dnn / layers / layers_common . lasx . cpp - - excluding from source files list : <build> / modules / dnn / int8layers / layers_common . lasx . cpp - - excluding from source files list : <build> / modules / dnn / layers / cpu_kernels / conv_depthwise . rvv . cpp - - excluding from source files list : <build> / modules / dnn / layers / cpu_kernels / conv_depthwise . lasx . cpp - - imgcodecs : openexr codec is disabled in runtime . details : <url> - - highgui : using builtin backend : none - - rgbd : eigen support is disabled . eigen is required for posegraph optimization - - wechat_qrcode : downloading detect . caffemodel from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : wechat_qrcode : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( ocv_download ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( message ) : wechatqrcode : can not get detect caffemodel file for wechat qrcode . - - wechat_qrcode : downloading detect . prototxt from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : wechat_qrcode : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( ocv_download ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( message ) : wechatqrcode : can not get detect prototxt file for wechat qrcode . - - wechat_qrcode : downloading sr . caffemodel from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : wechat_qrcode : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( ocv_download ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( message ) : wechatqrcode : can not get sr caffemodel file for wechat qrcode . - - wechat_qrcode : downloading sr . prototxt from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : wechat_qrcode : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( ocv_download ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / wechat_qrcode / cmakelists . txt : <number> ( message ) : wechatqrcode : can not get sr prototxt file for wechat qrcode . - - xfeatures2d / boostdesc : downloading boostdesc_bgm . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_bgm_bi . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_bgm_hd . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_binboost_064 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_binboost_128 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_binboost_256 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / boostdesc : downloading boostdesc_lbgm . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / boostdesc : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_boostdesc . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_boost_descriptors ) - - xfeatures2d / vgg : downloading vgg_generated_48 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / vgg : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_vgg . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_vgg_descriptors ) - - xfeatures2d / vgg : downloading vgg_generated_64 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / vgg : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_vgg . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_vgg_descriptors ) - - xfeatures2d / vgg : downloading vgg_generated_80 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / vgg : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_vgg . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_vgg_descriptors ) - - xfeatures2d / vgg : downloading vgg_generated_120 . i from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : xfeatures2d / vgg : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmake / download_vgg . cmake : <number> ( ocv_download ) / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( download_vgg_descriptors ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( message ) : features2d : boost descriptor implementation is not available due to missing data ( download failed : <url> cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / xfeatures2d / cmakelists . txt : <number> ( message ) : features2d : vgg descriptor implementation is not available due to missing data ( download failed : <url> - - data : downloading face_landmark_model . dat from <url> - - try <number> failed - - = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = could not download files from the internet . please check the internet access on this host . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / cmake / opencvdownload . cmake : <number> ( message ) : data : download failed : <number> ; "" could not resolve host name "" for details please refer to the download log file : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug / cmakedownloadlog . txt call stack ( most recent call first ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / face / cmakelists . txt : <number> ( ocv_download ) cmake warning at / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules / face / cmakelists . txt : <number> ( message ) : face : can not get model file for face alignment . - - found ' misc ' python modules from / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / modules / python / package / extra_modules - - found ' mat_wrapper ; utils ' python modules from / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / modules / core / misc / python / package - - found ' gapi ' python modules from / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / modules / gapi / misc / python / package - - - - general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - version control : unknown - - - - extra modules : - - location ( extra ) : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / contrib / modules - - version control ( extra ) : unknown - - - - platform : - - timestamp : <number> - <number> - 1 2 t <time> z - - host : linux <date> - <number> . el7 . elrepo . x86_64 x86_64 - - cmake : <number> . <number> - - cmake generator : unix makefiles - - cmake build tool : / usr / bin / gmake - - configuration : debug - - - - cpu / hw features : - - baseline : sse sse2 sse3 ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 - - requested : avx2 - - dispatched code generation : avx512_skx - - requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx - - avx512_skx ( <number> files ) : + avx_512f avx512_common avx512_skx - - - - c / c + + : - - built as dynamic libs ? : yes - - c + + standard : <number> - - c + + compiler : / ovde_plugins / gcc11 / linux / bin / g + + ( ver <number> . <number> ) - - c + + flags ( release ) : - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fno - omit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - mssse3 - msse4 . <number> - mpopcnt - msse4 . <number> - mf16c - mfma - mavx - mavx2 - fvisibility = hidden - fvisibility - inlines - hidden - o2 - dndebug - dndebug - - c + + flags ( debug ) : - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fno - omit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - mssse3 - msse4 . <number> - mpopcnt - msse4 . <number> - mf16c - mfma - mavx - mavx2 - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug - - c compiler : / ovde_plugins / gcc11 / linux / bin / gcc - - c flags ( release ) : - fsigned - char - w - wall - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fno - omit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - mssse3 - msse4 . <number> - mpopcnt - msse4 . <number> - mf16c - mfma - mavx - mavx2 - fvisibility = hidden - o2 - dndebug - dndebug - - c flags ( debug ) : - fsigned - char - w - wall - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fno - omit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - mssse3 - msse4 . <number> - mpopcnt - msse4 . <number> - mf16c - mfma - mavx - mavx2 - fvisibility = hidden - g - o0 - ddebug - d_debug - - linker flags ( release ) : - wl , - - gc - sections - wl , - - as - needed - wl , - - no - undefined - - linker flags ( debug ) : - wl , - - gc - sections - wl , - - as - needed - wl , - - no - undefined - - ccache : no - - precompiled headers : no - - extra dependencies : dl m pthread rt - - 3 rdparty dependencies : - - - - opencv modules : - - to be built : aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto - - disabled : world - - disabled by dependency : - - - unavailable : alphamat cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv freetype hdf java julia matlab ovis python2 python3 sfm ts viz - - applications : - - - documentation : no - - non - free algorithms : no - - - - gui : none - - gtk + : no - - vtk support : no - - - - media i / <surprise> - - zlib : zlib ( ver <date> ) - - jpeg : libjpeg - turbo ( ver <number> . <number> - <number> ) - - webp : build ( ver encoder : 0x0 2 0 f ) - - png : build ( ver <date> ) - - tiff : build ( ver <number> - <number> . <number> ) - - jpeg <number> : build ( ver <number> . <number> ) - - openexr : build ( ver <number> . <number> ) - - hdr : yes - - sunraster : yes - - pxm : yes - - pfm : yes - - - - video i / <surprise> - - dc1394 : no - - ffmpeg : no - - avcodec : no - - avformat : no - - avutil : no - - swscale : no - - avresample : no - - gstreamer : no - - v4l / v4l2 : yes ( linux / videodev2 . h ) - - - - parallel framework : pthreads - - - - trace : yes ( with intel itt ) - - - - other third - party libraries : - - va : yes - - lapack : no - - eigen : no - - custom hal : no - - protobuf : build ( <number> . <number> ) - - flatbuffers : builtin / 3 rdparty ( <number> . <number> ) - - - - opencl : yes ( intelva ) - - include path : / net / subnet - homes / development / user / ukoehler / local / frameworks / opencv / <number> . 0 _trial / source / lib / 3 rdparty / include / opencl / <number> - - link libraries : dynamic load - - - - python ( for build ) : / usr / bin / python2 . <number> - - - - java : - - ant : no - - java : no - - jni : no - - java wrappers : no - - java tests : no - - - - install to : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - default - debug - install - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - configuring done - - generating done - - build files have been written to : / build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - pthreads - openmp - debug ` ` ` compile the test using the following configuration : ` ` ` cmake3 - dcmake_build_type = debug - dcmake_install_prefix <annoyed> build / uk / frameworks / opencv - <number> . <number> -ov - avx2 - default - debug - install verbose = <number> ~ / local / frameworks / opencv / <number> . 0 _trial / source / ovtest ` ` ` the code to load and execute the model ` ` ` std : : string modelpath = r "" ( / build / uk / dnnmodels / selftrained / yolo - nas / yolo_nas_m_drone - sim . onnx ) "" ; / / load the network cv : : dnn : : net net = cv : : dnn : : readnetfromonnx ( modelpath ) ; net . setpreferablebackend ( cv : : dnn : : dnn_backend_opencv ) ; net . setpreferabletarget ( cv : : dnn : : dnn_target_cpu ) ; std : : vector < cv : : mat > outs ; std : : vector < std : : string > outputnames = dnnhelper : : getoutputsnames ( net ) ; for ( int i = <number> ; i < outputnames . size ( ); i + + ) { std : : cout < < "" dnnhelper : : getoutputsnames ( net ) "" < < i < < "" "" < < dnnhelper : : getoutputsnames ( net ) . at ( i ) < < std : : endl ; } net . forward ( outs , dnnhelper : : getoutputsnames ( net ) ); std : : cout < < "" outs . size ( ) "" < < outs . size ( ) < < std : : endl ; for ( size_t i = <number> ; i < outs . size ( ); + + i ) { std : : cout < < i < < "" "" < < outs [ i ] . size ( ) < < "" depth "" < < outs [ i ] . depth ( ) < < "" channels ( ) "" < < outs [ i ] . channels ( ) < < "" type ( ) "" < < outs [ i ] . type ( ) < < std : : endl ; } ` ` ` the output ` ` ` [ debug : <number> <user> . <number> ] global system . cpp : <number> restorefpdenormalsstate core : restore fp mxcsr flags = 0x0 0 0 0 1 fb0 [ debug : <number> <user> . <number> ] global system . cpp : <number> restorefpdenormalsstate core : restore fp mxcsr flags = 0x0 0 0 0 1 fb0 . <repeated> dnnhelper : : getoutputsnames ( net ) <number> <number> dnnhelper : : getoutputsnames ( net ) <number> <number> [ debug : <number> <user> . <number> ] global system . cpp : <number> setfpdenormalsignorehint core : update fp mxcsr flags = 0x0 0 0 0 9 ff0 [ debug : <number> <user> . <number> ] global system . cpp : <number> restorefpdenormalsstate core fp mxcsr flags = 0x0 0 0 0 9 ff0 . <repeated> outs . size ( ) <number> <number> [ <number> x <number> ] depth <number> channels ( ) <number> type ( ) <number> <number> [ <number> x <number> ] depth <number> channels ( ) <number> type ( ) <number> ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"cmake : do not export external target twice fixup # <number> error message on ` - dwith_openvino = on - dbuild_shared_libs = off ` : ` ` ` cmake error : install ( export "" opencvmodules "" . <repeated> ) includes target "" ocv . 3 rdparty . openvino "" more than once in the export set . cmake error in cmakelists . txt target "" ocv . 3 rdparty . openvino "" more than once . ` ` `",0
opencv/opencv,"fix stubs overload presence check every function has at least <number> overload ( the * main <emphasis> * one ) , so before the fix ` check_overload_presence ` always produced ` true ` as a coincide , removes unnecessary ` import typing ` in several modules stubs e . g . ` cv2 / gapi / core / cpu / __init__ . pyi ` # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"test_torch_nets . fastneuralstyle_accuracy / <number> randomly fails with cuda # # # system information platform : ubuntu + cuda full test name : test_torch_nets . fastneuralstyle_accuracy / <number> , where getparam ( ) = cuda / cuda_fp16 # # # detailed description ` ` ` [ run ] test_torch_nets . fastneuralstyle_accuracy / <number> , where getparam ( ) = cuda / cuda_fp16 / home / ci / opencv / modules / dnn / test / test_common . impl . hpp : <number> : failure expected : ( norminf ) <= ( linf ) , actual vs <number> | ref | = <number> [ failed ] test_torch_nets . fastneuralstyle_accuracy / <number> , where getparam ( ) = cuda / cuda_fp16 ( <number> ms ) ` ` ` # # # steps to reproduce <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"python type stubs does not handle cuda : : gpumat and cuda : : stream # # # system information python <number> + platform : cuda # # # detailed description ` ` ` ' failed to resolve "" cv2 . cudacodec "" namespace against "" cv2 "" . errors : [ \ \ ' failed to resolve "" cv2 . cudacodec . createvideowriter "" function against "" cv2 "" . errors : [ <number> <sad> failed to resolve "" stream "" argument : failed to resolve "" stream "" exposed as "" stream "" , [ <number> <sad> failed to resolve "" stream "" argument : failed to resolve "" stream "" exposed as "" stream "" \ \ ' , \ \ ' failed to resolve "" cv2 . cudacodec . encodeqp "" class against "" cv2 "" . errors : [ \ \ \ \ \ \ ' failed to resolve "" qpinterp "" property \ \ \ \ \ \ ' , \ \ \ \ \ \ ' failed to resolve "" qpinterb "" property \ \ \ \ \ \ ' , \ \ \ \ \ \ ' failed to resolve "" qpintra "" property \ \ \ \ \ \ ' ] \ \ ' , \ \ ' failed to resolve "" cv2 . cudacodec . encoderparams "" class against "" cv2 "" . errors : [ \ \ \ \ \ \ ' failed to resolve "" targetquality "" property \ \ \ \ \ \ ' ] \ \ ' , \ \ ' failed to resolve "" cv2 . cudacodec . videoreader "" class against "" cv2 "" . errors : [ \ \ \ \ \ \ ' failed to resolve "" cv2 . cudacodec . videoreader . nextframe "" function against "" cv2 "" . errors : [ <number> <sad> failed to resolve "" frame "" argument : failed to resolve one of "" gpumat | none "" items . errors : [ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' failed to resolve "" gpumat "" exposed as "" gpumat "" \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' ] , [ <number> <sad> failed to resolve "" stream "" argument : failed to resolve "" stream "" exposed as "" stream "" , [ <number> <sad> failed to resolve return type : failed to resolve one of "" tuple [ bool , gpumat ] "" items . errors : [ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' failed to resolve "" gpumat "" exposed as "" gpumat "" \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' ] \ \ \ \ \ \ ' , \ \ \ \ \ \ ' failed to resolve "" cv2 . cudacodec . videoreader . grab "" function against "" cv2 "" . errors : [ <number> <sad> failed to resolve "" stream "" argument : failed to resolve "" stream "" exposed as "" stream "" \ \ \ \ \ \ ' , \ \ \ \ \ \ ' failed to resolve "" cv2 . cudacodec . videoreader . retrieve "" function against "" cv2 "" . errors : [ <number> <sad> failed to resolve "" frame "" argument : failed to resolve one of "" gpumat | none "" items . errors : [ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' failed to resolve "" gpumat "" exposed as "" gpumat "" \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' ] , [ <number> <sad> failed to resolve return type : failed to resolve one of "" tuple [ bool , gpumat ] "" items . errors : [ \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' failed to resolve "" gpumat "" exposed as "" gpumat "" \ \ \ \ \ \ \ \ \ \ \ \ \ \ ' ] \ \ \ \ \ \ ' ] \ \ ' ] ' ] ` ` ` # # # steps to reproduce cmake options that work : ` cmake - dbuild_opencv_cudacodec = off - dbuild_opencv_cudaoptflow = off - dpython3_executable = ` which python3 . <number> ` - dpython_default_executable = ` which python3 . <number> ` - dwith_cuda = on - dcuda_arch_bin = <number> - dopencv_extra_modules_path = . <repeated> / opencv_contrib / modules / . <repeated> / opencv - master ` cmake options that doe not : ` cmake - dbuild_opencv_cudaoptflow = off - dpython3_executable = ` which python3 . <number> ` - dpython_default_executable = ` which python3 . <number> ` - dwith_cuda = on - dcuda_arch_bin = <number> - dopencv_extra_modules_path = . <repeated> / opencv_contrib / modules / . <repeated> / opencv - master ` cuda definitions for python # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"depthwise convolution layer with 5 x5 kernel much slower than <number> . <number> # # # system information macos m2 opencv <number> . <number> and <number> . <number> python # # # detailed description mediapipe palm detection model from opencv zoo show the inference time is double than the <number> . <number> version . reference : <url> model file : <url> layer by layer performance test results [ palm_4 . <number> . txt ] ( <url> depth wise convolution inference time is <number> times than <number> . <number> version . # # # steps to reproduce just run benchmark in opencv zoo # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"videocapture msmf plugin will not open using camera / device index . # # # system information opencv version : at least from <number> . <number> to master operating system / platform : windows <number> / <number> compiler & compiler version : visual studio <number> # # # detailed description it looks like unnecessary ` nullptr ` check of optional argument <url> below linked fragment with my comments . by commenting out pointed ` filename ` check i was able to open camera on my local machine . ` ` ` cpp static cvresult cv_api_call cv_capture_open_with_params ( const char * filename , int camera_index , int * params , unsigned n_params , cv_out cvplugincapture * handle ) { if ( handle ) return cv_error_fail ; * handle = null ; if ( ! filename ) / / < - - - filename is optional , see below return cv_error_fail ; capturet * cap = <number> ; try { cv : : videocaptureparameters parameters ( params , n_params ) ; cap = new capturet ( ); bool res ; if ( filename ) / / if filename is true then we use res = cap - > open ( std : : string ( filename ) , ¶ meters ) ; / / else / / however if it ' s nullptr res = cap - > open ( camera_index , ¶ meters ) ; / / then we use index to search device if ( res ) { * handle = ( cvplugincapture ) cap ; return cv_error_ok ; } } catch ( const std : : exception & e ) { cv_log_warning ( null , "" msmf : exception is raised : "" < < e . what ( )); } catch ( . <repeated> ) { cv_log_warning ( null , "" msmf c + + exception is raised "" ); } if ( cap ) delete cap ; return cv_error_fail ; } ` ` ` # # # steps to reproduce open video capture from camera ( for example , laptop ' s built - in camera ) using msmf backend ` cv : : videocapture vc ( <number> , cv : : cap_msmf ) ; ` built as a plugin . it ' s also very likely , that it ' s the same issue as <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"opencv solution for visual studio <number> is not building opencv_core . lib when avx2 is specified in ( cpu_dispatch = sse4_1 ; sse4_2 ; avx ; fp16 ; avx2 ) in x86 build # # # system information opencv solution for visual studio <number> is not building opencv_core . lib when avx2 is specified in ( cpu_dispatch = sse4_1 ; sse4_2 ; avx ; fp16 ; avx2 ) in x86 build opencv version : <number> . <number> operating system / platform : windows <number> compiler & compiler version : visual studio <number> # # # detailed description by default x64 cmake configuration for visual studio <number> uses : cpu_baseline = sse3 ; cpu_dispatch = sse4_1 ; sse4_2 ; avx ; fp16 ; avx2 and x86 configuration uses : cpu_baseline = sse2 ; cpu_dispatch = sse4_1 ; sse4_2 ; avx ; fp16 when i set the same flags in build for x86 as set in x64 build , building opencv_core module ends with failure : logs from building opencv_core module : build started . <repeated> <number> > - - - - - - build started : project : zero_check , configuration : release win32 - - - - - - <number> > checking build system <number> > - - - - - - build started : project : ippiw , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : ittnotify , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : opencv_core_sse4_1 , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : opencv_core_sse4_2 , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : opencv_core_avx , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : opencv_core_avx2 , configuration : release win32 - - - - - - <number> > - - - - - - build started : project : zlib , configuration : release win32 - - - - - - <number> > building custom rule g <annoyed> opencv - <number> . <number> / modules / core / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / build_x86_avx2 / 3 rdparty / ippicv / ippicv_win / iw / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / 3 rdparty / ittnotify / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / modules / core / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / modules / core / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / 3 rdparty / zlib / cmakelists . txt <number> > building custom rule g <annoyed> opencv - <number> . <number> / modules / core / cmakelists . txt <number> > iw_core . c <number> > mathfuncs_core . avx . cpp <number> > arithm . sse4_1 . cpp <number> > iw_image . c <number> > iw_image_color_convert_all . c <number> > iw_image_color_convert_rgbs . c <number> > ittnotify_static . c <number> > iw_image_filter_bilateral . c <number> > iw_image_filter_box . c <number> > matmul . sse4_1 . cpp <number> > jitprofiling . c <number> > iw_image_filter_canny . c <number> > iw_image_filter_gaussian . c <number> > stat . sse4_2 . cpp <number> > mathfuncs_core . avx2 . cpp <number> > stat . avx2 . cpp <number> > adler32 . c <number> > compress . c <number> > crc32 . c <number> > deflate . c <number> > arithm . avx2 . cpp <number> > convert . avx2 . cpp <number> > gzclose . c <number> > gzlib . c <number> > gzread . c <number> > convert_scale . avx2 . cpp <number> > count_non_zero . avx2 . cpp <number> > has_non_zero . avx2 . cpp <number> > matmul . avx2 . cpp <number> > gzwrite . c <number> > iw_image_filter_general . c <number> > iw_image_filter_laplacian . c <number> > iw_image_filter_morphology . c <number> > inflate . c <number> > infback . c <number> > inftrees . c <number> > iw_image_filter_scharr . c <number> > iw_image_filter_sobel . c <number> > iw_image_op_copy . c <number> > iw_image_op_copy_channel . c <number> > iw_image_op_copy_make_border . c <number> > iw_image_op_copy_merge . c <number> > iw_image_op_copy_split . c <number> > iw_image_op_scale . c <number> > inffast . c <number> > trees . c <number> > uncompr . c <number> > zutil . c <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ stat . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ stat . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ stat . avx2 . cpp ) <number> > mean . avx2 . cpp <number> > iw_image_op_set . c <number> > iw_image_op_set_channel . c <number> > iw_image_op_swap_channels . c <number> > iw_image_transform_mirror . c <number> > iw_image_transform_resize . c <number> > iw_image_transform_rotate . c <number> > iw_image_transform_warpaffine . c <number> > iw_own . c <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ arithm . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ arithm . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ arithm . avx2 . cpp ) <number> > merge . avx2 . cpp <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mathfuncs_core . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mathfuncs_core . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mathfuncs_core . avx2 . cpp ) <number> > split . avx2 . cpp <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ count_non_zero . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ count_non_zero . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ count_non_zero . avx2 . cpp ) <number> > sum . avx2 . cpp <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ matmul . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ matmul . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ matmul . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert_scale . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert_scale . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ convert_scale . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ has_non_zero . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ has_non_zero . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ has_non_zero . avx2 . cpp ) <number> > ittnotify . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ 3 rdparty \ \ lib \ \ release \ \ ittnotify . lib <number> > opencv_core_sse4_2 . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ opencv_core_sse4_2 . dir \ \ release \ \ opencv_core_sse4_2 . lib <number> > ippiw . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ 3 rdparty \ \ lib \ \ release \ \ ippiw . lib <number> > zlib . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ 3 rdparty \ \ lib \ \ release \ \ zlib . lib <number> > opencv_core_avx . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ opencv_core_avx . dir \ \ release \ \ opencv_core_avx . lib <number> > opencv_core_sse4_1 . vcxproj - > g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ opencv_core_sse4_1 . dir \ \ release \ \ opencv_core_sse4_1 . lib <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mean . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mean . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ mean . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ merge . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ merge . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ merge . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ split . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ split . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ split . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> error c3861 : ' _mm256_extract_epi64 ' : identifier not found ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ sum . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_avx . hpp ( <number> <sad> message : see reference to function template instantiation ' int64 cv : : hal_avx2 : : _v256_extract_epi64 < <number> > ( const __m256i & ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ sum . avx2 . cpp ) <number> > g :\\ opencv - <number> . <number> \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin . hpp ( <number> <sad> message : see reference to function template instantiation ' uint64 cv : : hal_avx2 : : v_extract_n < <number> > ( cv : : hal_avx2 : : v_uint64x4 ) ' being compiled ( compiling source file g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ sum . avx2 . cpp ) <number> > done building project "" opencv_core_avx2 . vcxproj "" - - failed . <number> > - - - - - - build started : project : opencv_core , configuration : release win32 - - - - - - <number> > processing opencl kernels ( core ) <number> > - - g <annoyed> opencv - <number> . <number> / build_x86_avx2 / modules / core / opencl_kernels_core . hpp contains the same content <number> > building custom rule g <annoyed> opencv - <number> . <number> / modules / core / cmakelists . txt <number> > cmake_pch . cxx <number> > algorithm . cpp <number> > arithm . cpp <number> > arithm . dispatch . cpp <number> > array . cpp <number> > async . cpp <number> > batch_distance . cpp <number> > bindings_utils . cpp <number> > buffer_area . cpp <number> > channels . cpp <number> > check . cpp <number> > command_line_parser . cpp <number> > conjugate_gradient . cpp <number> > convert . dispatch . cpp <number> > convert_c . cpp <number> > convert_scale . dispatch . cpp <number> > copy . cpp <number> > count_non_zero . dispatch . cpp <number> > cuda_gpu_mat . cpp <number> > cuda_gpu_mat_nd . cpp <number> > cuda_host_mem . cpp <number> > cuda_info . cpp <number> > cuda_stream . cpp <number> > datastructs . cpp <number> > directx . cpp <number> > downhill_simplex . cpp <number> > dxt . cpp <number> > gl_core_3_1 . cpp <number> > glob . cpp <number> > hal_internal . cpp <number> > has_non_zero . dispatch . cpp <number> > kmeans . cpp <number> > lapack . cpp <number> > lda . cpp <number> > logger . cpp <number> > lpsolver . cpp <number> > lut . cpp <number> > mathfuncs . cpp <number> > mathfuncs_core . dispatch . cpp <number> > matmul . dispatch . cpp <number> > matrix . cpp <number> > matrix_c . cpp <number> > matrix_decomp . cpp <number> > matrix_expressions . cpp <number> > matrix_iterator . cpp <number> > matrix_operations . cpp <number> > matrix_sparse . cpp <number> > matrix_transform . cpp <number> > matrix_wrap . cpp <number> > mean . dispatch . cpp <number> > merge . dispatch . cpp <number> > minmax . cpp <number> > norm . cpp <number> > ocl . cpp <number> > opencl_clblas . cpp <number> > opencl_clfft . cpp <number> > opencl_core . cpp <number> > opengl . cpp <number> > out . cpp <number> > ovx . cpp <number> > parallel_openmp . cpp <number> > parallel_tbb . cpp <number> > parallel_impl . cpp <number> > pca . cpp <number> > persistence . cpp <number> > persistence_base64_encoding . cpp <number> > persistence_json . cpp <number> > persistence_types . cpp <number> > persistence_xml . cpp <number> > persistence_yml . cpp <number> > rand . cpp <number> > softfloat . cpp <number> > split . dispatch . cpp <number> > stat . dispatch . cpp <number> > stat_c . cpp <number> > stl . cpp <number> > sum . dispatch . cpp <number> > system . cpp <number> > tables . cpp <number> > trace . cpp <number> > types . cpp <number> > umatrix . cpp <number> > datafile . cpp <number> > filesystem . cpp <number> > logtagconfigparser . cpp <number> > logtagmanager . cpp <number> > samples . cpp <number> > va_intel . cpp <number> > opencl_kernels_core . cpp <number> > alloc . cpp <number> > parallel . cpp <number> > parallel . cpp <number> > link : fatal error lnk1181 : cannot open input file ' g :\\ opencv - <number> . <number> \ \ build_x86_avx2 \ \ modules \ \ core \ \ opencv_core_avx2 . dir \ \ release \ \ mathfuncs_core . avx2 . obj ' <number> > done building project "" opencv_core . vcxproj "" - - failed . = = = = = = = = = = build succeeded , <number> failed , <number> up - to - date , <number> skipped = = = = = = = = = = # # # steps to reproduce <number> . set cmake configuration to visual studio <number> x86 <number> . set this flags cpu_baseline = sse3 ; cpu_dispatch = sse4_1 ; sse4_2 ; avx ; fp16 ; avx2 build_shared_libs = off <number> . build visual studio <number> solution <number> . open solution and build opencv_core module [ cmakecache . txt ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"python type stubs : class ` cv2 . error ` is missing properties # # # system information opencv python : opencv - python - headless = = <number> . <number> operating system / platform : <number> . <number> build <number> python version : <date> # # # detailed description class ` cv2 . error ` is currently typed as : ` ` ` py class error ( exception ) : . <repeated> ` ` ` but should be something like : ` ` ` py class error ( exception ) : code : int err : str file : str func : str line : int msg : str ` ` ` ( they are technically classvars , but would be none ) as inspected with : ` ` ` py > > > import cv2 > > > error_dir = dir ( cv2 . error ) > > > exception_dir = dir ( exception ) > > > [x for x in error_dir if x not in exception_dir ] [ ' __module__ ' , ' __weakref__ ' , ' code ' , ' err ' , ' file ' , ' func ' , ' line ' , ' msg ' ] > > > try : . <repeated> cv2 . videocapture ( ) . read ( "" a <elongated> "" ) . <repeated> except cv2 . error as error : . <repeated> print ( type ( error . code ) , type ( error . err ) , type ( error . file ) , type ( error . func ) , type ( error . line ) , type ( error . msg ) ) . <repeated> < class ' int ' > < class ' str ' > < class ' str ' > < class ' str ' > < class ' int ' > < class ' str ' > ` ` ` # # # steps to reproduce actual usage example # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"detection of multiple charuco boards with different ids in one image broken in <number> . <number> # # # system information opencv <number> . <number> ubuntu <number> python <number> . <number> # # # detailed description in opencv <number> . <number> you could separately detect two charuco boards visible in a single image : ` ` ` python dict_type = aruco . dict_6x6_250 squares = ( <number> , <number> ) square_length_mm = <number> marker_length_mm = <number> aruco_dict = aruco . getpredefineddictionary ( dict_type ) board0 = aruco . charucoboard ( squares , square_length_mm , marker_length_mm , aruco_dict , np . arange ( <number> ) ) # there are <number> white squares , provide <number> markers with ids [ <number> . <repeated> <number> ] detector = aruco . charucodetector ( board0 ) charuco_corners , charuco_ids , marker_corners , marker_ids = detector . detectboard ( image ) aruco . drawdetectedcornerscharuco ( image , charuco_corners , charuco_ids , ( <number> , <number> , <number> ) ) board1 = aruco . charucoboard ( squares , square_length_mm , marker_length_mm , aruco_dict , np . arange ( <number> ) + <number> ) # <number> markers with ids [ <number> . <repeated> <number> ] detector = aruco . charucodetector ( board1 ) charuco_corners , charuco_ids , marker_corners , marker_ids = detector . detectboard ( image ) aruco . drawdetectedcornerscharuco ( image , charuco_corners , charuco_ids , ( <number> , <number> , <number> ) ) ` ` ` [ screenshot from <number> - <number> - <number> <date> ] ( <url> in opencv <number> . <number> , the second ` detectboard ` does not return any charuco corners . ` charuco_corners ` and ` charuco_ids ` are both ` none ` . the markers are still returned , so it is not a detection issue . sample code and sample image ( see below ) return under <number> . <number> : ` ` ` running opencv <number> . <number> board0 charuco ids <number> . <repeated> <number> ( <number> ) marker ids <number> . <repeated> <number> ( <number> ) board1 charuco ids <number> . <repeated> <number> ( <number> ) marker ids <number> . <repeated> <number> ( <number> ) ` ` ` and same code under opencv <number> . <number> : ` ` ` running opencv <number> . <number> board0 charuco ids <number> . <repeated> <number> ( <number> ) marker ids <number> . <repeated> <number> ( <number> ) board1 no charuco ids marker ids <number> . <repeated> <number> ( <number> ) ` ` ` # # # steps to reproduce ` ` ` python import numpy as np import cv2 import cv2 . aruco as aruco if __name__ = = ' __main__ ' : # load image image = cv2 . imread ( ' image . png ' ) # generate aruco boards dict_type = aruco . dict_6x6_250 squares = ( <number> , <number> ) square_length_mm = <number> marker_length_mm = <number> aruco_dict = aruco . getpredefineddictionary ( dict_type ) board0 = aruco . charucoboard ( squares , square_length_mm , marker_length_mm , aruco_dict , np . arange ( <number> ) ) board1 = aruco . charucoboard ( squares , square_length_mm , marker_length_mm , aruco_dict , np . arange ( <number> ) + <number> ) print ( f ' running opencv { cv2 . __version__ } ' ) detector = aruco . charucodetector ( board0 ) charuco_corners , charuco_ids , marker_corners , marker_ids = detector . detectboard ( image ) print ( ' board0 ' ) if charuco_ids is none : print ( ' no charuco ids ' ) else : print ( f ' charuco ids { np . min ( charuco_ids ) } . <repeated> { np . max ( charuco_ids ) } ( { len ( charuco_ids ) } ) ' ) aruco . drawdetectedcornerscharuco ( image , charuco_corners , charuco_ids , ( <number> , <number> , <number> ) ) print ( f ' marker ids { np . min ( marker_ids ) } . <repeated> { np . max ( marker_ids ) } ( { len ( marker_ids ) } ) ' ) detector = aruco . charucodetector ( board1 ) charuco_corners , charuco_ids , marker_corners , marker_ids = detector . detectboard ( image ) print ( ' board1 ' ) if charuco_ids is none : print ( ' no charuco ids ' ) else charuco ids { np . min ( charuco_ids ) } . <repeated> { np . max ( charuco_ids ) } ( { len ( charuco_ids ) } ) ' ) aruco . drawdetectedcornerscharuco ( image , charuco_corners , charuco_ids , ( <number> , <number> , <number> ) ) print ( f ' marker ids { np . min ( marker_ids ) } . <repeated> { np . max ( marker_ids ) } ( { len ( marker_ids ) } ) ' ) cv2 . imshow ( f ' charuco ids with opencv { cv2 . __version__ } ' , image ) cv2 . waitkey ( <number> ) cv2 . destroyallwindows ( ) ` ` ` ! [ image ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"cuda <number> fp16 dnn compilation error # # # system information opencv version : <number> . <number> os : windows <number> compiler : visual studio <number> cuda : <number> # # # detailed description switching from cuda <number> to <number> results in several compilation like the one below when compiling the * dnn <emphasis> * module . <details> <summary> d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> error c2666 : ' operator = ' : overloaded functions have similar conversions </summary> ` ` ` failed : modules / dnn / cmakefiles / opencv_dnn . dir / src / layers / normalize_bbox_layer . cpp . obj c :\\ progra ~ <number> \ \ micros ~ <number> \ \ <number> \ \ commun ~ <number> \ \ vc \ \ tools \ \ msvc \ \ <number> ~ <number> \ \ bin \ \ hostx64 \ \x 6 4 \ \ cl . exe / nologo / tp - dcvapi_exports - dcv_cuda4dnn = <number> - dcv_ocl4dnn = <number> - denable_plugins - dhave_flatbuffers = <number> - dhave_protobuf = <number> - d_crt_secure_no_warnings = <number> - d_use_math_defines - d_variadic_max = <number> - d_win32_winnt =0 x0601 - d__opencv_build = <number> - d__stdc_constant_macros - d__stdc_format_macros - d__stdc_limit_macros - id :\\ build \ \ opencv \ \ 4 _8_0 \ \ cuda_12_2_test \ \ 3 rdparty \ \ ippicv \ \ ippicv_win \ \ icv \ \ include - id :\\ build \ \ opencv \ \ 4 _8_0 \ \ cuda_12_2_test \ \ 3 rdparty \ \ ippicv \ \ ippicv_win \ \ iw \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ include - id :\\ build \ \ opencv \ \ 4 _8_0 \ \ cuda_12_2_test \ \ modules \ \ dnn - id :\\ repos \ \ opencv \ \ contrib \ \ modules \ \ cudev \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ imgproc \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ misc \ \ caffe - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ misc \ \ tensorflow - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ misc \ \ onnx - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ misc \ \ tflite - id :\\ repos \ \ opencv \ \ opencv \ \ 3 rdparty \ \ include \ \ opencl \ \ <number> - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ ts \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ imgcodecs \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ videoio \ \ include - id :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ highgui \ \ include - external : id :\\ build \ \ opencv \ \ 4 _8_0 \ \ cuda_12_2_test - external : i "" c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include "" - external : id :\\ repos \ \ opencv \ \ opencv \ \ 3 rdparty \ \ flatbuffers \ \ include - external : id :\\ repos \ \ opencv \ \ opencv \ \ 3 rdparty \ \ protobuf \ \ src - external : w0 / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / fs / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / wd4244 / wd4267 / wd4018 / wd4355 / wd4800 / wd4251 / wd4996 / wd4146 / wd4305 / wd4127 / wd4100 / wd4512 / wd4125 / wd4389 / wd4510 / wd4610 / wd4702 / wd4456 / wd4457 / wd4065 / wd4310 / wd4661 / wd4506 / wd4125 / wd4267 / wd4127 / wd4244 / wd4512 / wd4702 / wd4456 / wd4510 / wd4610 / wd4800 / wd4701 / wd4703 / wd4505 / wd4458 / md / o2 / ob2 / dndebug / showincludes / fomodules \ \ dnn \ \ cmakefiles \ \ opencv_dnn . dir \ \ src \ \ layers \ \ normalize_bbox_layer . cpp . obj / fdlib \ \ opencv_dnn480 . pdb / fs - c d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> error c2666 : ' operator ! = ' : overloaded functions have similar conversions d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / persistence . hpp ( <number> <sad> note : could be ' bool cv : : operator ! =( const cv : : filenodeiterator & , const cv : : filenodeiterator & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : sparsematconstiterator & , const cv : : sparsematconstiterator & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : matconstiterator & , const cv : : matconstiterator & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' cv : : matexpr cv : : operator ! =( double , const cv : : mat & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' cv : : matexpr cv : : operator ! =( const cv : : mat & , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' cv : : matexpr cv : : operator ! =( const cv : : mat & , const cv : : mat & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : umatdata : : memoryflag & , const int & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : _inputarray : : kindflag & , const int & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : accessflag & , const int & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : range & , const cv : : range & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_int64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int64x2 & , const cv : : hal_baseline : : v_int64x2 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_uint64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint64x2 & , const cv : : hal_baseline : : v_uint64x2 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_float64x2 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_float64x2 & , const cv : : hal_baseline : : v_float64x2 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_float32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_float32x4 & , const cv : : hal_baseline : : v_float32x4 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_int32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int32x4 & , const cv : : hal_baseline : : v_int32x4 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_uint32x4 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint32x4 & , const cv : : hal_baseline : : v_uint32x4 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_int16x8 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int16x8 & , const cv : : hal_baseline : : v_int16x8 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_uint16x8 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint16x8 & , const cv : : hal_baseline : : v_uint16x8 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_int8x16 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_int8x16 & , const cv : : hal_baseline : : v_int8x16 & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / hal / intrin_sse . hpp ( <number> <sad> note : or ' cv : : hal_baseline : : v_uint8x16 cv : : hal_baseline : : operator ! =( const cv : : hal_baseline : : v_uint8x16 & , const cv : : hal_baseline : : v_uint8x16 & ) ' c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> note : or ' bool operator ! =( const __half & , const __half & ) ' [ found using argument - dependent lookup ] c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ( <number> <sad> note : or ' bool operator ! =( const __half2 & , const __half2 & ) ' [ found using argument - dependent lookup ] c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> note : or ' bool operator ! =( const __nv_bfloat16 & , const __nv_bfloat16 & ) ' [ found using argument - dependent lookup ] c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_bf16 . hpp ( <number> <sad> note : or ' bool operator ! =( const __nv_bfloat162 & , const __nv_bfloat162 & ) ' [ found using argument - dependent lookup ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( float , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( signed char , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( unsigned char , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( char , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( short , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( unsigned short , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( int , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( unsigned int , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( long , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( unsigned long , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( __int64 , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( unsigned __int64 , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : or ' built - in c + + operator ! =( bool , double ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : matiterator_ <_tp> & , const cv : : matiterator_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : matiterator_ <_tp> & , const cv : : matiterator_ <_tp> & <sad> could not deduce template argument for ' const cv : : matiterator_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . inl . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : matconstiterator_ <_tp> & , const cv : : matconstiterator_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : matconstiterator_ <_tp> & , const cv : : matconstiterator_ <_tp> & <sad> could not deduce template argument for ' const cv : : matconstiterator_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' cv : : matexpr cv : : operator ! =( const cv : : matx < _tp , m , n > & , const cv : : mat & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' cv : : matexpr cv : : operator ! =( const cv : : matx < _tp , m , n > & , const cv : : mat & <sad> could not deduce template argument for ' const cv : : matx < _tp , m , n > & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / mat . hpp ( <number> <sad> note : or ' cv : : matexpr cv : : operator ! =( const cv : : mat & , const cv : : matx < _tp , m , n > & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' cv : : matexpr cv : : operator ! =( const cv : : mat & , const cv : : matx < _tp , m , n > & <sad> could not deduce template argument for ' const cv : : matx < _tp , m , n > & ' from ' double ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : scalar_ <_tp> & , const cv : : scalar_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : scalar_ <_tp> & , const cv : : scalar_ <_tp> & <sad> could not deduce template argument for ' const cv : : scalar_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : rect_ <_tp> & , const cv : : rect_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : rect_ <_tp> & , const cv : : rect_ <_tp> & <sad> could not deduce template argument for ' const cv : : rect_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : size_ <_tp> & , const cv : : size_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : size_ <_tp> & , const cv : : size_ <_tp> & <sad> could not deduce template argument for ' const cv : : size_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : point3_ <_tp> & , const cv : : point3_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : point3_ <_tp> & , const cv : : point3_ <_tp> & <sad> could not deduce template argument for ' const cv : : point3_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : point_ <_tp> & , const cv : : point_ <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : point_ <_tp> & , const cv : : point_ <_tp> & <sad> could not deduce template argument for ' const cv : : point_ <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / types . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : complex <_tp> & , const cv : : complex <_tp> & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : complex <_tp> & , const cv : : complex <_tp> & <sad> could not deduce template argument for ' const cv : : complex <_tp> & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ core \ \ include \ \ opencv2 / core / matx . hpp ( <number> <sad> note : or ' bool cv : : operator ! =( const cv : : matx < _tp , m , n > & , const cv : : matx < _tp , m , n > & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : ' bool cv : : operator ! =( const cv : : matx < _tp , m , n > & , const cv : : matx < _tp , m , n > & <sad> could not deduce template argument for ' const cv : : matx < _tp , m , n > & ' from ' t ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : while trying to match the argument list ' ( t , double ) ' with [ t = half ] d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / cuda4dnn / primitives / normalize_bbox . hpp ( <number> <sad> note : while compiling class template member function ' void cv : : dnn : : cuda4dnn : : normalizeop <half> : : forward ( const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , const std : : vector < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > , std : : allocator < cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendwrapper > > > & , cv : : dnn : : cuda4dnn : : csl : : workspace & ) ' d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ . <repeated> / op_cuda . hpp ( <number> <sad> note : see reference to class template instantiation ' cv : : dnn : : cuda4dnn : : normalizeop <half> ' being compiled d :\\ repos \ \ opencv \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ normalize_bbox_layer . cpp ( <number> <sad> note reference to function template instantiation ' cv : : ptr < cv : : dnn : : dnn4_v20230620 : : backendnode > cv : : dnn : : make_cuda_node < cv : : dnn : : cuda4dnn : : normalizeop , cv : : dnn : : cuda4dnn : : csl : : stream , const cv : : mat & , cv : : dnn : : cuda4dnn : : normalizeconfiguration <float> & > ( int , cv : : dnn : : cuda4dnn : : csl : : stream & & , const cv : : mat & , cv : : dnn : : cuda4dnn : : normalizeconfiguration <float> & ) ' being compiled ` ` ` </details> it looks like a result of the [ cuda math api ' s ] ( <url> being reworked in this version of cuda , specifically changes to the ` c :\\ program files \ \ nvidia gpu computing toolkit \ \ cuda \ \ v12 . <number> \ \ include \ \ cuda_fp16 . hpp ` file . # # # steps to reproduce ` ` ` cmake . exe - bbuild_dir - hopencv_dir - gninja - dopencv_extra_modules_path = opencv_contrib_modules - dbuild_opencv_world = off - dwith_cuda = on - dcuda_toolkit_root_dir =""c <annoyed> program files / nvidia gpu computing toolkit / cuda / v12 . <number> "" - dcuda_arch_bin = <number> cmake . exe - - build build_dir - - target opencv_dnn ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"error : ( - <number> : assertion failed ) inputs . size ( ) in function ' cv : : dnn : : dnn4_v20230620 : : layer : : getmemoryshapes ' ` ` ` # # # system information general configuration for opencv <number> . <number> - pre = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> - <number> - gc982be3924 extra modules : location ( extra ) : c <annoyed> lib / opencv_contrib / modules version control ( extra ) : <number> . <number> - <number> - g4f66f867 platform : timestamp : <number> - <number> - 0 5 t <time> z host : windows <number> . <number> amd64 cmake : <number> . <number> cmake generator : visual studio <number> <number> cmake build tool : c <annoyed> program files / microsoft visual studio / <number> / community / msbuild / current / bin / amd64 / msbuild . exe msvc : <number> configuration : debug release cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe ( ver <number> . <number> ) c + + flags ( release ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / md / o2 / ob2 / dndebug c + + flags ( debug ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / mdd / zi / ob0 / od / rtc1 c compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe c flags ( release ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / md / o2 / ob2 / dndebug c flags ( debug ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / mdd / zi / ob0 / od / rtc1 linker flags ( release ) : / machine <kiss> 6 4 / incremental : no linker flags ( debug ) : / machine <kiss> 6 4 / debug / incremental ccache : no precompiled headers : yes extra dependencies : cudart_static . lib nppc . lib nppial . lib nppicc . lib nppidei . lib nppif . lib nppig . lib nppim . lib nppist . lib nppisu . lib nppitc . lib npps . lib cublas . lib cudnn . lib cufft . lib - libpath <sad> <annoyed> program files / nvidia gpu computing toolkit / cuda / v12 . <number> / lib / x64 3 rdparty dependencies : opencv modules : to be built : alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto disabled : world disabled by dependency : - unavailable : cvv freetype hdf julia matlab ovis python2 applications : tests perf_tests examples apps documentation : doxygen python javadoc non - free algorithms : yes windows rt support : no gui : win32ui win32 ui : yes opengl support : yes ( opengl32 glu32 ) vtk support : yes ( ver <number> . <number> ) media i / <surprise> zlib : optimized c <annoyed> install / zlib / lib / zlib . lib debug c <annoyed> install / zlib / lib / zlibd . lib ( ver <date> ) jpeg : build - libjpeg - turbo ( ver <number> . <number> - <number> ) simd support request : yes simd support : no webp : build ( ver encoder : 0x0 2 0 f ) png : optimized c <annoyed> install / libpng / lib / libpng16 . lib debug c <annoyed> install / libpng / lib / libpng16d . lib ( ver <date> ) tiff : build ( ver <number> - <number> . <number> ) jpeg <number> : build ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes ( prebuilt binaries ) avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : yes ( <number> . <number> ) gstreamer : no directshow : yes media foundation : yes dxva : yes parallel framework : concurrency other third - party libraries : intel ipp : <number> [ <number> . <number> ] at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / icv intel ipp iw : sources ( <number> . <number> ) at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / iw lapack : yes ( c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_intel_lp64 . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_sequential . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_core . lib ) openvino : yes ( <number> . <number> ) eigen : yes ( ver . <repeated> ) custom hal : no protobuf : build ( <number> . <number> ) flatbuffers : builtin / 3 rdparty ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas ) nvidia gpu arch : <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( nvd3d11 ) include path : c <annoyed> lib / opencv / 3 rdparty / include / opencl / <number> link libraries : dynamic load python <number> : interpreter : c <annoyed> program files / python310 / python . exe ( ver <date> ) libraries : optimized c <annoyed> program files / python310 / libs / python310 . lib debug c <annoyed> program files / python310 / libs / python310_d . lib ( ver <date> ) numpy : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / numpy / core / include ( ver <number> . <number> ) install path : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / cv2 / python - <number> python ( for build ) : c <annoyed> program files / python310 / python . exe java : ant : c <annoyed> apache - ant - <date> / bin / ant . bat ( ver <date> ) java : no jni : c <annoyed> program files / java / jdk - <number> / include c <annoyed> program files / java / jdk - <number> / include / win32 c <annoyed> program files / java / jdk - <number> / include java wrappers : yes ( ant ) java tests : yes install to : c <annoyed> install / opencv - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ` ` ` # # # detailed description i converted adabins_kitty . pth to onnx using pytorch : error is : ` ` ` [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : populatenet dnn / onnx : loading onnx v7 model produced by ' pytorch ' : <number> . <number> . number of nodes = <number> , initializers = <number> , inputs = <number> , outputs = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : parseoperatorset dnn / onnx : onnx opset version = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node / encoder / conv_stem / constant ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_3 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constantofshape ] <sad> onnx_node ! / encoder / conv_stem / constantofshape ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ concat ] <sad> onnx_node ! / encoder / conv_stem / concat ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_4 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reshape ] <sad> onnx_node ! / encoder / conv_stem / reshape ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_5 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_6 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_7 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_8 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ slice ] <sad> onnx_node ! / encoder / conv_stem / slice ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ transpose ] <sad> onnx_node ! / encoder / conv_stem / transpose ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / conv_stem / constant_9 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reshape ] <sad> onnx_node ! / encoder / conv_stem / reshape_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / encoder / conv_stem / cast ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ pad ] <sad> onnx_node ! / encoder / conv_stem / pad ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / conv_stem / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / reducemean ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_reduce / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_expand / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_pw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / reducemean ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_reduce / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_expand / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_pw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / add ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ reducemean ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / reducemean ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_reduce / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / conv_expand / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / se / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_pw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ add ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / add ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_pw / conv ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ sigmoid ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / sigmoid ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ mul ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / act1 / mul ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ shape ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / shape ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / constant ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ gather ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / gather ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ shape ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / shape_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / constant_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ gather ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / gather_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ constant ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / constant_2 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ div ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / div ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / cast ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ cast ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / cast_1 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ neg ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / neg ) from domain = ' ai . onnx ' opencv ( <number> . <number> - pre ) error : assertion failed ( inputs . size ( ) ) in cv : : dnn : : dnn4_v20230620 : : layer : : getmemoryshapes , file c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layer . cpp , line <number> [ error : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ neg ] <sad> onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / neg ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode input [ <number> ] = ' / encoder / blocks . <number> / blocks . <number> / conv_dw / cast_1_output_0 ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode output [ <number> ] = ' / encoder / blocks . <number> / blocks . <number> / conv_dw / neg_output_0 ' opencv ( <number> . <number> - pre ) error : unspecified error ( > node [ <email> <sad> ( onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / neg ) parse error : opencv ( <number> . <number> - pre ) c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layer . cpp : <number> : error : ( - <number> : assertion failed ) inputs . size ( ) in function ' cv : : dnn : : dnn4_v20230620 : : layer : : getmemoryshapes ' > ) in cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode , file c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp , line <number> opencv : terminate handler is called ! the last opencv error is : opencv ( <number> . <number> - pre ) error : unspecified error ( > node [ <email> <sad> ( onnx_node ! / encoder / blocks . <number> / blocks . <number> / conv_dw / neg ) parse error : opencv ( <number> . <number> - pre ) c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layer . cpp : <number> : error : ( - <number> : assertion failed ) inputs . size ( ) in function ' cv : : dnn : : dnn4_v20230620 : : layer : : getmemoryshapes ' > ) in cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode , file c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp , line <number> c :\\ users \ \ laurent \ \ documents \ \ visual studio <number> \ \ build \ \ dnn_edge \ \ debug \ \ dnn_edge . exe ( process <number> ) exited with code <number> . ` ` ` stack trace is > opencv_core480d . dll ! cv : : error ( const cv : : exception & exc ) line <number> c + + opencv_core480d . dll ! cv : : error ( int _code , const std : : string & _err , const char * _func , const char * _file , int _line ) line <number> c + + opencv_dnn480d . dll ! ` cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode ' : : ` <number> ' : : catch <money> ( ) line <number> c + + [ external code ] opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : handlenode ( const opencv_onnx : : nodeproto & node_proto ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : populatenet ( ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : onnximporter : : onnximporter ( cv : : dnn : : dnn4_v20230620 : : net & net , const char * onnxfile ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : detail : : readnet < cv : : dnn : : dnn4_v20230620 : : onnximporter , char const *>( const char * & & <args_0> ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : detail : : readnetdiagnostic < cv : : dnn : : dnn4_v20230620 : : onnximporter , char const *>( const char * & & <args_0> ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : readnetfromonnx ( const std : : string & onnxfile ) line <number> c + + opencv_dnn480d . dll ! cv : : dnn : : dnn4_v20230620 : : readnet ( const std : : string & _model , const std : : string & _config , const std : : string & _framework ) line <number> c + + # # # steps to reproduce model is imported using adress given in <url> ` ` ` import onnxscript import torch from models import unetadaptivebins import model_io import torch . onnx opset_version = <number> device = torch . device ( ' cpu ' ) model = unetadaptivebins . build ( n_bins = <number> , min_val = 1 e - <number> , max_val = <number> ) dummy_input = torch . rand ( ( <number> , <number> , <number> , <number> ) , requires_grad = true ) #. to ( device ) model . eval ( ) model , _ , _ = model_io . load_checkpoint ( r "" c :\\ users \ \ laurent \ \ downloads \ \ adabins_kitti . pt "" , model ) print ( "" export model to onnx "" ) torch . onnx . export ( model , dummy_input , ' adabins_kitti . onnx ' , verbose = true , opset_version = opset_version , input_names =[ ' image_in ' ] , output_names =[ ' depth_out ' ] ) ` ` ` then i try to download model in opencv ` ` ` net net ; net = readnet ( "" adabins_kitti . onnx "" ); ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"mouse coordinates seem to be off if opencv versionhigher than <number> # # # system information opencv version : <number> . <number> operating system / platform : windows <number> python version : <number> # # # detailed description opencv version <number> . <number> seams to be still ok , but somewhere for later versions mouse reading coordinates seems to be off , is that true ? not sure what version it starts but latest <number> or <number> for sure it could be tested in code below . it is a 2 x2 image , so with mouse increase the size of the window . tt should print proper mouse coordinates . but those latest versions show proper value only for top , left quadrant for a pixel # # # steps to reproduce ` ` ` import numpy as np import cv2 print ( f ' opencv version { cv2 . __version__ } ' ) img = np . zeros ( ( <number> , <number> ) , np . uint8 ) img [ <number> , <number> ] = ( <number> , <number> , <number> ) img [ <number> , <number> ] = ( <number> , <number> , <number> ) img [ <number> , <number> ] = ( <number> , <number> , <number> ) def mouseaction ( event , x , y , flags , param = none ) cv2 . namedwindow ( ' image ' , cv2 . window_normal ) cv2 . setwindowproperty ( ' image ' , cv2 . wnd_prop_fullscreen , cv2 . window_normal ) cv2 . setmousecallback ( ' image ' , mouseaction ) cv2 . imshow ( ' image ' , img ) cv2 . waitkey ( <number> ) ` ` `",0
opencv/opencv,"dnn overflow in sigmoid layer fixes # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"mingw - w64 standalone environment error : ' pthread_self ' was not declared in this scope # # # system information opencv version : <number> . <number> operating system : windows <number> 2 2 h2 compiler ( & version ) : gcc . exe (x 8 6 _64 - win32 - seh - rev1 , built by mingw - builds project ) <number> . <number> , mingw - w64 # # # detailed description ` ` ` [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / __ / core / src / parallel . cpp . obj c :\\ users \ \ thekp \ \ desktop \ \ opencv \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ src \ \ parallel . cpp : in function ' int cv : : getthreadnum ( <sad> c :\\ users \ \ thekp \ \ desktop \ \ opencv \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ src \ \ parallel . cpp : <number> <time> : error was not declared in this scope <number> | return ( int ) ( size_t ) ( void <wink> pthread_self ( ); / / no zero - based indexing | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ` ` ` in the case of my machine , i have had only ` <hashtag> define </hashtag> have_pthreads_pf ` enabled , but the ` pthread . h ` header was not included for this case , hence the error . # # # steps to reproduce ` ` ` cpp / / . <repeated> <hashtag> elif </hashtag> defined have_openmp <hashtag> include </hashtag> < omp . h > <hashtag> elif </hashtag> defined have_gcd <hashtag> include </hashtag> < dispatch / dispatch . h > <hashtag> include </hashtag> < pthread . h > <hashtag> elif </hashtag> defined winrt & & _msc_ver < <number> <hashtag> include </hashtag> < ppltasks . h > <hashtag> elif </hashtag> defined have_concurrency <hashtag> include </hashtag> < ppl . h > / / line <number> in modules / core / src / parallel . cpp <hashtag> elif </hashtag> defined have_pthreads_pf <hashtag> include </hashtag> < pthread . h > <hashtag> end if </hashtag> / / line <number> in modules / core / src / parallel . cpp / / . <repeated> ` ` ` adding an ` <hashtag> elif </hashtag> defined have_pthreads_pf ` condition and a ` <hashtag> include </hashtag> < pthread . h > ` snippet allowed ` ` ` cpp <hashtag> elif </hashtag> defined have_pthreads_pf return ( int ) ( size_t ) ( void <wink> pthread_self ( ); / / no zero - based indexing ` ` ` at line <number> to correctly find ` pthread_self ( ) ` and moves on correctly . i wonder if this was somehow overlooked , or was a design choice and that i am just setting my cmake configs wrong ? # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"g - api : fix incorrect opaquekind for kernel outputs # # # pull request readiness checklist # # # # overview the pr is going to fix several problems : <number> . major : ` gkernel ` does not hold ` kind ` for its outputs . since ` gmodelbuilder ` traverse graph from outputs to inputs once it reaches any output of the operation it will use its ` kind ` to create ` data ` meta for all operation outputs . since it essential for ` python ` to know ` gtypeinfo ` ( which is ` shape ` and ` kind ` ) it will be confused . consider this operation : ` ` ` <user> . gapi . op ( ' custom . square_mean ' , in_types =[ cv . garray . int ] , out_types =[ cv . gopaque . float , cv . garray . int ] ) class gsquaremean : <user> def outmeta ( desc ) : return cv . empty_gopaque_desc ( ) , cv . empty_array_desc ( ) ` ` ` even though ` gopaque ` is ` float ` , corresponding metadata might have ` int ` kind because it might be taken from ` cv . garray . int ` so it will be a problem if one of the outputs of these operation is graph output because python will cast it to the wrong type based on ` data ` meta . <number> . minor of the openvino ` ir ` ' s does not any layout information for input . it ' s usually true only for ` irv10 ` but since ` openvino <number> ` need this information to correctly configure resize we need to put default layout if there no such assigned in ` ov : : model ` . see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix : conditionally define generic numpy ndarray alias this patch introduces conditional alias definition . - fixes compatibility with numpy versions < <number> and python < <number> - updates enums handling to utilise introduced ` conditionalaliastypenode ` generated ` cv2 / typing / __init__ . py ` content after patch : ` ` ` python import cv2 . mat_wrapper import numpy import sys import typing if numpy . lib . numpyversion ( numpy . __version__ ) > "" <number> . <number> "" and sys . version_info >= ( <number> , <number> <sad> numpyarraygeneric = numpy . ndarray [ typing . any , numpy . dtype [ numpy . generic ] ] else : numpyarraygeneric = numpy . ndarray if numpy . lib . numpyversion ( numpy . __version__ ) > "" <number> . <number> "" and sys . version_info >= ( <number> , <number> <sad> numpyarrayfloat32 = numpy . ndarray [ typing . any , numpy . dtype [ numpy . float32 ] ] else : numpyarrayfloat32 = numpy . ndarray if numpy . lib . numpyversion ( numpy . __version__ ) > "" <number> . <number> "" and sys . version_info >= ( <number> , <number> <sad> numpyarrayfloat64 = numpy . ndarray [ typing . any , numpy . dtype [ numpy . float64 ] ] else : numpyarrayfloat64 = numpy . ndarray matlike = typing . union [ cv2 . mat_wrapper . mat , numpyarraygeneric ] matx33f = numpyarrayfloat32 "" "" "" ndarray ( shape =( <number> , <number> ) , dtype = numpy . float32 ) "" "" "" matx33d = numpyarrayfloat64 "" "" "" ndarray ( shape =( <number> , <number> ) , dtype = numpy . float64 ) "" "" "" matx44f = numpyarrayfloat32 "" "" "" ndarray ( shape =( <number> , <number> ) , dtype = numpy . float32 ) "" "" "" matx44d = numpyarrayfloat64 "" "" "" ndarray ( shape =( <number> , <number> ) , dtype = numpy . float64 ) "" "" "" ` ` ` resolves # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"python typing module is incompatible with some numpy versions # # # system information os : ubuntu <number> python : <number> . <number> ( from apt ) opencv : <number> . x # # # detailed description numpy : <number> . <number> importing issue : ` ` ` alexander <user> : ~ / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages $ python3 . <number> python <number> . <number> ( default , <date> , <time> ) [ gcc <number> . <number> ] on linux type "" help "" , "" copyright "" , "" credits "" or "" license "" for more information . > > > import cv2 traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in <module> bootstrap ( ) file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in bootstrap if __load_extra_py_code_for_module ( "" cv2 "" , submodule , debug ) : file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in __load_extra_py_code_for_module py_module = importlib . import_module ( module_name ) file "" / usr / lib / python3 . <number> / importlib / __init__ . py "" , line <number> , in import_module return _bootstrap . _gcd_import ( name [ level <happy> , package , level ) file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / typing / __init__ . py "" , line <number> , in <module> matlike = typing . union [ cv2 . mat_wrapper . mat , numpy . ndarray [ typing . any , numpy . dtype [ numpy . generic ] ] ] typeerror : type subscription requires python >= <number> ` ` ` numpy <number> . <number> : ` ` ` python <number> . <number> ( default , <date> , <time> ) [ gcc <number> . <number> ] on linux type "" help "" , "" copyright "" , "" credits "" or "" license "" for more information . > > > import cv2 traceback ( most recent call last ) : file "" <stdin> "" , line <number> , in <module> file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in <module> bootstrap ( ) file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in bootstrap if __load_extra_py_code_for_module ( "" cv2 "" , submodule , debug ) : file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / __init__ . py "" , line <number> , in __load_extra_py_code_for_module py_module = importlib . import_module ( module_name ) file "" / usr / lib / python3 . <number> / importlib / __init__ . py "" , line <number> , in import_module return _bootstrap . _gcd_import ( name [ level <happy> , package , level ) file "" / mnt / projects / projects / opencv / opencv - build / install / lib / python3 . <number> / site - packages / cv2 / typing / __init__ . py "" , line <number> , in <module> matlike = typing . union [ cv2 . mat_wrapper . mat , numpy . ndarray [ typing . any , numpy . dtype [ numpy . generic ] ] ] typeerror : ' numpy . _dtypemeta ' object is not subscriptable ` ` ` # # # steps to reproduce python3 . <number> import cv2 # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"export enums all_caps version to typing stub files - export all_caps versions alongside from normal names for enum constants , since both versions are available in runtime - change enum names entries comments to documentary strings before patch ` ` ` python rmat_access_r : int rmat_access_w : int rmat_access = int # one of [ r , w ] ` ` ` after patch ` ` ` python rmat_access_r : int rmat_access_r : int rmat_access_w : int rmat_access_w : int rmat_access = int "" "" "" one of [ rmat_access_r , rmat_access_r , rmat_access_w , rmat_access_w ] "" "" "" ` ` ` resolves # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"cap_images : can not find starting number # # # system information general configuration for opencv <number> . <number> - dev = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> - <number> - g5d913f4d72 extra modules : location ( extra ) : c <annoyed> lib / opencv_contrib / modules version control ( extra ) : <number> . <number> - <number> - g8dfeed73 platform : timestamp : <number> - <number> - 0 5 t <time> z host : windows <number> . <number> amd64 cmake : <number> . <number> cmake generator : visual studio <number> <number> cmake build tool : c <annoyed> program files / microsoft visual studio / <number> / community / msbuild / current / bin / amd64 / msbuild . exe msvc : <number> configuration : debug release cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe ( ver <number> . <number> ) c + + flags ( release ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / md / o2 / ob2 / dndebug c + + flags ( debug ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / mdd / zi / ob0 / od / rtc1 c compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe c flags ( release ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / md / o2 / ob2 / dndebug c flags ( debug ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / mdd / zi / ob0 / od / rtc1 linker flags ( release ) : / machine <kiss> 6 4 / incremental : no linker flags ( debug ) : / machine <kiss> 6 4 / debug / incremental ccache : no precompiled headers : yes extra dependencies : cudart_static . lib nppc . lib nppial . lib nppicc . lib nppidei . lib nppif . lib nppig . lib nppim . lib nppist . lib nppisu . lib nppitc . lib npps . lib cublas . lib cudnn . lib cufft . lib - libpath <sad> <annoyed> program files / nvidia gpu computing toolkit / cuda / v12 . <number> / lib / x64 3 rdparty dependencies : opencv modules : to be built : alphamat aruco barcode bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto disabled : world disabled by dependency : - unavailable : cvv freetype hdf julia matlab ovis python2 applications : tests perf_tests examples apps documentation : doxygen python javadoc non - free algorithms : yes windows rt support : no gui : win32ui win32 ui : yes opengl support : yes ( opengl32 glu32 ) vtk support : yes ( ver <number> . <number> ) media i / <surprise> zlib : optimized c <annoyed> install / zlib / lib / zlib . lib debug c <annoyed> install / zlib / lib / zlibd . lib ( ver <date> ) jpeg : build - libjpeg - turbo ( ver <number> . <number> - <number> ) simd support request : yes simd support : no webp : build ( ver encoder : 0x0 2 0 f ) png : optimized c <annoyed> install / libpng / lib / libpng16 . lib debug c <annoyed> install / libpng / lib / libpng16d . lib ( ver <date> ) tiff : build ( ver <number> - <number> . <number> ) jpeg <number> : build ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes ( prebuilt binaries ) avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : yes ( <number> . <number> ) gstreamer : no directshow : yes media foundation : yes dxva : yes parallel framework : concurrency other third - party libraries : intel ipp : <number> [ <number> . <number> ] at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / icv intel ipp iw : sources ( <number> . <number> ) at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / iw lapack : yes ( c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_intel_lp64 . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_sequential . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_core . lib ) openvino : yes ( <number> . <number> ) eigen : yes ( ver . <repeated> ) custom hal : no protobuf : build ( <number> . <number> ) flatbuffers : builtin / 3 rdparty ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas ) nvidia gpu arch : <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( nvd3d11 ) include path : c <annoyed> lib / opencv / 3 rdparty / include / opencl / <number> link libraries : dynamic load python <number> : interpreter : c <annoyed> program files / python310 / python . exe ( ver <date> ) libraries : optimized c <annoyed> program files / python310 / libs / python310 . lib debug c <annoyed> program files / python310 / libs / python310_d . lib ( ver <date> ) numpy : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / numpy / core / include ( ver <number> . <number> ) install path : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / cv2 / python - <number> python ( for build ) : c <annoyed> program files / python310 / python . exe java : ant : c <annoyed> apache - ant - <date> / bin / ant . bat ( ver <date> ) jni : c <annoyed> program files / java / jdk - <number> / include c <annoyed> program files / java / jdk - <number> / include / win32 c <annoyed> program files / java / jdk - <number> / include java wrappers : yes java tests : yes install to : c <annoyed> install / opencv - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # # # detailed description this code gives no error : import cv2 as cv ` ` ` img_read = cv . imread ( cv . samples . findfile ( "" lena . jpg "" ) ) vid = cv . videocapture ( cv . samples . findfile ( "" lena . jpg "" ) ) ret , img_vid = vid . read ( ) print ( "" video open : "" , img_vid [ <number> , <number> , :]) print ( "" imread : "" , img_read [ <number> , <number> , :]) ` ` ` and results are not equal because ffmpeg uses its own jpeg decoder ` ` ` video open : [ <number> <number> <number> ] imread : [ <number> <number> <number> ] ` ` ` so i want to use imread i add [ apipreference ] ( <url> ` ` ` import cv2 as cv vid = cv . videocapture ( cv . samples . findfile ( "" lena . jpg "" ) , cv . cap_images ) ` ` ` and there is an exception : ` ` ` exception [ warn : <number> <user> . <number> ] global samples . cpp : <number> cv : : samples : : findfile cv : : samples : : findfile ( ' lena . jpg ' ) => ' c :\\ lib \ \ opencv \ \ samples / data \ \ lena . jpg ' [ error : <number> <user> . <number> ] global cap . cpp : <number> cv : : videocapture : : open videoio ( cv_images ) : raised opencv exception : opencv ( <number> . <number> - dev ) c :\\ lib \ \ opencv \ \ modules \ \ videoio \ \ src \ \ cap_images . cpp : <number> : error : ( - <number> : bad argument ) cap_images : can not find starting number ( in the name of file ) : c :\\ lib \ \ opencv \ \ samples / data \ \ lena . jpg in function ' cv : : icvextractpattern ' [ warn : <number> <user> . <number> ] global cap . cpp : <number> cv : : videocapture : : open videoio ( cv_images ) is generally available but can not be used to capture by name ` ` ` many example in dnn module uses videocapture . open to read a file inference with videocapture . open and imread do not give same results . # # # steps to reproduce same problem in c + + all codes is already given # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"type stubs : ` cv2 . typing ` type aliases do not exist at runtime but are not marked as such # # # system information opencv python : opencv_python_headless - <number> . <number> + 7 7 2 5 9 9 a - cp37 - abi3 - win_amd64 . whl operating system / platform : <number> . <number> build <number> python version : <date> # # # detailed description trying to use ` cv2 . typing . anything ` at runtime will throw an ` attributeerror ` ( ie : ` attributeerror : module ' cv2 . typing ' has no attribute ' matlike ' ` ) . the details of when that ' s the case will depend on the python version and its annotations parser . this makes the ` cv2 . typing ` module dangerous to use , because static type checkers will find its use acceptable . the ` <user> . type_check_only ` decorator exists for those exact cases , but unfortunately can not be used with type aliases . there ' s a couple possible solutions for this , here ' s some i can think of : <number> . make them exist at runtime . this could be as simple as creating a ` cv2 / typing / __init__ . py ` where all the values are assigned the type ( but without generic typing to support python < <number> ) . this has the added bonus of making them runtime - comparable using ` isinstance ` . <number> . use classes instead of type aliases that inherits from the aliased type . those classes can now be marked as ` type_check_only ` . <number> . use ` typeshed ` ' s workaround of making them private . not great imo but at least is serves as a reminder for users that these do not exist at runtime . # # # steps to reproduce here ' s a example that fails in python <number> . an older version like <number> - <number> would have more cases . ` ` ` py from typing import cast import cv2 . typing # let us pretend this came from a method that returns a matlike , but is not typed correctly foo = object ( ) bar = cast ( cv2 . typing . matlike , foo ) ` ` ` ` ` ` traceback ( most recent call last ) : file "" c :\\ users \ \ avasam \ \ desktop \ \ import cv2 . py "" , line <number> , in <module> bar = cast ( cv2 . typing . matlike , foo ) attributeerror ' cv2 . typing ' has no attribute ' matlike ' ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"type stubs : slicing a ` cv2 . mat_wrapper . mat ` or derived type results in partially unknown ` ndarray ` # # # system information opencv python : opencv_python_headless - <number> . <number> + 7 7 2 5 9 9 a - cp37 - abi3 - win_amd64 . whl operating system / platform : <number> . <number> build <number> python version # # # detailed description pyright uses a special ` unknown ` type to represent , well , unknown type , types that can not be inferred , unspecified generic types , etc . it is semantically different than ` any ` ( which means literally any type is valid , or used as an escape hatch ) . this results in type errors when using strict typing . mypy does not separate that concept and simply infers ` any ` when type information is loss or unspecified . # # # steps to reproduce ` ` ` py import cv2 . mat_wrapper import cv2 . typing # using casts to simplify for static typing testing purposes mat_like = cast ( cv2 . typing . matlike , object ( ) ) mat = cast ( cv2 . mat_wrapper . mat , object ( ) ) # type of "" mat_like_sliced "" is partially unknown # type of "" mat_like_sliced "" is "" ndarray [ any , unknown ] | ndarray [ any , dtype [ generic ] ] "" pylance ( reportunknownvariabletype ) mat_like_sliced = mat_like [ :, :, <number> ] # type of "" mat_sliced "" is partially unknown # type of "" mat_sliced "" is "" ndarray [ any , unknown ] "" pylance ( reportunknownvariabletype ) mat_sliced = mat [ :, :, <number> ] ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"type stubs : ` cv2 . __init__ . pyi ` is missing module re - exports # # # system information opencv python : opencv_python_headless - <number> . <number> + 7 7 2 5 9 9 a - cp37 - abi3 - win_amd64 . whl operating system / platform : <number> . <number> build <number> python version : <date> # # # detailed description ` cv2 . __init__ . pyi ` is missing module re - exports , which should look something like this : ` ` ` py from cv2 import aruco as aruco from cv2 import cuda as cuda from cv2 import data as data from cv2 import detail as detail from cv2 import dnn as dnn from cv2 import error as error from cv2 import fisheye as fisheye from cv2 import flann as flann from cv2 import gapi as gapi from cv2 import ipp as ipp from cv2 import mat_wrapper as mat_wrapper from cv2 import misc as misc from cv2 import ml as ml from cv2 import ocl as ocl from cv2 import ogl as ogl from cv2 import parallel as parallel from cv2 import samples as samples from cv2 import segmentation as segmentation from cv2 import typing as typing from cv2 import utils as utils from cv2 import videoio_registry as videoio_registry ` ` ` # # # steps to reproduce the following should be acceptable to static type checkers , but is not : ` ` ` py import cv2 # argument type is unknown # argument corresponds to parameter "" values "" in function "" print "" pylancereportunknownargumenttype # "" cuda "" is not a known member of module "" cv2 "" pylancereportgeneraltypeissues # ( function ) cuda print ( cv2 . error ) ` ` ` ` ` ` py > > > import cv2 > > > cv2 . error . stsok <number> ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"converto value # # # system information opencv version : <number> . <number> operating system / windows compiler & compiler version <number> simple function frame1 . convertto ( frame1 , cv_32f , <date> ); from input value <number> give me <number> but real value must be something like this ` ` ` float value = <number> / <number> ; printf ( "" % . 1 0 f \ \ n "" , value ) ; / / display <number> decimal places printf ( "" % . 4 f \ \ n "" , value ) ; / / display <number> decimal places <number> . <phone> <number> <number> ` ` ` # # # detailed description not correct value from function # # # steps to reproduce input value <number> frame1 . convertto ( frame1 , cv_32f , <date> ); # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"python scalar is different with c + + hi , i suspect the following code has different behavior in c + + and python . ` ` ` py scalar a = <number> ; ` ` ` for python , the ` scalar a = <number> ` code will only replace the first ` <number> ` to ` <number> ` . do something like ` a [ <number> ] = <number> ` . for c + + , it will do the ` scalar a = scalar ( <number> ) ` . original link : <url> my code in python paramtf2cedn = cv . dnn . image2blobparams ( ) paramtf2cedn . datalayout = cv . dnn . dnn_layout_nhwc ; paramtf2cedn . ddepth = cv . cv_32f ; paramtf2cedn . mean = ( <number> , <number> , <number> ) paramtf2cedn . scalefactor = <number> / <number> . paramtf2cedn . size = ( <number> , <number> ) paramtf2cedn . swaprb = false ; paramtf2cedn . paddingmode = cv . dnn . dnn_pmode_null print ( "" paramtf2cedn . scalefactor = "" , paramtf2cedn . scalefactor ) ` ` ` result is ` ` ` paramtf2cedn . scalefactor = ( <number> , <number> , <number> , <number> ) ` ` ` in c + + ` ` ` cpp image2blobparams paramtf2cedn ; paramtf2cedn . datalayout = dnn_layout_nhwc ; paramtf2cedn . ddepth = cv_32f ; paramtf2cedn . mean = ( <number> , <number> , <number> ); paramtf2cedn . scalefactor = <number> / <number> . ; paramtf2cedn . size = size ( <number> , <number> ); paramtf2cedn . swaprb = false ; paramtf2cedn . paddingmode = dnn_pmode_null ; cout < < "" paramtf2cedn . scalefactor = "" < < paramtf2cedn . scalefactor < < endl ; ` ` ` result is ` ` ` paramtf2cedn . scalefactor = [ <number> , <number> , <number> , <number> ] ` ` ` # # # detailed description it is about opencv - python bing . # # # steps to reproduce install lastest opencv - python ` pip install opencv - python - rolling = = <number> . <number> ` . ` ` ` py param = cv . dnn . image2blobparams ( ) param . mean = np . array ( [ <number> , <number> , <number> ] ) # [ <number> , <number> , <number> , <number> ] # then redefine it . param . mean = <number> # it is [ <number> , <number> , <number> , <number> ] < < - - ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"g - api backend hotfix # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"keep inliers for linear remap with border_transparent # # # pull request readiness checklist resolves <url> i do think that this is a bug because with ` inter_cubic + border_transparent ` the last column and row are preserved . so same should be done for ` inter_linear ` see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"imgproc / cvtcolor invalid read in bgr2hls related pr # <number> * ` mhls3 ` is read outside of array ` intertmpm ` ( <number> elements ) , it is not used anywhere * ~ replacing ` max_nlanes ` with ` vlanes ` for array size is still in question , i think it should be ok ~ it was not ok : smiley :",0
opencv/opencv,"facerecognizersf . feature c + + throws vector subscript out of range # # # system information opencv version : <number> . <number> operating system / platform : windows <number> compiler & compiler version : clang <number> . <number> # # # detailed description the bug is on this line ` ` ` cv : : mat feature1 , feature2 ; fr - > feature ( aligned_face1 , feature1 ) ; / / throws an exception vector subscript out of range feature1 = feature1 . clone ( ); ` ` ` of this code ` ` ` <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> < opencv2 / imgcodecs . hpp > <hashtag> include </hashtag> < opencv2 / highgui . hpp > <hashtag> include </hashtag> < opencv2 / imgproc . hpp > <hashtag> include </hashtag> < opencv2 / objdetect . hpp > <hashtag> include </hashtag> < opencv2 / core / utils / logger . hpp > <hashtag> define </hashtag> is_cosine_similar ( cosine ) ( cosine >= <number> ) <hashtag> define </hashtag> is_l2_similar ( norm ) ( norm <= <number> ) char const * fd_model = "" d <annoyed> codding / face_detection_yunet_2022mar . onnx "" ; char const * fr_model = "" d <annoyed> codding / face_recognition_sface_2021dec . onnx "" ; double scale = <number> ; static void visualize ( cv : : mat & input , cv : : mat & faces , int thickness = <number> ) { for ( int i = <number> ; i < faces . rows ; i + + ) { / / draw bounding box cv : : rectangle ( input , cv : : rect2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; / / draw landmarks cv : : circle ( input , cv : : point2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , <number> , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; cv : : circle ( input , cv : : point2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , <number> , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; cv : : circle ( input , cv : : point2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , <number> , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; cv : : circle ( input , cv : : point2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , <number> , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; cv : : circle ( input , cv : : point2i ( int ( faces . at <float> ( i , <number> ) ) , int ( faces . at <float> ( i , <number> ) ) ) , <number> , cv : : scalar ( <number> , <number> , <number> ) , thickness ) ; } } int main ( ) { / / then set logging to silent cv : : utils : : logging : : setloglevel ( cv : : utils : : logging : : loglevel : : log_level_silent ) ; cv : : videocapture capture ( <number> ); if ( capture . isopened ( ) ) { std : : cerr < < "" fatal error : cannot open capture device ! \ \ n "" ; return exit_failure ; } cv : : size img_size ( int ( capture . get ( cv : : cap_prop_frame_width ) * scale ) , int ( capture . get ( cv : : cap_prop_frame_height ) * scale ) ); cv : : ptr < cv : : facedetectoryn > fd = cv : : facedetectoryn : : create ( fd_model , "" "" , img_size ) ; cv : : ptr < cv : : facerecognizersf > fr = cv : : facerecognizersf : : create ( fr_model , "" "" ); cv : : mat img1 , img2 , capimg1 , capimg2 ; capture . read ( capimg1 ) ; capture . read ( capimg2 ) ; cv : : resize ( capimg1 , img1 , img_size ) ; cv : : resize ( capimg2 , img2 , img_size ) ; cv : : mat faces1 , faces2 , debug1 = img1 . clone ( ) , debug2 = img2 . clone ( ); fd - > detect ( img1 , faces1 ) ; if ( faces1 . rows < <number> ) { std : : cerr < < "" fatal error : cannot find a face on owner image ! \ \ n "" ; return exit_failure ; } fd - > detect ( img2 , faces2 ) ; if ( faces1 . rows < <number> ) { std : : cerr < < "" fatal error find a face on webcam ! \ \ n "" ; return exit_failure ; } visualize ( debug1 , faces1 ) ; visualize ( debug2 , faces2 ) ; cv : : imshow ( "" face1 "" , debug1 ) ; cv : : imshow ( "" face2 "" , debug2 ) ; cv : : waitkey ( <number> ); cv : : mat aligned_face1 , aligned_face2 ; fr - > aligncrop ( img1 , faces1 . row ( <number> ) , aligned_face1 ) ; fr - > aligncrop ( img2 , faces2 . row ( <number> ) , aligned_face2 ) ; std : : cout < < "" aligned size - "" < < aligned_face1 . size ( ) < < ' / ' < < aligned_face2 . size ( ) < < ' \ \ n ' ; cv : : imshow ( "" aligned1 "" , aligned_face1 ) ; cv : : imshow ( "" aligned2 "" , aligned_face2 ) ; cv : : waitkey ( <number> ); cv : : mat feature1 , feature2 ; fr - > feature ( aligned_face1 , feature1 ) ; feature1 = feature1 . clone ( ); fr - > feature ( aligned_face2 , feature2 ) ; feature2 = feature2 . clone ( ); double cos_score = fr - > match ( feature1 , feature2 , cv : : facerecognizersf : : distype : : fr_cosine ) ; std : : cout < < "" score - "" < < cos_score < < "" , is same - "" < < is_cosine_similar ( cos_score ) < < "" \ \ n "" ; return exit_success ; } ` ` ` # # # steps to reproduce i have checked all your facerecognizersf related issues and i have not found any solution , do i also need to provide some additional info ? ! [ image_2023 - <number> - 0 5 _084517884 ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,opencv . js : qr code test failure <url> relates # <number> ` ` ` │ qr code detect and decode │ died on test # <number> : cannot construct qrcodedetector due to unbound types │ fail │ │ │ at object . global . test . qunit . test ( / build / 4 _x_javascript - emscripten - lin64 / build / bin / node_modules / node - qunit / lib / child . js : <number> <time> ) │ │ │ │ at object . <anonymous> ( / build / 4 _x_javascript - emscripten - lin64 / build / bin / test_objdetect . js : <number> : <number> ) │ │ │ │ at module . _compile ( module . js : <number> <time> ) │ │ │ │ at object . module . _extensions . <repeated> js ( module . js : <number> <time> ) │ │ │ │ at module . load ( module . js : <number> <time> ) │ │ │ │ at trymoduleload ( module . js : <number> <time> ) │ │ │ │ at function . module . _load ( module . js : <number> : <number> ) ` ` `,0
opencv/opencv,"how using open cv to maui net # # # system information how using open cv to maui net # # # detailed description how using open cv to maui net # # # steps to reproduce how using open cv to maui net # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"opencv . js missing arucodetector , gridboard , charucoboard # # # system information opencv . js version : <number> . <number> official build from <url> # # # detailed description the classes arucodetector , gridboard , and charucoboard , are not available in the opencv . js , despite being whitelisted in the opencv_js . config . py config file : <url> <url> <url> tests for such classes are also missing <url> thanks # # # steps to reproduce using node . js code : ` ` ` require ( ' . / opencv . js ' ) . then ( ( cv ) =>{ console . log ( cv . getbuildinformation ( )); / / correctly show build informations let detector = new cv . arucodetector ( ); / / generate runtime error of missing arucodetector constructor }); ` ` ` output : ` ` ` general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> platform : timestamp : <number> - <number> - 2 8 t <time> z host : linux <number> . <number> - <number> - generic x86_64 target : emscripten <number> x86 cmake : <number> . <number> cmake generator : unix makefiles cmake build tool : / usr / bin / make configuration : release cpu / hw features : baseline : c / c + + : built as dynamic libs ? : no c + + standard : <number> c + + compiler : / opt / emsdk - portable / upstream / emscripten / em + + ( ver <number> . <number> ) c + + flags ( release ) : - s use_pthreads = <number> - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winconsistent - missing - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - fdiagnostics - show - option - qunused - arguments - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - dndebug - o2 - dndebug c + + flags ( debug ) : - s use_pthreads = <number> - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winconsistent - missing - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - fdiagnostics - show - option - qunused - arguments - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug c compiler : / opt / emsdk - portable / upstream / emscripten / emcc c flags ( release ) : - s use_pthreads = <number> - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winconsistent - missing - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - fdiagnostics - show - option - qunused - arguments - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - dndebug - o2 - dndebug c flags ( debug ) : - s use_pthreads = <number> - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winconsistent - missing - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - fdiagnostics - show - option - qunused - arguments - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug linker flags ( release ) : - wl , - - gc - sections - wl , - - no - undefined - o2 linker flags ( debug ) : - wl , - - gc - sections - wl , - - no - undefined ccache : no precompiled headers : no extra dependencies : 3 rdparty dependencies : zlib libprotobuf quirc opencv modules : to be built : calib3d core dnn features2d flann imgproc js objdetect photo video disabled : highgui imgcodecs ml stitching videoio world disabled by dependency : ts unavailable : gapi java python2 python3 applications : examples documentation : js non - free algorithms : no gui : media i / <surprise> zlib : build ( ver <date> ) jpeg <number> : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> parallel framework : none other third - party libraries : va : no custom hal : no protobuf : build ( <number> . <number> ) python ( for build ) : / usr / bin / python install to : / usr / local - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - runtimeerror : abort ( typeerror is not a constructor ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"generation of getter and setter for the detectorparameters . cornerrefinementmethod property is skipped during java wrapper generation . # # # system information opencv version : <number> . <number> operating system / platform : windows <number> 2 2 h2 compiler & compiler version : cmake <number> . <number> # # # detailed description the creation of the get_cornerrefinementmethod ( ) and set_cornerrefinementmethod ( int cornerrefinementmethod ) methods are skipped when the java wrapper is generated . this problem has occurred since opencv <number> . <number> . it appears that opencv4 . <number> and opencv4 . <number> have changed the type of the cornerrefinementmethod variable . opencv4 . <number> <url> opencv4 . <number> <url> the following code is the java code generated for each of these versions . opencv4 . <number> ` ` ` / / / / c + + : int detectorparameters : : cornerrefinementmethod / / public int get_cornerrefinementmethod ( ) { return get_cornerrefinementmethod_0 ( nativeobj ) ; } / / / / c + + : void detectorparameters : : cornerrefinementmethod / / public void set_cornerrefinementmethod ( int cornerrefinementmethod ) { set_cornerrefinementmethod_0 ( nativeobj , cornerrefinementmethod ) ; } ` ` ` opencv4 . <number> ` ` ` / / / / c + + : cornerrefinemethod detectorparameters : : cornerrefinementmethod / / / / return type ' cornerrefinemethod ' is not supported , skipping the function / / / / c + + detectorparameters : : cornerrefinementmethod / / / / unknown type ' cornerrefinemethod ' ( i ) , skipping the function ` ` ` # # # steps to reproduce build with the build_opencv_java_bindings_generator flag on . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"python binding for cuda : : gpumat does not handle float16 properly when downloading to or uploading from a numpy array # # # system information opencv python version : <number> . <number> with opencv 8 7 3 3 1 ca built with cuda <number> operating system / platform : ubuntu <number> python version : <number> . <number> # # # detailed description trying to upload a float16 numpy array to a gpumat gives an ` arr data type = <number> is not supported ` error . while trying to download from a float16 gpumat gives a uint64 numpy array with garbage content . the test case below shows that float16 cupy and gpumat interoperability appears to work fine . here ' s the full output log to the code from "" steps to reproduce "" : ` ` ` original cupy array and pointer [ [ <number> . <number> . ] [ <number> . <number> . ] ] <number> gpumat , initialized and downloaded to numpy ; and pointer [ [ <number> <number> ] [ <phone> <number> ] ] <number> assert gpumat type is float16 : true numpy dtype : uint64 back to cupy from gpumat : value and pointer : [ [ <number> . <number> . ] [ <number> . <number> . ] ] <number> now try to upload a float16 numpy array to gpumat : traceback ( most recent call last ) : file "" * * */ tst_cupy_to_mat . py "" , line <number> , in <module> cv_a . upload ( np_a ) cv2 . error : opencv ( <number> . <number> ) : - <number> : error : ( - <number> : bad argument ) in function ' upload ' > overload resolution failed : > - arr data type = <number> is not supported > - expected ptr < cv : : cuda : : gpumat > for argument ' arr ' > - expected ptr < cv : : umat > for argument ' arr ' > - cuda_gpumat . upload ( ) missing required argument ' stream ' ( pos <number> ) > - cuda_gpumat . upload ( ) missing required argument ' stream ' ( pos <number> ) > - cuda_gpumat . upload ( ) missing required argument ' stream ' ( pos <number> ) ` ` ` # # # steps to reproduce ` ` ` python import cupy as cp import numpy as np import cv2 def cv2cp ( mat : cv2 . cuda . gpumat ) - > cp . ndarray : class cudaarrayinterface : def __init__ ( self , gpu_mat : cv2 . cuda . gpumat ) : w , h = gpu_mat . size ( ) type_map = { cv2 . cv_8u : "" | u1 "" , cv2 . cv_8s : "" | i1 "" , cv2 . cv_16u : "" < u2 "" , cv2 . cv_16s : "" < i2 "" , cv2 . cv_32s : "" < i4 "" , cv2 . cv_32f : "" < f4 "" , cv2 . cv_64f : "" < f8 "" , cv2 . cv_16f : "" < f2 "" } self . __cuda_array_interface__ = { "" version "" : <number> , "" shape "" : ( h , w , gpu_mat . channels ( ) ) if gpu_mat . channels ( ) > <number> else ( h , w ) , "" typestr "" : type_map [ gpu_mat . depth ( ) ] , "" descr "" : [ ( "" "" , type_map [ gpu_mat . depth ( ) ] ) ] , "" stream "" : <number> , "" strides "" : ( gpu_mat . step , gpu_mat . elemsize ( ) , gpu_mat . elemsize1 ( ) ) if gpu_mat . channels ( ) > <number> else ( gpu_mat . step , gpu_mat . elemsize ( ) ) , "" data "" : ( gpu_mat . cudaptr ( ) , false ) , } arr = cp . asarray ( cudaarrayinterface ( mat ) ) return arr def cp2cv ( arr : cp . ndarray ) - > cv2 . cuda . gpumat : assert len ( arr . shape ) in ( <number> , <number> ) , "" cupy array must have <number> or <number> dimensions to be a valid gpumat "" type_map = { cp . dtype ( ' uint8 ' <sad> cv2 . cv_8u , cp . dtype ( ' int8 ' <sad> cv2 . cv_8s , cp . dtype ( ' uint16 ' <sad> cv2 . cv_16u , cp . dtype ( ' int16 ' <sad> cv2 . cv_16s , cp . dtype ( ' int32 ' <sad> cv2 . cv_32s , cp . dtype ( ' float32 ' <sad> cv2 . cv_32f , cp . dtype ( ' float64 ' <sad> cv2 . cv_64f , cp . dtype ( ' float16 ' <sad> cv2 . cv_16f } depth = type_map . get ( arr . dtype ) assert depth is not none , "" unsupported cupy array dtype "" channels = <number> if len ( arr . shape ) = = <number> else arr . shape [ <number> ] mat_type = cv2 . cv_maketype ( depth , channels ) mat = cv2 . cuda . creategpumatfromcudamemory ( arr . __cuda_array_interface__ [ ' shape ' ] [ <number> : : - <number> ] , mat_type , arr . __cuda_array_interface__ [ ' data ' ] [ <number> ] ) return mat cp_a = cp . random . randint ( <number> , <number> , ( <number> , <number> ) ) . astype ( np . float16 ) print ( ' original cupy array and pointer ' ) print ( cp_a , cp_a . __cuda_array_interface__ [ ' data ' ] [ <number> ] ) print ( ' ' ) cv_a = cp2cv ( cp_a ) np_a = cv_a . download ( ) print ( ' ' ) print ( ' gpumat , initialized and downloaded to numpy ; and pointer ' ) print ( np_a , cv_a . cudaptr ( ) ) print ( f ' assert gpumat type is float16 : { cv_a . type ( ) = = cv2 . cv_16fc1 } ' ) print ( ' numpy dtype :', np_a . dtype ) print ( ' ' ) cp_a2 = cv2cp ( cv_a ) print ( ' back to cupy from gpumat and pointer <happy> print ( cp_a2 , cp_a2 . __cuda_array_interface__ [ ' data ' ] [ <number> ] ) print ( ' ' ) print ( ' now try to upload a float16 numpy array to gpumat <happy> np_a = np . random . randint ( <number> , <number> , ( <number> , <number> ) ) . astype ( np . float16 ) cv_a = cv2 . cuda_gpumat ( ) cv_a . upload ( np_a ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"dnn potential bug , stride should not be set as <number> . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"no rule to make target ' cublas ' , needed by ' lib / libopencv_sfm . so . <number> . <number> ' . # # # system information i use ubuntu <number> i have cuda <number> i have cudnn version <number> . <number> # # # detailed description i am trying to install opencv <number> . <number> with cuda <number> and cudnn <number> . <number> but i am having a problem with that and it ' s as follows ` ` ` ( aienv ) ai <user> : ~ / nvidia / opencv - <number> . <number> / build $ make - j20 - - detected processor : x86_64 - - looking for ccache - not found - - found zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( found suitable version "" <date> "" , minimum required is "" <number> . <number> "" ) cleaning internal cached variable : webp_library cleaning internal cached variable : webp_include_dir - - could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) . openjpeg will be built from sources - - openjpeg : version = <number> . <number> , build = opencv - <number> . <number> - openjp2 - <number> . <number> - - openjpeg libraries will be built from sources : libopenjp2 ( version "" <number> . <number> "" ) - - found zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( found version "" <date> "" ) - - found openexr : / usr / lib / x86_64 - linux - gnu / libilmimf . so - - found tbb ( cmake ) : / usr / lib / x86_64 - linux - gnu / libtbb . so . <number> - - found intel ipp ( icv version ) : <number> . <number> [ <number> . <number> gold ] - - at : / home / ai / nvidia / opencv - <number> . <number> / build / 3 rdparty / ippicv / ippicv_lnx / icv - - found intel ipp integration wrappers sources : <number> . <number> - - at : / home / ai / nvidia / opencv - <number> . <number> / build / 3 rdparty / ippicv / ippicv_lnx / iw - - cuda detected : <number> - - cuda : using cuda_arch_bin = <number> - - cuda nvcc target flags : - gencode ; arch = compute_86 , code = sm_86 ; - d_force_inlines - - lapack ( atlas ) : lapack_libraries : / usr / lib / x86_64 - linux - gnu / liblapack . so ;/ usr / lib / x86_64 - linux - gnu / libcblas . so ;/ usr / lib / x86_64 - linux - gnu / libatlas . so - - lapack ( atlas ) : support is enabled . - - could not find jni ( missing : java_include_path java_include_path2 awt ) - - vtk is not found . please set - dvtk_dir in cmake to vtk build directory , or to vtk install subdirectory with vtkconfig . cmake file - - opencv python : during development append to pythonpath : / home / ai / nvidia / opencv - <number> . <number> / build / python_loader - - caffe : no - - protobuf : no - - glog : yes - - freetype2 : yes ( ver <date> ) - - harfbuzz : yes ( ver <number> . <number> ) - - hdf5 c compiler wrapper is unable to compile a minimal hdf5 program . - - julia not found . not compiling julia bindings . - - module opencv_ovis disabled because ogre3d was not found - - found amd headers in : / usr / include / suitesparse - - found amd library : / usr / lib / x86_64 - linux - gnu / libamd . so - - found camd headers in : / usr / include / suitesparse - - found camd library : / usr / lib / x86_64 - linux - gnu / libcamd . so - - found ccolamd headers in : / usr / include / suitesparse - - found ccolamd library : / usr / lib / x86_64 - linux - gnu / libccolamd . so - - found cholmod headers in : / usr / include / suitesparse - - found cholmod library : / usr / lib / x86_64 - linux - gnu / libcholmod . so - - found colamd headers in : / usr / include / suitesparse - - found colamd library : / usr / lib / x86_64 - linux - gnu / libcolamd . so - - found spqr headers in : / usr / include / suitesparse - - found spqr library : / usr / lib / x86_64 - linux - gnu / libspqr . so - - found config headers in : / usr / include / suitesparse - - found config library : / usr / lib / x86_64 - linux - gnu / libsuitesparseconfig . so - - found intel thread building blocks ( tbb ) library ( <number> / <number> ) include location : . assuming suitesparseqr was compiled with tbb . - - adding librt to suitesparse_config libraries ( required on linux & unix [ not osx ] if suitesparse is compiled with timing ) . - - could not find metis ( missing : metis_include_dir metis_library ) - - tesseract : yes ( ver <number> . <number> ) - - allocator metrics storage type : ' long long ' - - hdf5 c compiler wrapper is unable to compile a minimal hdf5 program . - - registering hook ' init_module_sources_opencv_dnn ' : / home / ai / nvidia / opencv - <number> . <number> / modules / dnn / cmake / hooks / init_module_sources_opencv_dnn . cmake - - found amd headers in : / usr / include / suitesparse - - found amd library : / usr / lib / x86_64 - linux - gnu / libamd . so - - found camd headers in : / usr / include / suitesparse - - found camd library : / usr / lib / x86_64 - linux - gnu / libcamd . so - - found ccolamd headers in : / usr / include / suitesparse - - found ccolamd library : / usr / lib / x86_64 - linux - gnu / libccolamd . so - - found cholmod headers in : / usr / include / suitesparse - - found cholmod library : / usr / lib / x86_64 - linux - gnu / libcholmod . so - - found colamd headers in : / usr / include / suitesparse - - found colamd library : / usr / lib / x86_64 - linux - gnu / libcolamd . so - - found spqr headers in : / usr / include / suitesparse - - found spqr library : / usr / lib / x86_64 - linux - gnu / libspqr . so - - found config headers in : / usr / include / suitesparse - - found config library : / usr / lib / x86_64 - linux - gnu / libsuitesparseconfig . so - - found intel thread building blocks ( tbb ) library ( <number> / <number> ) include location : . assuming suitesparseqr was compiled with tbb . - - adding librt to suitesparse_config libraries ( required on linux & unix [ not osx ] if suitesparse is compiled with timing ) . - - could not find metis ( missing : metis_include_dir metis_library ) - - building with nvidia optical flow api <number> - - - - general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - version control : unknown - - - - extra modules : - - location ( extra ) : / home / ai / nvidia / opencv_contrib - <number> . <number> / modules - - version control ( extra ) : unknown - - - - platform : - - timestamp : <number> - <number> - 2 1 t <time> z - - host : linux <number> . <number> - <number> - generic x86_64 - - cmake : <number> . <number> - - cmake generator : unix makefiles - - cmake build tool : / usr / bin / make - - configuration : release - - - - cpu / hw features : - - baseline : sse sse2 sse3 - - requested : sse3 - - dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx - - requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx - - sse4_1 ( <number> files ) : + ssse3 sse4_1 - - sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 - - fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx - - avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx - - avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 - - avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx - - - - c / c + + : - - built as dynamic libs ? : yes - - c + + standard : <number> - - c + + compiler : / usr / bin / c + + ( ver <number> . <number> ) - - c + + flags ( release ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - o3 - dndebug - dndebug - - c + + flags ( debug ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug - - c compiler : / usr / bin / cc - - c flags ( release ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - o3 - dndebug - dndebug - - c flags ( debug ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - g - o0 - ddebug - d_debug - - linker flags ( release ) : - wl , - - exclude - libs , libippicv . a - wl , - - exclude - libs , libippiw . a - wl , - - gc - sections - wl , - - as - needed - - linker flags ( debug ) : - wl , - - exclude - libs , libippicv . a - wl , - - exclude - libs , libippiw . a - wl , - - gc - sections - wl , - - as - needed - - ccache : no - - precompiled headers : no - - extra dependencies : m pthread cudart_static dl rt nppc nppial nppicc nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cudnn cufft - l / usr / local / cuda - <number> / lib64 - l / usr / lib / x86_64 - linux - gnu - - 3 rdparty dependencies : - - - - opencv modules : - - to be built : alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hdf hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python2 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto - - disabled : cudacodec world - - disabled by dependency : - - - unavailable : cnn_3dobj cvv java julia matlab ovis python3 viz - - applications : tests perf_tests apps - - documentation : no - - non - free algorithms : yes - - - - gui : - - gtk + : yes ( ver <date> ) - - gthread : yes ( ver <number> . <number> ) - - gtkglext : no - - opengl support : no - - vtk support : no - - - - media i / <surprise> - - zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( ver <date> ) - - jpeg : / usr / lib / x86_64 - linux - gnu / libjpeg . so ( ver <number> ) - - webp : build ( ver encoder : 0x0 2 0 f ) - - png : / usr / lib / x86_64 - linux - gnu / libpng . so ( ver <date> ) - - tiff : / usr / lib / x86_64 - linux - gnu / libtiff . so ( ver <number> / <number> . <number> ) - - jpeg <number> : build ( ver <number> . <number> ) - - openexr : / usr / lib / x86_64 - linux - gnu / libimath . so / usr / lib / x86_64 - linux - gnu / libilmimf . so / usr / lib / x86_64 - linux - gnu / libiex . so / usr / lib / x86_64 - linux - gnu / libhalf . so / usr / lib / x86_64 - linux - gnu / libilmthread . so ( ver 2 _3 ) - - hdr : yes - - sunraster : yes - - pxm : yes - - pfm : yes - - - - video i / <surprise> - - dc1394 : yes ( <number> . <number> ) - - ffmpeg : yes - - avcodec : yes ( <number> . <number> ) - - avformat : yes ( <number> . <number> ) - - avutil : yes ( <number> . <number> ) - - swscale : yes ( <date> ) - - avresample : yes ( <number> . <number> ) - - gstreamer : yes ( <number> . <number> ) - - v4l / v4l2 : yes ( linux / videodev2 . h ) - - - - parallel framework : tbb ( ver <number> interface <number> ) - - - - trace : yes ( with intel itt ) - - - - other third - party libraries : - - intel ipp : <number> . <number> gold [ <number> . <number> ] - - at : / home / ai / nvidia / opencv - <number> . <number> / build / 3 rdparty / ippicv / ippicv_lnx / icv - - intel ipp iw : sources ( <number> . <number> ) - - at : / home / ai / nvidia / opencv - <number> . <number> / build / 3 rdparty / ippicv / ippicv_lnx / iw - - va : yes - - lapack : yes ( / usr / lib / x86_64 - linux - gnu / liblapack . so / usr / lib / x86_64 - linux - gnu / libcblas . so / usr / lib / x86_64 - linux - gnu / libatlas . so ) - - eigen : yes ( ver <number> . <number> ) - - custom hal : no - - protobuf : build ( <number> . <number> ) - - - - nvidia cuda : yes ( ver <number> , cufft cublas fast_math ) - - nvidia gpu arch : <number> - - nvidia ptx archs : - - - - cudnn : yes ( ver <number> . <number> ) - - - - opencl : yes ( intelva ) - - include path : / home / ai / nvidia / opencv - <number> . <number> / 3 rdparty / include / opencl / <number> - - link libraries : dynamic load - - - - python <number> : - - interpreter : / usr / bin / python2 . <number> ( ver <date> ) - - libraries : / usr / lib / x86_64 - linux - gnu / libpython2 . <number> . so ( ver <date> ) - - numpy : / usr / lib / python2 . <number> / dist - packages / numpy / core / include ( ver <number> . <number> ) - - install path : lib / python2 . <number> / dist - packages / cv2 / python - <number> - - - - python ( for build ) : / usr / bin / python2 . <number> - - - - java : - - ant : no - - jni : no - - java wrappers : no - - java tests : no - - - - install to : / usr / local - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - configuring done ( <number> . 2 s ) - - generating done ( <number> . 9 s ) - - build files have been written to : / home / ai / nvidia / opencv - <number> . <number> / build [ <percent> ] built target gen - pkgconfig [ <percent> ] built target opencv_videoio_plugins [ <percent> ] built target ittnotify [ <percent> ] built target quirc [ <percent> ] built target gen_opencv_python_source [ <percent> ] built target numeric [ <percent> ] built target ippiw [ <percent> ] built target libopenjp2 [ <percent> ] built target ade [ <percent> ] built target opencv_cudev [ <percent> ] building cxx object modules / sfm / src / libmv / libmv / multiview / cmakefiles / multiview . dir / fundamental . cc . o [ <percent> ] building cxx object modules / sfm / src / libmv / libmv / multiview / cmakefiles / multiview . dir / homography . cc . o [ <percent> ] built target libwebp [ <percent> ] built target libprotobuf [ <percent> ] built target opencv_core [ <percent> ] built target opencv_hdf [ <percent> ] built target opencv_version [ <percent> ] built target opencv_flann [ <percent> ] built target opencv_surface_matching [ <percent> ] built target opencv_ml [ <percent> ] built target opencv_cudaarithm [ <percent> ] built target opencv_imgproc [ <percent> ] built target opencv_plot [ <percent> ] built target opencv_phase_unwrapping [ <percent> ] built target opencv_freetype [ <percent> ] built target opencv_quality [ <percent> ] built target opencv_intensity_transform [ <percent> ] built target opencv_fuzzy [ <percent> ] built target opencv_img_hash [ <percent> ] built target opencv_cudawarping [ <percent> ] built target opencv_alphamat [ <percent> ] built target opencv_hfs [ <percent> ] built target opencv_reg [ <percent> ] built target opencv_cudafilters [ <percent> ] built target opencv_imgcodecs [ <percent> ] built target opencv_features2d [ <percent> ] built target opencv_line_descriptor [ <percent> ] built target opencv_cudafeatures2d [ <percent> ] built target opencv_saliency [ <percent> ] built target opencv_videoio [ <percent> ] built target opencv_cudaimgproc [ <percent> ] built target opencv_highgui [ <percent> ] built target opencv_annotation [ <percent> ] built target opencv_visualisation [ <percent> ] built target opencv_photo [ <percent> ] built target opencv_ts [ <percent> ] built target opencv_bioinspired [ <percent> ] built target opencv_xphoto [ <percent> ] built target opencv_test_phase_unwrapping [ <percent> ] built target opencv_test_intensity_transform [ <percent> ] built target opencv_test_cudafilters [ <percent> ] built target opencv_test_hdf [ <percent> ] built target opencv_test_flann [ <percent> ] built target opencv_test_reg [ <percent> ] built target opencv_perf_reg [ <percent> ] built target opencv_perf_cudaarithm [ <percent> ] built target opencv_calib3d [ <percent> ] built target opencv_test_quality [ <percent> ] built target opencv_perf_cudafilters [ <percent> ] built target opencv_perf_cudawarping [ <percent> ] built target opencv_test_cudaarithm [ <percent> ] built target opencv_perf_imgcodecs [ <percent> ] built target opencv_test_cudawarping [ <percent> ] built target opencv_test_fuzzy [ <percent> ] built target opencv_perf_features2d [ <percent> ] built target opencv_test_ml [ <percent> ] built target opencv_test_img_hash [ <percent> ] built target opencv_perf_cudaimgproc [ <percent> ] built target opencv_test_line_descriptor [ <percent> ] built target opencv_test_cudaimgproc [ <percent> ] built target opencv_perf_line_descriptor [ <percent> ] built target opencv_test_imgcodecs [ <percent> ] built target opencv_test_saliency [ <percent> ] built target opencv_perf_photo [ <percent> ] built target opencv_dnn [ <percent> ] built target opencv_perf_videoio [ <percent> ] built target opencv_test_cudev [ <percent> ] built target opencv_test_cudafeatures2d [ <percent> ] built target opencv_perf_xphoto [ <percent> ] built target opencv_perf_cudafeatures2d [ <percent> ] built target opencv_test_highgui [ <percent> ] built target opencv_rapid [ <percent> ] built target opencv_test_photo [ <percent> ] built target opencv_test_features2d [ <percent> ] built target opencv_perf_calib3d [ <percent> ] built target opencv_test_xphoto [ <percent> ] built target opencv_structured_light [ <percent> ] built target opencv_test_videoio [ <percent> ] built target opencv_test_bioinspired [ <percent> ] built target opencv_cudastereo [ <percent> ] built target opencv_perf_bioinspired [ <percent> ] built target opencv_dnn_objdetect [ <percent> ] built target opencv_model_diagnostics [ <percent> ] built target opencv_aruco [ <percent> ] built target opencv_perf_core [ <percent> ] built target opencv_ccalib [ <percent> ] built target opencv_shape [ <percent> ] built target opencv_objdetect [ <percent> ] built target opencv_dnn_superres [ <percent> ] built target opencv_perf_imgproc [ <percent> ] built target opencv_perf_cudastereo [ <percent> ] built target opencv_test_rapid [ <percent> ] built target opencv_test_cudastereo [ <percent> ] built target opencv_text [ <percent> ] built target opencv_test_dnn_superres [ <percent> ] built target opencv_perf_dnn [ <percent> ] built target opencv_test_structured_light [ <percent> ] built target opencv_test_aruco [ <percent> ] built target opencv_perf_dnn_superres [ <percent> ] built target opencv_interactive - calibration [ <percent> ] built target opencv_test_shape [ <percent> ] built target opencv_test_objdetect [ <percent> ] built target opencv_perf_objdetect [ <percent> ] built target opencv_xobjdetect [ <percent> ] built target opencv_mcc [ <percent> ] built target opencv_test_text [ <percent> ] built target opencv_dpm [ <percent> ] built target opencv_test_dnn [ <percent> ] built target opencv_waldboost_detector [ <percent> ] built target opencv_test_mcc [ <percent> ] built target opencv_test_calib3d [ <percent> ] built target opencv_video [ <percent> ] built target opencv_rgbd [ <percent> ] built target opencv_face [ <percent> ] built target opencv_bgsegm [ <percent> ] built target opencv_wechat_qrcode [ <percent> ] built target opencv_cudabgsegm [ <percent> ] built target opencv_perf_rgbd [ <percent> ] built target opencv_datasets [ <percent> ] built target opencv_test_core [ <percent> ] built target opencv_test_bgsegm [ <percent> ] built target opencv_test_wechat_qrcode [ <percent> ] built target opencv_test_rgbd [ <percent> ] built target opencv_test_face [ <percent> ] built target opencv_xfeatures2d [ <percent> ] built target opencv_test_video [ <percent> ] built target opencv_perf_video [ <percent> ] built target opencv_perf_cudabgsegm [ <percent> ] built target opencv_test_cudabgsegm [ <percent> ] built target opencv_cudalegacy [ <percent> ] built target opencv_test_imgproc [ <percent> ] built target opencv_cudaobjdetect [ <percent> ] built target opencv_perf_cudalegacy [ <percent> ] built target opencv_test_cudaobjdetect [ <percent> ] built target opencv_perf_cudaobjdetect [ <percent> ] built target opencv_perf_xfeatures2d [ <percent> ] built target opencv_test_xfeatures2d [ <percent> ] built target opencv_stitching [ <percent> ] built target opencv_test_cudalegacy [ <percent> ] built target opencv_tracking [ <percent> ] built target opencv_perf_tracking [ <percent> ] built target opencv_stereo [ <percent> ] built target opencv_perf_stitching [ <percent> ] built target opencv_test_tracking [ <percent> ] built target opencv_test_stitching [ <percent> ] built target opencv_perf_stereo [ <percent> ] built target opencv_test_stereo [ <percent> ] built target opencv_ximgproc [ <percent> ] built target opencv_gapi [ <percent> ] built target opencv_perf_ximgproc [ <percent> ] built target opencv_optflow [ <percent> ] built target opencv_perf_optflow [ <percent> ] built target opencv_test_optflow [ <percent> ] built target opencv_cudaoptflow [ <percent> ] built target opencv_test_ximgproc [ <percent> ] built target opencv_perf_gapi [ <percent> ] built target opencv_test_cudaoptflow [ <percent> ] built target opencv_perf_cudaoptflow [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_superres . so [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_videostab . so [ <percent> ] built target opencv_test_gapi [ <percent> ] built target opencv_superres [ <percent> ] built target opencv_videostab [ <percent> ] building cxx object modules / superres / cmakefiles / opencv_test_superres . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / superres / cmakefiles / opencv_test_superres . dir / test / test_superres . cpp . o [ <percent> ] building cxx object modules / superres / cmakefiles / opencv_perf_superres . dir / perf / perf_main . cpp . o [ <percent> ] building cxx object modules / superres / cmakefiles / opencv_perf_superres . dir / perf / perf_superres . cpp . o [ <percent> ] building cxx object modules / videostab / cmakefiles / opencv_test_videostab . dir / test / test_main . cpp . o [ <percent> ] building cxx object modules / videostab / cmakefiles / opencv_test_videostab . dir / test / test_motion_estimation . cpp . o [ <percent> ] building cxx object modules / videostab / cmakefiles / opencv_test_videostab . dir / test / test_stabilizer . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_superres [ <percent> ] built target opencv_test_superres [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_test_videostab [ <percent> ] built target opencv_test_videostab [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_perf_superres [ <percent> ] built target opencv_perf_superres [ <percent> ] linking cxx static library . <repeated> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libmultiview . a [ <percent> ] built target multiview [ <percent> ] built target correspondence [ <percent> ] building cxx object modules / sfm / src / libmv / libmv / simple_pipeline / cmakefiles / simple_pipeline . dir / bundle . cc . o [ <percent> ] building cxx object modules / sfm / src / libmv / libmv / simple_pipeline / cmakefiles / simple_pipeline . dir / intersect . cc . o [ <percent> ] building cxx object modules / sfm / src / libmv / libmv / simple_pipeline / cmakefiles / simple_pipeline . dir / keyframe_selection . cc . o [ <percent> ] linking cxx static library . <repeated> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libsimple_pipeline . a [ <percent> ] built target simple_pipeline make [ <number> <sad> * * * no rule to make target ' cublas ' , needed by ' lib / libopencv_sfm . so . <number> . <number> ' . stop . make [ <number> <sad> * * * [ cmakefiles / makefile2 : <number> : modules / sfm / cmakefiles / opencv_sfm . dir / all ] error <number> make : * * * [ makefile : <number> error <number> ` ` ` thanks for the help in advance # # # steps to reproduce none # # # issue submission checklist - [ ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"build fix for avx <number> <user> our ci environment started failing after you merged this [ commit ] ( <url> ` ` ` / build / build_cuda / 3 p / opencv / linux - x64 / ubuntu22 . <number> / debug / modules / dnn / src / layers / cpu_kernels / convolution . cpp : in function ' void cv : : dnn : : packdata8 ( char * & , float * & , int & , int & , int & , const int * , int , int , int ) ' : / build / build_cuda / 3 p / opencv / linux - x64 / ubuntu22 . <number> / debug / modules / dnn / src / layers / cpu_kernels / convolution . cpp : <number> <time> : error was not declared in this scope ; did you mean ' conv_3d ' ? <number> | vx_store ( inpbufc_fp32 + k*conv_nr <censored> , vx_load ( inptrinc + k1 ) ); | ^ ~ ~ ~ ~ ~ ~ | conv_3d ` ` ` # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"build issue and usage of std : : random_shuffle # # # system information c + + user - building from source requires building all the tests , and _opencv / modules / objectdetect / test / test_qrcode_encode . cpp_ uses the function ` ` ` std : : random_shuffle ( ) ` ` ` to generate the input info . however , this function is no longer in std since c + + <number> . # # # detailed description c + + user - building from source requires building all the tests , and _opencv / modules / objectdetect / test / test_qrcode_encode . cpp_ uses the function ` ` ` std : : random_shuffle ( ) ` ` ` to generate the input info . however , this function is no longer in std since c + + <number> . my solution , as for now , is to just comment all the code . however , it should be able to build with all versions of c + + . a solution would be replacing it with ` ` ` std : : shuffle ( ) ` ` ` . # # # steps to reproduce build with c + + std >= <number> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"add assert to check if layer input size is not empty # # # check if tf model layer input is not empty # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fix even input dimensions for inter_nearest_exact # # # pull request readiness checklist resolves <url> related / cc <user> see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fixed mask handling in affinefeature address <url> # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"fixes pixel info color font for dark qt themes for dark qt themes , it is hard to read the pixel color information on the bottom left , like the coordinates or rgb values . this pr proposes a way on how the dynamically sets the font colors based on the system ' s theme . original example : [ original ] ( <url> with patch for windows , nothing is changed ( tested on a windows <number> system ) , because the font color is # <number> when using the default qt theme . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"unused variable in frameprocessor . cpp # # # system information opencv <number> . x # # # detailed description in <url> rejected is unused variable # # # steps to reproduce read source code # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"wrong version for opencv <number> . <number> in the releases website for ios # # # system information does not apply . # # # detailed description the downloaded version for ios ( ios pack ) in <url> for the <number> . <number> seems to download the opencv version <date> . # # # steps to reproduce download the <number> . <number> for ios # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"opencv <number> . <number> much slower than <number> . <number> for java apps # # # system information performance comparisons are between opencv <number> . <number> ( slow ) and <number> . <number> . ( faster ) . [ <number> . <number> runs essentially same faster speed as <number> . <number> . ] all runs use windows <number> ; hp elitebook laptop . my test app ( the mre ) built with java compiler : openjdk version "" <number> . <number> "" <number> - <number> - <number> openjdk runtime environment temurin - <number> . <number> + <number> ( build <number> . <number> + <number> ) openjdk <number> - bit server vm temurin - <number> . <number> + <number> ( build <number> . <number> + <number> , mixed mode , sharing ) essentially identical results running test on two java runtime versions : java version "" <number> "" <number> - <number> - <number> java ( tm ) se runtime environment ( build <number> + <number> ) java hotspot ( tm ) <number> - bit server vm ( build <number> + <number> , mixed mode , sharing ) and openjdk version "" <number> . <number> "" <number> - <number> - <number> openjdk runtime environment temurin - <number> . <number> + <number> ( build <number> . <number> + <number> ) openjdk <number> - bit server vm temurin - <number> . <number> + <number> ( build <number> . <number> + <number> , mixed mode , sharing ) # # # detailed description opencv <number> . <number> is much slower than <number> . <number> or <number> . <number> for the test run as shown using ` imread ` in the test mre , for a variety of other methods in another app summarized below , and for other java apps using opencv . ` ` ` mre run times opencv <number> . <number> [ seconds ] <number> ` imread ` elapsed time <number> , total cpu time <number> , user cpu <number> mre run times opencv <number> . <number> [ seconds ] <number> ` imread ` elapsed time <number> , total cpu time <number> , user cpu <number> ` ` ` the mre test case scales correctly as expected : <number> iterations takes twice as long as <number> iterations . mre run times opencv <number> . <number> with ` imread ` commented out are nearly zero [ seconds ] ` imread ` elapsed time <number> , total cpu time <number> , user cpu <number> instrumenting a complete app showed similar performance differences for most opencv java methods ( <number> . <number> and <number> . <number> as are essentially identical in performance ) . ` ` ` [ version / milliseconds ] <number> . <number> <number> . <number> _____ _____ imread <number> <number> convert <number> <number> resize <number> <number> blur <number> <number> split <number> <number> normalize <number> <number> equalize <number> <number> resize <number> <number> adaptive threshold <number> <number> ` ` ` # # # steps to reproduce a mre app with the single method , ` imread ` was written to demonstrate the slow performance . it is attached and includes generating its own test file to read and it displays the opencv version build information for both versions which is included as comments in the file . [ app . java . txt ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"the ` matchtemplate ` function changes the data of the ` mask ` . # # # system information opencv version : <number> . <number> operating system / platform : windows <number> compiler & compiler version : visual studio <number> # # # detailed description when the ` matchtemplate ` function is used with a ` mask ` , the data of the ` mask ` changes and it is no longer usable . ` mask ` is an ` inputarray ` and should not change , it should be read only . # # # steps to reproduce ` ` ` c + + mat src = mat ( <number> , <number> , cv_8uc1 , scalar ( <number> )); mat templ = mat ( <number> , <number> , cv_8uc1 , scalar ( <number> )); mat mask1 = mat ( <number> , <number> , cv_8uc1 , scalar ( <number> )); printf ( "" before % d \ \ n "" , mask1 . at <uchar> ( <number> , <number> )); mat res ; matchtemplate ( src , templ , res , templatematchmodes : : tm_ccoeff_normed , mask1 ) ; printf ( "" after % d \ \ n "" , mask1 . at <uchar> ( <number> , <number> )); ` ` ` output before <number> after <number> ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"losing last mat row and column during ` remap ( ) ` with ` border_transparent ` # # # system information opencv python version : <number> . <number> operating system / platform : windows <number> python revision : <number> . <number> # # # detailed description [ edit : unquoted relevant part of my question . <repeated> ] hi , first off thanks for the great work you guys are doing i will try and break this down with the attached python snippet . the docs say > when bordermode = border_transparent , it means that the pixels in the destination image that corresponds to the "" outliers "" in the source image are not modified by the function . and sadly i cannot seem to find much more on this anywhere , so i may be using it wrong . in that case apologies for creating this issue . but my understanding is that in the example code , this should lead to pixels in the destination staying black when trying to map to a pixel in the source that does not exist ( e . g . ` <number> ` ) . however , in the example , all pixels in the mapping mats exist in the source , but still , the last row and column in the destination always remain ` <number> ` when using ` border_transparent ` . it works as expected with ` border_constant ` ! [ grafik ] ( <url> * * ` border_transparent ` * * ! [ grafik ] ( <url> # # # steps to reproduce ` ` ` python bgimage = np . zeros ( ( <number> , <number> , <number> ) , dtype = np . float32 ) source = np . array ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = np . float32 ) . reshape ( <number> , <number> , <number> ) imagesize = bgimage . shape [ <number> ] , bgimage . shape [ <number> ] map1 = np . array ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = np . float32 ) map2 = np . array ( [ [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] , [ <number> , <number> , <number> ] ] , dtype = np . float32 ) destination = np . zeros ( ( <number> , <number> , <number> ) , dtype = np . float32 ) cv2 . remap ( source , map1 , map2 , interpolation = cv2 . inter_linear , dst = destination , bordermode = cv2 . border_transparent ) result = bgimage . copy ( ) result [ <number> : <number> + <number> , <number> : <number> + <number> ] = destination upscaledimage = cv2 . resize ( result , np . array ( [ <number> , <number> ] ) , interpolation = cv2 . inter_nearest ) cv2 . imshow ( "" test "" , upscaledimage ) cv2 . waitkey ( <number> ) cv2 . destroyallwindows ( ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"segfault at countnonzero8u on risc - v with rvv <number> . <number> # # # system information opencv version : 6 dbc5e032fa851d1a25fb4a895d735ed3441975b operating system / platform : [ 2 0 2 1 1 2 3 0 _licheerv_debian_d1_hdmi_8723ds ] ( <url> / sipeed lichee rv dock ( allwinner d1 cpu ) , compiler & compiler version : [ xuantie - <number> - gcc - linux - <number> . <number> - glibc - x86_64 - v2 . <number> - <number> . tar . gz ] ( <url> # # # detailed description ` ` ` # <number> 0x0 0 0 0 0 0 3 ff7d146ae in vsetvl_e64m1 ( a = <number> ) at / home / dkurt / xuantie - <number> - gcc - linux - <number> . <number> - glibc - x86_64 - v2 . <number> / lib / gcc / riscv64 - unknown - linux - gnu / <number> . <number> / include / riscv_vector . h : <number> # <number> vmv_v_x_u64m1 ( vl = <number> , a = <number> ) at / home / dkurt / xuantie - <number> - gcc - linux - <number> . <number> - glibc - x86_64 - v2 . <number> / lib / gcc / riscv64 - unknown - linux - gnu / <number> . <number> / include / riscv_vector . h : <number> # <number> cv : : hal_baseline : : v_reduce_sum ( a = . <repeated> ) at / home / dkurt / halide_riscv / 3 rdparty / opencv / modules / core / include / opencv2 / core / hal / intrin_rvv . hpp : <number> # <number> 0x0 0 0 0 0 0 3 ff7d15ae8 in cv : : cpu_baseline : : countnonzero8u ( src =0 x3 ff65e8040 ' \ \ <number> ' < repeats <number> times > . <repeated> , len = <number> ) at / home / dkurt / halide_riscv / 3 rdparty / opencv / modules / core / src / count_non_zero . simd . hpp : <number> - - type <ret> for more , q to quit , c to continue without paging - - # <number> 0x0 0 0 0 0 0 3 ff7d1655c in cv : : countnonzero ( _src = . <repeated> ) at / home / dkurt / halide_riscv / 3 rdparty / opencv / modules / core / src / count_non_zero . dispatch . cpp : <number> ` ` ` # # # steps to reproduce build options : ` ` ` bash - dcmake_build_type = release - dcmake_toolchain_file <annoyed> path / to / opencv / platforms / linux / riscv64 - gcc . toolchain . cmake - driscv_rvv_scalable = off - dcpu_baseline = rvv - dcpu_rvv_flags_on = - march = rv64gcv0p7 ` ` ` <details> <summary> build summary </summary> ` ` ` - - general configuration for opencv <number> . <number> - dev = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - version control : <number> . <number> - <number> - g6dbc5e032f - - - - platform : - - timestamp : <number> - <number> - 2 8 t <time> z - - host : linux <number> . <number> - microsoft - standard - wsl2 x86_64 - - target : linux <number> riscv64 - - cmake : <number> . <number> - - cmake generator : unix makefiles - - cmake build tool : / usr / bin / make - - configuration : debug - - - - cpu / hw features : - - baseline : rvv - - - - c / c + + : - - built as dynamic libs ? : yes - - c + + standard : <number> - - c + + compiler : / home / dkurt / xuantie - <number> - gcc - linux - <number> . <number> - glibc - x86_64 - v2 . <number> / bin / riscv64 - unknown - linux - gnu - g + + ( ver <number> . <number> ) - - c + + flags ( release ) : - march = rv64gc - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - march = rv64gcv0p7 - fvisibility = hidden - fvisibility - inlines - hidden - o3 - dndebug - dndebug - - c + + flags ( debug ) : - march = rv64gc - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - march = rv64gcv0p7 - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug - - c compiler : / home / dkurt / xuantie - <number> - gcc - linux - <number> . <number> - glibc - x86_64 - v2 . <number> / bin / riscv64 - unknown - linux - gnu - gcc - - c flags ( release ) : - march = rv64gc - fsigned - char - w - wall - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - march = rv64gcv0p7 - fvisibility = hidden - o3 - dndebug - dndebug - - c flags ( debug ) : - march = rv64gc - fsigned - char - w - wall - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - march = rv64gcv0p7 - fvisibility = hidden - g - o0 - ddebug - d_debug - - linker flags ( release ) : - wl , - - gc - sections - wl , - - as - needed - wl , - - no - undefined - - linker flags ( debug ) : - wl , - - gc - sections - wl , - - as - needed - wl , - - no - undefined - - ccache : yes - - precompiled headers : no - - extra dependencies : dl m pthread rt - - 3 rdparty dependencies : - - - - opencv modules : - - to be built : core dnn highgui imgcodecs imgproc ts videoio - - disabled : world - - disabled by dependency : calib3d features2d flann java_bindings_generator js_bindings_generator ml objc_bindings_generator objdetect photo python_bindings_generator python_tests stitching video - - unavailable : gapi java python2 python3 - - applications : tests - - documentation : no - - non - free algorithms : no - - - - gui : none - - gtk + : no - - - - media i / <surprise> - - zlib : zlib ( ver <date> ) - - jpeg : libjpeg - turbo ( ver <number> . <number> - <number> ) - - png : build ( ver <date> ) - - jpeg <number> : build ( ver <number> . <number> ) - - hdr : yes - - sunraster : yes - - pxm : yes - - pfm : yes - - - - video i / <surprise> - - dc1394 : no - - gstreamer : no - - v4l / v4l2 : yes ( linux / videodev2 . h ) - - - - parallel framework : pthreads - - - - trace : yes ( built - in ) - - - - other third - party libraries : - - lapack : no - - custom hal : no - - protobuf : build ( <number> . <number> ) - - flatbuffers : builtin / 3 rdparty ( <date> ) - - - - python ( for build ) : / usr / bin / python2 . <number> - - - - install to ` ` ` </details> ` ` ` cpp mat src0 ( <number> , <number> , cv_8uc1 ) ; mat src1 ( <number> , <number> , cv_8uc1 ) ; randu ( src0 , <number> , <number> ); randu ( src1 , <number> , <number> ); countnonzero ( src0 = src1 ) ; ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"nary elementwise broadcast failure cases i wrote a brute force test to test elementwise which cases will be wrong , here is the test report [ report . md ] ( <url> i am not sure if i should put this brute force test into ` test_layer . cpp ` , because it will test about * * <number> * * times for each backend and it can not detect case which is fallback from gpu to cpu unless we add something to record this situation . # # # steps to reproduce <number> . if you want to test on cuda , modify file ` nary_eltwise_layer . cpp ` . change all ` return ptr <backendnode> (); ` to ` throw std : : logic_error ( "" fallback "" ); ` and it will be detected when fallback to cpu . <number> . build opencv with cuda and in * release <emphasis> * mode . <number> . use code here to generate this report ` ` ` cpp / / config std : : vector < std : : pair < int , int > > backend_target_list = { { dnn_backend_opencv , dnn_target_cpu } , { dnn_backend_cuda , dnn_target_cuda } }; std : : vector <int> dims_list = { <number> , <number> , <number> , <number> , <number> }; struct util { / / give n to generate all n - d arrays with <number> or <number> static void get_all_arr ( std : : vector < std : : vector <int> > & arr , int n ) { int total = <number> < < n ; arr . assign ( total , std : : vector <int> ( n , - <number> )); for ( int i = <number> ; i < total ; i + + ) for ( int j = <number> ; j < n ; j + + ) arr [ i ] [ j ] = ( i > > ( n - j - <number> ) ) & <number> ; } / / zero will replace all <number> , one will replace all <number> static void replace ( std : : vector < std : : vector <int> > & arr , int zero , int one ) { for ( int i = <number> ; i < arr . size ( ); i + + ) for ( int j = <number> ; j < arr [ <number> ] . size ( ); j + + ) arr [ i ] [ j ] = arr [ i ] [ j ] ? one : zero ; } / / test if the shape can be forwarded static int test_bcast ( const std : : vector <int> & a_shape , const std : : vector <int> & b_shape , const string & op , const std : : pair < int , int > & backend_target ) { mat a = mat : : zeros ( ( int ) a_shape . size ( ) , a_shape . data ( ) , cv_32fc1 ) ; mat b = mat : : ones ( ( int ) b_shape . size ( ) , b_shape . data ( ) , cv_32fc1 ) ; net net ; layerparams lp ; lp . type = "" naryeltwise "" ; lp . name = "" testlayer "" ; lp . set ( "" operation "" , op ) ; int id = net . addlayertoprev ( lp . name , lp . type , lp ) ; net . connect ( <number> , <number> , id , <number> ); std : : vector <string> inpnames ( <number> ); inpnames [ <number> ] = "" a "" ; inpnames [ <number> ] = "" b "" ; net . setinputsnames ( inpnames ) ; net . setinput ( a , inpnames [ <number> ]); net . setinput ( b , inpnames [ <number> ]); net . setpreferablebackend ( backend_target . first ) ; net . setpreferabletarget ( backend_target . second ) ; try { mat re = net . forward ( ); auto ptr_re = ( float <wink> re . data ; / / check if result is right for ( int i = <number> ; i < re . total ( ); i + + ) if ( op = = "" sum "" & & ptr_re [ i ] = <number> ) return - <number> ; / / sum result is wrong return <number> ; / / all right } catch ( std : : logic_error & e ) { if ( ( std : : string ) e . what ( ) = = "" fallback "" ) return - <number> ; / / fallback to cpu else return - <number> ; / / other error } catch ( . <repeated> ) { return - <number> ; / / runtime error } } static void print_result ( int type , const std : : vector <int> & shp1 , const std : : vector <int> & shp2 ) { std : : string error_content ; switch ( type ) { case <number> : return ; case - <number> : error_content = "" runtime error "" ; break ; case - <number> : error_content = "" result wrong "" ; break ; case - <number> : error_content = "" fallback to cpu "" ; break ; default : error_content = "" "" ; break ; } std : : cout < < tostring ( shp1 ) < < "" op "" < < tostring ( shp2 ) < < "" , fail reason is "" < < error_content < < std : : endl ; } }; std : : vector < std : : vector <int> > dim_shape_list ; std : : vector < std : : vector <int> > sub_shape_list ; std : : cout < < "" # nary elementwise broadcast failure cases "" < < std : : endl ; for ( auto backend_target : backend_target_list ) { std : : cout < < "" # # backendid : "" < < backend_target . first < < "" , targetid : "" < < backend_target . second < < std : : endl ; for ( int dim : dims_list ) { std : : cout < < "" # # # dimension : "" < < dim < < std : : endl ; sub_shape_list . insert ( sub_shape_list . end ( ) , dim_shape_list . begin ( ) , dim_shape_list . end ( )); util : : get_all_arr ( dim_shape_list , dim ) ; util : : replace ( dim_shape_list , <number> , <number> ); / / same shape std : : cout < < "" - * * same shape * * "" < < std : : endl ; for ( int i = <number> ; i < dim_shape_list . size ( ); i + + ) for ( int j = <number> ; j < dim_shape_list . size ( ); j + + ) util : : print_result ( util : : test_bcast ( dim_shape_list [ i ] , dim_shape_list [ j ] , "" sum "" , backend_target ) , dim_shape_list [ i ] , dim_shape_list [ j ] ); / / diff shape std : : cout < < "" - * * different shape * * "" < < std : : endl ; for ( const auto & shp1 : dim_shape_list ) for ( const auto & shp2 : sub_shape_list ) util : : print_result ( util : : test_bcast ( shp1 , shp2 , "" sum "" , backend_target ) , shp1 , shp2 ) ; / / diff shape for ( const auto & shp1 : sub_shape_list ) for ( const auto & shp2 util : : print_result ( util : : test_bcast ( shp1 , shp2 , "" sum "" , backend_target ) , shp1 , shp2 ) ; } } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"rotatedrectangleintersection does not return all intersection points as expected # # # system information opencv python version : <number> . <number> operating system / platform : windows <number> python version : <date> # # # detailed description during some testing of a system that used ` rotatedrectangleintersection ` and then ` contourarea ` to get the intersection area of <number> overlapping rectangles , we got a size of ~ 3 5 0 0 mm² instead of the expected ~ 7 0 0 0 mm² . it seems to be related to the fact that one edge of each rectangle are on the same line . * rectangle <number> : ` rotatedrect ( center =( <number> , <number> ) , extent =( <number> , <number> ) , angle = - <number> ) ` * rectangle <number> <number> ) , extent =( <number> , <number> ) , angle = <number> ) ` this seems related to # <number> , which should be fixed by # <number> , but i have not tested that as it has not been released . # # # steps to reproduce ` ` ` py import cv2 as cv res , points = cv . rotatedrectangleintersection ( ( ( <number> , <number> ) , ( <number> , <number> ) , - <number> ) , ( ( <number> , <number> ) , ( <number> , <number> ) , <number> ) , ) assert res = = <number> assert points is not none points = points . reshape ( - <number> , <number> ) area = cv . contourarea ( points ) print ( area ) # expected ~ <number> , got ~ <number> ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"dnn / cuda ' abcd op 1 b11 ' broadcast eltwise operator support cuda this pr will fix # <number> current implement is a temp impl . i will try to make more eltwise broadcast cases support cuda . the inference time of [ model ] ( <url> is from * * <number> ms * * to * * <number> ms * * . * * perf_test result * * run this script to generate result ` ` ` shell . \ \ bin \ \ opencv_perf_dnn . exe ' - - gtest_filter = cuda / layer_naryeltwise . */ * : cuda / layer_naryeltwise / * . * ' - - gtest_output = xml : . <repeated> / tmp / 1 th . xml - - perf_threads = <number> ` ` ` use this script to generate summary ` ` ` shell python . <repeated> / modules / ts / misc / summary . py - m min 1 th . xml 0 th . xml - o markdown ` ` ` result | name of test |0 th | 1 th | 1 th vs 0 th (x - factor ) | | - - - <sad> - <sad> : - <sad> : - <sad> | nhwc_h : : cuda / layer_naryeltwise : : cuda / cuda | <number> ( fallback to cpu ) | <number> | <number> | * * layer by layer data : * * - before being fixed ` ` ` shell onnx_noderesnet18 / 0 _conv / conv2d <number> . 1 5 1 5 ms onnx_node ! resnet18 / 0 _prelu / relu <number> . 0 1 9 3 ms onnx_node ! resnet18 / 0 _prelu / neg_1 <number> . 0 1 4 5 ms onnx_node ! resnet18 / 0 _prelu / relu_1 <number> . 0 1 2 1 ms resnet18 / 0 _prelu / neg : <number> <number> . 0 1 6 7 ms onnx_node ! resnet18 / 0 _prelu / mul <number> . 0 7 1 ms onnx_node ! resnet18 / 0 _prelu / add <number> . 0 5 8 5 ms onnx_node ! resnet18 / stack1_block1_shortcut_conv / conv2d <number> . 1 6 4 3 ms onnx_node ! resnet18 / stack1_block1_1_bn / fusedbatchnormv3 <number> . 0 1 6 6 ms onnx_node ! resnet18 / stack1_block1_1_conv / conv2d <number> . 1 1 7 9 ms onnx_node ! resnet18 / stack1_block1_2_prelu / relu <number> . 0 1 9 2 ms onnx_node ! resnet18 / stack1_block1_2_prelu / neg_1 <number> . 0 1 1 4 ms onnx_node ! resnet18 / stack1_block1_2_prelu / relu_1 <number> . 0 0 9 5 ms onnx_node ! resnet18 / stack1_block1_2_prelu / mul <number> . 0 5 2 2 ms onnx_node ! resnet18 / stack1_block1_2_prelu / add <number> . 0 8 5 7 ms onnx_node ! resnet18 / stack1_block1_2_conv / conv2d <number> . 1 8 0 3 ms onnx_node ! resnet18 / stack1_block2_1_bn / fusedbatchnormv3 <number> . 0 1 3 ms onnx_node ! resnet18 / stack1_block2_1_conv / conv2d <number> . 0 5 3 3 ms onnx_node ! resnet18 / stack1_block2_2_prelu / relu <number> . 0 1 4 5 ms onnx_node ! resnet18 / stack1_block2_2_prelu / neg_1 <number> . 0 1 1 6 ms onnx_node ! resnet18 / stack1_block2_2_prelu / relu_1 <number> . 0 0 9 3 ms onnx_node ! resnet18 / stack1_block2_2_prelu / mul <number> . 3 4 6 ms onnx_node ! resnet18 / stack1_block2_2_prelu / add <number> . 0 4 8 3 ms onnx_node ! resnet18 / stack1_block2_2_conv / conv2d <number> . 0 7 4 8 ms onnx_node ! resnet18 / stack2_block1_shortcut_conv / conv2d <number> . 1 0 1 5 ms onnx_node ! resnet18 / stack2_block1_1_bn / fusedbatchnormv3 <number> . 0 1 3 5 ms onnx_node ! resnet18 / stack2_block1_1_conv / conv2d <number> . 0 6 3 9 ms onnx_node ! resnet18 / stack2_block1_2_prelu / relu <number> . 0 1 6 1 ms onnx_node ! resnet18 / stack2_block1_2_prelu / neg_1 <number> . 0 1 3 7 ms onnx_node ! resnet18 / stack2_block1_2_prelu / relu_1 <number> . 0 1 3 3 ms resnet18 / stack2_block2_2_prelu / neg : <number> <number> . 0 1 7 7 ms onnx_node ! resnet18 / stack2_block1_2_prelu / mul <number> . 7 3 1 8 ms onnx_node ! resnet18 / stack2_block1_2_prelu / add <number> . 0 6 4 3 ms onnx_node ! resnet18 / stack2_block1_2_conv / conv2d <number> . 1 0 8 3 ms onnx_node ! resnet18 / stack2_block2_1_bn / fusedbatchnormv3 <number> . 0 1 3 9 ms onnx_node ! resnet18 / stack2_block2_1_conv / conv2d <number> . 0 4 9 6 ms onnx_node ! resnet18 / stack2_block2_2_prelu / relu <number> . 0 1 4 7 ms onnx_node ! resnet18 / stack2_block2_2_prelu / neg_1 <number> . 0 1 1 5 ms onnx_node ! resnet18 / stack2_block2_2_prelu / relu_1 <number> . 0 0 9 6 ms onnx_node ! resnet18 / stack2_block2_2_prelu / mul <number> . 7 9 ms onnx_node ! resnet18 / stack2_block2_2_prelu / add <number> . 0 4 5 ms onnx_node ! resnet18 / stack2_block2_2_conv / conv2d <number> . 0 7 0 1 ms onnx_node ! resnet18 / stack3_block1_shortcut_conv / conv2d <number> . 0 7 7 6 ms onnx_node ! resnet18 / stack3_block1_1_bn / fusedbatchnormv3 <number> . 0 1 6 ms onnx_node ! resnet18 / stack3_block1_1_conv / conv2d <number> . 0 4 7 9 ms onnx_node ! resnet18 / stack3_block1_2_prelu / relu <number> . 0 1 5 9 ms onnx_node ! resnet18 / stack3_block1_2_prelu / neg_1 <number> . 0 1 3 5 ms onnx_node ! resnet18 / stack3_block1_2_prelu / relu_1 <number> . 0 1 2 1 ms resnet18 / stack3_block2_2_prelu / neg : <number> <number> . 0 1 7 3 ms onnx_node ! resnet18 / stack3_block1_2_prelu / mul <number> . 1 2 5 1 ms onnx_node ! resnet18 / stack3_block1_2_prelu / add <number> . 0 4 3 ms onnx_node ! resnet18 / stack3_block1_2_conv / conv2d <number> . 0 7 9 3 ms onnx_node ! resnet18 / stack3_block2_1_bn / fusedbatchnormv3 <number> . 0 1 2 ms onnx_node ! resnet18 / stack3_block2_1_conv / conv2d <number> . 0 4 5 8 ms onnx_node ! resnet18 / stack3_block2_2_prelu / relu <number> . 0 1 3 3 ms onnx_node ! resnet18 / stack3_block2_2_prelu / neg_1 <number> . 0 1 0 6 ms onnx_node ! resnet18 / stack3_block2_2_prelu / relu_1 <number> . 0 0 9 1 ms onnx_node ! resnet18 / stack3_block2_2_prelu / mul <number> . 7 7 6 6 ms onnx_node ! resnet18 / stack3_block2_2_prelu / add <number> . 0 4 3 ms onnx_node ! resnet18 / stack3_block2_2_conv / conv2d <number> . 0 7 5 1 ms onnx_node ! resnet18 / stack4_block1_shortcut_conv / conv2d <number> . 0 7 5 8 ms onnx_node ! resnet18 / stack4_block1_1_bn / fusedbatchnormv3 <number> . 0 1 5 3 ms onnx_node ! resnet18 / stack4_block1_1_conv / conv2d <number> . 0 4 8 ms onnx_node ! resnet18 / stack4_block1_2_prelu / relu <number> . 0 1 5 1 ms onnx_node ! resnet18 / stack4_block1_2_prelu / neg_1 <number> . 0 1 3 ms onnx_node ! resnet18 / stack4_block1_2_prelu / relu_1 <number> . 0 1 2 ms resnet18 / stack4_block1_2_prelu / neg : <number> <number> . 0 1 7 6 ms onnx_node ! resnet18 / stack4_block1_2_prelu / mul <number> . 1 1 6 3 ms onnx_node ! resnet18 / stack4_block1_2_prelu / add <number> . 0 3 9 6 ms onnx_node ! resnet18 / stack4_block1_2_conv / conv2d <number> . 0 7 5 1 ms onnx_node ! resnet18 / stack4_block2_1_bn / fusedbatchnormv3 <number> . 0 1 2 1 ms onnx_node ! resnet18 / stack4_block2_1_conv / conv2d <number> . 0 4 8 5 ms onnx_node ! resnet18 / stack4_block2_2_prelu / relu <number> . 0 1 5 8 ms onnx_node ! resnet18 / stack4_block2_2_prelu / neg_1 <number> . 0 1 3 ms onnx_node ! resnet18 / stack4_block2_2_prelu / relu_1 <number> . 0 1 2 1 ms onnx_node ! resnet18 / stack4_block2_2_prelu / mul <number> . 0 3 5 1 ms onnx_node ! resnet18 / stack4_block2_2_prelu / add <number> . 0 3 7 ms onnx_node ! resnet18 / stack4_block2_2_conv / conv2d <number> . 0 7 2 ms onnx_node ! resnet18 / e_batchnorm / fusedbatchnormv3 <number> . 0 1 4 2 ms onnx_node ! resnet18 / e_batchnorm / fusedbatchnormv3__210 <number> . 0 1 6 9 ms onnx_node ! resnet18 / e_flatten / reshape <number> . 0 0 1 4 ms onnx_node ! resnet18 / e_dense / matmul <number> . 0 4 4 5 ms resnet18 / e_batchnorm / readvariableop_1 : <number> <number> . 0 1 6 5 ms onnx_node ! resnet18 / pre_embedding / batchnorm / mul_1 <number> . 0 1 5 6 ms embedding <number> . 0 0 1 ms ` ` ` - after being fixed ` ` ` shell onnx_node ! resnet18 / 0 _conv / conv2d <number> . 2 5 5 ms onnx_node ! resnet18 / 0 _prelu / relu <number> . 0 3 0 9 ms onnx_node ! resnet18 / 0 _prelu / neg_1 <number> . 0 1 8 1 ms onnx_node ! resnet18 / 0 _prelu / relu_1 <number> . 0 1 4 7 ms resnet18 / 0 _prelu / neg : <number> <number> . 0 5 3 9 ms onnx_node ! resnet18 / 0 _prelu / mul <number> . 0 2 7 6 ms onnx_node ! resnet18 / 0 _prelu / add <number> . 0 1 8 ms onnx_node ! resnet18 / stack1_block1_shortcut_conv / conv2d <number> . 1 7 1 8 ms onnx_node ! resnet18 / stack1_block1_1_bn / fusedbatchnormv3 <number> . 0 2 1 5 ms onnx_node ! resnet18 / stack1_block1_1_conv / conv2d <number> . 1 7 6 2 ms onnx_node ! resnet18 / stack1_block1_2_prelu / relu <number> . 0 2 0 1 ms onnx_node ! resnet18 / stack1_block1_2_prelu / neg_1 <number> . 0 1 5 6 ms onnx_node ! resnet18 / stack1_block1_2_prelu / relu_1 <number> . 0 1 4 2 ms onnx_node ! resnet18 / stack1_block1_2_prelu / mul <number> . 0 1 9 9 ms onnx_node ! resnet18 / stack1_block1_2_prelu / add <number> . 0 4 7 8 ms onnx_node ! resnet18 / stack1_block1_2_conv / conv2d <number> . 1 1 9 8 ms onnx_node ! resnet18 / stack1_block2_1_bn / fusedbatchnormv3 <number> . 0 1 3 9 ms onnx_node ! resnet18 / stack1_block2_1_conv / conv2d <number> . 2 3 3 4 ms onnx_node ! resnet18 / stack1_block2_2_prelu / relu <number> . 0 2 4 4 ms onnx_node ! resnet18 / stack1_block2_2_prelu / neg_1 <number> . 0 2 3 8 ms onnx_node ! resnet18 / stack1_block2_2_prelu / relu_1 <number> . 0 1 9 6 ms onnx_node ! resnet18 / stack1_block2_2_prelu / mul <number> . 0 2 5 6 ms onnx_node ! resnet18 / stack1_block2_2_prelu / add <number> . 0 2 0 4 ms onnx_node ! resnet18 / stack1_block2_2_conv / conv2d <number> . 1 1 0 1 ms onnx_node ! resnet18 / stack2_block1_shortcut_conv / conv2d <number> . 1 6 4 1 ms onnx_node ! resnet18 / stack2_block1_1_bn / fusedbatchnormv3 <number> . 0 2 9 6 ms onnx_node ! resnet18 / stack2_block1_1_conv / conv2d <number> . 0 8 6 7 ms onnx_node ! resnet18 / stack2_block1_2_prelu / relu <number> . 0 2 5 3 ms onnx_node ! resnet18 / stack2_block1_2_prelu / neg_1 <number> . 0 2 2 3 ms onnx_node ! resnet18 / stack2_block1_2_prelu / relu_1 <number> . 0 2 0 8 ms resnet18 / stack2_block2_2_prelu / neg : <number> <number> . 0 3 3 7 ms onnx_node ! resnet18 / stack2_block1_2_prelu / mul <number> . 0 3 3 4 ms onnx_node ! resnet18 / stack2_block1_2_prelu / add <number> . 0 3 0 6 ms onnx_node ! resnet18 / stack2_block1_2_conv / conv2d <number> . 1 6 0 5 ms onnx_node ! resnet18 / stack2_block2_1_bn / fusedbatchnormv3 <number> . 0 2 6 6 ms onnx_node ! resnet18 / stack2_block2_1_conv / conv2d <number> . 0 9 0 4 ms onnx_node ! resnet18 / stack2_block2_2_prelu / relu <number> . 0 7 1 2 ms onnx_node ! resnet18 / stack2_block2_2_prelu / neg_1 <number> . 0 3 0 5 ms onnx_node ! resnet18 / stack2_block2_2_prelu / relu_1 <number> . 0 2 3 7 ms onnx_node ! resnet18 / stack2_block2_2_prelu / mul <number> . 0 2 9 9 ms onnx_node ! resnet18 / stack2_block2_2_prelu / add <number> . 0 2 5 7 ms onnx_node ! resnet18 / stack2_block2_2_conv / conv2d <number> . 1 6 4 8 ms onnx_node ! resnet18 / stack3_block1_shortcut_conv / conv2d <number> . 1 4 7 ms onnx_node ! resnet18 / stack3_block1_1_bn / fusedbatchnormv3 <number> . 0 2 6 9 ms onnx_node ! resnet18 / stack3_block1_1_conv / conv2d <number> . 0 8 0 5 ms onnx_node ! resnet18 / stack3_block1_2_prelu / relu <number> . 0 2 7 4 ms onnx_node ! resnet18 / stack3_block1_2_prelu / neg_1 <number> . 0 2 1 4 ms onnx_node ! resnet18 / stack3_block1_2_prelu / relu_1 <number> . 0 9 6 9 ms resnet18 / stack3_block2_2_prelu / neg : <number> <number> . 0 3 ms onnx_node ! resnet18 / stack3_block1_2_prelu / mul <number> . 0 2 7 2 ms onnx_node ! resnet18 / stack3_block1_2_prelu / add <number> . 0 2 4 7 ms onnx_node ! resnet18 / stack3_block1_2_conv / conv2d <number> . 1 3 1 6 ms onnx_node ! resnet18 / stack3_block2_1_bn / fusedbatchnormv3 <number> . 0 2 4 1 ms onnx_node ! resnet18 / stack3_block2_1_conv / conv2d <number> . 0 7 9 2 ms onnx_node ! resnet18 / stack3_block2_2_prelu / relu <number> . 0 2 5 9 ms onnx_node ! resnet18 / stack3_block2_2_prelu / neg_1 <number> . 0 2 1 3 ms onnx_node ! resnet18 / stack3_block2_2_prelu / relu_1 <number> . 0 9 6 2 ms onnx_node ! resnet18 / stack3_block2_2_prelu / mul <number> . 0 6 3 3 ms onnx_node ! resnet18 / stack3_block2_2_prelu / add <number> . 0 2 4 6 ms onnx_node ! resnet18 / stack3_block2_2_conv / conv2d <number> . 1 1 3 1 ms onnx_node ! resnet18 / stack4_block1_shortcut_conv / conv2d <number> . 1 0 2 8 ms onnx_node ! resnet18 / stack4_block1_1_bn / fusedbatchnormv3 <number> . 0 2 7 3 ms onnx_node ! resnet18 / stack4_block1_1_conv / conv2d <number> . 0 8 3 4 ms onnx_node ! resnet18 / stack4_block1_2_prelu / relu <number> . 0 3 1 ms onnx_node ! resnet18 / stack4_block1_2_prelu / neg_1 <number> . 1 0 3 1 ms onnx_node ! resnet18 / stack4_block1_2_prelu / relu_1 <number> . 0 8 5 8 ms resnet18 / stack4_block1_2_prelu / neg : <number> <number> . 0 3 2 ms onnx_node ! resnet18 / stack4_block1_2_prelu / mul <number> . 0 3 3 3 ms onnx_node ! resnet18 / stack4_block1_2_prelu / add <number> . 0 2 2 9 ms onnx_node ! resnet18 / stack4_block1_2_conv / conv2d <number> . 1 6 0 9 ms onnx_node ! resnet18 / stack4_block2_1_bn / fusedbatchnormv3 <number> . 0 3 3 6 ms onnx_node ! resnet18 / stack4_block2_1_conv / conv2d <number> . 0 8 6 9 ms onnx_node ! resnet18 / stack4_block2_2_prelu / relu <number> . 0 3 1 4 ms onnx_node ! resnet18 / stack4_block2_2_prelu / neg_1 <number> . 0 2 3 5 ms onnx_node ! resnet18 / stack4_block2_2_prelu / relu_1 <number> . 0 2 3 6 ms onnx_node ! resnet18 / stack4_block2_2_prelu / mul <number> . 0 3 6 8 ms onnx_node ! resnet18 / stack4_block2_2_prelu / add <number> . 0 2 3 4 ms onnx_node ! resnet18 / stack4_block2_2_conv / conv2d <number> . 1 9 1 3 ms onnx_node ! resnet18 / e_batchnorm / fusedbatchnormv3 <number> . 0 2 6 9 ms onnx_node ! resnet18 / e_batchnorm / fusedbatchnormv3__210 <number> . 0 2 3 4 ms onnx_node ! resnet18 / e_flatten / reshape <number> . 0 0 1 6 ms onnx_node ! resnet18 / e_dense / matmul <number> . 1 4 7 2 ms resnet18 / e_batchnorm / readvariableop_1 : <number> <number> . 0 6 3 5 ms onnx_node ! resnet18 / pre_embedding / batchnorm / mul_1 <number> . 0 6 9 2 ms embedding <number> . 0 0 1 9 ms ` ` ` # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"opencv ( <number> . <number> ) error : gpu api call ( cuda_error_file_not_found [ code = <number> ] # # # system information opencv - <number> . <number> opencv_contrib - <number> . <number> video_codec_sdk_11 . <number> cmake <number> . <number> win <number> # # # detailed description hello , i followed the online tutorial to compile opencv hard decoding ; i will first attempt to open a local mp4 file for hard decoding testing , which can be run through ; when i switch to real - time video rtsp / rtmp , the following issues will occur : opencv ( <number> . <number> ) error : gpu api call ( cuda_error_file_not_found [ code = <number> ] ) in cv : : cudacodec : : detail : : cuvidvideosource : : cuvidvideosource , file e :\\ software \ \ opencv4 . <number> . build \ \ opencv_contrib - <number> . <number> \ \ modules \ \ cudacodec \ \ src \ \ cuvid_video_source . cpp , line <number> # # # steps to reproduce / / local mp4 video is ok std : : string fname = "" rtmp :// ns8 . indexforce . com / home / mystream "" ; std : : cout < < "" gpu个数为 < < cv : : cuda : : getcudaenableddevicecount ( ) < < std : : endl ; cv : : cuda : : gpumat d_frame ; / / error cv : : ptr < cv : : cudacodec : : videoreader > d_reader = cv : : cudacodec : : createvideoreader ( fname ) ; # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"solve the "" unrecognized selector "" issue in objective - c / swift binding # # # system information opencv version : <number> . x latest operating system / platform any # # # detailed description as originally reported in <url> there is a frequently occurring problem when trying to use the * *-[ mat touiimage ] * * method and other similar methods . the "" unrecognized selector sent to instance "" error can be worked round by modifying the "" other linker flags "" but its a pain to always have to do this . other functionality like debug quicklook ( <url> also requires these flags to be set in order to work correctly and there is no hint in the ui to indicate that this is the case . to solve the problem we need to move the functionality added to * mat <emphasis> * using categories in * * mat + converters * * and * * mat + quicklook * * directly into the * mat <emphasis> * implemenation # # # steps to reproduce refer to <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"error in ` generation_onnx_models . py ` onnx test data generation script . # # # system information hi running ` python generate_onnx_models . py ` inside ` opencv_extra / testdata / dnn / onnx ` to generate new test models and data results in error it seem that ` postprocess_model ( ) ` function is not located in wrong place opencv python version : <date> operating system / platform : ubuntu <number> , focal python version : <date> # # # detailed description ` ` ` python . . . = = = = = = = = = = = = = = diagnostic run torch . onnx . export version <number> . <number> + cpu = = = = = = = = = = = = = = verbose : false , log level : level . error = = = = = = = = = = = = = = = = = = = = = = = <number> none <number> note <number> warning <number> error = = = = = = = = = = = = = = = = = = = = = = = = slice_opset_11_steps_2d input has sizes torch . size ( [ <number> , <number> ] ) slice_opset_11_steps_2d output has sizes torch . size ( [ <number> , <number> ] ) = = = = = = = = = = = = = = diagnostic run torch . onnx . export version <number> . <number> + cpu = = = = = = = = = = = = = = verbose : false , log level : level . error = = = = = = = = = = = = = = = = = = = = = = = <number> none <number> note <number> warning <number> error = = = = = = = = = = = = = = = = = = = = = = = = traceback ( most recent call last ) : file "" generate_onnx_models . py "" , line <number> , in <module> postprocess_model ( "" models / slice_opset_11_steps_2d . onnx "" , [ [ ' height ' , ' width ' ] ] ) nameerror ' postprocess_model ' is not defined ` ` ` # # # steps to reproduce running ` python generate_onnx_models . py ` inside ` opencv_extra / testdata / dnn / onnx ` to generate new test models and data results in error # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",0
opencv/opencv,"fix aruco module corner_refine_contour parameter gets skipped # # # pull request readiness checklist hi <user> the aruco module is using the wrong variable to check whether it should perform corner refinement . the previous buggy version of code since opencv <number> is using the candidates supplied in the function argument from user instead of the candidates returned from ` _identifycandidates ` . our current workaround is always to run the detectmarkers twice , so the second time will use the ids returned from the first run , however that workaround is not optimal and greatly hurt the performance . could you please take a look and get this merged ? thanks see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work <url> - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"dnn : support the split node of onnx opset >= <number> merge with test case : <url> the attribute of ` split ` in ` split layer ` has been moved from ` attribute ` to ` input ` . related link the purpose of this pr is to support the ` split ` with input type . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",0
opencv/opencv,"using the java version of opencv , the camera encountered a memory overflow issue while iteratively fetching images # # # system information opencv python version : <number> operating system / platform : windows <number> java version : <number> # # # detailed description exception in thread "" main "" java . lang . exception : std : : exception allocation at org . opencv . imgcodecs . imgcodecs . imencode_1 ( native method ) at org . opencv . imgcodecs . imgcodecs . imencode ( imgcodecs . java : <number> ) at com . benfei . grab_callback . main ( grab_callback . java : <number> ) # # # steps to reproduce when i loop through the code below from the camera , there will be a memory overflow bug matofbyte mob = new matofbyte ( ); imencode ( "" . jpg "" , mat , mob ) ; byte [ ] array = mob . toarray ( ); # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"error compiling opencv with cuda at <percent> , please help # # # system information i put the following <number> commands to compile opencv with cuda : wget - o opencv . zip <url> ; wget - o opencv_contrib . zip <url> ; unzip opencv . zip ; unzip opencv_contrib . zip ; mv opencv_contrib - <number> . <number> opencv_contrib ; cd opencv - <number> . <number> / build / ; and : cmake - d cmake_build_type = release - d with_cuda = on - d opencv_dnn_cuda = on - d cuda_arch_bin = <number> - d cmake_c_compiler <annoyed> usr / bin / gcc - <number> - d cmake_install_prefix <annoyed> usr / local - d build_opencv_python2 = off - d build_opencv_java = off - d python3_executable <annoyed> home / dani / anaconda3 / bin / python3 - d with_tbb = on - d enable_fast_math = <number> - d cuda_fast_math = <number> - d with_cublas = <number> - d build_opencv_cudacodec = on - d with_cudnn = on - d cudnn_version = <number> . <number> - d with_v4l = on - d with_qt = on - d with_opengl = on - d with_gstreamer = on - d opencv_generate_pkgconfig = yes - d opencv_pc_file_name = opencv . pc - d opencv_enable_nonfree = on - d opencv_python3_install_path <annoyed> home / dani / anaconda3 / lib / python3 . <number> / site - packages - d python_executable <annoyed> home / dani / anaconda3 / bin / python - d opencv_extra_modules_path <annoyed> home / dani / opencv_contrib / modules - d install_python_examples = off - d install_c_examples = off - d build_examples = off … \ \ and at <percent> i had the next error : error : conversion from ‘ cv : : cuda : : stream ’ to non - scalar type ‘ cv : : ptrcv : : cuda : : stream ’ requested <number> | ptrcv : : cuda : : stream stream = cuda : : stream : : null ( ); my cuda version is <number> , my ubuntu version is <number> , my cudnn is <number> . <number> and opencv is <number> thanks in advance for help ! <repeated> # # # detailed description i put the following <number> commands to compile opencv with cuda : wget - o opencv . zip <url> ; wget - o opencv_contrib . zip <url> ; unzip opencv . zip ; unzip opencv_contrib . zip ; mv opencv_contrib - <number> . <number> opencv_contrib ; cd opencv - <number> . <number> / build / ; and : cmake - d cmake_build_type = release - d with_cuda = on - d opencv_dnn_cuda = on - d cuda_arch_bin = <number> - d cmake_c_compiler <annoyed> usr / bin / gcc - <number> - d cmake_install_prefix <annoyed> usr / local - d build_opencv_python2 = off - d build_opencv_java = off - d python3_executable <annoyed> home / dani / anaconda3 / bin / python3 - d with_tbb = on - d enable_fast_math = <number> - d cuda_fast_math = <number> - d with_cublas = <number> - d build_opencv_cudacodec = on - d with_cudnn = on - d cudnn_version = <number> . <number> - d with_v4l = on - d with_qt = on - d with_opengl = on - d with_gstreamer = on - d opencv_generate_pkgconfig = yes - d opencv_pc_file_name = opencv . pc - d opencv_enable_nonfree = on - d opencv_python3_install_path <annoyed> home / dani / anaconda3 / lib / python3 . <number> / site - packages - d python_executable <annoyed> home / dani / anaconda3 / bin / python - d opencv_extra_modules_path <annoyed> home / dani / opencv_contrib / modules - d install_python_examples = off - d install_c_examples = off - d build_examples = off … \ \ and at <percent> i had the next error : error : conversion from ‘ cv : : cuda : : stream ’ to non - scalar type ‘ cv : : ptrcv : : cuda : : stream ’ requested <number> | ptrcv : : cuda : : stream stream = cuda : : stream : : null ( ); my cuda version is <number> , my ubuntu version is <number> , my cudnn is <number> . <number> and opencv is <number> thanks in advance for help ! <repeated> # # # steps to reproduce i put the following <number> commands to compile opencv with cuda : wget - o opencv . zip <url> ; wget - o opencv_contrib . zip <url> ; unzip opencv . zip ; unzip opencv_contrib . zip ; mv opencv_contrib - <number> . <number> opencv_contrib ; cd opencv - <number> . <number> / build / ; and : cmake - d cmake_build_type = release - d with_cuda = on - d opencv_dnn_cuda = on - d cuda_arch_bin = <number> - d cmake_c_compiler <annoyed> usr / bin / gcc - <number> - d cmake_install_prefix <annoyed> usr / local - d build_opencv_python2 = off - d build_opencv_java = off - d python3_executable <annoyed> home / dani / anaconda3 / bin / python3 - d with_tbb = on - d enable_fast_math = <number> - d cuda_fast_math = <number> - d with_cublas = <number> - d build_opencv_cudacodec = on - d with_cudnn = on - d cudnn_version = <number> . <number> - d with_v4l = on - d with_qt = on - d with_opengl = on - d with_gstreamer = on - d opencv_generate_pkgconfig = yes - d opencv_pc_file_name = opencv . pc - d opencv_enable_nonfree = on - d opencv_python3_install_path <annoyed> home / dani / anaconda3 / lib / python3 . <number> / site - packages - d python_executable <annoyed> home / dani / anaconda3 / bin / python - d opencv_extra_modules_path <annoyed> home / dani / opencv_contrib / modules - d install_python_examples = off - d install_c_examples = off - d build_examples = off … \ \ and at <percent> i had the next error : error from ‘ cv : : cuda : : stream ’ to non - scalar type ‘ cv : : ptrcv : : cuda : : stream ’ requested <number> | ptrcv : : cuda : : stream stream = cuda : : stream : : null ( ); my cuda version is <number> , my ubuntu version is <number> , my cudnn is <number> . <number> and opencv is <number> thanks in advance for help ! <repeated> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"can not set camera to 4 k resolution # # # system information opencv python version : <number> . <number> operating system / platform : ubuntu <number> python version : <date> camera model : dell ultrasharp wb7022 # # # detailed description i can not set the camera resolution to 4 k with the normal method : ` ` ` import cv2 cap = cv2 . videocapture ( <number> ) print ( cap . isopened ( ) ) cap . set ( <number> ) cap . set ( <number> ) ret , frame = cap . read ( ) cv2 . imshow ( ' display ' , frame ) print ( frame . shape ) if cv2 . waitkey ( <number> ) & 0 xff = = ord ( ' q ' <sad> exit ( <number> ) ` ` ` this prints : true ( <number> , <number> , <number> ) the same code works on windows and the cap is successfully set to 4 k this is the output of v4l2 - ctl - d0 - - list - formats - ext > ioctl : vidioc_enum_fmt > type : video capture > > [ <number> <sad> ' yuyv ' ( yuyv <number> : <number> : <number> ) > size : discrete 6 4 0 x480 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 6 4 0 x360 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 1 2 8 0 x720 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 1 9 2 0 x1080 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > [ <number> <sad> ' mjpg ' ( motion - jpeg , compressed ) > size : discrete 6 4 0 x480 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 1 2 8 0 x720 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > interval : discrete <number> . 0 1 7 s ( <number> fps ) > size : discrete 1 9 2 0 x1080 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > interval : discrete <number> . 0 1 7 s ( <number> fps ) > size : discrete 2 5 6 0 x1440 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 3 8 4 0 x2160 > interval : discrete <number> . 0 4 2 s ( <number> fps ) > interval : discrete <number> . 0 3 3 s ( <number> fps ) > [ <number> <sad> ' nv12 ' ( y / cbcr <number> : <number> : <number> ) > size : discrete 6 4 0 x480 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 6 4 0 x360 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 1 2 8 0 x720 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > size : discrete 1 9 2 0 x1080 > interval : discrete <number> . 0 3 3 s ( <number> fps ) > i have tried this line : ` cap = cv2 . videocapture ( <number> , cv2 . cap_opencv_mjpeg ) ` but i get this error : > [ warn : <number> <user> . <number> ] global cap . cpp : <number> open videoio ( cv_mjpeg ) : backend is generally available but can not be used to capture by index > false > traceback ( most recent call last ) : > file "" test_open_camera . py "" , line <number> , in <module> > cv2 . imshow ( ' display ' , frame ) > cv2 . error / io with software like webcamoid , if i set the video format to mjpg then i can get 4 k , this is not the case with yuyv . any help would be greatly appreciated . # # # steps to reproduce i do not know if it can be reproduced by other cameras # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"color_rgb2lab is not reliable for float32 values # # # system information opencv python version : <number> . <number> operating system / platform : win11 python version : <number> x64 # # # detailed description the convert color function does not convert all rgb - values to the correct corresponding l a <emphasis> b * - values if the rgb values are of type float32 . # # # steps to reproduce import numpy as np import cv2 as cv color = np . array ( [ [ [ <number> , <number> ] ] ] ) . astype ( np . float32 ) lab_color_cv = cv . cvtcolor ( color , cv . color_rgb2lab ) print ( lab_color_cv ) output is : [ [ [ <number> . <number> . <number> . ] ] ] should be <number> <number> - <number> ] ] ] any value from <number> to <number> for the green value in rgb will lead to the wrong output of [ [ [ <number> , <number> , <number> ] ] ] while [ [ [ <number> , <number> ] ] ] and [ [ [ <number> , <number> ] ] ] are correct # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"cross compiling arm64 in windows x86_64 fails # # # system information opencv version : <number> . <number> os : windows <number> compiler & version ( as reported by cmake ) - - building for : visual studio <number> <number> - - the c compiler identification is msvc <number> . <number> - - the cxx compiler identification is msvc <number> . <number> # # # detailed description i am trying to cross - compile : achieve arm64 binaries on a x64 windows <number> desktop . i create the developer shell for windows visual c + + compilation and linking like this : ` ` ` c :\\ windows \ \ system32 \ \ cmd . exe / k "" c :\\ program files \ \ microsoft visual studio \ \ <number> \ \ community \ \ common7 \ \ tools \ \ vsdevcmd . bat "" - arch = arm64 - host_arch = amd64 ` ` ` here is my . bat script to run cmake and to compile & link : ` ` ` : : setting the cl env variable makes no difference : : : set cl <annoyed> d_m_arm64 = <number> : : set cl <annoyed> arch : armv8 . <number> set cl = echo echo extra vars for compiler : % cl % echo set ' cmake_generator_options = - g "" visual studio <number> <number> "" ' set "" cmake_options = - dbuild_perf_tests : bool = off - dbuild_tests : bool = off - dbuild_docs : bool = off - dwith_cuda : bool = off - dbuild_examples : bool = off - dinstall_create_distrib = on - dbuild_opencv_java = off - dbuild_opencv_python = off - dbuild_protobuf = off - dbuild_opencv_dnn = off "" : : this is required to find the correct compiler set "" cmake_architecture = - a arm64 "" cd opencv - <number> . <number> rmdir / s build mkdir build cd build cmake . <repeated> % cmake_generator_options % % cmake_architecture % % cmake_options % : : cmake . <repeated> % cmake_generator_options % % cmake_options % cd . <repeated> cmake - - build build - - config release ` ` ` i can see that cmake picks up the correct compiler allright : ` ` ` . <repeated> - - check for working cxx compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / arm64 / cl . exe - skipped . <repeated> ` ` ` finally , i get tons of these : ` ` ` c :\\ program files \ \ microsoft visual studio \ \ <number> \ \ community \ \ vc \ \ tools \ \ msvc \ \ <number> . <number> \ \ include \ \ emmintrin . h ( <number> <sad> fatal error c11 <number> : <hashtag> error </hashtag> : this header is specific to x86 , x64 , arm64 , and arm64ec targets ( compiling source file c :\\ users \ \ v - sriikonen \ \ open cv \ \ opencv - <number> . <number> \ \ modules \ \ core \ \ src \ \ async . cpp ) [ c :\\ users \ \ v - sriikonen \ \ opencv \ \ opencv - <number> . <number> \ \ build \ \ modules \ \ world \ \ opencv_world . vcxpro j ] ` ` ` with emphasis on : ` ` ` fatal error c1189 : <hashtag> error </hashtag> : this header is specific to x86 , x64 , arm64 , and arm64ec targets ` ` ` which is stupid , since we are indeed compiling for ` ` arm64 ` ` target . origin of the problem is in the header files <hashtag> if </hashtag> ! defined ( _m_ix86 ) & & ! defined ( _m_x64 ) & & ! ( defined ( _m_arm64 ) & & defined ( use_soft_intrinsics ) ) <hashtag> error </hashtag> this header is specific to x86 , x64 , arm64 , and arm64ec targets <hashtag> end if </hashtag> ` ` ` so it seems that ` ` _m_arm64 ` ` preprocessor directive is lost somewhere on the way - indeed i can not find it in any of the cache files produced by cmake . # # # steps to reproduce ( see above ) # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"does "" cv : : videocapture "" have any thread lock ? # # # system information opencv python version : <number> . <number> operating system / platform : macos <number> & & windows <number> python version : <number> . <number> # # # detailed description videocapture multithreading # # # steps to reproduce - when i using threadpool , ` ` ` python import cv2 import time from concurrent . futures import threadpoolexecutor as poolexecutor def open_capture ( i ) : capture = cv2 . videocapture ( ) begin_time = time . time ( ) capture . open ( ' <url> if capture . isopened ( <sad> print ( f ' [ open capture { i } ] <sad> time . time ( ) - begin_time } s ' ) if __name__ = = ' __main__ ' : pool = poolexecutor ( <number> ) pool . submit ( open_capture , <number> ) pool . submit ( open_capture , <number> ) pool . shutdown ( ) ` ` ` ` ` ` [ open capture <number> ] : <number> . 0 3 7 6 3 1 0 3 4 8 5 1 0 7 4 s [ open capture <number> ] : <number> . 2 6 7 1 6 6 8 5 2 9 5 1 0 5 s ` ` ` ` ` ` [ open capture <number> ] : <number> . 4 7 3 8 4 0 9 5 1 9 1 9 5 5 5 7 s [ open capture <number> ] : <number> . 7 9 4 5 2 4 9 0 8 0 6 5 7 9 6 s ` ` ` ` ` ` [ open capture <number> ] : <number> . 3 1 0 3 3 4 9 2 0 8 8 3 1 7 8 7 s [ open capture <number> ] : <number> . 6 2 9 1 0 5 8 0 6 3 5 0 7 0 8 s ` ` ` - when i using processpool , they are opened almost at the same time . ` ` ` python import cv2 import time from concurrent . futures import processpoolexecutor as poolexecutor def open_capture ( i ) : capture = cv2 . videocapture ( ) begin_time = time . time ( ) capture . open ( ' <url> if capture . isopened ( <sad> print ( f ' [ open capture { i } ] <sad> time . time ( ) - begin_time } s ' ) if __name__ = = ' __main__ ' = poolexecutor ( <number> ) pool . submit ( open_capture , <number> ) pool . submit ( open_capture , <number> ) pool . shutdown ( ) ` ` ` ` ` ` [ open capture <number> ] : <number> . 9 3 1 2 0 7 8 9 5 2 7 8 9 3 0 7 s [ open capture <number> ] : <number> . 9 1 3 4 5 1 9 1 0 0 1 8 9 2 1 s ` ` ` ` ` ` [ open capture <number> ] : <number> . 5 7 6 0 6 5 0 6 3 4 7 6 5 6 2 5 s [ open capture <number> ] : <number> . 5 8 8 1 6 0 9 9 1 6 6 8 7 0 1 2 s ` ` ` ` ` ` [ open capture <number> ] : <number> . 0 8 4 3 5 6 7 8 4 8 2 0 5 5 6 6 s [ open capture <number> ] : <number> . 0 8 0 4 7 5 0 9 1 9 3 4 2 0 4 s ` ` ` # # # issue submission checklist - [ ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"opencv includes not found after build # # # describe the doc issue hello , i built the opencv using cmake . now i need to add opencv into my application and call its functions . i could not find the include \ \ version . hpp to include in my application . ( header file ) your input is appreciated . # # # fix suggestion hello , i have build the opencv using camke . now i need to refer opencv in my c application . i need to call the opencv functions from my application . i could not find the include \ \ version . hpp to include in my application . appreciate your input .",2
opencv/opencv,"can cv2 . videocapture read <number> channel video ? # # # describe the doc issue i want to read <number> channel video , using : ` ` ` cv2 . videocapture ( path ) while true frame = video . read ( ) ` ` ` but , i find the frame is <number> channel , # # # fix suggestion _no response_",2
opencv/opencv,"facerecognizersf . feature ( alingcrop , feature ) always return the same features for different faces # # # system information [ opencv version : <number> . <number> ] ( <url> operating system / platform : win11 ` ` ` <dependency> <groupid> org . bytedeco </groupid> <artifactid> opencv </artifactid> <version> <number> . <number> - <number> . <number> </version> < - - <classifier> macosx - x86_64 </classifier> - - > <classifier> windows - x86_64 </classifier> </dependency> ` ` ` # # # detailed description ` ` ` facerecognizersf . feature ( alingcrop , feature ) ` ` ` traverse different images to extract features , and the returned dataaddr is always the same . ` ` ` java mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af89090 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af89a90 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8aa30 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af89db0 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af88eb0 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8a350 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8a990 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8d690 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8c790 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8bbb0 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8b930 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8bd90 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8db90 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8d730 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8c150 , dataaddr =0 x2b67b2f7540 ] mat [ <number> * <number> * cv_32fc1 , iscont = true , issubmat = true , nativeobj =0 x2b67af8d870 , dataaddr =0 x2b67b2f7540 ] ` ` ` # # # steps to reproduce reproduction demo [ face_detection_yunet_2022mar_int8 . onnx ] ( <url> [ face_recognition_sface_2021dec_int8 . onnx ] ( <url> ` ` ` java package com . example . asyncdemo . sface ; import cn . hutool . core . io . fileutil ; import lombok . extern . slf4j . slf4j ; import org . opencv . core . mat ; import org . opencv . core . rect ; import org . opencv . core . size ; import org . opencv . imgcodecs . imgcodecs ; import org . opencv . objdetect . facedetectoryn ; import org . opencv . objdetect . facerecognizersf ; import java . io . file ; import java . util . map ; import java . util . concurrent . concurrenthashmap ; <user> public class demo1 { private static facedetectoryn facedetectorynimg ; private static facerecognizersf facerecognizersf ; private static map < string , mat > features = new concurrenthashmap < >(); static { system . load ( "" d :\\\\ asyncdemo \ \ \ \ src \ \ \ \ main \ \ \ \ java \ \ \ \ com \ \ \ \ example \ \ \ \ asyncdemo \ \ \ \ sface \ \ \ \ opencv_java470 . dll "" ); facedetectorynimg = facedetectoryn . create ( "" d :\\\\ asyncdemo \ \ \ \ face_detection_yunet_2022mar_int8 . onnx "" , "" "" , new size ( <number> , <number> ) , ( float ) <number> , ( float ) <number> , <number> ); facerecognizersf = facerecognizersf . create ( "" d :\\\\ asyncdemo \ \ \ \ src \ \ \ \ main \ \ \ \ java \ \ \ \ com \ \ \ \ example \ \ \ \ asyncdemo \ \ \ \ sface \ \ \ \ face_recognition_sface_2021dec_int8 . onnx "" , "" "" ); } public static void main ( string [ ] args ) { initall ( ); } private static void initall ( ) { file [ ] fs = fileutil . ls ( "" d :\\\\ asyncdemo \ \ \ \ src \ \ \ \ main \ \ \ \ java \ \ \ \ com \ \ \ \ example \ \ \ \ asyncdemo \ \ \ \ sface \ \ \ \ images "" ); for ( file file { mat mat = imgcodecs . imread ( file . getabsolutepath ( )); mat feature = new mat ( ); detect ( mat , feature ) ; if ( ! feature . empty ( ) ) { features . put ( file . getname ( ) , feature ) ; } else { log . error ( file . getname ( )); } / / log . info ( "" {}: : { } "" , file . getname ( ) , feature ) ; } log . info ( "" all files : : { } , features . size : : { } "" , fs . length , features . size ( )); } private static mat detect ( mat mat1 , mat feature ) { / / facerecognizersf = facerecognizersf . create ( "" d :\\\\ asyncdemo \ \ \ \ src \ \ \ \ main \ \ \ \ java \ \ \ \ com \ \ \ \ example \ \ \ \ asyncdemo \ \ \ \ sface \ \ \ \ face_recognition_sface_2021dec_int8 . onnx "" , "" "" ); mat face = new mat ( ); mat alingcrop = new mat ( ); mat maxface = null ; double maxsize = <number> ; try { facedetectorynimg . setinputsize ( mat1 . size ( )); facedetectorynimg . detect ( mat1 , face ) ; for ( int i = <number> ; i < face . rows ( ); i + + ) { rect rect = new rect ( ( int ) face . get ( i , <number> ) [ <number> ] , ( int ) face . get ( i , <number> ) [ <number> ] , ( int ) face . get ( i , <number> ) [ <number> ] , ( int ) face . get ( i , <number> ) [ <number> ]); if ( rect . area ( ) > maxsize ) { maxface = face . row ( i ) ; maxsize = rect . area ( ); } } if ( maxface ! = null ) { facerecognizersf . aligncrop ( mat1 , maxface , alingcrop ) ; facerecognizersf . feature ( alingcrop , feature ) ; system . out . println ( feature ) ; } mat1 . release ( ); } catch ( exception e ) { e . printstacktrace ( ); } finally { alingcrop . release ( ); face . release ( ); } return feature ; } } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"imshow not implemented # # # system information * * opencv version * * : <number> . <number> ( also occurs in version <number> . <number> ) * * operating system * * : windows * compiler <emphasis> * : microsoft visual c + + v19 . <number> (x 8 6 ) # # # detailed description * * my code is in c + + . * * i am using the highgui module ( ` <hashtag> include </hashtag> "" opencv2 / highgui / highgui . hpp "" ` ) , and it seems to me that _the ` imshow ` function is not implemented_ . ( compiler error below . ) why am i sure it ' s a bug ? <number> . the function is defined in ` highgui . hpp ` , and the error says it ' s _unresolved_ but not undefined . * * that means there ' s no issue with importing the prototypes . * * <number> . a ` namedwindow ` call from the same module compiles successfully . ( yes , that ' s after commenting out the ` imshow ` call . ) * * that means there ' s no issue with importing the implementations . * * here ' s the full error message : ` ` ` error lnk2019 to unresolved external symbol "" void __cdecl cv : : imshow ( class std : : basic_string < char , struct std : : char_traits <char> , class std : : allocator <char> > const & , class cv : : debug_build_guard : : _inputarray const & ) "" ( ? imshow <user> @ <user> ? $ basic_string <user> ? $ char_traits <user> <user> @ <user> ? $ allocator <user> <user> @ <user> @ <user> <user> <user> @ <user> ) in function main . ` ` ` * * this issue does not occur in opencv <date> . * * # # # steps to reproduce ` ` ` cpp <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> "" opencv2 / opencv . hpp "" <hashtag> include </hashtag> "" opencv2 / highgui / highgui . hpp "" using namespace cv ; int main ( ) { videocapture capture ( cap_any ) ; if ( capture . isopened ( ) ) { std : : cout < < "" cannot open the video stream "" ; return - <number> ; } namedwindow ( "" camera "" ); while ( true ) { mat frame ; capture > > frame ; imshow ( "" camera "" , frame ) ; if ( waitkey ( <number> ) >= <number> ) return <number> ; } } ` ` ` this is my first program working with opencv . it just shows a video stream from the built - in camera . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"rotate parameter for puttext ( ) function # # # describe the feature and motivation i am trying to rotate my puttext ( ) without rotating the whole frame . but it seems like a hard thing to do . is it possible to add a new "" rotate "" parameter to puttext ( ) function ? tia # # # additional context _no response_",2
opencv/opencv,"how to cite opencv ? # # # describe the doc issue i write a paper with opencv , and i want to cite opencv in my paper . so can i cite opencv in my paper . # # # fix suggestion _no response_",2
opencv/opencv,ordering of images in stitcher i was performing a <number> degree images stitch operation using sift for feature extraction and brute force matcher for matching . i observed that the order of images which i provide does not matter as the output i get is in different order . i used following program for stitching [ stitching detailed ] ( <url> questions : can i modify the order of images in matching stage so that i get a particular scene in the centre of my panorama ? do i need to reorder my feature vector before providing it to matcher ? example <number> degree image i recenter the “ river ” to middle ? [ output1 ] ( <url> # # # additional context _no response_,2
opencv/opencv,"error : ' mutex ' in namespace ' cv ' does not name a type and error : ' recursive_mutex ' in namespace ' std ' does not name a type # # # system information opencv version <number> . <number> - dev * * host platform os * * : ubuntu <number> * * cross compiler used for compilation * * - arm - none - eabi - gcc ( version gcc - arm - none - eabi - <number> - <number> ) used cmake - gui for configuring and generation - not able to get the command line equivalent of the build command ( advise me how to get ) * * target platform * * : arm cortex - r5 cpu , am273x high performance r5f cpu mcu from texas instruments [ trm ] ( <url> here is my toolchain_cmake file ` ` ` # set ( cmake_system_name generic ) # set ( cmake_system_processor arm ) # set ( baremetal_arm_toolchain_path / home / kowshik / documents / gcc / gcc - arm - <number> - <number> - x86_64 - arm - none - linux - gnueabihf / bin / ) # # without that flag cmake is not able to pass test compilation check # set ( cmake_try_compile_target_type static_library ) # set ( cmake_ar ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - ar ${ cmake_executable_suffix } ) # set ( cmake_asm_compiler ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - gcc ${ cmake_executable_suffix } ) # set ( cmake_c_compiler ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - gcc ${ cmake_executable_suffix } ) # set ( cmake_cxx_compiler ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - g + + ${ cmake_executable_suffix } ) # set ( cmake_linker ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - ld ${ cmake_executable_suffix } ) # set ( cmake_objcopy ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - objcopy ${ cmake_executable_suffix } cache internal "" "" ) # set ( cmake_ranlib ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - ranlib ${ cmake_executable_suffix } cache internal "" "" ) # set ( cmake_size ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - size ${ cmake_executable_suffix } cache internal "" "" ) # set ( cmake_strip ${ baremetal_arm_toolchain_path } arm - none - linux - gnueabihf - strip ${ cmake_executable_suffix } cache internal "" "" ) # set ( cmake_c_flags "" - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections "" cache internal "" "" ) # set ( cmake_cxx_flags "" ${ cmake_c_flags } - std =c + + <number> - fno - pie - fno - pic - wno - dev - fpermissive - dcmake_crosscompiling "" cache internal "" "" ) # # - - specs = nosys . specs # set ( cmake_c_flags_debug "" - os - g "" cache internal "" "" ) # set ( cmake_c_flags_release "" - os - dndebug "" cache internal "" "" ) # set ( cmake_cxx_flags_debug "" ${ cmake_c_flags_debug } "" cache internal "" "" ) # set ( cmake_cxx_flags_release "" ${ cmake_c_flags_release } "" cache internal "" "" ) # set ( cmake_find_root_path_mode_program never ) # set ( cmake_find_root_path_mode_library only ) # set ( cmake_find_root_path_mode_include only ) # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = set ( cmake_system_name generic ) set ( cmake_system_processor arm ) set ( baremetal_arm_toolchain_path / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / bin / ) # without that flag cmake is not able to pass test compilation check set ( cmake_try_compile_target_type static_library ) set ( cmake_ar ${ baremetal_arm_toolchain_path } arm - none - eabi - ar ${ cmake_executable_suffix } ) set ( cmake_asm_compiler ${ baremetal_arm_toolchain_path } arm - none - eabi - gcc ${ cmake_executable_suffix } ) set ( cmake_c_compiler ${ baremetal_arm_toolchain_path } arm - none - eabi - gcc ${ cmake_executable_suffix } ) set ( cmake_cxx_compiler ${ baremetal_arm_toolchain_path } arm - none - eabi - g + + ${ cmake_executable_suffix } ) set ( cmake_linker ${ baremetal_arm_toolchain_path } arm - none - eabi - ld ${ cmake_executable_suffix } ) set ( cmake_objcopy ${ baremetal_arm_toolchain_path } arm - none - eabi - objcopy ${ cmake_executable_suffix } cache internal "" "" ) set ( cmake_ranlib ${ baremetal_arm_toolchain_path } arm - none - eabi - ranlib ${ cmake_executable_suffix } cache internal "" "" ) set ( cmake_size ${ baremetal_arm_toolchain_path } arm - none - eabi - size ${ cmake_executable_suffix } cache internal "" "" ) set ( cmake_strip ${ baremetal_arm_toolchain_path } arm - none - eabi - strip ${ cmake_executable_suffix } cache internal "" "" ) set ( cmake_c_flags "" - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections "" cache internal "" "" ) set ( cmake_cxx_flags "" ${ cmake_c_flags } - - std =c + + <number> - fno - pie - fno - pic - wno - dev - fpermissive "" cache internal "" "" ) # - - specs = nosys . specs set ( cmake_c_flags_debug "" - os - g "" cache internal "" "" ) set ( cmake_c_flags_release "" - os - dndebug "" cache internal "" "" ) set ( cmake_cxx_flags_debug "" ${ cmake_c_flags_debug } "" cache internal "" "" ) set ( cmake_cxx_flags_release "" ${ cmake_c_flags_release } "" cache internal "" "" ) set ( cmake_find_root_path_mode_program never ) set ( cmake_find_root_path_mode_library only ) set ( cmake_find_root_path_mode_include only ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #, - l / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / arm - none - eabi / lib / libstdc + + . a - i / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / arm - none - eabi / include / c + + / <number> . <number> / arm - none - eabi / thumb / v7 - r + fp . sp / hard / - i / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / arm - none - eabi / include / c + + / <number> . <number> / - include / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / arm - none - eabi / include / c + + / <number> . <number> / arm - none - eabi / thumb / v7 - r + fp . sp / hard / bits / c + + config . h - include / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / arm - none - eabi / include / c + + / <number> . <number> / bits / std_mutex . h # set ( cmake_system_name generic ) core , calib3d , flann , imgproc # set ( cmake_system_processor armv7 - r ) # set ( cmake_c_compiler arm - none - eabi - gcc ) # set ( cmake_cxx_compiler arm - none - eabi - g + + ) # set ( cmake_asm_compiler arm - none - eabi - gcc ) # set ( cmake_find_root_path_mode_program never ) # set ( cmake_find_root_path_mode_library only ) # set ( cmake_find_root_path_mode_include only ) # set ( cmake_find_root_path_mode_package only ) # # define the toolchain variables # set ( toolchain_prefix / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / bin / arm - none - eabi - ) # set ( cmake_ar ${ toolchain_prefix } ar cache filepath "" archiver "" ) # set ( cmake_linker ${ toolchain_prefix } ld cache filepath "" linker "" ) # set ( cmake_nm ${ toolchain_prefix } nm cache filepath "" name tool "" ) # set ( cmake_objcopy ${ toolchain_prefix } objcopy cache filepath "" object copy tool "" ) # set ( cmake_objdump ${ toolchain_prefix } objdump cache filepath "" object dump tool "" ) # set ( cmake_ranlib ${ toolchain_prefix } ranlib cache filepath "" ranlib "" ) ` ` ` # # # detailed description once i have configured and generated my project here ' s my configuration log ` ` ` opencv : system - specific configuration file is not found : ' generic ' detected processor : arm cleaning internal cached variable : zlib_library cleaning internal cached variable : zlib_include_dir could not find zlib ( missing : zlib_library zlib_include_dir ) ( required is at least version "" <number> . <number> "" ) could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) . openjpeg will be built from sources openjpeg : version = <number> . <number> , build = opencv - <number> . <number> - dev - openjp2 - <number> . <number> openjpeg libraries will be built from sources : libopenjp2 ( version "" <number> . <number> "" ) math lib ' libm ' not found ; floating point support disabled processing world modules . <repeated> module opencv_core . <repeated> module opencv_imgproc . <repeated> module opencv_features2d . <repeated> module opencv_imgcodecs . <repeated> processing world modules . <repeated> done excluding from source files list ( optimization is disabled ) : modules / imgproc / src / corner . avx . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / imgwarp . avx2 . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / imgwarp . lasx . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / imgwarp . sse4_1 . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / resize . avx2 . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / resize . lasx . cpp excluding from source files list ( optimization is disabled ) : modules / imgproc / src / resize . sse4_1 . cpp excluding from source files list ( optimization is disabled ) : modules / features2d / src / fast . avx2 . cpp general configuration for opencv <number> . <number> - dev = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> - <number> - g0052d46b8e platform : timestamp : <number> - <number> - 3 0 t <time> z host : linux <number> . <number> - <number> - generic x86_64 target : generic arm cmake : <number> . <number> cmake generator : unix makefiles cmake build tool : / usr / bin / gmake configuration : release cpu / hw features : baseline : requested : detect disabled : vfpv3 neon c / c + + : built as dynamic libs ? : no c + + standard : <number> c + + compiler : / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / bin / arm - none - eabi - g + + ( ver <number> . <number> ) c + + flags ( release ) : - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections - - std =c + + <number> - fno - pie - fno - pic - wno - dev - fpermissive - fsigned - char - ffast - math - w - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wno - psabi - wsuggest - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - fno - omit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - os - dndebug - dndebug c + + flags ( debug ) : - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections - - std =c + + <number> - fno - pie - fno - pic - wno - dev - fpermissive - fsigned - char - ffast - math - w - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wno - psabi - wsuggest - override - wno - delete - non - virtual - dtor - wno - unnamed - type - template - args - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - fno - omit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - os - g - ddebug - d_debug c compiler : / home / kowshik / documents / gcc / gcc - arm - none - eabi - <number> - <number> / bin / arm - none - eabi - gcc c flags ( release ) : - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections - fsigned - char - ffast - math - w - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - psabi - wno - unnamed - type - template - args - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - fno - omit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - os - dndebug - dndebug c flags ( debug ) : - mcpu = cortex - r5 - mthumb - mfloat - abi = hard - mfpu = vfpv3 - d16 - fpic - wno - psabi - wno - dev - fshort - enums - fshort - wchar - fdata - sections - ffunction - sections - wl , - - gc - sections - fsigned - char - ffast - math - w - wreturn - type - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - psabi - wno - unnamed - type - template - args - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - fno - omit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - os - g - ddebug - d_debug linker flags ( release ) : - wl , - - gc - sections linker flags ( debug ) : - wl , - - gc - sections ccache : no precompiled headers : no filesystem support is disabled extra dependencies : 3 rdparty dependencies : libpng libopenjp2 zlib opencv modules : to be built : core features2d imgcodecs imgproc world disabled : calib3d flann highgui java_bindings_generator js_bindings_generator ml objc_bindings_generator objdetect photo python_bindings_generator python_tests stitching video videoio disabled by dependency : - unavailable : dnn gapi java python2 python3 ts applications : - documentation : no non - free algorithms : no gui : media i / <surprise> zlib : build ( ver <date> ) png : build ( ver <date> ) jpeg <number> : build ( ver <number> . <number> ) hdr : no sunraster : no pxm : no pfm : no video i / <surprise> parallel framework : none other third - party libraries : custom hal : no python ( for build ) : / usr / bin / python2 . <number> install to : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / build / install - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - configuring done generating done ` ` ` and when the ` make - s - j ` command is used to compile the source , i am hit with the below error ` ` ` [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / thread . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / bio . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / cio . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / dwt . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / event . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / image . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / invert . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / j2k . c . obj / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : in function ' opj_j2k_dump_tile_info ' : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' %x ' expects argument of type ' unsigned int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t csty = % <hashtag> x </hashtag> \ \ n "" , l_default_tile - > csty ) ; | ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | unsigned int opj_uint32 { aka long unsigned int } | % <hashtag> lx </hashtag> / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t numlayers = % d \ \ n "" , l_default_tile - > numlayers ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' %x ' expects argument of type ' unsigned int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t mct =%x \ \ n "" , l_default_tile - > mct ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | unsigned int opj_uint32 { aka long unsigned int } | % lx / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_int32 ' { aka ' long int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t comp % d { \ \ n "" , compno ) ; | ~ ^ ~ ~ ~ ~ ~ ~ | | | | int opj_int32 { aka long int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' %x ' expects argument of type ' unsigned int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t csty = % <hashtag> x </hashtag> \ \ n "" , l_tccp - > csty ) ; | ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | | opj_uint32 { aka long unsigned int } | unsigned int | % <hashtag> lx </hashtag> / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t numresolutions = % d \ \ n "" , l_tccp - > numresolutions ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t cblkw = <number> ^ % d \ \ n "" , l_tccp - > cblkw ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t cblkh = <number> ^ % d \ \ n "" , l_tccp - > cblkh ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' %x ' expects argument of type ' unsigned int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t cblksty = % <hashtag> x </hashtag> \ \ n "" , l_tccp - > cblksty ) ; | ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | | opj_uint32 { aka long unsigned int } | unsigned int | % <hashtag> lx </hashtag> / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t qmfbid = % d \ \ n "" , l_tccp - > qmfbid ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" ( % d , % d ) "" , l_tccp - > prcw [ resno ] , l_tccp - > prch [ resno ] ); | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" ( % d , % d ) "" , l_tccp - > prcw [ resno ] , l_tccp - > prch [ resno ] ); | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t qntsty = % d \ \ n "" , l_tccp - > qntsty ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t numgbits = % d \ \ n "" , l_tccp - > numgbits ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_int32 ' { aka ' long int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" ( % d , % d ) "" , l_tccp - > stepsizes [ bandno ] . mant , | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_int32 { aka long int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_int32 ' { aka ' long int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" ( % d , % d ) "" , l_tccp - > stepsizes [ bandno ] . mant , | ~ ^ | | | int | % ld <number> | l_tccp - > stepsizes [ bandno ] . expn ) ; | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | opj_int32 { aka long int } / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_int32 ' { aka ' long int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t roishift = % d \ \ n "" , l_tccp - > roishift ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_int32 { aka long int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : in function ' opj_j2k_dump_mh_index ' : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> : <number> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t nb of tile - part in tile [ % d ] = % d \ \ n "" , it_tile , | ~ ^ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> : <number> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t nb of tile - part in tile [ % d ] = % d \ \ n "" , it_tile , | ~ ^ | | | int | % ld <number> | nb_of_tile_part ) ; | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | opj_uint32 { aka long unsigned int } / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t \ \ t \ \ t tile - part [ % d ] : star_pos = % "" prii64 "" , end_header = % "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | prii64 "" , end_pos = % "" prii64 "" . \ \ n "" , <number> | it_tile_part , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | opj_uint32 { aka long unsigned int } / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : in function ' opj_j2k_dump_mh_info ' : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tx0 = % d , ty0 = % d \ \ n "" , p_j2k - > m_cp . tx0 , p_j2k - > m_cp . ty0 ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tx0 = % d , ty0 = % d \ \ n "" , p_j2k - > m_cp . tx0 , p_j2k - > m_cp . ty0 ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tdx = % d , tdy = % d \ \ n "" , p_j2k - > m_cp . tdx , p_j2k - > m_cp . tdy ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tdx = % d , tdy = % d \ \ n "" , p_j2k - > m_cp . tdx , p_j2k - > m_cp . tdy ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tw = % d , th = % d \ \ n "" , p_j2k - > m_cp . tw , p_j2k - > m_cp . th ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" \ \ t tw = % d , th = % d \ \ n "" , p_j2k - > m_cp . tw , p_j2k - > m_cp . th ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : in function ' j2k_dump_image_header ' : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s x0 = % d , y0 = % d \ \ n "" , tab , img_header - >x0 , img_header - > y0 ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s x0 = % d , y0 = % d \ \ n "" , tab , img_header - >x0 , img_header - > y0 ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s x1 = % d , y1 = % d \ \ n "" , tab , img_header - >x 1 , | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s x1 = % d , y1 = % d \ \ n "" , tab , img_header - >x 1 , | ~ ^ | | | int | % ld <number> | img_header - > y1 ) ; | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | opj_uint32 { aka long unsigned int } / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s numcomps = % d \ \ n "" , tab , img_header - > numcomps ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s \ \ t component % d { \ \ n "" , tab , compno ) ; | ~ ^ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : in function ' j2k_dump_image_comp_header ' : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s <sad> = % d , dy = % d \ \ n "" , tab , comp_header - > <sad> , comp_header - > dy ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s <sad> = % d , dy = % d \ \ n "" , tab , comp_header - > <sad> , comp_header - > dy ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s prec = % d \ \ n "" , tab , comp_header - > prec ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : <number> <time> : warning : format ' % d ' expects argument of type ' int ' , but argument <number> has type ' opj_uint32 ' { aka ' long unsigned int ' } [ - wformat <happy> <number> | fprintf ( out_stream , "" %s sgnd = % d \ \ n "" , tab , comp_header - > sgnd ) ; | ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | | | | int opj_uint32 { aka long unsigned int } | % ld / home / kowshik / desktop / opencv_porting / opencv_master / opencv / 3 rdparty / openjpeg / openjp2 / j2k . c : at top level : cc1 : note : unrecognized command - line option ' - wno - implicit - const - int - float - conversion ' may have been intended to silence earlier diagnostics cc1 : note : unrecognized command - line option ' - wno - unnamed - type - template - args ' may have been intended to silence earlier diagnostics cc1 : note : unrecognized command - line option ' - wno - dev ' may have been intended to silence earlier diagnostics [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / jp2 . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / mct . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / mqc . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / openjpeg . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / opj_clock . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / pi . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / t1 . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / t2 . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / tcd . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / tgt . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / function_list . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / opj_malloc . c . obj [ <percent> ] building c object 3 rdparty / openjpeg / openjp2 / cmakefiles / libopenjp2 . dir / sparse_array . c . obj [ <percent> ] linking c static library . <repeated> / . <repeated> / lib / liblibopenjp2 . a [ <percent> ] built target libopenjp2 [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / adler32 . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / compress . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / crc32 . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / deflate . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / gzclose . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / gzlib . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / gzread . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / gzwrite . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / inflate . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / infback . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / inftrees . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / inffast . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / trees . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / uncompr . c . obj [ <percent> ] building c object 3 rdparty / zlib / cmakefiles / zlib . dir / zutil . c . obj [ <percent> ] linking c static library . <repeated> / lib / libzlib . a [ <percent> ] built target zlib [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / png . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngerror . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngget . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngmem . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngpread . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngread . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngrio . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngrtran . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngrutil . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngset . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngtrans . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngwio . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngwrite . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngwtran . c . obj [ <percent> ] building c object 3 rdparty / libpng / cmakefiles / libpng . dir / pngwutil . c . obj [ <percent> ] linking c static library . <repeated> / lib / liblibpng . a [ <percent> ] built target libpng [ <percent> ] processing opencl kernels ( imgproc ) - - / home / kowshik / desktop / opencv_porting / opencv_master / opencv / build / modules / world / opencl_kernels_imgproc . hpp contains the same content [ <percent> ] processing opencl kernels ( core ) - - / home / kowshik / desktop / opencv_porting / opencv_master / opencv / build / modules / world / opencl_kernels_core . hpp contains the same content [ <percent> ] processing opencl kernels ( features2d ) - - / home / kowshik / desktop / opencv_porting / opencv_master / opencv / build / modules / world / opencl_kernels_features2d . hpp contains the same content [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / __ / core / src / algorithm . cpp . obj in file included from / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / src / precomp . hpp : <number> , from / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / src / algorithm . cpp : <number> : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / include / opencv2 / core / utility . hpp : <number> <time> : error : ' recursive_mutex ' in namespace ' std ' does not name a type <number> | typedef std : : recursive_mutex mutex ; | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / include / opencv2 / core / utility . hpp : <number> : <number> : note : ' std : : recursive_mutex ' is defined in header ' <mutex> ' ; did you forget to ' <hashtag> include </hashtag> <mutex> ' ? <number> | <hashtag> include </hashtag> <mutex> / / std : : mutex , std : : lock_guard + + + | + <hashtag> include </hashtag> <mutex> <number> | <hashtag> end if </hashtag> / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / include / opencv2 / core / utility . hpp : <number> <time> : error : ' mutex ' is not a member of ' cv ' <number> | typedef std : : lock_guard < cv : : mutex > autolock ; | ^ ~ ~ ~ ~ / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / include / opencv2 / core / utility . hpp : <number> <time> : error : ' mutex ' is not a member of ' cv ' / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / include / opencv2 / core / utility . hpp : <number> <time> : error : template argument <number> is invalid <number> | typedef std : : lock_guard < cv : : mutex > autolock ; | ^ in file included from / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / src / algorithm . cpp : <number> : / home / kowshik / desktop / opencv_porting / opencv_master / opencv / modules / core / src / precomp . hpp : <number> : <number> : error : ' mutex ' in namespace ' cv ' does not name a type <number> | cv : : mutex & getinitializationmutex ( ); | ^ ~ ~ ~ ~ cc1plus : note : unrecognized command - line option ' - wno - unnamed - type - template - args ' may have been intended to silence earlier diagnostics cc1plus : note : unrecognized command - line option ' - wno - dev ' may have been intended to silence earlier diagnostics cc1plus : note : unrecognized command - line option ' - wno - dev ' may have been intended to silence earlier diagnostics make [ <number> <sad> * * * [ modules / world / cmakefiles / opencv_world . dir / build . make : <number> : modules / world / cmakefiles / opencv_world . dir / __ / core / src / algorithm . cpp . obj ] error <number> make [ <number> <sad> * * * [ cmakefiles / makefile2 : <number> : modules / world / cmakefiles / opencv_world . dir / all ] error <number> make : * * * [ makefile : <number> error <number> ` ` ` # # # steps to reproduce <number> . download the opencv source code <number> . download the arm - none - eabi - gcc compiler from this [ link ] ( <url> <number> . configure the opencv with cmake - gui by using the module as stated in the first section i . e . , ( core , features2d , imgproc , imgcodecs build list ) <number> . compile the ` build / ` with ` make - s ` and observe the same errors are received . please note that the reason behind this error is know that the arm - none - eabi - gcc compiler does not support the threads by default and when tried to run ` arm - none - eabi - gcc - v ` it gave the following output where it says ` thread model : single ` instead of posix however i want to solve this error because i am trying to compile opencv for a embedded platform as stated before . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"how to compile opencv into a single static library file ? # # # describe the feature and motivation i compiled using - dbuild_opencv_world = on - dbuild_shared_libs = off , which resulted in libopencv_world . a in the lib directory and several . a files in the lib / opencv / 3 rdparty / directory . i would like to merge all the . a files from the 3 rdparty directory with the libopencv_world . a file into one . i have tried various methods , including using the ar command for merging , but the resulting static library throws various errors when used . is there any way to compile all the modules or dependencies into a single static library ? # # # additional context _no response_",2
opencv/opencv,"image reshape in c + + - input image = cv : : dnn : : blobfromimage ( image , scalefactor , cv : : size ( width , height ) , false , false ) ; - this function return <number> x <number> x width x height . - but some model requested <number> x width x height x <number> . - in case difficult to reshape image . - how to convert <number> x <number> x width x height image size to <number> x width x height x <number> in c + + ?",2
opencv/opencv,"creating an roi on a live video # # # system information jetson agx xavier jetpack <number> . <number> opencv <number> . <number> cmake <number> . <number> # # # detailed description i am trying to create an roi on a live video and for a while it seems to work but after some time the programme seems to think the roi has moved out of the image plane . i am not to sure on how this is happening and i have not found any potential fixes which have worked so far . i have attached an image of the error i receive . [ img_20230517_143253641_hdr ] ( <url> # # # steps to reproduce ` ` ` int rx = <number> ; int ry = <number> ; int rw = <number> ; int rh = <number> ; videocapture cap ( <number> ); while ( true ) { cap > > img ; rect roirect ; roirect = rect ( rx , ry , rw , rh ) ; mat roi = img ( roirect ) ; imshow ( "" frame "" , img ) ; } ` ` ` this is just a small snippet of the main code . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"cannot load tensorflow savedmodel created with teachable machine # # # system information opencv version : current branch <number> . x operating system : windows <number> compiler : visual studio <number> <number> , win32 release build # # # detailed description i built a simple model with teachable machine and downloaded it in the saved model format . after unzipping , i tried to load the model in opencv : ` ` ` cpp cv : : dnn : : readnet ( onnxpath . getstring ( )); ` ` ` this gave me a ` cv : : exception ` with the following message : ` ` ` opencv ( <number> . <number> - dev ) c :\\ tools \ \ opencv - <number> . <number> - pre \ \ source \ \ modules \ \ dnn \ \ src \ \ tensorflow \ \ tf_io . cpp : <number> : error : ( - <number> : unspecified error ) failed : readprotofrombinaryfile ( param_file , param ) . failed to parse graphdef file in function ' cv : : dnn : : readtfnetparamsfrombinaryfileordie ' ` ` ` # # # steps to reproduce to reproduce it , just head over to <url> and train a model with two classes , then export it to tensorflow savedmodel , and try to load the model in opencv . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"the parameter patternsize behaves unexpected in findcirclesgrid ( ) # # # system information opencv python version : <number> . <number> operating system / platform : macos <number> . <number> python version : <date> # # # detailed description according to the [ documentation ] ( <url> ` patternsize ` should be ` ( points_per_row , points_per_colum ) ` . however , it seems that only when i set ` patternsize ` as ` ( points_per_colum , points_per_row ) ` can make the return value ` retval ` of ` findcirclesgrid ( ) ` become ` true ` . # # # steps to reproduce code : ` ` ` python import cv2 img = cv2 . imread ( f ' test . png ' ) patternsize = ( <number> , <number> ) retval , centers = cv2 . findcirclesgrid ( img , patternsize , cv2 . circlesgridfinderparameters_symmetric_grid ) print ( f ' { patternsize =} { retval =} ' ) patternsize = ( <number> , <number> ) retval , centers = cv2 . findcirclesgrid ( img , patternsize , cv2 . circlesgridfinderparameters_symmetric_grid ) print ( f ' { patternsize =} { retval =} ' ) ` ` ` output : ` ` ` patternsize =( <number> , <number> ) retval = false patternsize =( <number> , <number> ) retval = true ` ` ` test data # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"how about edges ? # # # descripe the feature and motivation they can be detected using edge detection algorithms like sobel , canny , or laplacian . edges are useful for tasks like object detection , image segmentation , and feature matching . # # # additional context _no response_",2
opencv/opencv,"does dnn support intel n100 processor i ' d like to buy a mini pc with intel n100 processor . i am not sure if "" cv2 . dnn . readnetfromtensorflow "" support the new processor . i use the function to detect face in real time . thanks in advance for your help",2
opencv/opencv,"how to image process the on the original image like the of the given output . # # # system information i have this image original image : - [ img1 ] ( <url> , and after processing it above we want the following output [ output image ] : - ! [ <number> ] ( <url> # # # detailed description ` i expecting this result is when i process on the original image , the output image given above should come in the same result as this . ' ` # # # steps to reproduce ` code main ( ) { / / read image mat imgoffice = imread ( "" img29 . jpg "" ); / / remove background using chroma keying mat imgnobg ; double threshold = <number> ; / / adjust as necessary / / cv : : cvtcolor ( imgoffice , imgoffice , color_bgr2hsv ) ; / / convert to hsv color space / / cv : : inrange ( imgoffice , scalar ( - <number> , - <number> , - <number> ) , scalar ( <number> , <number> , <number> ) , imgnobg ) ; / / detect green color range and set as the background cv : : inrange ( imgoffice , scalar ( <number> , <number> , <number> ) , scalar ( <number> , <number> , <number> ) , imgnobg ) ; / / cv : : morphologyex ( imgnobg , imgnobg , cv : : morph_close , cv : : getstructuringelement ( cv : : morph_erode , cv : : size ( <number> , <number> ))); / / fill small holes in the foreground / / cv : : morphologyex ( imgnobg , imgnobg , cv : : morph_close , cv : : getstructuringelement ( cv : : morph_ellipse , cv : : size ( <number> , <number> ))); cv : : morphologyex ( imgnobg , imgnobg , cv : : morph_close , cv : : getstructuringelement ( cv : : morph_ellipse , cv : : size ( <number> , <number> ))); / / display both images namedwindow ( "" image "" , window_normal ) ; namedwindow ( "" output "" , window_normal ) ; imshow ( "" image "" , imgoffice ) ; imshow ( "" output "" , imgnobg ) ; / / imwrite ( "" final - output1 ( morph_ellipse ) . jpg "" , imgnobg ) ; / / imwrite ( "" o2 . jpg "" , imgnobg ) ; waitkey ( <number> ); destroyallwindows ( ); return <number> ; ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"opencv no longer builds with cuda <number> . <number> # # # system information opencv version : <number> . <number> operating system / platform : gentoo linux compiler & compiler version <number> . 1 _p20230121 - r1 # # # detailed description fails to build due to missing symbols . [ build . log ] ( <url> # # # steps to reproduce ` ` ` bash emerge - avudut - - backtrack = <number> - - verbose - conflicts <user> ` ` ` this rebuilds anything needing an update , and the command i used . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,colud we use usac with pnp ? # # # descripe the feature and motivation because using ransac with pnp shows bad effects . # # # additional context _no response_,2
opencv/opencv,"unable to compile <number> . <number> with arm # # # system information opencv version : <number> . <number> operating system / platform : ubuntu <number> compiler & compiler version : gcc <number> . <number> # # # detailed description i tried to build opencv through the options of the command in * * steps to reproduce * * . however , i checked the following error message : ` ` ` [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / superres / src / super_resolution . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / opencl_kernels_superres . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / deblurring . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / fast_marching . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / frame_source . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / global_motion . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / inpainting . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / log . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / motion_stabilizing . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / optical_flow . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / outlier_rejection . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / stabilizer . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / home / nvidia / opencv_contrib - <number> . <number> / modules / videostab / src / wobble_suppression . cpp . o [ <percent> ] building cxx object modules / world / cmakefiles / opencv_world . dir / src / world_init . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_world . so [ <percent> ] built target opencv_world scanning dependencies of target opencv_img_hash scanning dependencies of target opencv_annotation scanning dependencies of target opencv_version scanning dependencies of target opencv_interactive - calibration scanning dependencies of target opencv_visualisation scanning dependencies of target opencv_waldboost_detector [ <percent> ] building cxx object apps / version / cmakefiles / opencv_version . dir / opencv_version . cpp . o [ <percent> ] building cxx object apps / annotation / cmakefiles / opencv_annotation . dir / opencv_annotation . cpp . o [ <percent> ] building cxx object apps / visualisation / cmakefiles / opencv_visualisation . dir / opencv_visualisation . cpp . o [ <percent> ] building cxx object modules / world / tools / waldboost_detector / cmakefiles / opencv_waldboost_detector . dir / waldboost_detector . cpp . o [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / calibcontroller . cpp . o [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / average_hash . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / . <repeated> / . <repeated> / bin / opencv_waldboost_detector [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_version [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / block_mean_hash . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_annotation . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_add_match_subsystem ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_get_list_entry ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_unref ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_unref ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_parent_with_subsystem_devtype ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_sysattr_value ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_scan_devices ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_new ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_name ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_next ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_new_from_syspath ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_devnode ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_unref ' . <repeated> / . <repeated> / . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_new ' collect2 : error : ld returned <number> exit status modules / world / tools / waldboost_detector / cmakefiles / opencv_waldboost_detector . dir / build . make : <number> : recipe for target ' bin / opencv_waldboost_detector ' failed make [ <number> <sad> * * * [ bin / opencv_waldboost_detector ] error <number> cmakefiles / makefile <time> <number> : recipe for target ' modules / world / tools / waldboost_detector / cmakefiles / opencv_waldboost_detector . dir / all ' failed make [ <number> <sad> * * * [ modules / world / tools / waldboost_detector / cmakefiles / opencv_waldboost_detector . dir / all ] error <number> make [ <number> <sad> * * * waiting for unfinished jobs . <repeated> [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / color_moment_hash . cpp . o . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_add_match_subsystem ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_get_list_entry ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_parent_with_subsystem_devtype ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_sysattr_value ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_scan_devices ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_new ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_name ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_next ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_new_from_syspath ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_devnode ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_new ' collect2 : error : ld returned <number> exit status apps / version / cmakefiles / opencv_version . dir / build . make : <number> : recipe for target ' bin / opencv_version ' failed make [ <number> <sad> * * * [ bin / opencv_version ] error <number> cmakefiles / makefile <time> <number> : recipe for target ' apps / version / cmakefiles / opencv_version . dir / all ' failed make [ <number> <sad> * * * [ apps / version / cmakefiles / opencv_version . dir / all ] error <number> [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / img_hash_base . cpp . o [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_visualisation . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_add_match_subsystem ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_get_list_entry ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_parent_with_subsystem_devtype ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_sysattr_value ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_scan_devices ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_new ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_name ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_next ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_new_from_syspath ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_devnode ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_new ' collect2 : error : ld returned <number> exit status apps / annotation / cmakefiles / opencv_annotation . dir / build . make : <number> : recipe for target ' bin / opencv_annotation ' failed make [ <number> <sad> * * * [ bin / opencv_annotation ] error <number> cmakefiles / makefile <time> <number> : recipe for target ' apps / annotation / cmakefiles / opencv_annotation . dir / all ' failed make [ <number> <sad> * * * [ apps / annotation / cmakefiles / opencv_annotation . dir / all ] error <number> [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / marr_hildreth_hash . cpp . o [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / calibpipeline . cpp . o . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_add_match_subsystem ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_get_list_entry ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_parent_with_subsystem_devtype ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_sysattr_value ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_scan_devices ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_new ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_name ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_next ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_new_from_syspath ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_devnode ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_new ' collect2 : error : ld returned <number> exit status apps / visualisation / cmakefiles / opencv_visualisation . dir / build . make : <number> : recipe for target ' bin / opencv_visualisation ' failed make [ <number> <sad> * * * [ bin / opencv_visualisation ] error <number> cmakefiles / makefile <time> <number> : recipe for target ' apps / visualisation / cmakefiles / opencv_visualisation . dir / all ' failed make [ <number> <sad> * * * [ apps / visualisation / cmakefiles / opencv_visualisation . dir / all ] error <number> [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / frameprocessor . cpp . o [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / main . cpp . o [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / phash . cpp . o [ <percent> ] building cxx object modules / img_hash / cmakefiles / opencv_img_hash . dir / src / radial_variance_hash . cpp . o [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / parameterscontroller . cpp . o [ <percent> ] building cxx object apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / rotationconverters . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_img_hash . so [ <percent> ] built target opencv_img_hash [ <percent> ] linking cxx executable . <repeated> / . <repeated> / bin / opencv_interactive - calibration . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_add_match_subsystem ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_get_list_entry ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_parent_with_subsystem_devtype ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_sysattr_value ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_scan_devices ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_new ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_name ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_list_entry_get_next ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_new_from_syspath ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_device_get_devnode ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_unref ' . <repeated> / . <repeated> / lib / libopencv_world . so . <number> . <number> : undefined reference to ` udev_enumerate_new ' collect2 : error : ld returned <number> exit status apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / build . make : <number> : recipe for target ' bin / opencv_interactive - calibration ' failed make [ <number> <sad> * * * [ bin / opencv_interactive - calibration ] error <number> cmakefiles / makefile <time> <number> : recipe for target ' apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / all ' failed make [ <number> <sad> * * * [ apps / interactive - calibration / cmakefiles / opencv_interactive - calibration . dir / all ] error <number> makefile : <number> : recipe for target ' all ' failed make : * * * [ all ] error <number> ` ` ` i thought libudev was not installed on my system . however , it exists in the system . ` ` ` $ pkg - config - - cflags - - libs libudev - ludev $ apt list - - installed | grep libudev warning does not have a stable cli interface . use with caution in scripts . libudev - dev / bionic - updates , bionic - security , now <number> - 3 ubuntu10 . <number> arm64 [ installed ] libudev1 / bionic - updates , bionic - security , now <number> - 3 ubuntu10 . <number> arm64 [ installed ] $ ll / lib / aarch64 - linux - gnu / | grep libudev * lrwxrwxrwx <number> root root <date> <time> libudev . so - > libudev . so . <number> lrwxrwxrwx <number> root root <date> <time> libudev . so . <number> - > libudev . so . <number> . <number> - rw - r - - r - - <number> root root <number> <date> <time> libudev . so . <number> . <number> ` ` ` # # # steps to reproduce ` ` ` cmake - d cmake_build_type = release \ \ - d cmake_install_prefix <annoyed> usr / local \ \ - d build_opencv_python3 = on \ \ - d build_opencv_world = on \ \ - d with_cuda = on \ \ - d cuda_arch_bin = "" <number> "" \ \ - d cuda_arch_ptx = "" "" \ \ - d with_cudnn = on \ \ - d enable_fast_math = on \ \ - d cuda_fast_math = on \ \ - d opencv_dnn_cuda = on \ \ - d install_python_examples = off \ \ - d opencv_extra_modules_path = ~ / opencv_contrib - <number> . <number> / modules \ \ - d opencv_enable_nonfree = on \ \ - d opencv_generate_pkgconfig = on \ \ - d build_examples = off \ \ - d with_gphoto2 = off \ \ - d build_tests = off \ \ - d with_vtk = off \ \ - d with_gstreamer = off \ \ - d build_perf_tests = off . <repeated> ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"puttext ( ) documentation is misleading about origin point # # # descripe the doc issue there is a mistake in ` ` ` puttext ( ) ` ` ` function documentation : <url> <url> > puttext ( . <repeated> , point org , . <repeated> , bool bottomleftorigin = false ) <number> ) ` ` ` org ` ` ` - bottom - left corner of the text string in the image . <number> ) ` ` ` bottomleftorigin ` ` ` - when true , the image data origin is at the bottom - left corner . otherwise , it is at the top - left corner . by default ` ` ` bottomleftorigin = false ` ` ` , so documentation about ` ` ` org ` ` ` argument is wrong . # # # fix suggestion i see two alternatives : <number> ) if ` ` ` bottomleftorigin ` ` ` will be changed to ` ` ` true ` ` ` - documentation is nearly ok ( but i think documentation of ` ` ` org ` ` ` argument should mention ` ` ` bottomleftorigin ` ` ` ) . probably this is not an option because of backward compatibility concerns . <number> ) update documentation like this <user> org top - left corner of the text string in the image if bottomleftorigin is false . otherwise , it is at the bottom - left corner . <user> bottomleftorigin when false , the image data origin is at the top - left corner . otherwise , it is at the bottom - left corner . ` ` ` if this documentation update is ok - i can create a pr , if i miss something or there are a better way - i will be glad to know .",2
opencv/opencv,"unable to run the simple rtsp program in docker opencv => <number> operating system / platform => ubuntu <number> compiler => python i need to take and rtsp stream which is currently from vlc player rtsp :// localhost : <number> to the program running inside the docker and the docker container has to give the video output . for that i did like this but everytime i am getting the same error which is related qt and xcb please look into below details ind dockerfile and command kindly help me to resolve this issue hi i am trying to dockerize this code * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * code * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ` ` ` import cv2 import os rtsp_url = ' rtsp :// <number> . <number> : <number> / ' os . environ [ ' opencv_ffmpeg_capture_options ' ] = ' rtsp_transport ; udp ' cap = cv2 . videocapture ( rtsp_url , cv2 . cap_ffmpeg ) if not cap . isopened ( <sad> print ( ' cannot open rtsp stream ' ) exit ( - <number> ) while true : _ , frame = cap . read ( ) cv2 . imshow ( ' rtsp stream ' , frame ) if cv2 . waitkey ( <number> ) = = <number> : break cap . release ( ) cv2 . destroyallwindows ( ) ` ` ` * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * code * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * dockerfile below : - * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * dockerfile <emphasis> * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ` ` ` from ubuntu : <number> arg debian_frontend = noninteractive run mkdir / app workdir / app run chmod - r <number> / app run apt update run debian_frontend = noninteractive tz = asia / kolkata apt install - y tzdata run apt install - - no - install - recommends - y python3 - pip run apt - get install - y libgl1 - mesa - dev run apt - get install - y libglib2 . <number> - <number> run pip install opencv - python workdir / app copy . . cmd [ "" python3 "" , "" - u "" , "" import_cv2 . py "" ] ` ` ` * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * end <emphasis> * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * run command : docker run - it - - net = host - - device / dev / video1 - p <number> : <number> - v / tmp / . x11 - unix / <annoyed> tmp / . x11 - unix abab670c544b ` ` ` error i am getting is qt . qpa . plugin : could not load the qt platform plugin "" xcb "" in "" / app / cv2 / qt / plugins "" even though it was found . this application failed to start because no qt platform plugin could be initialized . reinstalling the application may fix this problem . available platform plugins are ` ` `",2
opencv/opencv,"how disable mt default on msvc ? # # # descripe the doc issue got errors when link opencv . lib with unreal engine : ` ` ` 检测到 “ runtimelibrary ” 的不匹配项 中 ) ` ` ` ue default using md , but opencv default using mt how to enable md while still using static lib ? # # # fix suggestion _no response_",2
opencv/opencv,"cv : : setnumthreads does not work with apple gcd # # # system information opencv version : <number> . <number> os : macos <number> . <number> on apple m1 compiler : apple clang version <number> . <number> ( clang - <number> . <number> ) target : arm64 - apple - darwin22 . <number> # # # detailed description compile opencv with default configuration , which results in using gcd for parallel framework . in this case , calling ` cv : : setnumthreads ` always fails and ` cv : : getnumthreads ( ) ` always returns <number> . using <number> threads on apple m1 would result in worse performance since half of the cores are relatively weak . one possible solution is using openmp instead on apple platform . tried to build with option ` - dwith_openmp = on ` , but got the following error when building : ` ` ` modules / core / src / parallel . cpp : <number> <time> : fatal error file not found <hashtag> include </hashtag> < omp . h > ` ` ` my ` libomp ` is installed via homebrew . # # # steps to reproduce ` ` ` cpp <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> <numeric> / / accumulate <hashtag> include </hashtag> <algorithm> / / min_element <hashtag> include </hashtag> "" opencv2 / opencv . hpp "" using namespace std ; using namespace cv ; int main ( ) { const vector <int> input_shape { <number> , <number> , <number> , <number> }; mat blob ( <number> , input_shape . data ( ) , cv_32fc1 ) ; randu ( blob , <number> . f , <number> . f ) ; dnn : : net net = dnn : : readnet ( "" . <repeated> / some_nets . onnx "" ); net . setinput ( blob ) ; / / set thread num int thread_num = <number> ; setnumthreads ( thread_num ) ; std : : cout < < "" thread_num = "" < < getnumthreads ( ) < < std : : endl ; / / warmup net . forward ( ); / / benchmark tickmeter tm ; vector <double> times ; for ( int i = <number> ; i < <number> ; + + i ) { tm . reset ( ); tm . start ( ); net . forward ( ); tm . stop ( ); times . push_back ( tm . gettimemilli ( )); } double mean = std : : accumulate ( times . begin ( ) , times . end ( ) , <number> . f ) / times . size ( ); double median = ( times [ <number> ] + times [ <number> ] ) / <number> ; double minimum = *( std : : min_element ( times . begin ( ) , times . end ( ))); std : : cout < < cv : : format ( "" mean = % f , median = % f , min = % f \ \ n "" , mean , median , minimum ) ; return <number> ; } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,avoid using system zlib # # # descripe the feature and motivation how to forely using static build zlib rather than system one ? # # # additional context how to forely using static build zlib rather than system one ?,2
opencv/opencv,"error trying to build opencv <number> . <number> with openvino <number> # # # system information opencv version : <number> . <number> operating system / platform : windows <number> inference engine ( openvino ) version : <number> python version : <number> . <number> # # # detailed description hey guys , so basically i was trying to build opencv <number> . <number> with openvino <number> and cuda and got the following error : c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - package \ \ opencv_mirror \ \ modules \ \ dnn \ \ src \ \ ie_ngraph . cpp ( <number> <sad> error c2440 : ' initializin g ' : cannot convert from ' initializer list ' to ' ov : : discretetypeinfo ' [ c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - package \ \ opencv _mirror \ \ build \ \ modules \ \ world \ \ opencv_world . vcxproj ] c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - package \ \ opencv_mirror \ \ modules \ \ dnn \ \ src \ \ ie_ngraph . cpp ( <number> <sad> message : no constructor could take the source type , or constructor overload resolution was ambiguous [ c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - packag e \ \ opencv_mirror \ \ build \ \ modules \ \ world \ \ opencv_world . vcxproj ] c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - package \ \ opencv_mirror \ \ modules \ \ dnn \ \ src \ \ ie_ngraph . cpp ( <number> <sad> error c2131 : expression did not evaluate to a constant [ c :\\ users \ \ cesar . gouveia \ \ projects \ \ opencv - package \ \ opencv_mirror \ \ build \ \ modules \ \ world \ \ opencv_wor ld . vcxproj ] i have done this process multiple times and the only thing that i changed was the openvino version ( changed from <number> to <number> ) . the cmake finds openvino new version ( <number> ) correctly , but when i am building opencv_world cannot be generated because there are code errors . does opencv <number> . <number> supports openvino <number> ? is there any new procedure to build opencv with openvino , like a new flag that you need to turn on or something ? if there is anything that i should try and if you can help i really appreciate . note : in the meantime i am building openvino <number> with the new released version of opencv ( <number> . <number> ) to check if the problem remains . thanks , césar . # # # steps to reproduce opencv flags used . <repeated> - dbuild_opencv_apps = off - dbuild_opencv_aruco = off - dbuild_opencv_bgsegm = off - dbuild_opencv_bioinspired = off - dbuild_opencv_calib3d = on - dbuild_opencv_ccalib = on - dbuild_opencv_core = on - dbuild_opencv_datasets = off - dbuild_opencv_dnn = on - dbuild_opencv_dnn_objdetect = off - dbuild_opencv_dnn_superres = off - dbuild_opencv_dpm = off - dbuild_opencv_face = off - dbuild_opencv_features2d = on - dbuild_opencv_flann = on - dbuild_opencv_fuzzy = off - dbuild_opencv_gapi = off - dbuild_opencv_hfs = off - dbuild_opencv_highgui = on - dbuild_opencv_img_hash = on - dbuild_opencv_imgcodecs = on - dbuild_opencv_imgproc = on - dbuild_opencv_intensity_transform = off - dbuild_java_bindings_generator = off - dbuild_opencv_js = off - dbuild_opencv_line_descriptor = off - dbuild_opencv_mcc = off - dbuild_opencv_ml = on - dbuild_opencv_objc_bindings_generator = off - dbuild_opencv_objdetect = on - dbuild_opencv_optflow = off - dbuild_opencv_phase_unwrapping = off - dbuild_opencv_photo = on - dbuild_opencv_plot = on - dbuild_opencv_python3 = off - dbuild_opencv_python_bindings_generator = off - dbuild_opencv_python_tests = off - dbuild_opencv_quality = off - dbuild_opencv_rapid = off - dbuild_opencv_reg = off - dbuild_opencv_rgdb = off - dbuild_opencv_saliency = off - dbuild_opencv_shape = off - dbuild_opencv_stereo = off - dbuild_opencv_stitching = off - dbuild_opencv_structured_light = off - dbuild_opencv_superres = off - dbuild_opencv_surface_matching = off - dbuild_opencv_text = off - dbuild_opencv_tracking = off - dbuild_opencv_ts = off - dbuild_opencv_video = on - dbuild_opencv_videoio = on - dbuild_opencv_videostab = off - dbuild_opencv_world = on - dbuild_opencv_xfeatures2d = on - dbuild_opencv_ximgproc = on - dbuild_opencv_xobjdetect = on - dbuild_opencv_xphoto = on - dcmake_configuration_types = release - dcmake_generator_platform =x 6 4 - t v142 - dwith_ffmpeg = on - dwith_inf_engine = on - denable_cxx11 = on - dopencv_enable_nonfree = off - dbuild_java = off - dwith_msmf = off - dwith_msmf_dxva = off - dopencv_extra_modules_path =""c <annoyed> users / cesar . gouveia / projects / opencv - package / opencv_contrib / modules "" - dbuild_zlib = on - dbuild_opencv_hdf = off - dwith_cuda = on - denable_fast_math = <number> - dcuda_fast_math = <number> - dwith_cublas = <number> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"how to disable zlib ? # # # system information i using ` build_zlib = off ` , but it still calls some ` gzgets ` api from zlib . and i found in code , it hard coded width = "" <number> "" alt = "" image "" src = "" <url> am confused here . <repeated> how to disable zlib totally ? * * don ; t say features2d or some module need zlib , i just want disable all of them , i do not need zlib at all * * # # # detailed description no # # # steps to reproduce no # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"opencv contrib attribute not found - attributeerror : module ' cv2 ' has no attribute ' face ' # # # system information in case you get ` attributeerror : module ' cv2 ' has no attribute ' face ' ` ( or any other [ module ] ( <url> ( ` rgbd ` , ` ximgproc ` , ` bgsegm ` , etc . ) and you already have opencv - contrib - python installed , it might be that ` import cv2 ` is actually importing ` opencv - python ` library instead of ` opencv - contrib - python ` . my solution was to uninstall opencv - python . ` ` ` pip install opencv - contrib - python - u - - force # when running python script that uses cv2 . rgbd it crashes python . \ \ depth_preview . py file "" . \ \ depth_preview . py "" , line <number> , in <module> depthcleaner = cv2 . rgbd . depthcleaner_create ( cv2 . cv_16u , <number> , cv2 . rgbd . depthcleaner_depth_cleaner_nil ) attributeerror ' cv2 ' has no attribute ' rgbd ' # uninstalling opencv - python made it work python - mpip uninstall opencv - python ` ` ` # # # detailed description opencv - python used instead of opencv - contrib - python # # # steps to reproduce opencv - python used instead of opencv - contrib - python # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"an nparray that uses an inconsistent save from cv2 . imwrite # # # system information / / example for python user opencv python version : <number> operating system / platform : ubuntu <number> python version : <number> # # # detailed description i ran the following code , the outputs should be the same . why are they different ? # # # steps to reproduce import cv2 import numpy as np from pil import image a = np . zeros ( [ <number> , <number> ] ) . astype ( np . uint8 ) for i in range ( <number> <sad> if i > <number> : a [ i ] [ i ] [ <number> ] = i a [ i ] [ i ] [ <number> ] = i a [ i ] [ i ] [ <number> ] = i else = i a [ i ] [ i ] [ <number> ] = i a [ i ] [ i ] [ <number> ] = i print ( a . max ( ) ) cv2 . imwrite ( "" <number> . jpg "" , a ) img = cv2 . imread ( "" <number> . jpg "" ) print ( img . max ( ) ) # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"give it option to build opencv x86 static lib on a arm m1 mac # # # descripe the feature and motivation give it option to build opencv x86 static lib on a arm m1 mac . currently , whenever enviroment set , opencv always build arm static on a arm mac , this is no problem , but sometimes users need x86 lib on m1 mac , for example , build a thirdparty lib for ue5 . # # # additional context give it option to build opencv x86 static lib on a arm m1 mac . currently , whenever enviroment set , opencv always build arm static on a arm mac , this is no problem , but sometimes users need x86 lib on m1 mac , for example , build a thirdparty lib for ue5 .",2
opencv/opencv,"android jni videocapture can not open videofile # # # system information opencv version : <number> . <number> operating system / platform : android <number> compiler & compiler version : cmake <number> . <number> # # # detailed description ` ` ` e / cv : : error ( <sad> opencv ( <number> . <number> ) error : requested object was not found ( could not open directory : / data / app / ~ ~ sbu7r1tb_qjqdnkvkr9abq ==/ com . tencent . yolov5ncnn - 5 hoztn5fqskrrikcilvuuq ==/ base . apk / lib / arm64 - v8a ) in glob_rec , file / build / master_pack - android / opencv / modules / core / src / glob . cpp , line <number> a / libc signal <number> ( sigsegv ) , code <number> ( segv_accerr ) , fault addr 0x 7 9 7 3 0 2 f480 in tid <number> ( cent . yolov5ncnn ) , pid <number> ( cent . yolov5ncnn ) ` ` ` i use cv : : imread and cv2 : : imwrite normal , but when i use videocapture open video , it will report the error even if i use <number> . <number> , it return the same error my code is simple , error in line2 ` capture . open ` # # # steps to reproduce ` ` ` / / / / read video cv : : videocapture capture ; capture . open ( "" / storage / emulated / <number> / dcim / test . mp4 "" ); ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"protobuf compatibility . <repeated> # # # descripe the feature and motivation my * env <emphasis> * : - ubuntu <number> - gcc : ( ubuntu <number> . <number> - 1 ubuntu1 ~ <number> ) <number> . <number> - protobuf : <number> . <number> - opencv <number> . <number> ( building ) failed to build * * opencv dnn caffe * * with this version of * protobuf <emphasis> * : ` ` ` console / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : cleartoempty ( ) ’ <number> | void cleartoempty ( ); | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : _internal_set_layer ( const string & ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : set ( const string * , const string & , google : : protobuf : : arena <wink> ’ <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , value , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ template < class refwrappedtype > void google : : protobuf : : internal : : arenastringptr : : set ( std : : reference_wrapper <_tp> , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : reference_wrapper <refwrappedtype> const_string_ref , | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : template argument deduction / substitution failed : in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : note : mismatched types ‘ std : : reference_wrapper <_tp> ’ and ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , value , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( google : : protobuf : : conststringparam , google : : protobuf : : arena <wink> ’ <number> | void set ( conststringparam value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( std : : string & & , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : string & & value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , size_t , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : no known conversion for argument <number> from ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } to ‘ const char * ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : set_layer ( std : : string & & ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : set ( const string * , std : : remove_reference < std : : __cxx11 : : basic_string <char> & <sad> : type , google : : protobuf : : arena <wink> ’ <number> | layer_ . set ( | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ <number> | & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : move ( value ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ template < class refwrappedtype > void google : : protobuf : : internal : : arenastringptr : : set ( std : : reference_wrapper <_tp> , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : reference_wrapper <refwrappedtype> const_string_ref , | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : template argument deduction / substitution failed : in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : note : mismatched types ‘ std : : reference_wrapper <_tp> ’ and ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } <number> | layer_ . set ( | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ <number> | & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : move ( value ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( google : : protobuf : : conststringparam , google : : protobuf : : arena <wink> ’ <number> | void set ( conststringparam value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( std : : string & & , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : string & & value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , size_t , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : no known conversion for argument <number> from ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } to ‘ const char * ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : set_layer ( const char <wink> ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : set ( const string * , std : : string , google : : protobuf : : arena <wink> ’ <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : string ( value ) , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ template < class refwrappedtype > void google : : protobuf : : internal : : arenastringptr : : set ( std : : reference_wrapper <_tp> , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : reference_wrapper <refwrappedtype> const_string_ref , | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : template argument deduction / substitution failed : in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : note : mismatched types ‘ std : : reference_wrapper <_tp> ’ and ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : string ( value ) , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( google : : protobuf : : conststringparam , google : : protobuf : : arena <wink> ’ <number> | void set ( conststringparam value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( std : : string & & , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : string & & value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , size_t , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : no known conversion for argument <number> from ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } to ‘ const char * ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : set_layer ( const char * , size_t ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : set ( const string * , std : : string , google : : protobuf : : arena <wink> ’ <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : string ( | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | reinterpret_cast < const char *>( value ) , size ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ template < class refwrappedtype > void google : : protobuf : : internal : : arenastringptr : : set ( std : : reference_wrapper <_tp> , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : reference_wrapper <refwrappedtype> const_string_ref , | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : template argument deduction / substitution failed : in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : note : mismatched types ‘ std : : reference_wrapper <_tp> ’ and ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } <number> | layer_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , : : std : : string ( | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | reinterpret_cast < const char *>( value ) , size ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( google : : protobuf : : conststringparam , google : : protobuf : : arena <wink> ’ <number> | void set ( conststringparam value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( std : : string & & , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : string & & value , arena * arena ) ; | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate expects <number> arguments , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : set ( const char * , size_t , google : : protobuf : : arena <wink> ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : no known conversion for argument <number> from ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } to ‘ const char * ’ <number> | inline void arenastringptr : : set ( const char * s , size_t n , arena * arena ) { | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ std : : string * opencv_caffe : : pythonparameter : : _internal_mutable_layer ( ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : mutable ( const string * , google : : protobuf : : arena <wink> ’ <number> | return layer_ . mutable ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ std : : string * google : : protobuf : : internal : : arenastringptr : : mutable ( google : : protobuf : : arena <wink> ’ <number> | std : : string * mutable ( arena * arena ) ; | ^ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate expects <number> argument , <number> provided / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : candidate : ‘ std : : string * google : : protobuf : : internal : : arenastringptr : : mutable ( const google : : protobuf : : internal : : lazystring & , google : : protobuf : : arena <wink> ’ <number> | std : : string * mutable ( const lazystring & default_value , arena * arena ) ; | ^ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> <time> : note : no known conversion for argument <number> from ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } to ‘ const google : : protobuf : : internal : : lazystring & ’ <number> | std : : string * mutable ( const lazystring & default_value , arena * arena ) ; | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ std : : string * opencv_caffe : : pythonparameter : : release_layer ( ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : ‘ struct google : : protobuf : : internal : : arenastringptr ’ has no member named ‘ releasenondefault ’ ; did you mean ‘ cleartodefault ’ ? <number> | return layer_ . releasenondefault ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , getarena ( )); | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ | cleartodefault . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : set_allocated_layer ( std : : string <wink> ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : setallocated ( const string * , std : : string * & , google : : protobuf : : arena <wink> ’ <number> | layer_ . setallocated ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , layer , | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ <number> | getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : setallocated ( std : : string * , google : : protobuf : : arena <wink> ’ <number> | void setallocated ( std : : string * value , arena * arena ) ; | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : clear_param_str ( ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : cleartoempty ( const string * , google : : protobuf : : arena <wink> ’ <number> | param_str_ . cleartoempty ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ void google : : protobuf : : internal : : arenastringptr : : cleartoempty ( ) ’ <number> | void cleartoempty ( ); | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate expects <number> arguments , <number> provided in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : in member function ‘ void opencv_caffe : : pythonparameter : : _internal_set_param_str ( const string & ) ’ : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : error : no matching function for call to ‘ google : : protobuf : : internal : : arenastringptr : : set ( const string * , const string & , google : : protobuf : : arena <wink> ’ <number> | param_str_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , value , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> , from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : candidate : ‘ template < class refwrappedtype > void google : : protobuf : : internal : : arenastringptr : : set ( std : : reference_wrapper <_tp> , google : : protobuf : : arena <wink> ’ <number> | void set ( std : : reference_wrapper <refwrappedtype> const_string_ref , | ^ ~ ~ / usr / local / include / google / protobuf / arenastring . h : <number> : <number> : note : template argument deduction / substitution failed : in file included from . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . h : <number> <time> : note types ‘ std : : reference_wrapper <_tp> ’ and ‘ const string * ’ { aka ‘ const std : : __cxx11 : : basic_string <char> * ’ } <number> | param_str_ . set ( & : : protobuf_namespace_id : : internal : : getemptystringalreadyinited ( ) , value , getarena ( )); | ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ` ` ` did anybody meet the same issue ? # # # additional context _no response_",2
opencv/opencv,"remap cv2 # # # system information opencv python version : <number> . <number> operating system / platform : ubuntu <number> python version : <number> . <number> # # # detailed description remap function cut end of image i have x_coords and y_coord and use remap , but output missing end of image [ image ] ( <url> ori img : ! [ original ] ( <url> # # # steps to reproduce here is code use remap [ y_coords . txt ] ( <url> ` ` ` import numpy as np import cv2 x_coords = np . loadtxt ( ' x_coords . txt ' ) y_coords = np . loadtxt ( ' y_coords . txt ' ) remapped = cv2 . remap ( ori , x_coords , y_coords , cv2 . inter_cubic , none , cv2 . border_replicate ) cv2 . imshow ( "" remap "" , remapped ) cv2 . waitkey ( ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"unable to build c + + solution using opencv <number> . <number> with openvino <number> . <number> # # # system information opencv version : <number> . <number> operating system / platform : ubuntu <number> cuda version : <number> cudnn version : <number> . <number> # # # detailed description hey , so my ultimate goal is to build opencv <number> . <number> with support for cuda <number> and openvino <number> . <number> and install it so that i can further use it within my c + + solution . opencv builds correctly with support for both cuda and openvino versions mencioned above but then when i am compiling my c + + code i get the following error : / usr / bin / ld : warning : libinference_engine_transformations . so , needed by / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so , not found ( try using - rpath or - rpath - link ) / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` vtable for ngraph : : pass : : smartreshape ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` vtable for ngraph : : pass : : setbatchsize ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : op : : util : : has_f16_constants ( std : : shared_ptr < ngraph : : function const > const & ) ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : pass : : disableconvertconstantfoldingonconstpath : : disableconvertconstantfoldingonconstpath ( std : : vector < ngraph : : element : : type , std : : allocator < ngraph : : element : : type > > const & ) ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : primitivespriority : : getprimitivespriority [ abi : cxx11 ] ( ) const ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` vtable for ngraph : : pass : : serialize ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : dequantizationattr : : getdequantizationattr [ abi : cxx11 ] ( ) const ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` typeinfo for ngraph : : variantwrapper < ngraph : : dequantizationattr > ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : pass : : serialize : : run_on_function ( std : : shared_ptr < ngraph : : function > ) ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : pass : : serialize : : serialize ( std : : ostream & , std : : ostream & , ngraph : : pass : : serialize : : version , std : : map < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , ngraph : : opset , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , std : : allocator < std : : pair < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const , ngraph : : opset > > > ) ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : fusednames : : getnames [ abi : cxx11 ] ( ) const ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` typeinfo for ngraph : : variantwrapper < ngraph : : primitivespriority > ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` typeinfo for ngraph : : variantwrapper < ngraph : : fusednames > ' / / opt / intel / openvino_2021 / inference_engine / lib / intel64 / libinference_engine . so : undefined reference to ` ngraph : : pass : : serialize : : serialize ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const & , std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const & , ngraph : : pass : : serialize : : version , std : : map < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > , ngraph : : opset , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , std : : allocator < std : : pair < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const , ngraph : : opset > > > ) ' any other information that you need feel free to ask and thank you so much for the help . # # # steps to reproduce the opencv cmake flags that i used were the following ones : cmake . <repeated> - dbuild_opencv_apps = off - dbuild_opencv_aruco = off - dbuild_opencv_bgsegm = off - dbuild_opencv_bioinspired = off - dbuild_opencv_calib3d = on - dbuild_opencv_ccalib = on - dbuild_opencv_core = on - dbuild_opencv_datasets = off - dbuild_opencv_dnn = on - dbuild_opencv_dnn_objdetect = off - dbuild_opencv_dnn_superres = off - dbuild_opencv_dpm = off - dbuild_opencv_face = off - dbuild_opencv_features2d = on - dbuild_opencv_flann = on - dbuild_opencv_fuzzy = off - dbuild_opencv_gapi = off - dbuild_opencv_hfs = off - dbuild_opencv_highgui = on - dbuild_opencv_img_hash = on - dbuild_opencv_imgcodecs = on - dbuild_opencv_imgproc = on - dbuild_opencv_intensity_transform = off - dbuild_java_bindings_generator = off - dbuild_opencv_js = off - dbuild_opencv_line_descriptor = off - dbuild_opencv_mcc = off - dbuild_opencv_ml = on - dbuild_opencv_objc_bindings_generator = off - dbuild_opencv_objdetect = on - dbuild_opencv_optflow = off - dbuild_opencv_phase_unwrapping = off - dbuild_opencv_photo = on - dbuild_opencv_plot = on - dbuild_opencv_python3 = off - dbuild_opencv_python_bindings_generator = off - dbuild_opencv_python_tests = off - dbuild_opencv_quality = off - dbuild_opencv_rapid = off - dbuild_opencv_reg = off - dbuild_opencv_rgdb = off - dbuild_opencv_saliency = off - dbuild_opencv_shape = off - dbuild_opencv_stereo = off - dbuild_opencv_stitching = off - dbuild_opencv_structured_light = off - dbuild_opencv_superres = off - dbuild_opencv_surface_matching = off - dbuild_opencv_text = off - dbuild_opencv_tracking = off - dbuild_opencv_ts = off - dbuild_opencv_video = on - dbuild_opencv_videoio = on - dbuild_opencv_videostab = off - dbuild_opencv_world = on - dbuild_opencv_xfeatures2d = on - dbuild_opencv_ximgproc = on - dbuild_opencv_xobjdetect = on - dbuild_opencv_xphoto = on - dcmake_configuration_types = release - dwith_ffmpeg = on - dwith_inf_engine = on - dinf_engine_lib_dirs =""/ opt / intel / openvino_2021 / inference_engine / lib / intel64 "" - dinf_engine_include_dirs =""/ opt / intel / openvino_2021 / inference_engine / include "" - dcmake_find_root_path =""/ opt / intel / openvino_2021 / deployment_tools / ngraph ;/ opt / intel / openvino_2021 / inference_engine / external / tbb "" - dinf_engine_release = <phone> - denable_cxx11 = on - dopencv_enable_nonfree = off - dbuild_java = off - dwith_msmf = off - dwith_msmf_dxva = off - dopencv_extra_modules_path = ~ / development / opencv_contrib - <number> . <number> / modules / - dbuild_zlib = on - dbuild_opencv_hdf = off - dwith_cuda = on - denable_fast_math = <number> - dcuda_fast_math = <number> - dwith_cublas = <number> - dcuda_arch_bin = <number> the output of those cmake flags is the following # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"the arm32 linux platform uses opencv3 . <number> # # # descripe the doc issue hello , i use the arm32 linux platform and the operating system is ubuntu16 . <number> . after compiling opencv3 . <number> with the cross - compiler , the following errors occurred when i generated an opencv program separately above is my error information [ 图片 ] ( <url>",2
opencv/opencv,"unable to link statically to opencv <number> # # # system information opencv version : <number> operating system / platform : ubuntu <number> compiler & compiler version : gcc <number> . <number> # # # detailed description i am trying to build a shared library that uses opencv . i have opencv installed via ` apt - get install libopencv - dev ` in the docker image where i am building my library , but i need my library to be used in a lean ubuntu <number> machine ( ie ` apt - get install libopencv - dev ` , all required ` . so ` need to be placed in a specific folder ) . this means that i need to either <number> . place all opencv . so and ( their pre - requisites as well ) in a specific folder or <number> . statically link with opencv ( and its pre - requisites ) since <number> . seems very painful i was trying to follow <number> ; apparently it should be a matter of simply setting ` set ( opencv_static on ) ` before doing ` find_package ( opencv required ) ` on my library ' s ` cmakelists . txt ` unfortunately this does not seem to work as opencv is still dynamically linked . i saw numerous threads about statically linking with opencv but they all seem to build opencv from source instead of using the system - installed one . is it not possible to statically link opencv without building from source ? # # # steps to reproduce build a simple ` cmake ` project using opencv with ` set ( opencv_static on ) ` and check that the produced library is dynamically linking opencv see snippet below for an example ` ` ` set ( opencv_static on ) find_package ( opencv required ) # set library target add_library ( ${ library_name } shared ${ library_dir } / src / my_lib_file . cpp ) # include headers target_include_directories ($ { library_name } public ${ opencv_include_dirs } ) target_link_libraries ( ${ library_name } public ${ opencv_libs } ) link_directories ( ${ opencv_lib_dir } ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"scan image coords issues # # # system information we use opencv version <number> . <number> . even the issue is there on the opencv version <number> . <number> also . "" cv : : exception : opencv ( <number> . <number> ) / home / runner / work / opencv / opencv / opencv - <number> . <number> / modules / imgproc / src / resize . cpp : <number> : error : ( - <number> : assertion failed ) ssize . empty ( ) in function ' resize ' "" when we try to scan the image coordinates , and then crop the image . we are getting this issue . need urgent help on it . we are blocked here for many days . # # # detailed description we use opencv version <number> . <number> . even the issue is there on the opencv version <number> . <number> also . "" cv : : exception : opencv ( <number> . <number> ) / home / runner / work / opencv / opencv / opencv - <number> . <number> / modules / imgproc / src / resize . cpp : <number> : error : ( - <number> : assertion failed ) ! ssize . empty ( ) in function ' resize ' "" when we try to scan the image coordinates , and then crop the image . we are getting this issue . need urgent help on it . we are blocked here for many days . # # # steps to reproduce we use opencv version <number> . <number> . even the issue is there on the opencv version <number> . <number> also . "" cv : : exception : opencv ( <number> . <number> ) / home / runner / work / opencv / opencv / opencv - <number> . <number> / modules / imgproc / src / resize . cpp : <number> : error failed ) ! ssize . empty ( ) in function ' resize ' "" when we try to scan the image coordinates , and then crop the image . we are getting this issue . need urgent help on it . we are blocked here for many days . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"how to infer unet by opencv ? # # # descripe the feature and motivation is there any example with "" . cpp "" or "" . py "" i can refer to unet inference ？ so this can be used to unet inference ? please help me . # # # additional context _no response_",2
opencv/opencv,"when compiling opencv4 . <number> , i find that the objdetect module is not compiled . # # # system information when compiling opencv4 . <number> , i find that the objdetect module is not compiled . the libopencv_objdetect . so and libopencv_objdetect4 . <number> . so files are not found in the lib directory . in addition , the objdetect module is included in opencv4 . <number> compilation . why is that ? # # # detailed description <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / cuda_gpu_mat_nd . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / cuda_host_mem . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / cuda_info . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / cuda_stream . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / datastructs . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / directx . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / downhill_simplex . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / dxt . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / gl_core_3_1 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / glob . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / hal_internal . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / kmeans . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / lapack . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / lda . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / logger . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / lpsolver . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / lut . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / mathfuncs . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / mathfuncs_core . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matmul . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_c . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_decomp . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_expressions . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_iterator . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_operations . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_sparse . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_transform . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / matrix_wrap . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / mean . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / merge . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / minmax . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / norm . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / ocl . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / opencl / runtime / opencl_clblas . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / opencl / runtime / opencl_clfft . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / opencl / runtime / opencl_core . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / opengl . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / out . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / ovx . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / parallel . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / parallel / parallel . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / parallel / parallel_openmp . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / parallel / parallel_tbb . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / parallel_impl . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / pca . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence_base64_encoding . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence_json . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence_types . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence_xml . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / persistence_yml . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / rand . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / softfloat . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / split . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / stat . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / stat_c . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / stl . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / sum . dispatch . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / system . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / tables . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / trace . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / types . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / umatrix . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / utils / datafile . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / utils / filesystem . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / utils / logtagconfigparser . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / utils / logtagmanager . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / utils / samples . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / va_intel . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / opencl_kernels_core . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / arithm . sse4_1 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / matmul . sse4_1 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / stat . sse4_2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / mathfuncs_core . avx . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / mathfuncs_core . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / stat . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / arithm . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / convert . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / convert_scale . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / count_non_zero . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / matmul . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / mean . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / merge . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / split . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / sum . avx2 . cpp . o [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / matmul . avx512_skx . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_core . so [ <percent> ] built target opencv_core [ <percent> ] processing opencl kernels ( imgproc ) scanning dependencies of target opencv_flann [ <percent> ] building cxx object modules / flann / cmakefiles / opencv_flann . dir / src / miniflann . cpp . o scanning dependencies of target opencv_ml [ <percent> ] building cxx object modules / flann / cmakefiles / opencv_flann . dir / src / flann . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / ann_mlp . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / boost . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / data . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / gbt . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / em . cpp . o scanning dependencies of target opencv_imgproc [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / accum . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / inner_functions . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / accum . dispatch . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / kdtree . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / approx . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / knearest . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / bilateral_filter . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / blend . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / lr . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / box_filter . dispatch . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / nbayes . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / canny . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / rtrees . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / svm . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / clahe . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / svmsgd . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / color . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / testset . cpp . o [ <percent> ] building cxx object modules / ml / cmakefiles / opencv_ml . dir / src / tree . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / color_hsv . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / color_lab . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / color_rgb . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / color_yuv . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / colormap . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / connectedcomponents . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / contours . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / convhull . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_ml . so [ <percent> ] built target opencv_ml [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / corner . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / cornersubpix . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / demosaicing . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / deriv . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / distransform . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / drawing . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / emd . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / featureselect . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / filter . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / floodfill . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / gabor . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / generalized_hough . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / geometry . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / grabcut . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / hershey_fonts . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / histogram . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_flann . so [ <percent> ] built target opencv_flann [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / hough . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / imgwarp . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / intelligent_scissors . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / intersection . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / linefit . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / lsd . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / main . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / matchcontours . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / median_blur . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / min_enclosing_triangle . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / moments . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / morph . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / phasecorr . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / pyramids . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / resize . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / rotcalipers . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / samplers . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / segmentation . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / shapedescr . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / smooth . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / spatialgradient . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / subdivision2d . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / sumpixels . dispatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / tables . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / templmatch . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / thresh . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / utils . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / opencl_kernels_imgproc . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / imgwarp . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / resize . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / accum . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / box_filter . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / filter . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_hsv . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_rgb . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_yuv . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / median_blur . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / morph . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / smooth . sse4_1 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / corner . avx . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / accum . avx . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / imgwarp . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / src / resize . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / accum . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / bilateral_filter . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / box_filter . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / filter . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_hsv . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_rgb . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / color_yuv . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / median_blur . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / morph . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / smooth . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / sumpixels . avx2 . cpp . o [ <percent> ] building cxx object modules / imgproc / cmakefiles / opencv_imgproc . dir / sumpixels . avx512_skx . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_imgproc . so [ <percent> ] built target opencv_imgproc [ <percent> ] processing opencl kernels ( photo ) [ <percent> ] processing opencl kernels ( features2d ) scanning dependencies of target opencv_photo [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / contrast_preserve . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / calibrate . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / denoise_tvl1 . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / align . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / denoising . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / denoising . cuda . cpp . o scanning dependencies of target opencv_imgcodecs [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / loadsave . cpp . o scanning dependencies of target opencv_features2d [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / affine_feature . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / agast . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / agast_score . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / utils . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / hdr_common . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / inpaint . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / akaze . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_base . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / merge . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_bmp . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_exr . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_gdal . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / bagofwords . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_gdcm . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / blobdetector . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / brisk . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / draw . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_hdr . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / npr . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / seamless_cloning . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / seamless_cloning_impl . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / src / tonemap . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_jpeg . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_jpeg2000 . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_jpeg2000_openjpeg . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_pam . cpp . o [ <percent> ] building cxx object modules / photo / cmakefiles / opencv_photo . dir / opencl_kernels_photo . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / dynamic . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / evaluation . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / fast . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_pfm . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_png . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_pxm . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_sunras . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_tiff . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / fast_score . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / grfmt_webp . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / feature2d . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / gftt . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / kaze . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / bitstrm . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / kaze / akazefeatures . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / rgbe . cpp . o [ <percent> ] building cxx object modules / imgcodecs / cmakefiles / opencv_imgcodecs . dir / src / exif . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / kaze / kazefeatures . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / kaze / fed . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / kaze / nldiffusion_functions . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / keypoint . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / main . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / matchers . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_imgcodecs . so [ <percent> ] built target opencv_imgcodecs [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / mser . cpp . o scanning dependencies of target opencv_videoio [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_photo . so [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / videoio_registry . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / orb . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / videoio_c . cpp . o [ <percent> ] built target opencv_photo [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / cap . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / cap_images . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / sift . dispatch . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / opencl_kernels_features2d . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / sift . sse4_1 . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / cap_mjpeg_encoder . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / cap_mjpeg_decoder . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / backend_plugin . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / src / fast . avx2 . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / backend_static . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / container_avi . cpp . o [ <percent> ] building cxx object modules / videoio / cmakefiles / opencv_videoio . dir / src / cap_v4l . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / sift . avx2 . cpp . o [ <percent> ] building cxx object modules / features2d / cmakefiles / opencv_features2d . dir / sift . avx512_skx . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_videoio . so [ <percent> ] built target opencv_videoio scanning dependencies of target opencv_highgui [ <percent> ] building cxx object modules / highgui / cmakefiles / opencv_highgui . dir / src / backend . cpp . o [ <percent> ] building cxx object modules / highgui / cmakefiles / opencv_highgui . dir / src / roiselector . cpp . o [ <percent> ] building cxx object modules / highgui / cmakefiles / opencv_highgui . dir / src / window . cpp . o [ <percent> ] building cxx object modules / highgui / cmakefiles / opencv_highgui . dir / src / window_gtk . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_features2d . so [ <percent> ] built target opencv_features2d [ <percent> ] processing opencl kernels ( calib3d ) scanning dependencies of target opencv_calib3d [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / calibinit . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / ap3p . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / calibration . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / calibration_handeye . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / checkchessboard . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / chessboard . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / circlesgrid . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / compat_ptsetreg . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / dls . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / epnp . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / fisheye . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_highgui . so [ <percent> ] built target opencv_highgui [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / five - point . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / fundam . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / homography_decomp . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / ippe . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / levmarq . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / main . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / p3p . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / polynom_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / posit . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / ptsetreg . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / quadsubpix . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / rho . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / solvepnp . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / sqpnp . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / stereobm . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / stereosgbm . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / triangulate . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / undistort . dispatch . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / upnp . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / degeneracy . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / dls_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / essential_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / estimator . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / fundamental_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / gamma_values . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / homography_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / local_optimization . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / pnp_solver . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / quality . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / ransac_solvers . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / sampler . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / termination . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / src / usac / utils . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / opencl_kernels_calib3d . cpp . o [ <percent> ] building cxx object modules / calib3d / cmakefiles / opencv_calib3d . dir / undistort . avx2 . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_calib3d . so [ <percent> ] built target opencv_calib3d [ <percent> ] processing opencl kernels ( stitching ) [ <percent> ] processing opencl kernels ( video ) scanning dependencies of target opencv_stitching [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / camera . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / blenders . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / exposure_compensate . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / matchers . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / motion_estimators . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / seam_finders . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / autocalib . cpp . o scanning dependencies of target opencv_video [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / bgfg_knn . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / bgfg_gaussmix2 . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / camshift . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / stitcher . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / dis_flow . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / timelapsers . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / ecc . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / util . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / warpers . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / src / warpers_cuda . cpp . o [ <percent> ] building cxx object modules / stitching / cmakefiles / opencv_stitching . dir / opencl_kernels_stitching . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / kalman . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / lkpyramid . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / optflowgf . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / optical_flow_io . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_feature . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_feature_set . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_mil_model . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_mil_state . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_model . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_sampler . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_sampler_algorithm . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracker_state_estimator . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracking_feature . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / detail / tracking_online_mil . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / tracker . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / tracker_dasiamrpn . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / tracker_goturn . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / tracking / tracker_mil . cpp . o [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / src / variational_refinement . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_stitching . so [ <percent> ] built target opencv_stitching [ <percent> ] building cxx object modules / video / cmakefiles / opencv_video . dir / opencl_kernels_video . cpp . o [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_video . so [ <percent> ] built target opencv_video # # # steps to reproduce cmake_option - dcmake_build_type = release - dwith_protobuf = off - dwith_webp = off - dwith_ipp = off - dwith_ade = off - dbuild_zlib = on - dbuild_jpeg = on - dbuild_png = on - dwith_openexr = off - dbuild_tests = off - dbuild_perf_tests = off - dbuild_opencv_apps = off - dcmake_skip_rpath = true - dbuild_opencv_python3 = off - dbuild_opencv_videoio = off - dwith_ffmpeg = off - dwith_tiff = on - dbuild_tiff = on - dwith_jasper = off - dbuild_jasper = off - dcv_trace = off # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"getting bad argument ( can not read onnx file : . \ \ yolov5s . onnx ) when trying to load yolov5 onnx model . # # # system information opencv version <number> . <number> operating system / platform : windows <number> pro ( <number> ) compiler & compiler version : microsoft visual c + + compiler ( msvc toolset ) v142 ( visual studio <number> ) # # # detailed description i am trying to deploy my yolov5 model with opencv <number> . <number> using c + + . however , no matter what i try to do open cv ' s onnx importer throws the following runtime error : ` ` ` text opencv ( <number> . <number> ) error : bad argument ( can not read onnx file : . \ \ yolov5s . onnx ) in cv : : dnn : : dnn4_v20220524 : : onnximporter : : onnximporter , file c :\\ opencv - <number> . <number> \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp , line <number> ` ` ` this is the command i am using to export the model in powershell : ` ` ` text python . \ \ yolov5 \ \ export . py - - data "" c :\\ vehicle_engine_yolov5s_r11 \ \ data . yaml "" - - weights "" c :\\ vehicle_engine_yolov5s_r11 \ \ weights \ \ best . pt "" - - simplify - - include onnx ` ` ` i tried it with my own exported weights using export . py and with the default weights and the result is still the same . - i have also switched between the ` master ` , ` v7 . <number> ` and ` v7 . <number> ` branches and retrying the export . - i also tried exporting the model in protobuf ( . pb ) format . - i even tried building open cv <number> . <number> from source . * * all to no avail . * * i have honestly fired all my bullets and could not find anything online related to the problem and error , i cannot even know where the process is failing and if there ' s an error from my end . here ' s my ` main ( void ) ` function where i load the model . i believe there ' s a problem with the opencv code that loads the onnx model . below is the code that produces the said problem steps to reproduce ` ` ` c + + <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> <fstream> const float input_width { <number> . 0 f }; const float input_height { <number> . 0 f }; const float score_threshold { <number> . 5 f }; const float nms_threshold { <number> . 4 5 f }; const float confidence_threshold { <number> . 4 5 f }; const float font_scale { <number> . 7 f }; const int font_face { cv : : font_hershey_simplex }; const int thickness { <number> }; cv : : scalar black { cv : : scalar ( <number> , <number> , <number> ) }; cv : : scalar blue { cv : : scalar ( <number> , <number> , <number> ) }; cv : : scalar yellow { cv : : scalar ( <number> , <number> , <number> ) }; cv : : scalar red { cv : : scalar ( <number> , <number> , <number> ) }; void drawlabel ( cv : : mat & inputimage , std : : string label , int left , int top ) { int baseline { <number> }; cv : : size labelsize { cv : : gettextsize ( label , font_face , font_scale , thickness , & baseline ) }; top = cv : : max ( top , labelsize . height ) ; cv : : point topleftcorner { cv : : point ( left , top ) }; cv : : point bottomrightcorner { cv : : point ( ( left + labelsize . width ) , ( top + labelsize . height + baseline ) ) }; cv : : rectangle ( inputimage , topleftcorner , bottomrightcorner , black , cv : : filled ) ; cv : : puttext ( inputimage , label , cv : : point ( left , ( top + labelsize . height ) ) , font_face , font_scale , yellow , thickness ); } std : : vector < cv : : mat > processinput ( cv : : mat & inputimage , cv : : dnn : : net & net ) { cv : : mat blob {}; cv : : dnn : : blobfromimage ( inputimage , blob , <number> . / <number> . , cv : : size ( input_width , input_height ) , cv : : scalar ( ) , true , false ); net . setinput ( blob ) ; std : : vector < cv : : mat > outputs { }; net . forward ( outputs , net . getunconnectedoutlayersnames ( )); return outputs ; } cv : : mat postprocess ( cv : : mat & inputimage , std : : vector < cv : : mat > & outputs , const std : : vector < std : : string > & classname ) { std : : vector <int> classids { }; std : : vector <float> confidences { }; std : : vector < cv : : rect > boxes { }; float xfactor { inputimage . cols / input_width }; float yfactor { inputimage . rows / input_height }; float * data { ( float <wink> outputs [ <number> ] . data }; const size_t dims { <number> }; const size_t rows { <number> }; for ( size_t i { <number> }; i < rows ; i + + ) { float confidence { data [ <number> ] }; if ( confidence >= confidence_threshold ) { float * classesscores { data + <number> }; cv : : mat scores { <number> , ( int ) classname . size ( ) , cv_32fc1 , classesscores }; cv : : point classid { }; double maxclasscore <elongated> { <number> }; cv : : minmaxloc ( scores , <number> , & maxclasscore <elongated> , <number> , & classid ) ; if ( maxclasscore <elongated> > score_threshold ) { confidences . push_back ( confidence ) ; classids . push_back ( classid . x) ; float centerx { data [ <number> ] }; float centery { data [ <number> ] }; float boxwidth { data [ <number> ] }; float boxheight { data [ <number> ] }; int left { ( int ) ( ( centerx - <number> * boxwidth ) * xfactor ) }; int top { ( int ) ( ( centery - <number> * boxheight ) * yfactor ) }; int width { ( int ) ( boxwidth * xfactor ) }; int height { ( int ) ( boxheight * yfactor ) }; boxes . push_back ( cv : : rect ( left , top , width , height ) ); } } data + = <number> ; } std : : vector <int> indices { }; cv : : dnn : : nmsboxes ( boxes , confidences , score_threshold , nms_threshold , indices ) ; for ( size_t i { <number> }; i < indices . size ( ); i + + ) { int idx { indices [ i ] }; cv : : rect box { boxes [ idx ] }; int left { box . x }; int top { box . y }; int width { box . width }; int height { box . height }; cv : : rectangle ( inputimage , cv : : point ( left , top ) , cv : : point ( ( left + width ) , ( top + height ) ) , blue , <number> * thickness ); std : : string label { cv : : format ( "" % . 2 f "" , confidences [ idx ] ) }; label = classname [ classids [ idx ] ] + "" : "" + label ; drawlabel ( inputimage , label , left , top ) ; } return inputimage ; } cv : : mat postprocess ( cv : : mat & & inputimage , std : : vector < cv : : mat > & outputs , const std : : vector < std : : string > & classname ) { std : : vector <int> classids { }; std : : vector <float> confidences { }; std : : vector < cv : : rect > boxes { }; float xfactor { inputimage . cols / input_width }; float yfactor { inputimage . rows / input_height }; float * data { ( float <wink> outputs [ <number> ] . data }; const size_t dims { <number> }; const size_t rows { <number> }; for ( size_t i { <number> }; i < rows ; i + + ) { float confidence { data [ <number> ] }; if ( confidence >= confidence_threshold ) { float * classesscores { data + <number> }; cv : : mat scores { <number> , ( int ) classname . size ( ) , cv_32fc1 , classesscores }; cv : : point classid { }; double maxclasscore <elongated> { <number> }; cv : : minmaxloc ( scores , <number> , & maxclasscore <elongated> , <number> , & classid ) ; if ( maxclasscore <elongated> > score_threshold ) { confidences . push_back ( confidence ) ; classids . push_back ( classid . x) ; float centerx { data [ <number> ] }; float centery { data [ <number> ] }; float boxwidth { data [ <number> ] }; float boxheight { data [ <number> ] }; int left { ( int ) ( ( centerx - <number> * boxwidth ) * xfactor ) }; int top { ( int ) ( ( centery - <number> * boxheight ) * yfactor ) }; int width { ( int ) ( boxwidth * xfactor ) }; int height { ( int ) ( boxheight * yfactor ) }; boxes . push_back ( cv : : rect ( left , top , width , height ) ); } } data + = <number> ; } std : : vector <int> indices { }; cv : : dnn : : nmsboxes ( boxes , confidences , score_threshold , nms_threshold , indices ) ; for ( size_t i { <number> }; i < indices . size ( ); i + + ) { int idx { indices [ i ] }; cv : : rect box { boxes [ idx ] }; int left { box . x }; int top { box . y }; int width { box . width }; int height { box . height }; cv : : rectangle ( inputimage , cv : : point ( left , top ) , cv : : point ( ( left + width ) , ( top + height ) ) , blue , <number> * thickness ); std : : string label { cv : : format ( "" % . 2 f "" , confidences [ idx ] ) }; label = classname [ classids [ idx ] ] + "" : "" + label ; drawlabel ( inputimage , label , left , top ) ; } return inputimage ; } int main ( void ) { try { std : : vector < std : : string > classlist { "" bus "" , "" car "" , "" truck "" }; cv : : mat frame { cv : : imread ( "" c :\\\\ yolo \ \ \ \ datasets \ \ \ \ vehicle_dataset \ \ \ \ test \ \ \ \ images \ \ \ \ <number> . jpg "" ) }; cv : : dnn : : net net { cv : : dnn : : readnet ( "" . \ \ \ \ yolov5s . onnx "" ) }; / / the line that throws the runtime error . std : : vector <double> layerstimes { }; std : : vector < cv : : mat > detections { processinput ( frame , net ) }; cv : : mat img { postprocess ( frame . clone ( ) , detections , classlist ) }; double freq { cv : : gettickfrequency ( ) / <number> }; double timetaken { net . getperfprofile ( layerstimes ) / freq }; std : : string label { cv : : format ( "" inference time :\\ t % . 4 f ms "" , timetaken ) }; cv : : puttext ( img , label , cv : : point ( <number> , <number> ) , font_face , font_scale , red ) ; cv : : imshow ( "" output "" , img ) ; cv : : waitkey ( <number> ); return <number> ; } catch ( cv : : exception & e ) { std : : cout < < std : : endl < < e . what ( ) < < std : : endl ; return - <number> ; } } ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",2
opencv/opencv,"' haarcascade_fontalface_default . xml ' in read mode # # # descripe the doc issue i am very new to opencv and was just running my first program ever in the terminal when i get the phrase : [ error : <number> <user> . <number> ] global / users / runner / work / opencv - python / opencv - python / opencv / modules / core / src / persistence . cpp ( <number> ) open can not open file : ' haarcascade_fontalface_default . xml ' in read mode does anyone know what this means ? here is my code for reference import cv2 trained_face_data = cv2 . cascadeclassifier ( ' haarcascade_fontalface_default . xml ' ) img = cv2 . imread ( ' rd . png ' ) cv2 . imshow ( ' clever programmer face detector ' , img ) cv2 . waitkey ( ) print ( ' code completed ' ) ` ` ` # # # fix suggestion _no response_",2
opencv/opencv,"cv : : videocapture will affect gpu performance ? my model takes 6 ms when testing a single frame of image , but it takes 8 ms when using cv : : videocapture to get video frame , and after dozens of inferences the time - consuming increases to 1 2 ms . win10 opencv version <number>",2
opencv/opencv,"opencv ios use in <number> hello , i am just getting into opencv and want to understand the following as i understand opencv [ does not support metal ] ( <url> so as a result there is no way for opencv to use gpu on ios devices . is that correct ? <number> . is my understanding correct that it only makes sense to use opencv on ios devices where opencv performance on cpu is good for app use cases ?",2
opencv/opencv,"why is the image merge area white [ savedscreen1212 ] ( <url> i do not know why two pictures merged , the overlapping area is white i hope the teacher can give me a solution this my scripts ： ` ` ` private void warptriangle ( mat img1 , mat img2 , list <point2f> tri1 , list <point2f> tri2 , point2f [ ] _points2f ) { ( list < list <point2f> > inputpoints , list < list <point2f> > keypoints ) = this . init ( _points2f ) ; for ( int i = <number> ; i < inputpoints . count ; i + + ) / / 遍历每个点 { tri1 = keypoints [ i ] ; / / 一个点就是一个三角形像素 tri2 = inputpoints [ i ] ; opencvsharp . rect r1 = cv2 . boundingrect ( tri1 ) ; opencvsharp . rect r2 = cv2 . boundingrect ( tri2 ) ; list <point2f> tri1cropped = new list <point2f> ( ) , tri2cropped = new list <point2f> (); list <point> tri2croppedint = new list <point> (); for ( int ii = <number> ; ii < <number> ; ii + + ) { tri1cropped . add ( new point2f ( tri1 [ ii ] . x - r1 . x , tri1 [ ii ] . y - r1 . y ) ); tri2cropped . add ( new point2f ( tri2 [ ii ] . x - r2 . x , tri2 [ ii ] . y - r2 . y ) ); / / fillconvexpoly needs a vector of point and not point2f tri2croppedint . add ( new point ( mathf . floor ( tri2 [ ii ] . x - r2 . x) , mathf . floor ( tri2 [ ii ] . y - r2 . y ) )); } mat img1cropped = new mat ( ); img1cropped = img1 . getrectsubpix ( r1 . size , r1 . center ) ; mat warpmat = cv2 . getaffinetransform ( tri1cropped , tri2cropped ) ; mat img2cropped = mat . zeros ( size , size , img1cropped . type ( )); cv2 . warpaffine ( img1cropped , img2cropped , warpmat , img2cropped . size ( ) , interpolationflags . linear , bordertypes . reflect101 ) ; mat mask = mat . zeros ( size , size , mattype . cv_32fc3 ) ; cv2 . fillconvexpoly ( mask , tri2croppedint , new scalar ( <number> , <number> , <number> ) , linetypes . antialias , <number> ); cv2 . multiply ( img2cropped , mask , img2cropped ) ; / / mat matout = mat . zeros ( new size ( size , size ) , img1 . type ( )); / / cv2 . add ( matout , img2cropped , matout ) ; float tx = tri2 . min ( x => x . x) ; float ty = tri2 . min ( y => y . y ) ; float [ ] warp_values = { <number> . 0 f , <number> . 0 f , tx , <number> . 0 f , <number> . 0 f , ty }; / / 图片偏移 mat translation_matrix = new mat ( <number> , <number> , mattype . cv_32f , warp_values ) ; cv2 . warpaffine ( img2cropped , img2cropped , translation_matrix , new size ( size , size ) ); cv2 . multiply ( img2cropped , new scalar ( <number> , <number> , <number> ) - mask , img2cropped ) ; cv2 . add ( img2 , img2cropped , img2 ) ; / / float tx = tri2 . min ( x => x . x) ; / / float ty = tri2 . min ( y => y . y ) ; / / create the translation matrix using tx and ty / / float [ ] warp_values = { <number> . 0 f , <number> . 0 f , tx , <number> . 0 f , <number> . 0 f , ty }; / / 图片偏移 / / mat translation_matrix = new mat ( <number> , <number> , mattype . cv_32f , warp_values ) ; / / cv2 . warpaffine ( matout , matout , translation_matrix , new size ( size , size ) ); / / cv2 . add ( img2 , matout , img2 ) ; } cv2 . flip ( img2 , img2 , flipmode . x) ; / / 图片翻转 cv2 . imshow ( "" output "" , img2 ) ; / / img2 . convertto ( img2 , mattype . cv_8uc3 , <number> ); / / texture2d _tex = unity . mattotexture ( img2 ) ; / / byte [ ] bytes = _tex . encodetopng ( ); / / system . io . file . writeallbytes ( application . streamingassetspath + "" / texture / savedscreen . png "" , bytes ) ; debug . log ( "" 输出图像 "" ); } ` ` `",2
opencv/opencv,"bug ( javascript ) mean / meanstddev broken for 8 uc3 images < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => <number> . <number> - operating system / platform => macos <number> # # # # # detailed description javascript functions ` mean ` and ` meanstddev ` ( and probably others ) cause an uncaught exception when applied with masks on <number> - channel images . < img width = "" <number> "" alt = "" image "" src = "" <url> # # # # # steps to reproduce ` ` ` let src = . <repeated> let dst1 = cv . mat . zeros ( src . cols , src . rows , cv . cv_8uc1 ) ; let mask1 = cv . mat . zeros ( src . cols , src . rows , dst1 . type ( )); let dst3 = cv . mat . zeros ( src . cols , src . rows , cv . cv_8uc3 ) ; let mask3 = cv . mat . zeros ( src . cols , src . rows , dst3 . type ( )); const mean_val = cv . mean ( dst1 , mask1 ) ; / / < - - - works const mean_val = cv . mean ( dst3 , mask3 ) ; / / < - - - causes uncaught exception error ` ` `",2
opencv/opencv,"in file included from / home / x <elongated> / opencv / modules / python / src2 / cv2 . cpp : <number> : <number> : / home / x <elongated> / opencv / build / modules / python_bindings_generator / pyopencv_generated_include . h : <time> : fatal error : opencv2 / hdf / hdf5 . hpp file or directory <hashtag> include </hashtag> "" opencv2 / hdf / hdf5 . hpp "" ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ i am building opencv - cuda , it happens at the end .",2
opencv/opencv,how to build for freebsd <number> ? system information ( version ) opencv => <number> . <number> - operating system / platform => freebsd <number> - compiler => clang / llvm and gcc <number> - kde5 plasma has anyone able to compile opencv <number> . <number> successfully for freebsd <number> ? if so what was used to compile opencv and also what commands was used ? thanks .,2
opencv/opencv,"how opencv load . dylibs under macos im working on python = = <date> , macox = = <number> i import cv2 as cv , and i find the some dylib under cv2 / . dylibs are loaded , but i can not find out where the code loads them . i need help ! <repeated>",2
opencv/opencv,"failed to install opencv <number> . <number> on ubuntu < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> . <number> - operating system / platform => linux <number> bit - compiler => gnu compilers - - > - opencv => : grey_question : - operating system / platform => : grey_question : - compiler => : grey_question : # # # # # detailed description hi i am a newbie on installing opencv from scratch . i am trying to install opencv - <number> . <number> on ubuntu followed the instruction . the first step is successful without any error : cmake - d cmake_install_prefix = my_opencv_4 . <number> / scratch / softwares / packages / opencv - <number> . <number> however , in the second step “ cmake - - build . ” i met the following errors : / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_image . h : <number> : <number> : note : in expansion of macro ‘ iw_inline ’ <number> | static iw_inline iwibordersize iwisizetobordersize ( | ^ ~ ~ ~ ~ ~ ~ ~ ~ in file included from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw_own . h : <number> , from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / src / iw_core . c : <number> : / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_image . h : <number> <time> : error : expected ‘ = ’ , ‘ , ’ , ‘ ; ’ , ‘ asm ’ or ‘ attribute ’ before ‘ iwisizetobordersize ’ <number> | static iw_inline iwibordersize iwisizetobordersize ( | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ in file included from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw_own . h : <number> , from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / src / iw_core . c : <number> : / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_core . h : <number> <time> : error : unknown type name ‘ inline ’ <number> | <hashtag> define </hashtag> iw_inline inline | ^ ~ ~ ~ ~ ~ / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_image . h : <number> : <number> : note : in expansion of macro ‘ iw_inline ’ <number> | static iw_inline iwibordersize iwisizesymtobordersize ( | ^ ~ ~ ~ ~ ~ ~ ~ ~ in file included from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw_own . h : <number> , from / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / src / iw_core . c : <number> : / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_image . h : <number> <time> : error : expected ‘ = ’ , ‘ , ’ , ‘ ; ’ , ‘ asm ’ or ‘ attribute ’ before ‘ iwisizesymtobordersize ’ <number> | static iw_inline iwibordersize iwisizesymtobordersize ( | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ / home / btan / prodisk / softwares / tmp / 3 rdparty / ippicv / ippicv_lnx / iw / include / iw / iw_image . h : <number> <time> : error : expected ‘ ; ’ before ‘ void ’ <number> | static iw_inline void * iwishiftptr ( | ^ ~ ~ ~ does anyone can help how to correct it ? thanks # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"fatal error cloning_demo . cpp : <time> : fatal error : opencv2 / photo . hpp such file or directory <number> | <hashtag> include </hashtag> "" opencv2 / photo . hpp "" | ^ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ compilation terminated .",2
opencv/opencv,"how to quickly convert rgb to yuv422 hello ， i am using ` cvtcolor ` will rgb to yuv422 . but this funciton param ` mode ` do not have bgr2yuv ( <number> ) . i use the in - down method to transform ` ` ` void cvtcolor_rgb2yuv422 ( cv : : mat & rgb , cv : : mat & yuv ) { cv : : mat yuv444 ( rgb . rows , rgb . cols , cv_8uc3 ) ; cv : : cvtcolor ( rgb , yuv444 , cv_bgr2yuv ) ; / / chroma subsampling - > yuv422 ; for ( int row = <number> ; row < yuv444 . rows ; row + + ) { for ( int col = <number> ; col < yuv444 . cols ; col + = <number> ) { cv : : vec3b p0_in = yuv444 . at < cv : : vec3b > ( row , col ) ; cv : : vec3b p1_in = yuv444 . at < cv : : vec3b > ( row , col + <number> ); cv : : vec2b p0_out , p1_out ; p0_out . val [ <number> ] = p0_in . val [ <number> ]; p0_out . val [ <number> ] = p0_in . val [ <number> ]; p1_out . val [ <number> ] = p1_in . val [ <number> ]; p1_out . val [ <number> ] = p0_in . val [ <number> ]; yuv . at < cv : : vec2b > ( row , col ) = p0_out ; yuv . at < cv : : vec2b > ( row , col + <number> ) = p1_out ; } } } ` ` ` but this approach takes a long time . how to use the parallel computing method for transformation ? or how to make opencv support yuv422 conversion ? please provide me with reference",2
opencv/opencv,can opencv <date> using cuda <number> ? can opencv <date> using cuda <number> ?,2
opencv/opencv,how to automatically obtain the input image size from the onnx model ？ is there a function in ` opencv dnn ` that automatically gets the input image size from the only ` onnx ` model ？ thanks,2
opencv/opencv,"error : ' off64_t ' undeclared here ( not in a function ) ; did you mean ' off_t ' ? < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => : <number> . <number> - operating system / platform => : ubuntu <number> - compiler => : gcc # # # # # detailed description i want to build opencv <number> . <number> via these options ` ` ` cmake - d cmake_build_type = release \ \ - d cmake_install_prefix <annoyed> usr / local \ \ - d python_default_executable =$( which python3 ) \ \ - d python_executable = ~ / . virtualenvs / cv / bin / python \ \ - d python3_executable =$( which python3 ) \ \ - d python2_executable =$( which python2 ) \ \ - d python3_include_dir =$( python3 - c "" from distutils . sysconfig import get_python_inc ; print ( get_python_inc ( ) ) "" ) \ \ - d python3_packages_path =$( python3 - c "" from distutils . sysconfig import get_python_lib ; print ( get_python_lib ( ) ) "" ) \ \ - d opencv_extra_modules_path = . <repeated> / . <repeated> / opencv_contrib / modules \ \ - d opencv_python3_install_path =$ cwd / opencv - $ cvversion - py3 / lib / python3 . <number> / site - packages \ \ - d build_png = on \ \ - d build_tiff = on \ \ - d build_tbb = on \ \ - d build_jpeg = on \ \ - d build_jasper = on \ \ - d build_zlib = on \ \ - d build_examples = on \ \ - d build_tests = on \ \ - d build_opencv_java = on \ \ - d build_opencv_python2 = on \ \ - d build_opencv_python3 = on \ \ - d build_opencv_enable_nonfree = on \ \ - d opencv_generate_pkgconfig = on \ \ - d with_opencl = on \ \ - d with_ipp = on \ \ - d with_eigen = on \ \ - d build_perf_tests = on \ \ - d build_opencv_python3 = yes \ \ - d with_ffmpeg = on \ \ - d with_v4l = on \ \ - d with_gstreamer = on \ \ - d with_gstreamer_0_10 = on \ \ - d with_gtk = on \ \ - d enable_fast_math = on \ \ - d with_vtk = on \ \ - d with_tbb = on \ \ - d with_qt = on \ \ - d with_opengl = on \ \ - d install_python_examples = on \ \ - d install_c_examples = on \ \ - d install_tests = on \ \ - d enable_neon = on \ \ - d with_libv4l = off . <repeated> ` ` ` but after building these errors occurred so configuring incomplete . cmakeerror . log content : ` ` ` opencv / build / cmakefiles / checktypesize / off64_t . c : <time> : error : ‘ off64_t ’ undeclared here ( not in a function ) ; did you mean ‘ off_t ’ ? opencv / build / cmakefiles / checktypesize / int8 . c : <time> : error : ‘ int8 ’ undeclared here ( not in a function ) ; did you mean ‘ int8_c ’ ? opencv / build / cmakefiles / checktypesize / int16 . c : <time> : error : ‘ int16 ’ undeclared here ( not in a function ) ; did you mean ‘ int16_c ’ ? opencv / build / cmakefiles / checktypesize / int32 . c : <time> : error : ‘ int32 ’ undeclared here ( not in a function ) ; did you mean ‘ int32_c ’ ? ` ` ` how could i resolve these errors ? # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > < ! - - - [ ] i updated to the latest opencv version and the issue is still there - - > < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > < ! - - - [ ] there is reproducer code and related data files : videos , images , onnx , etc - - > < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"imshow ( ) for . png transparent images with rgba <number> channels looks like * * imshow ( )* * cannot show transparency correctly ? i prefer * not <emphasis> * to show rgb some times , with alpha mask , how can i make the transparency displayed ? i can load , i can save , but with imshow ( ) , i cannot display correctly . <repeated>",2
opencv/opencv,"convcert cvmat to pcd file hi , i would like to convert a cv : : mat to pcl format . have you an idea ? thanks",2
opencv/opencv,getting cv2 . error [ screenshot ( <number> ) ] ( <url> above picture is the code and i am getting this error message which is shown below . could anyone help me fix this issue ? ! [ screenshot ( <number> ) ] ( <url>,2
opencv/opencv,"videocapture : read ( ) use video orientation if the video orientation is available if should be used by default with reading frames . - opencv => <number> . x - operating system / platform => linux x64 - compiler => gcc # # # # # detailed description by default imread for images use the exif orientation information . if possible videocapture : : read ( ) should use the orientation information ( if available ) by default too . # # # # # steps to reproduce happens on portrait videos taken by smartphones . if needed i can provide one . # # # # # issue submission checklist - [ x ] i report the issue , it ' s not a question - [ x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files images , onnx , etc",2
opencv/opencv,"about debug i debug opencv in one step , but i could not find the function entry of the matrix plus method .",2
opencv/opencv,"syntaxerror : invalid syntax - - - > frame = cv2 . circle ( frame , ( int ( a ) , int ( b ) ) , <number> , color [ i ] . tolist ( ) , - <number> ) when i tried to use lucas - kanade on optical flow , i could not figure out why there ' s always invalid syntax when i use "" frame = cv2 . circle ( frame , ( int ( a ) , int ( b ) ) , <number> , color [ i ] . tolist ( ) , - <number> ) "" here ' s the code , guys please help me . import numpy as np import cv2 cap = cv2 . videocapture ( ' 0 2 _video / 0 2 _foreground . avi ' ) feature_params = dict ( maxcorners = <number> , qualitylevel = <number> , mindistance = <number> ) lk_params = dict ( winsize = ( <number> ) , maxlevel = <number> ) color = np . random . randint ( <number> , ( <number> ) ) ret , old_frame = cap . read ( ) old_gray = cv2 . cvtcolor ( old_frame , cv2 . color_bgr2gray ) p0 = cv2 . goodfeaturestotrack ( old_gray , mask = none , * * feature_params ) mask = np . zeros_like ( old_frame ) while ( true ) : ret , frame = cap . read ( ) frame_gray = cv2 . cvtcolor ( frame , cv2 . color_bgr2gray ) p1 , st , err = cv2 . calcopticalflowpyrlk ( old_gray , frame_gray , p0 , none , * * lk_params ) good_new = p1 [ st = = <number> ] good_old = p0 [ st = = <number> ] for i , ( new , old ) in enumerate ( zip ( good_new , good_old ) <sad> a , b = new . ravel ( ) c , d = old . ravel ( ) mask = cv2 . line ( mask , ( int ( a ) , int ( b ) , ( int ( c ) , int ( d ) ) , color [ i ] . tolist ( ) , <number> ) frame = cv2 . circle ( frame , ( int ( a ) , int ( b ) ) , <number> , color [ i ] . tolist ( ) , - <number> ) #< - - - - - - - - - - - - - - - the problem is here ! <repeated> <hashtag> image </hashtag> = cv2 . circle ( image , center_coordinates , radius , color , thickness ) img = cv2 . add ( frame , mask ) cv2 . imshow ( ' frame ' , img ) k = cv2 . waitkey ( <number> ) & 0 xff if k = = <number> old_gray = frame_gray . copy ( ) p0 = good_new . reshape ( - <number> , <number> ) cv2 . destroyallwindows ( ) cap . release ( )",2
opencv/opencv,"how to properly use opencl acceleration applied to <number> surround view <number> surround view , multi - threaded processing of pictures from different cameras , opencv ( <number> . <number> ) for image correction and perspective transformation ( remap ) , the cpu usage is very high , so use opencl ( <number> ) to accelerate , but with the increase of the number of threads , the processing time of a single thread increases several times , and the gpu occupancy rate is also high . how to optimize ? <number> . use opencl <number> ? <number> . picture copy , umat has no data pointer , cannot use hardware rga acceleration , can only use copyto , which is time - consuming <number> . it feels that in multi - threading , the operation of opencl is blocked and queued , and the use of mutex has almost no effect on the time consumption",2
opencv/opencv,"cv2 . drawframeaxes broken using python <number> , macos <number> . <number> , opencv <number> . <number> i am not getting the correct axes displayed . the axes are defined as "" ox is drawn in red , oy in green and oz in blue . "" in the [ reference docs ] ( <url> using the code below : - zero rotation has the z - axis ( blue ) right and y - axis ( green ) down ( wrong orientation ) - <number> deg around z - axis , rotates instead around the x - axis ( red ) - <number> deg around y - axis , seems to work - <number> deg around x - axis , rotates instead around the z - axis ( blue ) ` ` ` python def drawaxes ( rvecs ) = np . array ( [ <number> , <number> . ] ) width = <number> im = np . zeros ( ( width , width , <number> ) ) cx , cy = width / / <number> , width / / <number> f = width k = np . array ( [ [ f , <number> , cx ] , [ <number> , f , cy ] , [ <number> , <number> , <number> ] ] , dtype = float ) dist = np . array ( [ <number> , <number> , <number> , <number> , <number> ] , dtype = float ) # set the axes size as a function of the marker ' s # square size and project onto image space s = <number> axis = np . float32 ( [ [ s , <number> ] , [ <number> , s , <number> ] , [ <number> , s ] ] ) . reshape ( - <number> ) imgpts , jac = cv2 . projectpoints ( axis , rvecs , tvecs , k , dist ) cv2 . drawframeaxes ( im , k , dist , rvecs , tvecs , length = . <number> , thickness = <number> ) return im plt . figure ( figsize =( <number> ) ) plt . subplot ( <number> , <number> ) plt . imshow ( drawaxes ( np . array ( [ <number> , <number> . ] ) ) ) plt . title ( "" xyz = ( <number> , <number> ) "" ); plt . subplot ( <number> , <number> ) plt . imshow ( drawaxes ( np . array ( [ <number> , pi / <number> ] ) ) ) plt . title ( "" xyz = ( <number> , <number> ) "" ); plt . subplot ( <number> , <number> ) plt . imshow ( drawaxes ( np . array ( [ <number> , pi / <number> ] ) ) ) plt . title ( "" xyz = ( <number> , <number> ) "" ); plt . subplot ( <number> , <number> ) plt . imshow ( drawaxes ( np . array ( [ pi / <number> , <number> ] ) ) ) plt . title ( "" xyz = ( <number> , <number> ) "" ); ` ` ` [ download ] ( <url>",2
opencv/opencv,"orb featuredetector has different result between c + + and python i use orb featuredetector to extract feature using python and c + + , code sees below . but the output results is different completely , i am sure my input is same , and opencv version all is <number> . <number> . i am confuse , i need some help . [ 3 6 dd59d464c83944dc2cbe65eb050be ] ( <url> ! [ 8 2 9 fc43294916a32f4ca0a56b405f43 ] ( <url>",2
opencv/opencv,"cv2 . imread ( ) fails when filenames have multiple dots ( . ) < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => <number> . <number> - operating system / platform => windows - compiler => pycharm community edition <number> . <number> # # # # # detailed description when the ` cv2 . imread ( ) ` method encounters a filename with multiple dots ( . ) , a ` nonetype ` is returned . i think this might be because the file extension is assumed to be everything after the first dot , and not the last , but it could be something completely different . example : # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` - - > running the following code , with two images in the same directory , ` ` ` . py import cv2 im = cv2 . imread ( "" c <annoyed> users / . <repeated> / videos / captures / test project – main . py 1 8 _05_2022 1 5 _44_50 . png "" ) im2 = cv2 . imread ( "" c <annoyed> users / . <repeated> / videos / captures / screenshot 1 6 _05_2022 2 3 _40_07 . png "" ) print ( im ) print ( im2 ) ` ` ` gives the result : ` ` ` . py [ warn : <number> <user> . <number> ] global d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ imgcodecs \ \ src \ \ loadsave . cpp ( <number> ) cv : : finddecoder imread_ ( ' c <annoyed> users / . <repeated> / videos / captures / test project – main . py 1 8 _05_2022 1 5 _44_50 . png ' <sad> can not open / read file : check file path / integrity none [ [ [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] . <repeated> [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] ] [ [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] . <repeated> [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] ] [ [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] . <repeated> [ <number> <number> <number> ] [ <number> <number> <number> ] [ <number> <number> <number> ] ] ] ` ` ` which shows that the first image ( one dot ) loaded successfully , but the second one ( with the multiple dots ) failed . # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [x ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"systemerror : < built - in function writetextgraph > returned null without setting an error when using cv2 . dnn . writetextgraph - opencv = <number> . <number> - operating system / platform = windows <number> bit - compiler = google colab < - - your description - - > so i recently created a machine learning model with keras and saved it using the save_model function into a . pb file . i want to use this model with opencv , and therefore have been trying to obtain a . pbtxt file to use alongside the dnn module of opencv ( net = cv2 . dnn_detectionmodel ( weightspath , configpath ) ) . yet for some reason , i keep getting the error in the title : systemerror : < built - in function writetextgraph > returned null without setting an error . what i have done : i have been saving the model on google colab , so i have tried restarting the runtime a couple of times . i have made sure my file path was correct . i am not sure how to check if the . pb file is corrupted , but i do not imagine it is since i directly downloaded it from keras . this is the code below model . save ( ' mymodel5 ' , save_format = ' pb ' ) loaded_model = tf . keras . models . load_model ( ' / content / mymodel6 ' ) / / this is the genereated folder from the . save function of keras import cv2 cv2 . dnn . writetextgraph ( loaded_model , ' graph . pbtxt ' ) ` ` `",2
opencv/opencv,"error while installing opencv <number> . <number> on ubuntu <number> ( jetson nano production module ) . # # # # # system information ( version ) < - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => : grey_question : - operating system / platform => : ubuntu <number> : - compiler => : gcc ( ubuntu / linaro <number> . <number> - 3 ubuntu1 ~ <number> ) <number> . <number> : # # # # # detailed description hello ! i am using a jetson production module running on ubuntu <number> ! and i can not install open cv <number> . <number> . i have already done and sudo apt - get update , and sudo apt - get upgrade . # # # # # # <hashtag> steps </hashtag> : i am running this script : # ! / bin / bash set - e echo "" installing opencv <number> . <number> on your jetson nano "" echo "" it will take <number> hours ! "" # reveal the cuda location cd ~ sudo sh - c "" echo ' / usr / local / cuda / lib64 ' > > / etc / ld . so . conf . d / nvidia - tegra . conf "" sudo ldconfig # install the dependencies sudo apt - get install - y build - essential cmake git unzip pkg - config sudo apt - get install - y libjpeg - dev libpng - dev libtiff - dev sudo apt - get install - y libavcodec - dev libavformat - dev libswscale - dev sudo apt - get install - y libgtk2 . <number> - dev libcanberra - gtk * sudo apt - get install - y python3 - dev python3 - numpy python3 - pip sudo apt - get install - y libxvidcore - dev libx264 - dev libgtk - <number> - dev sudo apt - get install - y libtbb2 libtbb - dev libdc1394 - <number> - dev sudo apt - get install - y gstreamer1 . <number> - tools libv4l - dev v4l - utils sudo apt - get install - y libgstreamer1 . <number> - dev libgstreamer - plugins - base1 . <number> - dev sudo apt - get install - y libavresample - dev libvorbis - dev libxine2 - dev sudo apt - get install - y libfaac - dev libmp3lame - dev libtheora - dev sudo apt - get install - y libopencore - amrnb - dev libopencore - amrwb - dev sudo apt - get install - y libopenblas - dev libatlas - base - dev libblas - dev sudo apt - get install - y liblapack - dev libeigen3 - dev gfortran sudo apt - get install - y libhdf5 - dev protobuf - compiler sudo apt - get install - y libprotobuf - dev libgoogle - glog - dev libgflags - dev # remove old versions or previous builds cd ~ sudo rm - rf opencv * # download the latest version wget - o opencv . zip <url> wget - o opencv_contrib . zip <url> # unpack unzip opencv . zip unzip opencv_contrib . zip # some administration to make live easier later on mv opencv - <number> . <number> opencv mv opencv_contrib - <number> . <number> opencv_contrib # clean up the zip files rm opencv . zip rm opencv_contrib . zip # set install dir cd ~ / opencv mkdir build cd build # run cmake cmake - d cmake_build_type = release \ \ - d cmake_install_prefix <annoyed> usr \ \ - d opencv_extra_modules_path = ~ / opencv_contrib / modules \ \ - d eigen_include_path <annoyed> usr / include / eigen3 \ \ - d with_opencl = off \ \ - d with_cuda = on \ \ - d cuda_arch_bin = <number> \ \ - d cuda_arch_ptx = "" "" \ \ - d with_cudnn = on \ \ - d with_cublas = on \ \ - d enable_fast_math = on \ \ - d cuda_fast_math = on \ \ - d opencv_dnn_cuda = on \ \ - d enable_neon = on \ \ - d with_qt = off \ \ - d with_openmp = on \ \ - d build_tiff = on \ \ - d with_ffmpeg = on \ \ - d with_gstreamer = on \ \ - d with_tbb = on \ \ - d build_tbb = on \ \ - d build_tests = off \ \ - d with_eigen = on \ \ - d with_v4l = on \ \ - d with_libv4l = on \ \ - d opencv_enable_nonfree = on \ \ - d install_c_examples = off \ \ - d install_python_examples = off \ \ - d build_new_python_support = on \ \ - d build_opencv_python3 = true \ \ - d opencv_generate_pkgconfig = on \ \ - d build_examples = off . <repeated> # run make free_mem =""$ ( free - m | awk ' / ^ swap / { print <money> } ' ) "" # use "" - j <number> "" only swap space is larger than <number> . 5 gb if [ [ "" free_mem "" - gt "" <number> "" ]]; then no_job = <number> else echo "" due to limited swap , make only uses <number> core "" no_job = <number> fi make - j ${ no_job } sudo rm - r / usr / include / opencv4 / opencv2 sudo make install sudo ldconfig # cleaning ( frees <number> mb ) make clean sudo apt - get update echo "" congratulations ! "" echo "" you have successfully installed opencv <number> . <number> on your jetson nano "" ~ # # # # # # # # # # # # # # # # # # # # # # # # # # # # <hashtag> output </hashtag> - - found pythoninterp : / usr / bin / python2 . <number> ( found suitable version "" <date> "" , minimum required is "" <number> "" ) - - could not find pythonlibs ( missing : python_libraries python_include_dirs ) ( required is exact version "" <date> "" ) traceback ( most recent call last ) : file "" "" , line <number> , in importerror : no module named numpy . distutils - - found pythoninterp : / usr / bin / python3 ( found suitable version "" <number> . <number> "" , minimum required is "" <number> "" ) - - found pythonlibs : / usr / lib / aarch64 - linux - gnu / libpython3 . 6 m . so ( found suitable exact version "" <number> . <number> "" ) - - looking for ccache - not found - - performing test have_cxx_fsigned_char - - performing test have_cxx_fsigned_char - success - - performing test have_c_fsigned_char - - performing test have_c_fsigned_char - success - - performing test have_cxx_ffast_math - - performing test have_cxx_ffast_math - success - - performing test have_c_ffast_math - - performing test have_c_ffast_math - success - - performing test have_cxx_w - - performing test have_cxx_w - success - - performing test have_c_w - - performing test have_c_w - success - - performing test have_cxx_wall - - performing test have_cxx_wall - success - - performing test have_c_wall - - performing test have_c_wall - success - - performing test have_cxx_werror_return_type - - performing test have_cxx_werror_return_type - success - - performing test have_c_werror_return_type - - performing test have_c_werror_return_type - success - - performing test have_cxx_werror_non_virtual_dtor - - performing test have_cxx_werror_non_virtual_dtor - success - - performing test have_c_werror_non_virtual_dtor - - performing test have_c_werror_non_virtual_dtor - failed - - performing test have_cxx_werror_address - - performing test have_cxx_werror_address - success - - performing test have_c_werror_address - - performing test have_c_werror_address - success - - performing test have_cxx_werror_sequence_point - - performing test have_cxx_werror_sequence_point - success - - performing test have_c_werror_sequence_point - - performing test have_c_werror_sequence_point - success - - performing test have_cxx_wformat - - performing test have_cxx_wformat - success - - performing test have_c_wformat - - performing test have_c_wformat - success - - performing test have_cxx_werror_format_security - - performing test have_cxx_werror_format_security - success - - performing test have_c_werror_format_security - - performing test have_c_werror_format_security - success - - performing test have_cxx_wmissing_declarations - - performing test have_cxx_wmissing_declarations - success - - performing test have_c_wmissing_declarations - - performing test have_c_wmissing_declarations - success - - performing test have_cxx_wmissing_prototypes - - performing test have_cxx_wmissing_prototypes - failed - - performing test have_c_wmissing_prototypes - - performing test have_c_wmissing_prototypes - success - - performing test have_cxx_wstrict_prototypes - - performing test have_cxx_wstrict_prototypes - failed - - performing test have_c_wstrict_prototypes - - performing test have_c_wstrict_prototypes - success - - performing test have_cxx_wundef - - performing test have_cxx_wundef - success - - performing test have_c_wundef - - performing test have_c_wundef - success - - performing test have_cxx_winit_self - - performing test have_cxx_winit_self - success - - performing test have_c_winit_self - - performing test have_c_winit_self - success - - performing test have_cxx_wpointer_arith - - performing test have_cxx_wpointer_arith - success - - performing test have_c_wpointer_arith - - performing test have_c_wpointer_arith - success - - performing test have_cxx_wshadow - - performing test have_cxx_wshadow - success - - performing test have_c_wshadow - - performing test have_c_wshadow - success - - performing test have_cxx_wsign_promo - - performing test have_cxx_wsign_promo - success - - performing test have_c_wsign_promo - - performing test have_c_wsign_promo - failed - - performing test have_cxx_wuninitialized - - performing test have_cxx_wuninitialized - success - - performing test have_c_wuninitialized - - performing test have_c_wuninitialized - success - - performing test have_cxx_wsuggest_override - - performing test have_cxx_wsuggest_override - success - - performing test have_c_wsuggest_override - - performing test have_c_wsuggest_override - failed - - performing test have_cxx_wno_delete_non_virtual_dtor - - performing test have_cxx_wno_delete_non_virtual_dtor - success - - performing test have_c_wno_delete_non_virtual_dtor - - performing test have_c_wno_delete_non_virtual_dtor - failed - - performing test have_cxx_wno_unnamed_type_template_args - - performing test have_cxx_wno_unnamed_type_template_args - failed - - performing test have_c_wno_unnamed_type_template_args - - performing test have_c_wno_unnamed_type_template_args - failed - - performing test have_cxx_wno_comment - - performing test have_cxx_wno_comment - success - - performing test have_c_wno_comment - - performing test have_c_wno_comment - success - - performing test have_cxx_wimplicit_fallthrough_3 - - performing test have_cxx_wimplicit_fallthrough_3 - success - - performing test have_c_wimplicit_fallthrough_3 - - performing test have_c_wimplicit_fallthrough_3 - success - - performing test have_cxx_wno_strict_overflow - - performing test have_cxx_wno_strict_overflow - success - - performing test have_c_wno_strict_overflow - - performing test have_c_wno_strict_overflow - success - - performing test have_cxx_fdiagnostics_show_option - - performing test have_cxx_fdiagnostics_show_option - success - - performing test have_c_fdiagnostics_show_option - - performing test have_c_fdiagnostics_show_option - success - - performing test have_cxx_pthread - - performing test have_cxx_pthread - success - - performing test have_c_pthread - - performing test have_c_pthread - success - - performing test have_cxx_fomit_frame_pointer - - performing test have_cxx_fomit_frame_pointer - success - - performing test have_c_fomit_frame_pointer - - performing test have_c_fomit_frame_pointer - success - - performing test have_cxx_ffunction_sections - - performing test have_cxx_ffunction_sections - success - - performing test have_c_ffunction_sections - - performing test have_c_ffunction_sections - success - - performing test have_cxx_fdata_sections - - performing test have_cxx_fdata_sections - success - - performing test have_c_fdata_sections - - performing test have_c_fdata_sections - success - - performing test have_cpu_neon_support ( check file : cmake / checks / cpu_neon . cpp ) - - performing test have_cpu_neon_support - success - - performing test have_cpu_fp16_support ( check file : cmake / checks / cpu_fp16 . cpp ) - - performing test have_cpu_fp16_support - success - - performing test have_cpu_baseline_flags - - performing test have_cpu_baseline_flags - success - - performing test have_cxx_fvisibility_hidden - - performing test have_cxx_fvisibility_hidden - success - - performing test have_c_fvisibility_hidden - - performing test have_c_fvisibility_hidden - success - - performing test have_cxx_fvisibility_inlines_hidden - - performing test have_cxx_fvisibility_inlines_hidden - success - - performing test have_c_fvisibility_inlines_hidden - - performing test have_c_fvisibility_inlines_hidden - failed - - performing test have_link_as_needed - - performing test have_link_as_needed - success - - looking for pthread . h - - looking for pthread . h - found - - looking for posix_memalign - - looking for posix_memalign - found - - looking for malloc . h - - looking for malloc . h - found - - looking for memalign - - looking for memalign - found - - check if the system is big endian - - searching <number> bit integer - - looking for sys / types . h - - looking for sys / types . h - found - - looking for stdint . h - - looking for stdint . h - found - - looking for stddef . h - - looking for stddef . h - found - - check size of unsigned short - - check size of unsigned short - done - - using unsigned short - - check if the system is big endian - little endian - - found openmp_c : - fopenmp ( found version "" <number> "" ) - - found openmp_cxx : - fopenmp ( found version "" <number> "" ) - - found openmp : true ( found version "" <number> "" ) - - found zlib : / usr / lib / aarch64 - linux - gnu / libz . so ( found suitable version "" <date> "" , minimum required is "" <number> . <number> "" ) - - found jpeg : / usr / lib / aarch64 - linux - gnu / libjpeg . so - - looking for assert . h - - looking for assert . h - found - - looking for dlfcn . h - - looking for dlfcn . h - found - - looking for fcntl . h - - looking for fcntl . h - found - - looking for inttypes . h - - looking for inttypes . h - found - - looking for io . h - - looking for io . h - not found - - looking for limits . h - - looking for limits . h - found - - looking for memory . h - - looking for memory . h - found - - looking for search . h - - looking for search . h - found - - looking for string . h - - looking for string . h - found - - looking for strings . h - - looking for strings . h - found - - looking for sys / time . h - - looking for sys / time . h - found - - looking for unistd . h - - looking for unistd . h - found - - performing test c_has_inline - - performing test c_has_inline - success - - check size of signed short - - check size of signed short - done - - check size of unsigned short - - check size of unsigned short - done - - check size of signed int - - check size of signed int - done - - check size of unsigned int - - check size of unsigned int - done - - check size of signed long - - check size of signed long - done - - check size of unsigned long - - check size of unsigned long - done - - check size of signed long long - - check size of signed long long - done - - check size of unsigned long long - - check size of unsigned long long - done - - check size of unsigned char * - - check size of unsigned char * - done - - check size of size_t - - check size of size_t - done - - check size of ptrdiff_t - - check size of ptrdiff_t - done - - check size of int8 - - check size of int8 - failed - - check size of int16 - - check size of int16 - failed - - check size of int32 - - check size of int32 - failed - - looking for floor - - looking for floor - found - - looking for pow - - looking for pow - found - - looking for sqrt - - looking for sqrt - found - - looking for isascii - - looking for isascii - found - - looking for memset - - looking for memset - found - - looking for mmap - - looking for mmap - found - - looking for getopt - - looking for getopt - found - - looking for memmove - - looking for memmove - found - - looking for setmode - - looking for setmode - not found - - looking for strcasecmp - - looking for strcasecmp - found - - looking for strchr - - looking for strchr - found - - looking for strrchr - - looking for strrchr - found - - looking for strstr - - looking for strstr - found - - looking for strtol - - looking for strtol - found - - looking for strtol - - looking for strtol - found - - looking for strtoull - - looking for strtoull - found - - looking for lfind - - looking for lfind - found - - performing test have_snprintf - - performing test have_snprintf - success - - check if the system is big endian - - searching <number> bit integer - - using unsigned short - - check if the system is big endian - little endian - - performing test have_c_wno_unused_but_set_variable - - performing test have_c_wno_unused_but_set_variable - success - - performing test have_c_wno_missing_prototypes - - performing test have_c_wno_missing_prototypes - success - - performing test have_c_wno_missing_declarations - - performing test have_c_wno_missing_declarations - success - - performing test have_c_wno_undef - - performing test have_c_wno_undef - success - - performing test have_c_wno_unused - - performing test have_c_wno_unused - success - - performing test have_c_wno_sign_compare - - performing test have_c_wno_sign_compare - success - - performing test have_c_wno_cast_align - - performing test have_c_wno_cast_align - success - - performing test have_c_wno_shadow - - performing test have_c_wno_shadow - success - - performing test have_c_wno_maybe_uninitialized - - performing test have_c_wno_maybe_uninitialized - success - - performing test have_c_wno_pointer_to_int_cast - - performing test have_c_wno_pointer_to_int_cast - success - - performing test have_c_wno_int_to_pointer_cast - - performing test have_c_wno_int_to_pointer_cast - success - - performing test have_c_wno_misleading_indentation - - performing test have_c_wno_misleading_indentation - success - - performing test have_c_wno_implicit_fallthrough - - performing test have_c_wno_implicit_fallthrough - success - - performing test have_c_wno_unused_parameter - - performing test have_c_wno_unused_parameter - success - - performing test have_cxx_wno_missing_declarations - - performing test have_cxx_wno_missing_declarations - success - - performing test have_cxx_wno_unused_parameter - - performing test have_cxx_wno_unused_parameter - success - - performing test have_cxx_wno_missing_prototypes - - performing test have_cxx_wno_missing_prototypes - failed - - performing test have_cxx_wno_undef - - performing test have_cxx_wno_undef - success - - performing test have_c_std_c99 - - performing test have_c_std_c99 - success - - performing test have_c_wno_unused_variable - - performing test have_c_wno_unused_variable - success - - performing test have_c_wno_unused_function - - performing test have_c_wno_unused_function - success - - could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) . openjpeg will be built from sources - - performing test have_c_wno_implicit_const_int_float_conversion - - performing test have_c_wno_implicit_const_int_float_conversion - failed - - openjpeg : version = <number> . <number> , build = opencv - <number> . <number> - openjp2 - <number> . <number> - - check if the system is big endian - - searching <number> bit integer - - using unsigned short - - check if the system is big endian - little endian - - looking for stdlib . h - - looking for stdlib . h - found - - looking for stdio . h - - looking for stdio . h - found - - looking for math . h - - looking for math . h - found - - looking for float . h - - looking for float . h - found - - looking for time . h - - looking for time . h - found - - looking for stdarg . h - - looking for stdarg . h - found - - looking for ctype . h - - looking for ctype . h - found - - looking for stdint . h - - looking for stdint . h - found - - looking for inttypes . h - - looking for inttypes . h - found - - looking for sys / stat . h - - looking for sys / stat . h - found - - looking for include file malloc . h - - looking for include file malloc . h - found - - looking for _aligned_malloc - - looking for _aligned_malloc - not found - - looking for posix_memalign - - looking for posix_memalign - found - - looking for memalign - - looking for memalign - found - - performing test have_c_wno_strict_prototypes - - performing test have_c_wno_strict_prototypes - success - - performing test have_c_wno_cast_function_type - - performing test have_c_wno_cast_function_type - failed - - openjpeg libraries will be built from sources : libopenjp2 ( version "" <number> . <number> "" ) - - found zlib : / usr / lib / aarch64 - linux - gnu / libz . so ( found version "" <date> "" ) - - found png : / usr / lib / aarch64 - linux - gnu / libpng . so ( found version "" <date> "" ) - - looking for / usr / include / libpng / png . h - - looking for / usr / include / libpng / png . h - found - - looking for semaphore . h - - looking for semaphore . h - found - - performing test have_cxx_wno_shadow - - performing test have_cxx_wno_shadow - success - - performing test have_cxx_wno_unused - - performing test have_cxx_wno_unused - success - - performing test have_cxx_wno_sign_compare - - performing test have_cxx_wno_sign_compare - success - - performing test have_cxx_wno_uninitialized - - performing test have_cxx_wno_uninitialized - success - - performing test have_cxx_wno_switch - - performing test have_cxx_wno_switch - success - - performing test have_cxx_wno_parentheses - - performing test have_cxx_wno_parentheses - success - - performing test have_cxx_wno_array_bounds - - performing test have_cxx_wno_array_bounds - success - - performing test have_cxx_wno_extra - - performing test have_cxx_wno_extra - success - - performing test have_cxx_wno_deprecated_declarations - - performing test have_cxx_wno_deprecated_declarations - success - - performing test have_cxx_wno_misleading_indentation - - performing test have_cxx_wno_misleading_indentation - success - - performing test have_cxx_wno_deprecated - - performing test have_cxx_wno_deprecated - success - - performing test have_cxx_wno_suggest_override - - performing test have_cxx_wno_suggest_override - success - - performing test have_cxx_wno_inconsistent_missing_override - - performing test have_cxx_wno_inconsistent_missing_override - failed - - performing test have_cxx_wno_implicit_fallthrough - - performing test have_cxx_wno_implicit_fallthrough - success - - performing test have_cxx_wno_tautological_compare - - performing test have_cxx_wno_tautological_compare - success - - performing test have_cxx_wno_reorder - - performing test have_cxx_wno_reorder - success - - performing test have_cxx_wno_unused_result - - performing test have_cxx_wno_unused_result - success - - performing test have_cxx_wno_implicit_const_int_float_conversion - - performing test have_cxx_wno_implicit_const_int_float_conversion - failed - - checking for module ' gtk + - <number> ' - - found gtk + - <number> , version <date> - - checking for module ' gthread - <number> ' - - found gthread - <number> , version <number> . <number> - - tbb : download : v2020 . <number> . tar . gz - - performing test have_cxx_wno_class_memaccess - - performing test have_cxx_wno_class_memaccess - failed cmake warning at cmake / opencvfindlibsperf . cmake : <number> ( message ) : opencv is not able to find / configure cuda sdk ( required by with_cuda ) . cuda support will be disabled in opencv build . to eliminate this warning remove with_cuda = on cmake configuration option . call stack ( most recent call first ) : cmakelists . txt : <number> ( include ) - - could not find openblas include . turning openblas_found off - - could not find openblas lib . turning openblas_found off - - could not find atlas ( missing : atlas_clapack_include_dir ) - - looking for sgemm_ - - looking for sgemm_ - found - - found threads : true - - a library with blas api found . - - looking for cheev_ - - looking for cheev_ - found - - a library with lapack api found . - - performing test have_cxx_wno_unused_local_typedefs - - performing test have_cxx_wno_unused_local_typedefs - success - - performing test have_cxx_wno_sign_promo - - performing test have_cxx_wno_sign_promo - success - - performing test have_cxx_wno_tautological_undefined_compare - - performing test have_cxx_wno_tautological_undefined_compare - failed - - performing test have_cxx_wno_ignored_qualifiers - - performing test have_cxx_wno_ignored_qualifiers - success - - performing test have_cxx_wno_unused_function - - performing test have_cxx_wno_unused_function - success - - performing test have_cxx_wno_unused_const_variable - - performing test have_cxx_wno_unused_const_variable - success - - performing test have_cxx_wno_shorten_64_to_32 - - performing test have_cxx_wno_shorten_64_to_32 - failed - - performing test have_cxx_wno_invalid_offsetof - - performing test have_cxx_wno_invalid_offsetof - success - - performing test have_cxx_wno_enum_compare_switch - - performing test have_cxx_wno_enum_compare_switch - failed - - could not find jni ( missing : java_awt_library java_jvm_library java_include_path java_include_path2 java_awt_include_path ) - - vtk is not found . please set - dvtk_dir in cmake to vtk build directory , or to vtk install subdirectory with vtkconfig . cmake file - - looking for dlerror in dl - - looking for dlerror in dl - found - - ade : download : v0 . <number> . 1 f . zip - - opencv python : during development append to pythonpath : / home / nvidia / opencv / build / python_loader - - checking for modules ' libavcodec ; libavformat ; libavutil ; libswscale ' - - found libavcodec , version <number> . <number> - - found libavformat , version <number> . <number> - - found libavutil , version <number> . <number> - - found libswscale , version <date> - - checking for module ' libavresample ' - - found libavresample , version <number> . <number> - - checking for module ' gstreamer - base - <number> ' - - found gstreamer - base - <number> , version <number> . <number> - - checking for module ' gstreamer - app - <number> ' - - found gstreamer - app - <number> , version <number> . <number> - - checking for module ' gstreamer - riff - <number> ' - - found gstreamer - riff - <number> , version <number> . <number> - - checking for module ' gstreamer - pbutils - <number> ' - - found gstreamer - pbutils - <number> , version <number> . <number> - - checking for module ' libdc1394 - <number> ' - - found libdc1394 - <number> , version <number> . <number> - - caffe : no - - protobuf : no - - glog : yes - - checking for module ' freetype2 ' - - found freetype2 , version <date> - - checking for module ' harfbuzz ' - - found harfbuzz , version <number> . <number> - - freetype2 : yes ( ver <date> ) - - harfbuzz : yes ( ver <number> . <number> ) - - hdf5 : using hdf5 compiler wrapper to determine c configuration - - found hdf5 : / usr / lib / aarch64 - linux - gnu / hdf5 / serial / libhdf5 . so ;/ usr / lib / aarch64 - linux - gnu / libpthread . so ;/ usr / lib / aarch64 - linux - gnu / libsz . so ;/ usr / lib / aarch64 - linux - gnu / libz . so ;/ usr / lib / aarch64 - linux - gnu / libdl . so ;/ usr / lib / aarch64 - linux - gnu / libm . so ( found version "" <number> . <number> "" ) - - julia not found . not compiling julia bindings . - - module opencv_ovis disabled because ogre3d was not found - - checking sfm glog / gflags deps . <repeated> true - - ceres support is disabled . ceres solver for reconstruction api is required . - - checking for module ' tesseract ' - - no package ' tesseract ' found - - tesseract : no - - allocator metrics storage type : ' int ' - - hdf5 : using hdf5 compiler wrapper to determine c configuration - - excluding from source files list : modules / imgproc / src / corner . avx . cpp - - excluding from source files list : modules / imgproc / src / imgwarp . avx2 . cpp - - excluding from source files list : modules / imgproc / src / imgwarp . sse4_1 . cpp - - excluding from source files list : modules / imgproc / src / resize . avx2 . cpp - - excluding from source files list : modules / imgproc / src / resize . sse4_1 . cpp cmake error at modules / dnn / cmakelists . txt : <number> ( message ) : dnn : cuda backend requires cuda toolkit . please resolve dependency or disable opencv_dnn_cuda = off - - registering hook ' init_module_sources_opencv_dnn ' : / home / nvidia / opencv / modules / dnn / cmake / hooks / init_module_sources_opencv_dnn . cmake - - opencv_dnn : filter out ocl4dnn source code - - opencv_dnn : filter out cuda4dnn source code - - excluding from source files list : / modules / dnn / layers / layers_common . avx . cpp - - excluding from source files list : / modules / dnn / layers / layers_common . avx2 . cpp - - excluding from source files list : / modules / dnn / layers / layers_common . avx512_skx . cpp - - excluding from source files list : modules / features2d / src / fast . avx2 . cpp - - performing test have_cxx_wno_overloaded_virtual - - performing test have_cxx_wno_overloaded_virtual - success - - rgbd : ceres support is disabled . ceres solver is required for posegraph optimization - - xfeatures2d / boostdesc : download : boostdesc_bgm . i - - xfeatures2d / boostdesc : download : boostdesc_bgm_bi . i - - xfeatures2d / boostdesc : download : boostdesc_bgm_hd . i - - xfeatures2d / boostdesc : download : boostdesc_binboost_064 . i - - xfeatures2d / boostdesc : download : boostdesc_binboost_128 . i - - xfeatures2d / boostdesc : download : boostdesc_binboost_256 . i - - xfeatures2d / boostdesc : download : boostdesc_lbgm . i - - xfeatures2d / vgg : download : vgg_generated_48 . i - - xfeatures2d / vgg : download : vgg_generated_64 . i - - xfeatures2d / vgg : download : vgg_generated_80 . i - - xfeatures2d / vgg : download : vgg_generated_120 . i - - data : download : face_landmark_model . dat - - ceres support is disabled . ceres solver for reconstruction api is required . - - performing test have_cxx_wno_unused_but_set_variable - - performing test have_cxx_wno_unused_but_set_variable - success - - performing test have_cxx_wno_unused_private_field - - performing test have_cxx_wno_unused_private_field - failed - - general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = - - version control : unknown - - extra modules : - - location ( extra ) : / home / nvidia / opencv_contrib / modules - - version control ( extra ) : unknown - - platform : - - timestamp : <number> - <number> - 0 9 t <time> z - - host : linux <date> - tegra aarch64 - - cmake : <number> . <number> - - cmake generator : unix makefiles - - cmake build tool : / usr / bin / make - - configuration : release - - cpu / hw features : - - baseline : neon fp16 - - required : neon - - c / c + + : - - built as dynamic libs ? : yes - - c + + standard : <number> - - c + + compiler : / usr / bin / c + + ( ver <number> . <number> ) - - c + + flags ( release ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - fopenmp - o3 - dndebug - dndebug - - c + + flags ( debug ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fvisibility - inlines - hidden - fopenmp - g - o0 - ddebug - d_debug - - c compiler : / usr / bin / cc - - c flags ( release ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fopenmp - o3 - dndebug - dndebug - - c flags ( debug ) : - fsigned - char - ffast - math - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - fvisibility = hidden - fopenmp - g - o0 - ddebug - d_debug - - linker flags ( release ) : - wl , - - gc - sections - wl , - - as - needed - - linker flags ( debug ) : - wl , - - gc - sections - wl , - - as - needed - - ccache : no - - precompiled headers : no - - extra dependencies : dl m pthread rt - - 3 rdparty dependencies : - - opencv modules : - - to be built : alphamat aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hdf hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto - - disabled : world - - disabled by dependency : - - - unavailable : cnn_3dobj cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv java julia matlab ovis python2 viz - - applications : perf_tests apps - - documentation : no - - non - free algorithms : yes - - gui : - - gtk + : yes ( ver <date> ) - - gthread : yes ( ver <number> . <number> ) - - gtkglext : no - - vtk support : no - - media i / <surprise> - - zlib : / usr / lib / aarch64 - linux - gnu / libz . so ( ver <date> ) - - jpeg : / usr / lib / aarch64 - linux - gnu / libjpeg . so ( ver <number> ) - - webp : build ( ver encoder : 0x0 2 0 f ) - - png : / usr / lib / aarch64 - linux - gnu / libpng . so ( ver <date> ) - - tiff : build ( ver <number> - <date> ) - - jpeg <number> : build ( ver <number> . <number> ) - - openexr : build ( ver <number> . <number> ) - - hdr : yes - - sunraster : yes - - pxm : yes - - pfm : yes - - video i / <surprise> - - dc1394 : yes ( <number> . <number> ) - - ffmpeg : yes - - avcodec : yes ( <number> . <number> ) - - avformat : yes ( <number> . <number> ) - - avutil : yes ( <number> . <number> ) - - swscale : yes ( <date> ) - - avresample : yes ( <number> . <number> ) - - gstreamer : yes ( <number> . <number> ) - - v4l / v4l2 : yes ( linux / videodev2 . h ) - - parallel framework : tbb ( ver <number> interface <number> ) - - trace : yes ( with intel itt ) - - other third - party libraries : - - lapack : no - - eigen : yes ( ver <number> . <number> ) - - custom hal : yes ( carotene ( ver <number> . <number> ) ) - - protobuf : build ( <number> . <number> ) - - nvidia cuda : no - - cudnn : no - - python <number> : - - interpreter : / usr / bin / python3 ( ver <number> . <number> ) - - libraries : / usr / lib / aarch64 - linux - gnu / libpython3 . 6 m . so ( ver <number> . <number> ) - - numpy : / usr / lib / python3 / dist - packages / numpy / core / include ( ver <number> . <number> ) - - install path : lib / python3 . <number> / dist - packages / cv2 / python - <number> - - python ( for build ) : / usr / bin / python2 . <number> - - java : - - ant : no - - jni : no - - java wrappers : no - - java tests : no - - install to - - - - configuring incomplete , errors occurred ! see also "" / home / nvidia / opencv / build / cmakefiles / cmakeoutput . log "" . see also "" / home / nvidia / opencv / build / cmakefiles / cmakeerror . log "" .",2
opencv/opencv,"js version cannot find the undistortpoints function 。 how to compute distorted image points position in opencv . js ？ < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => : grey_question : - operating system / platform => : grey_question : - compiler => : grey_question : # # # # # detailed description < ! - - your description - - > # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"common . hpp ` ` ` # # # # # system information - opencv => <number> . <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> ` ` ` hi i just installing opencv , and i trying this code : <url> but , i have an error can not found "" common . hpp "" . someone know how to resolve this ? i need to replace it with another lib ? thanks a lot : d",2
opencv/opencv,"iplimage has not been declared ` <hashtag> if n def </hashtag> mm3d_h <hashtag> define </hashtag> mm3d_h <hashtag> include </hashtag> < math . h > <hashtag> include </hashtag> "" 3 dmmglobal . h "" / / <hashtag> include </hashtag> < cv . h > pf注释的 , 找不到这个文件 <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> <algorithm> / / <hashtag> include </hashtag> < windows . h > pf注释的 , 找不到这个文件 / / <hashtag> include </hashtag> < gl \ \ gl . h > pf注释的 , 找不到这个文件 / / <hashtag> include </hashtag> < gl \ \ glu . h > pf注释的 , 找不到这个文件 using namespace std ; using namespace cv ; <hashtag> define </hashtag> inf 1 e20 class mm3d { public 3 dmm and reference frame mapping void cartesian2ref ( double * vertex , double * tri , double * texture , int width , int height , int nver , int ntri , double * ref , double * refco , double * tri_ind ) ; void zbuffer ( double * vertex , double * tri , double * texture , int nver , int ntri , double * src_img , int width , int height , int nchannels , double * img , double * tri_ind ) ; void zbuffertri ( double * vertex , double * tri , double * texture_tri , int nver , int ntri , double * src_img , int width , int height , int nchannels , double * img , double * tri_ind ) ; void getcovertri ( double * vertex , double * tri , double * r , double * p , int nver , int ntri , double * covertri ) ; void visiblesurf ( double * vertex , double * tri , double * r , int nver , int ntri , double * vis_bin ) ; bool pointintri ( mat * point , mat * pt1 , mat * pt2 , mat * pt3 ) ; void distancetransform ( double * dt , double * im , int width , int height ) ; void dt ( float * d , float * f , int n ) ; void lighting ( double * vertex , double * tri , double * tex , int _nv , int _nt , illum_para para , iplimage * img ) ; void drawmodal ( float * _shape , unsigned int * _triangle , float * _color , int _nt , int _nv , illum_para para , unsigned char * face , int widthstep ) ; void normdirection ( float * vertex , unsigned int * tri , int nt , int nv , float * norm ) ; void meshmap ( double * vertex , double * tri , int nver , int ntri , double * meshmap , int width , int height ) ; void occlusionquery ( double * vertex , double * tri , int ntri , int nver , int width , int height , double * visibility , double threshold ) ; / / opengl related hglrc _hrc ; hdc _hdc ; byte * _data ; hbitmap _bitmap ; / / for face int _width ; int _height ; bool preparegl ( ); bool releasegl ( ); double xmin ; double xmax ; double ymin ; double ymax ; double zmin ; double zmax ; }; <hashtag> end if </hashtag> ` - - - - - - - - - - - [ <number> ] ( <url> - - - - my opencv version is <number> . <number> . i use matlab mex compile opencv , but i got a bug . i have no idea",2
opencv/opencv,"difference in predictions when running on gpu vs cpu < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv = <number> . <number> - operating system / platform = ubuntu <number> . <number> - compiler => visual studio <number> - - > - opencv = <number> . <number> - operating system / platform = ubuntu <number> . <number> lts - python <date> - cuda driver version / runtime version : <number> / <number> - cuda capability major / minor version number : <number> # # # # # detailed description hi there , at the moment i am trying to use my gpu ( nvidia rtx 3 0 6 0 ti ) for inference of some darknet models using opencv dnn module . however , i have run into something that i can not quite explain . when running on cpu i get better results in terms of prediction accuracy compared to when running on gpu . outcomes : [(x - position , y - position , width , height , confidence ) ] using cpu : [ ( <number> , <number> , <number> , <number> , <number> ) ] using gpu : [ ( <number> , <number> , <number> , <number> , <number> ) ] the big difference here is the confidence scores which seem to be off . this happens when running on the out - of - the - box yolov3 model ( weights and config ) but also when running custom models . # # # # # steps to reproduce ( uncomment lines stating setpreferablebackend and setpreferabletarget to enable gpu usage ) also did a quick check on the output of cv2 . dnn . blobfromimage ( ) . this returns the exact same image when running with cpu or gpu enabled . ` ` ` import cv2 import numpy as np if __name__ = = ' __main__ ' = cv2 . dnn . readnet ( r "" network . weights "" , r "" network . cfg "" ) # net . setpreferablebackend ( cv2 . dnn . dnn_backend_cuda ) # net . setpreferabletarget ( cv2 . dnn . dnn_target_cuda ) img = cv2 . imread ( r "" test_image . jpg "" ) blob = cv2 . dnn . blobfromimage ( img , scalefactor = <number> , size =( <number> , <number> ) , mean =( <number> , <number> , <number> ) , swaprb = true , crop = false ) output_layers = net . getunconnectedoutlayersnames ( ) net . setinput ( blob ) outputs = net . forward ( output_layers ) detections = np . array ( [ detection for output in outputs for detection in output ] ) scores = detections [ :, <number> <happy> data = detections [ :, : <number> ] class_ids = np . argmax ( scores , axis = <number> ) main_scores = scores [ np . arange ( len ( scores ) ) , class_ids ] # keep only detections which have score higher than conf suff_scores = main_scores > <number> # class id and score class_ids = class_ids [ suff_scores ] confs = main_scores [ suff_scores ] data = data [ suff_scores ] w = data [ :, <number> ] * <number> h = data [ :, <number> ] * <number> center_x = data [ :, <number> ] * <number> center_y = data [ :, <number> ] * <number> print ( [ ( a , b , c , d , e ) for a , b , c , d , e in zip ( center_x , center_y , w , h , confs ) ] ) ` ` ` thanks in advance !",2
opencv/opencv,missing libraries in opencv <number> . <number> i tried installing opencv3 . <number> using the exe at <url> however it seems to be missing some libraries that i need to run my application . how can i obtain the complete set of libraries ? [ image ] ( <url>,2
opencv/opencv,"why are there <number> compiled dynamic libraries ? what ' s the difference between them ? < img width = "" <number> "" alt = "" 5 7 3 8 e7a7591e792495cc5b2252574c2 "" src = "" <url>",2
opencv/opencv,"` cv . imdecode ` can not open the dib ( bmp ) images from clipboard # # # # # system information ( version ) - opencv => opencv - python <number> . <number> - operating system / platform => win10 - compiler => pre - compiled # # # # # detailed description ` cv . imdecode ` can not open the dib ( bmp ) images from clipboard , but can from file # # # # # steps to reproduce work : ` ` ` py with open ( ' untitled . bmp ' , ' rb ' ) as f : src = f . read ( ) cv . imdecode ( np . frombuffer ( src , dtype = np . uint8 ) , cv . imread_color ) ` ` ` can not work : ` ` ` py # take a screenshoot or anything win32clipboard . openclipboard ( ) src = win32clipboard . getclipboarddata ( win32con . cf_dib ) # or win32con . cf_dibv5 win32clipboard . closeclipboard ( ) cv . imdecode ( np . frombuffer ( src , dtype = np . uint8 ) , cv . imread_color ) ` ` ` the ` imdecode ` return ` null ` . is it able to support this kind of format of ` bmp ( aka dib ) ` for opencv ? # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files images , onnx , etc",2
opencv/opencv,"python and java do not decode jpg in the same way # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => linux / debian - compiler => system default # # # # # detailed description jpg images read from python or java are not exactly equal . i would expect that they get decoded to exactly the same uint8 values . below an example # # # # # steps to reproduce ` ` ` python img1 = cv . imread ( "" . <number> . jpg "" ) img1 . max ( axis =( <number> ) ) ` ` ` result : array ( [ <number> , <number> , <number> ] , dtype = uint8 ) ` ` ` java string imgfile = "" <number> . jpg "" ; mat img = imgcodecs . imread ( imgfile ) ; list <mat> channels = new arraylist <mat> (); core . split ( img , channels ) ; channels . foreach ( ch - > system . out . println ( core . minmaxloc ( ch ) . maxval ) ); result : <number> <number> <number> ` ` ` i have attached the input file ` <number> . jpg ` # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [ x] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ x] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ x] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - > ! [ <number> ] ( <url>",2
opencv/opencv,"undefined symbols for architecture arm64 : "" std : : __1 : : basic_istream , ( built on m1 mac ) i followed that instructions on # <number> and successfully built an xcframework . when i tried to import it the follow error show . ` undefined symbols for architecture arm64 : "" std : : __1 : : basic_istream < char , std : : __1 : : char_traits <char> <sad> : sentry : : sentry ( std : : __1 : : basic_istream < char , std : : __1 : : char_traits <char> > & , bool ) "" , referenced from : ` platform : macbook pro with m1 pro chip swift - driver version : <number> . <number> apple swift version <number> ( swiftlang - <number> . <number> . <number> clang - <number> . <number> ) target apple clang version <number> . <number> ( clang - <number> . <number> )",2
opencv/opencv,"some issue with cmake and opencv < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # detailed description recently i am working on my first project , which is using intel d455 to track skeleton . i am planning use opencv to complete . i had read some articles , and one of them is from intel . [ <url> this site lead me to [ <url> i follow the description down below , and find out an error which i can not solved . ↓ after clicking configure and this occurred . [ <url> ↓ error description [ <url> honestly , i had no idea what happened to my cmake , and what i done so far seem to be unrelated to my current project . i am confused . can someone point out the problem in my cmake , and how can i do to solve . # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,opencv : include directory does not exist : ' / usr / local / stow / absl / include / opencv4 ' . opencv installation may be broken . system information ( version ) opencv <number> . <number> operating system linux ubuntu <number> cmake & make & install : cmake - d cmake_build_type = release - d cmake_install_prefix <annoyed> usr / local - d opencv_extra_modules_path <annoyed> home / algo / software / opencv / opencv_contrib_4 . <number> / modules - d with_eigen = on - d with_qt = on - d opencv_enable_nonfree = on - d install_c_examples = off - d install_python_examples = off - d python3_packages_path <annoyed> usr / lib / python3 / dist - packages - d opencv_generate_pkgconfig = on - d build_examples = off . <repeated> make - j8 sudo make install algo <user> : ~ / software / opencv / opencv_4 . <number> / samples / cpp / example_cmake / build $ pkg - config - - modversion opencv4 <number> . <number> algo <user> : ~ / software / opencv / opencv_4 . <number> / samples / cpp / example_cmake / build $ pkg - config - - libs opencv4 - l / usr / local / lib - lopencv_gapi - lopencv_stitching - lopencv_alphamat - lopencv_aruco - lopencv_barcode - lopencv_bgsegm - lopencv_bioinspired - lopencv_ccalib - lopencv_cvv - lopencv_dnn_objdetect - lopencv_dnn_superres - lopencv_dpm - lopencv_face - lopencv_freetype - lopencv_fuzzy - lopencv_hdf - lopencv_hfs - lopencv_img_hash - lopencv_intensity_transform - lopencv_line_descriptor - lopencv_mcc - lopencv_quality - lopencv_rapid - lopencv_reg - lopencv_rgbd - lopencv_saliency - lopencv_sfm - lopencv_stereo - lopencv_structured_light - lopencv_phase_unwrapping - lopencv_superres - lopencv_optflow - lopencv_surface_matching - lopencv_tracking - lopencv_highgui - lopencv_datasets - lopencv_text - lopencv_plot - lopencv_videostab - lopencv_videoio - lopencv_viz - lopencv_wechat_qrcode - lopencv_xfeatures2d - lopencv_shape - lopencv_ml - lopencv_ximgproc - lopencv_video - lopencv_xobjdetect - lopencv_objdetect - lopencv_calib3d - lopencv_imgcodecs - lopencv_features2d - lopencv_dnn - lopencv_flann - lopencv_xphoto - lopencv_photo - lopencv_imgproc - lopencv_core cmakelist . txt : cmake_minimum_required ( version <number> ) project ( opencv_helloworld languages cxx ) find_package ( opencv required ) include_directories ( ${ opencv_include_dirs } ) find_package ( opencv required ) add_executable ( opencv_helloworld main . cpp ) target_link_libraries ( opencv_helloworld ${ opencv_libs } ) error : cmake warning at / usr / local / lib / cmake / opencv4 / opencvconfig . cmake : <number> ( message ) : opencv : include directory does not exist : ' / usr / local / stow / absl / include / opencv4 ' . opencv installation may be broken . skip . <repeated> call stack ( most recent call first ) ( find_package ),2
opencv/opencv,"readnetfromtensorflow do not work < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => <number> . <number> - operating system / platform => windows <number> <number> bit - compiler => python <number> . <number> # # # # # detailed description when i want to load my ` ` saved_model . pb ` ` which is on the same directory with ` ` cv . dnn . readnetfromtensorflow ( ' saved_model . pb ' ) ` ` , i got this error : ` ` cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ tensorflow \ \ tf_io . cpp : <number> : error : ( - <number> : unspecified error ) failed : readprotofrombinaryfile ( param_file , param ) . failed to parse graphdef file : saved_model . pb in function ' cv : : dnn : : readtfnetparamsfrombinaryfileordie ' ` ` i saw everyone use already trained model and nobody train its own model for use it in opencv . i do not see anything related with my error on internet or to load a custom model in opencv . <annoyed> # # # # # steps to reproduce ` ` ` model = tf . keras . models . sequential ( [ tf . keras . layers . rescaling ( <number> . / <number> ) , tf . keras . layers . conv2d ( <number> , <number> , padding = ' same ' , activation = ' relu ' ) , tf . keras . layers . maxpooling2d ( ) , tf . keras . layers . conv2d ( <number> , <number> , padding = ' same ' , activation = ' relu ' ) , tf . keras . layers . maxpooling2d ( ) , tf . keras . layers . flatten ( ) , tf . keras . layers . dense ( <number> , activation = ' relu ' ) , tf . keras . layers . dense ( <number> ) ] ) model . compile ( optimizer = ' adam ' , loss = tf . keras . losses . sparsecategoricalcrossentropy ( from_logits = true ) , metrics =[ ' accuracy ' ] ) model . fit ( train_ds , validation_data = val_ds , epochs = <number> ) model . save ( ' . / ' ) model = cv . dnn . readnetfromtensorflow ( ' saved_model . pb ' ) ` ` ` < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [x ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"confused about the principle of cvfindextrinsiccameraparams2 ( ) interface ? could tell me about the principle of following code in cvfindextrinsiccameraparams2 ( ) interface ? any theory or paper ? ` ` ` / / initialize extrinsic parameters if ( w [ <number> ] / w [ <number> ] < 1 e - <number> ) { / / a planar structure case ( all m ' s lie in the same plane ) double tt [ <number> ] , h [ <number> ] , h1_norm , h2_norm ; cvmat * r_transform = & matv ; cvmat t_transform = cvmat ( <number> , <number> , cv_64f , tt ); cvmat math = cvmat ( <number> , <number> , cv_64f , h ); cvmat _h1 , _h2 , _h3 ; if ( v [ <number> ] * v [ <number> ] + v [ <number> ] * v [ <number> ] < 1 e - <number> ) cvsetidentity ( r_transform ); if ( cvdet ( r_transform ) < <number> ) cvscale ( r_transform , r_transform , - <number> ); cvgemm ( r_transform , & _mc , - <number> , <number> , <number> , & t_transform , cv_gemm_b_t ); for ( i = <number> ; i < count ; i + + ) { const double * rp = r_transform - > data . db ; const double * tp = t_transform . data . db ; const double * src = matm - > data . db + i * <number> ; double * dst = _mxy - > data . db + i * <number> ; dst [ <number> ] = rp [ <number> ] * src [ <number> ] + rp [ <number> ] * src [ <number> ] + rp [ <number> ] * src [ <number> ] + tp [ <number> ]; dst [ <number> ] = rp [ <number> ] * src [ <number> ] + rp [ <number> ] * src [ <number> ] + rp [ <number> ] * src [ <number> ] + tp [ <number> ]; } cvfindhomography ( _mxy , _mn , & math ); if ( cvcheckarr ( & math , cv_check_quiet ) ) { cvgetcol ( & math , & _h1 , <number> ); _h2 = _h1 ; _h2 . data . db + + ; _h3 = _h2 ; _h3 . data . db + + ; h1_norm = std : : sqrt ( h [ <number> ] * h [ <number> ] + h [ <number> ] * h [ <number> ] + h [ <number> ] * h [ <number> ]); h2_norm = std : : sqrt ( h [ <number> ] * h [ <number> ] + h [ <number> ] * h [ <number> ] + h [ <number> ] * h [ <number> ]); cvscale ( & _h1 , & _h1 , <number> . / max ( h1_norm , dbl_epsilon ) ); cvscale ( & _h2 , & _h2 , <number> . / max ( h2_norm , dbl_epsilon ) ); cvscale ( & _h3 , & _t , <number> . / max ( h1_norm + h2_norm , dbl_epsilon ) ); cvcrossproduct ( & _h1 , & _h2 , & _h3 ); cvrodrigues2 ( & math , & _r ); cvrodrigues2 ( & _r , & math ); cvmatmuladd ( & math , & t_transform , & _t , & _t ); cvmatmul ( & math , r_transform , & matr ); } else { cvsetidentity ( & matr ); cvzero ( & _t ); } cvrodrigues2 ( & matr , & _r ); } ` ` `",2
opencv/opencv,"cv : : cudacodec : : videoreader throw_no_cuda < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => <number> . <number> - operating system / platform => windows <number> 6 4 bit - compiler => visual studio <number> - cuda => <number> - gpu => gtx <number> # # # # detailed description < ! - - your description - - > this is my source code ： < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > ` ` ` . cpp <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> "" opencv2 / opencv . hpp "" <hashtag> include </hashtag> "" opencv2 / cudacodec . hpp "" using namespace std ; using namespace cv ; int main ( ) { const string videoname = "" d <annoyed> renzhenfeng / vframe_segment_pose_server / pre_data . mp4 "" ; cv : : cuda : : gpumat d_frame ; cv : : ptr < cv : : cudacodec : : videoreader > d_reader = cv : : cudacodec : : createvideoreader ( videoname ) ; while ( true ) { if ( ! d_reader - > nextframe ( d_frame ) ) break ; cv : : waitkey ( <number> ); } return <number> ; } ` ` ` when i run the "" cv : : ptr < cv : : cudacodec : : videoreader > d_reader = cv : : cudacodec : : createvideoreader ( videoname ) ;"", an error occurred in the program ： ` ` ` opencv : terminate handler is called ! the last opencv error is : opencv ( <number> . <number> ) error : the function / feature is not implemented ( the called functionality is disabled for current build or platform ) in throw_no_cuda , file d :\\ opencv \ \ opencv \ \ sources \ \ modules \ \ core \ \ include \ \ opencv2 / core / private . cuda . hpp , line <number> ` ` ` the information of cmake is as follows ： ` ` ` selecting windows sdk version <number> . <number> to target windows <number> . <number> . detected processor : amd64 libjpeg - turbo : version = <number> . <number> , build = opencv - <number> . <number> - libjpeg - turbo could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) . openjpeg will be built from sources openjpeg : version = <number> . <number> , build = opencv - <number> . <number> - openjp2 - <number> . <number> openjpeg libraries will be built from sources : libopenjp2 ( version "" <number> . <number> "" ) found intel ipp ( icv version ) : <number> . <number> [ <number> . <number> gold ] at : d <annoyed> opencv / opencv / build_cuda / 3 rdparty / ippicv / ippicv_win / icv found intel ipp integration wrappers sources : <number> . <number> at : d <annoyed> opencv / opencv / build_cuda / 3 rdparty / ippicv / ippicv_win / iw cuda detected : <number> cuda : using cuda_arch_bin = <number> cuda nvcc target flags : - gencode ; arch = compute_86 , code = sm_86 ; - d_force_inlines cuda : msvs generator is detected . disabling cmake re - run checks ( cmake_suppress_regeneration = on ) . you need to run cmake manually if updates are required . could not find openblas include . turning openblas_found off could not find openblas lib . turning openblas_found off could not find blas ( missing : blas_libraries ) could not find lapack ( missing : lapack_libraries ) reason given by package : lapack could not be found because dependency blas could not be found . vtk is not found . please set - dvtk_dir in cmake to vtk build directory , or to vtk install subdirectory with vtkconfig . cmake file opencv python : during development append to pythonpath : d <annoyed> opencv / opencv / build_cuda / python_loader module opencv_alphamat disabled because the following dependencies are not found : eigen caffe : no protobuf : no glog : no freetype2 : no harfbuzz : no julia not found . not compiling julia bindings . module opencv_ovis disabled because ogre3d was not found no preference for use of exported gflags cmake configuration set , and no hints for include / library directories provided . defaulting to preferring an installed / exported gflags cmake configuration if available . failed to find installed gflags cmake configuration , searching for gflags build directories exported with cmake . failed to find gflags - failed to find an installed / exported cmake configuration for gflags , will perform search for installed gflags components . failed to find gflags - could not find gflags include directory , set gflags_include_dir to directory containing gflags / gflags . h failed to find glog - could not find glog include directory , set glog_include_dir to directory containing glog / logging . h module opencv_sfm disabled because the following dependencies are not found : eigen glog / gflags tesseract : no processing world modules . <repeated> module opencv_cudev . <repeated> module opencv_core . <repeated> allocator metrics storage type : ' long long ' module opencv_cudaarithm . <repeated> module opencv_flann . <repeated> module opencv_imgproc . <repeated> module opencv_intensity_transform . <repeated> module opencv_ml . <repeated> module opencv_phase_unwrapping . <repeated> module opencv_plot . <repeated> module opencv_quality . <repeated> module opencv_reg . <repeated> module opencv_surface_matching . <repeated> module opencv_cudafilters . <repeated> module opencv_cudaimgproc . <repeated> module opencv_cudawarping . <repeated> module opencv_dnn . <repeated> registering hook ' init_module_sources_opencv_dnn ' : d <annoyed> opencv / opencv / sources / modules / dnn / cmake / hooks / init_module_sources_opencv_dnn . cmake module opencv_dnn_superres . <repeated> module opencv_features2d . <repeated> module opencv_fuzzy . <repeated> module opencv_hfs . <repeated> module opencv_imgcodecs . <repeated> module opencv_line_descriptor . <repeated> module opencv_photo . <repeated> module opencv_saliency . <repeated> module opencv_text . <repeated> module opencv_videoio . <repeated> module opencv_xphoto . <repeated> module opencv_calib3d . <repeated> module opencv_cudacodec . <repeated> module opencv_cudafeatures2d . <repeated> module opencv_cudastereo . <repeated> module opencv_datasets . <repeated> module opencv_highgui . <repeated> module opencv_mcc . <repeated> module opencv_objdetect . <repeated> module opencv_rapid . <repeated> module opencv_rgbd . <repeated> module opencv_shape . <repeated> module opencv_structured_light . <repeated> module opencv_video . <repeated> module opencv_xfeatures2d . <repeated> module opencv_ximgproc . <repeated> module opencv_xobjdetect . <repeated> module opencv_aruco . <repeated> module opencv_bgsegm . <repeated> module opencv_bioinspired . <repeated> module opencv_ccalib . <repeated> module opencv_cudabgsegm . <repeated> module opencv_cudalegacy . <repeated> module opencv_cudaobjdetect . <repeated> module opencv_dnn_objdetect . <repeated> module opencv_dpm . <repeated> module opencv_face . <repeated> module opencv_gapi . <repeated> module opencv_optflow . <repeated> module opencv_stitching . <repeated> module opencv_tracking . <repeated> module opencv_cudaoptflow . <repeated> module opencv_stereo . <repeated> module opencv_superres . <repeated> module opencv_videostab . <repeated> processing world modules . <repeated> done cmake warning at cmake / opencvgensetupvars . cmake : <number> ( message ) : configuration is not supported : validate setupvars script in install directory call stack ( most recent call first ) : cmakelists . txt : <number> ( include ) general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : unknown extra modules : location ( extra ) : d <annoyed> opencv / opencv_contrib - <number> . <number> / opencv_contrib - <number> . <number> / modules version control ( extra ) : unknown platform : timestamp : <number> - <number> - 0 4 t <time> z host : windows <number> . <number> amd64 cmake : <number> . <number> cmake generator : visual studio <number> <number> cmake build tool : c <annoyed> program files (x 8 6 ) / microsoft visual studio / <number> / community / msbuild / current / bin / msbuild . exe msvc : <number> cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : c <annoyed> program files (x 8 6 ) / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe ( ver <number> . <number> ) c + + flags ( release ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : fast / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / mp / md / o2 / ob2 / dndebug c + + flags ( debug ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : fast / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / mp / mdd / zi / ob0 / od / rtc1 c compiler : c <annoyed> program files (x 8 6 ) / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe c flags ( release ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : fast / mp / md / o2 / ob2 / dndebug c flags ( debug ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : fast / mp / mdd / zi / ob0 / od / rtc1 linker flags ( release ) : / machine <kiss> 6 4 / incremental : no linker flags ( debug ) : / machine <kiss> 6 4 / debug / incremental ccache : no precompiled headers : no extra dependencies : opengl32 glu32 cudart_static . lib nppc . lib nppial . lib nppicc . lib nppidei . lib nppif . lib nppig . lib nppim . lib nppist . lib nppisu . lib nppitc . lib npps . lib cublas . lib cudnn . lib cufft . lib - libpath <sad> <annoyed> program files / nvidia gpu computing toolkit / cuda / v11 . <number> / lib / x64 3 rdparty dependencies : opencv modules : to be built : aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab world xfeatures2d ximgproc xobjdetect xphoto disabled : python3 python_bindings_generator python_tests disabled by dependency : - unavailable : alphamat cnn_3dobj cvv freetype hdf java js julia matlab ovis python2 python2 sfm viz applications : tests perf_tests apps documentation : no non - free algorithms : yes windows rt support : no gui : win32 ui : yes opengl support : yes ( opengl32 glu32 ) vtk support : no media i / <surprise> zlib : build ( ver <date> ) jpeg : build - libjpeg - turbo ( ver <number> . <number> - <number> ) webp : build ( ver encoder : 0x0 2 0 f ) png : build ( ver <date> ) tiff : build ( ver <number> - <date> ) jpeg <number> : build ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes ( prebuilt binaries ) avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : yes ( <number> . <number> ) gstreamer : no directshow : yes media foundation : yes dxva : yes parallel framework : concurrency trace : yes ( with intel itt ) other third - party libraries : intel ipp : <number> . <number> gold [ <number> . <number> ] at : d <annoyed> opencv / opencv / build_cuda / 3 rdparty / ippicv / ippicv_win / icv intel ipp iw : sources ( <number> . <number> ) at : d <annoyed> opencv / opencv / build_cuda / 3 rdparty / ippicv / ippicv_win / iw lapack : no eigen : no custom hal : no protobuf : build ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas nvcuvid fast_math ) nvidia gpu arch : <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( nvd3d11 ) include path : d <annoyed> opencv / opencv / sources / 3 rdparty / include / opencl / <number> link libraries : dynamic load python ( for build ) : d <annoyed> python39 / python . exe java : ant : no jni : d <annoyed> java / include d <annoyed> java / include / win32 d <annoyed> java / include java wrappers : no java tests : no install to - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - configuring done generating done ` ` ` why it does not run successfully with “ nvcuvid ” displayed ？ i am very confused , please help me ？",2
opencv/opencv,"undefined reference to ` cv : : mat : : mat ( int , int , int , cv : : scalar_ <double> const & ) ' when i ran this code , it went wrong . # # # # # detailed description cv : : mat m ( <number> , cv_8uc3 , cv : : scalar ( <number> , <number> )); # # # # # system information ( version ) < - - opencv <number> . <number> ( code in question pulled from [ here ] ( <url> ubuntu <number> - - > - opencv => : <number> . <number> : - ubuntu => <number> it reported reference to ` cv : : mat : : mat ( int , int , int , cv : : scalar_ <double> const & ) ' undefined reference to ` cv : : formatter : : get ( cv : : formatter : : formattype ) ' issue submission checklist [ x] i report the issue , it ' s not a question [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution [ x] i updated to the latest opencv version and the issue is still there",2
opencv/opencv,"inconsistant conversion from bgra to bgr on png image # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => ubuntu <number> - compiler => g + + # # # # # detailed description usually in opencv , the conversion from bgra to bgr via ` imread_color ` or ` cvtcolor ` set the transparent background as black . however , for this image it seem to generate an inconsistant background . < - - your description - - > # # # # # steps to reproduce ` ` ` python3 import cv2 img = cv2 . imread ( path , cv2 . imread_color ) cv2 . imshow ( ' window ' , img ) cv2 . waitkey ( <number> ) ` ` ` original image : ! [ img_rgba ] ( <url> output image : ! [ out_rgba ] ( <url> # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [x ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"unable to launch app with brew version opencv <number> . <number> linked # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => macos <number> . <number> ( apple m1 ) - compiler => apple clang version <number> . <number> ( c + + ) # # # # # detailed description when i write cmakefiles . txt to link opencv <number> . <number> installed by brew , the built program will not be able to run . the os kills my program . i think errors of loading ` . dylib ` s occur when the application is launched . reproduction is very easy . here is a simple code . am i doing something wrong ? # # # # # steps to reproduce <number> . c + + source code ` main . cpp ` ( anything is ok ) ` ` ` main . cpp int main ( int argc , char * argv [ ] ) { } ` ` ` <number> . cmakelists . txt just link this code ( above ) with the opencv library . ` ` ` cmake cmake_minimum_required ( version <number> . <number> ) set ( cmake_verbose_makefile <number> ) # for verbose mode . run like "" verbose = <number> make - j8 . <repeated> "" project ( cvlink ) add_executable ( cvlink ) target_sources ( cvlink private . / main . cpp ) find_package ( opencv required ) target_link_libraries ( cvlink ${ opencv_libs } ) ` ` ` <number> . build build normally in the manner of cmake ` ` ` zsh $ brew install opencv # if not yet $ mkdir build ; cd build ; cmake . <repeated> ; make ` ` ` <number> . run , but fails . ` ` ` zsh $ . / cvlink zsh : killed . / cvlink ` ` ` <number> . i would like to confirm my opencv version . ` ` ` zsh $ opencv_version <number> . <number> $ which opencv_version / opt / homebrew / bin / opencv_version ` ` ` i think it is an issue with the brew version of opencv . here is the build log of my reproduction code . ` ` ` [ mymac : <time> : build ] $ make / opt / homebrew / cellar / cmake / <number> . <number> / bin / cmake - s / users / myname / tmp / cvlink - b / users / myname / tmp / cvlink / build - - check - build - system cmakefiles / makefile . cmake <number> / opt / homebrew / cellar / cmake / <number> . <number> / bin / cmake - e cmake_progress_start / users / myname / tmp / cvlink / build / cmakefiles / users / myname / tmp / cvlink / build / / cmakefiles / progress . marks / applications / xcode . app / contents / developer / usr / bin / make - f cmakefiles / makefile2 all / applications / xcode . app / contents / developer / usr / bin / make - f cmakefiles / cvlink . dir / build . make cmakefiles / cvlink . dir / depend cd / users / myname / tmp / cvlink / build & & / opt / homebrew / cellar / cmake / <number> . <number> / bin / cmake - e cmake_depends "" unix makefiles "" / users / myname / tmp / cvlink / users / myname / tmp / cvlink / users / myname / tmp / cvlink / build / users / myname / tmp / cvlink / build / users / myname / tmp / cvlink / build / cmakefiles / cvlink . dir / dependinfo . cmake - - color = / applications / xcode . app / contents / developer / usr / bin / make - f cmakefiles / cvlink . dir / build . make cmakefiles / cvlink . dir / build [ <percent> ] building cxx object cmakefiles / cvlink . dir / main . cpp . o / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / c + + - isystem / opt / homebrew / cellar / opencv / <number> . <number> / include / opencv4 - arch arm64 - isysroot / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx12 . <number> . sdk - mmacosx - version - min = <number> - std = gnu + + <number> - md - mt cmakefiles / cvlink . dir / main . cpp . o - mf cmakefiles / cvlink . dir / main . cpp . o . d - o cmakefiles / cvlink . dir / main . cpp . o - c / users / myname / tmp / cvlink / main . cpp [ <percent> ] linking cxx executable cvlink / opt / homebrew / cellar / cmake / <number> . <number> / bin / cmake - e cmake_link_script cmakefiles / cvlink . dir / link . txt - - verbose = <number> / applications / xcode . app / contents / developer / toolchains / xcodedefault . xctoolchain / usr / bin / c + + - arch arm64 - isysroot / applications / xcode . app / contents / developer / platforms / macosx . platform / developer / sdks / macosx12 . <number> . sdk - mmacosx - version - min = <number> - wl , - search_paths_first - wl , - headerpad_max_install_names - l / opt / homebrew / opt / libffi / lib - l / opt / homebrew / opt / openblas / lib - l / opt / homebrew / opt / python <user> . <number> / lib - l / opt / homebrew / opt / libffi / lib - l / opt / homebrew / opt / openblas / lib - l / opt / homebrew / opt / python <user> . <number> / lib - l / opt / homebrew / lib cmakefiles / cvlink . dir / main . cpp . o - o cvlink / opt / homebrew / lib / libopencv_gapi . <number> . <number> . dylib / opt / homebrew / lib / libopencv_stitching . <number> . <number> . dylib / opt / homebrew / lib / libopencv_alphamat . <number> . <number> . dylib / opt / homebrew / lib / libopencv_aruco . <number> . <number> . dylib / opt / homebrew / lib / libopencv_barcode . <number> . <number> . dylib / opt / homebrew / lib / libopencv_bgsegm . <number> . <number> . dylib / opt / homebrew / lib / libopencv_bioinspired . <number> . <number> . dylib / opt / homebrew / lib / libopencv_ccalib . <number> . <number> . dylib / opt / homebrew / lib / libopencv_dnn_objdetect . <number> . <number> . dylib / opt / homebrew / lib / libopencv_dnn_superres . <number> . <number> . dylib / opt / homebrew / lib / libopencv_dpm . <number> . <number> . dylib / opt / homebrew / lib / libopencv_face . <number> . <number> . dylib / opt / homebrew / lib / libopencv_freetype . <number> . <number> . dylib / opt / homebrew / lib / libopencv_fuzzy . <number> . <number> . dylib / opt / homebrew / lib / libopencv_hfs . <number> . <number> . dylib / opt / homebrew / lib / libopencv_img_hash . <number> . <number> . dylib / opt / homebrew / lib / libopencv_intensity_transform . <number> . <number> . dylib / opt / homebrew / lib / libopencv_line_descriptor . <number> . <number> . dylib / opt / homebrew / lib / libopencv_mcc . <number> . <number> . dylib / opt / homebrew / lib / libopencv_quality . <number> . <number> . dylib / opt / homebrew / lib / libopencv_rapid . <number> . <number> . dylib / opt / homebrew / lib / libopencv_reg . <number> . <number> . dylib / opt / homebrew / lib / libopencv_rgbd . <number> . <number> . dylib / opt / homebrew / lib / libopencv_saliency . <number> . <number> . dylib / opt / homebrew / lib / libopencv_sfm . <number> . <number> . dylib / opt / homebrew / lib / libopencv_stereo . <number> . <number> . dylib / opt / homebrew / lib / libopencv_structured_light . <number> . <number> . dylib / opt / homebrew / lib / libopencv_superres . <number> . <number> . dylib / opt / homebrew / lib / libopencv_surface_matching . <number> . <number> . dylib / opt / homebrew / lib / libopencv_tracking . <number> . <number> . dylib / opt / homebrew / lib / libopencv_videostab . <number> . <number> . dylib / opt / homebrew / lib / libopencv_viz . <number> . <number> . dylib / opt / homebrew / lib / libopencv_wechat_qrcode . <number> . <number> . dylib / opt / homebrew / lib / libopencv_xfeatures2d . <number> . <number> . dylib / opt / homebrew / lib / libopencv_xobjdetect . <number> . <number> . dylib / opt / homebrew / lib / libopencv_xphoto . <number> . <number> . dylib / opt / homebrew / lib / libopencv_shape . <number> . <number> . dylib / opt / homebrew / lib / libopencv_highgui . <number> . <number> . dylib / opt / homebrew / lib / libopencv_datasets . <number> . <number> . dylib / opt / homebrew / lib / libopencv_plot . <number> . <number> . dylib / opt / homebrew / lib / libopencv_text . <number> . <number> . dylib / opt / homebrew / lib / libopencv_ml . <number> . <number> . dylib / opt / homebrew / lib / libopencv_phase_unwrapping . <number> . <number> . dylib / opt / homebrew / lib / libopencv_optflow . <number> . <number> . dylib / opt / homebrew / lib / libopencv_ximgproc . <number> . <number> . dylib / opt / homebrew / lib / libopencv_video . <number> . <number> . dylib / opt / homebrew / lib / libopencv_videoio . <number> . <number> . dylib / opt / homebrew / lib / libopencv_imgcodecs . <number> . <number> . dylib / opt / homebrew / lib / libopencv_objdetect . <number> . <number> . dylib / opt / homebrew / lib / libopencv_calib3d . <number> . <number> . dylib / opt / homebrew / lib / libopencv_dnn . <number> . <number> . dylib / opt / homebrew / lib / libopencv_features2d . <number> . <number> . dylib / opt / homebrew / lib / libopencv_flann . <number> . <number> . dylib / opt / homebrew / lib / libopencv_photo . <number> . <number> . dylib / opt / homebrew / lib / libopencv_imgproc . <number> . <number> . dylib / opt / homebrew / lib / libopencv_core . <number> . <number> . dylib [ <percent> ] built target cvlink / opt / homebrew / cellar / cmake / <number> . <number> / bin / cmake - e cmake_progress_start / users / myname / tmp / cvlink / build / cmakefiles <number> [ mymac : <time> : build ] $ ` ` ` because there were too many . dylibs included in the build , i tried to link and launch only one library to isolate the problem for all of the libraries . i found that out of <number> libraries , the following <number> libraries failed to launch if i linked even one of each of them . the other <number> can be linked all at once without any problem . i do not know but , is there any common characteristics of these ? ` ` ` bad / opt / homebrew / lib / libopencv_bioinspired . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_ccalib . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_dnn_objdetect . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_dpm . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_highgui . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_stereo . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_superres . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_tracking . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_videoio . <number> . <number> . dylib bad / opt / homebrew / lib / libopencv_videostab . <number> . <number> . dylib ` ` ` # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files images , onnx , etc",2
opencv/opencv,"cmake detects a non - existent external dependency < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> . <number> - operating system / platform => ubuntu <number> - compiler => gcc <number> . <number> - cmake => <number> . <number> - - > - opencv => <number> . <number> - operating system / platform => ubuntu <number> - compiler => gcc <number> . <number> - cmake => <number> . <number> - cuda = <number> - cudnn = <number> . <number> - nvidia driver = <number> . <number> - # # # # # detailed description cmake detects non - existent library "" lib "" . i am trying to compile opencv with cuda support . when i run cmake , it detects the following external dependencies : - - extra dependencies : m pthread cudart_static dl rt nppc nppial nppicc nppidei nppif nppig nppim nppist nppisu nppitc npps cublas lib cufft - l / usr / local / cuda - <number> / lib64 - l / usr / lib / x86_64 - linux - gnu - l / home / boyangli / . conda / envs / denseflow the problem is that lib is not a valid library . this leads to failures during linking . # # # # # steps to reproduce i used the following cmake command . ` ` ` cmake - d cmake_build_type = release - d cmake_install_prefix <annoyed> home / boyangli / app - d with_cuda = on - d enable_fast_math = <number> - d cuda_fast_math = <number> - d with_cublas = <number> - d install_python_examples = on - d opencv_extra_modules_path = . <repeated> / . <repeated> / opencv_contrib / modules - d python_executable =$ python_exec - d python_default_executable =$ default_exec - d python_include_dirs =$ include_dir - d python_library =$ library - d build_examples = on - dbuild_opencv_dpm = off \ \ - dbuild_opencv_face = off \ \ - dbuild_opencv_dnn_superres = off \ \ - dbuild_opencv_dnn_objdetect = off \ \ - dbuild_opencv_bgsegm = off \ \ - dbuild_opencv_cvv = off \ \ - dbuild_opencv_ccalib = off \ \ - dbuild_opencv_bioinspired = off \ \ - dbuild_opencv_dnn_modern = off \ \ - dbuild_opencv_dnns_easily_fooled = off \ \ - dbuild_java = off \ \ - dbuild_opencv_python2 = off \ \ - dbuild_new_python_support = on \ \ - dbuild_opencv_python3 = on \ \ - dhave_opencv_python3 = on \ \ - dwith_opengl = off \ \ - dwith_vtk = off \ \ - dforce_vtk = off \ \ - dwith_tbb = on \ \ - dwith_gdal = on \ \ - dcuda_fast_math = on \ \ - dwith_cublas = on \ \ - dwith_mkl = on \ \ - dmkl_use_multithread = on \ \ - dopencv_enable_nonfree = on \ \ - dwith_cuda = on \ \ - dnvcc_flags_extra = "" - - default - stream per - thread "" \ \ - dwith_nvcuvid = off \ \ - dbuild_opencv_cudacodec = off \ \ - dmkl_with_tbb = on \ \ - dwith_ffmpeg = on \ \ - dmkl_with_openmp = on \ \ - dwith_xine = on \ \ - denable_precompiled_headers = off \ \ - dcmake_install_prefix =""$ rootdir "" \ \ - dopencv_generate_pkgconfig = on \ \ - dopencv_extra_modules_path = . <repeated> / . <repeated> / opencv_contrib / modules \ \ - dcudnn_include_dir ='/ home / boyangli / . conda / envs / denseflow / include ' \ \ - dcudnn_library ='/ home / boyangli / . conda / envs / denseflow / lib ' \ \ - dc_include_path ='/ home / boyangli / . conda / envs / denseflow / include <annoyed> usr / local / include <annoyed> usr / include / x86_64 - linux - gnu ' \ \ - dinclude_path ='/ home / boyangli / . conda / envs / denseflow / include <annoyed> usr / local / include <annoyed> usr / include / x86_64 - linux - gnu ' \ \ - dc_path ='/ home / boyangli / . conda / envs / denseflow / include <annoyed> usr / local / include <annoyed> usr / include / x86_64 - linux - gnu ' \ \ - dld_libary_path ='/ home / boyangli / . conda / envs / denseflow / lib <annoyed> usr / lib / x86_64 - linux - gnu ' \ \ - dcmake_verbose_makefile = on . <repeated> ` ` ` # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ x] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [x ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => : <number> . <number> : - operating system / platform => : ubuntu <number> . <number> <number> bit : - compiler => : gcc / g + + <number> . <number> : # # # # # detailed description just use * cmake <emphasis> * to build current * * opencv <number> . x* * with * * opencv_contrib <number> . x* * . ` ` ` console $ git show commit 9 2 3 1 2 fbc0cbd8d110c4eeb480f1cfc6de224eeea ( head - > <number> . x , upstream / master , upstream / head , upstream / <number> . x , origin / <number> . x) merge : 2 efcaa9e8e 3 3 f219dfe6 author : alexander alekhin < <email> > date : tue <date> <time> <number> + <number> merge pull request # <number> from yusukekameda : patch - <number> ` ` ` # # # # # steps to reproduce ` ` ` configure - build_protobuf off - protobuf_update_files on - build_opencv_datasets on - build_opencv_dnn on - build_opencv_dnn_objdetect on - build_opencv_dnn_superres on - build_opencv_dpm on - opencv_dnn_cuda on - opencv_dnn_opencl off - opencv_dnn_openvino off - opencv_dnn_perf_caffe off - opencv_dnn_perf_clcaffe off ` ` ` # # # # # error messages after * * make - j16 * * <details> ` ` ` console [ <percent> ] linking cxx shared library . <repeated> / . <repeated> / lib / libopencv_cvv . so cd . <repeated> / opencv / build / modules / cvv & & / usr / local / bin / cmake - e cmake_link_script cmakefiles / opencv_cvv . dir / link . txt - - verbose = <number> / usr / local / bin / c + + - fpic - fsigned - char - ffast - math - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wundef - winit - self - wpointer - arith - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fno - omit - frame - pointer - pg - g - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - fopenmp - wno - shadow - wno - missing - declarations - o3 - dndebug - dndebug - wl , - - as - needed - wl , - - no - undefined - shared - wl , - soname , libopencv_cvv . so . <number> - o . <repeated> / . <repeated> / lib / libopencv_cvv . so . <number> . <number> cmakefiles / opencv_cvv . dir / opencv_cvv_autogen / mocs_compilation . cpp . o cmakefiles / opencv_cvv . dir / src / controller / view_controller . cpp . o cmakefiles / opencv_cvv . dir / src / extension_api / api . cpp . o cmakefiles / opencv_cvv . dir / src / gui / call_window . cpp . o cmakefiles / opencv_cvv . dir / src / gui / image_call_tab . cpp . o cmakefiles / opencv_cvv . dir / src / gui / main_call_window . cpp . o cmakefiles / opencv_cvv . dir / src / gui / overview_group_subtable . cpp . o cmakefiles / opencv_cvv . dir / src / gui / overview_panel . cpp . o cmakefiles / opencv_cvv . dir / src / gui / overview_table . cpp . o cmakefiles / opencv_cvv . dir / src / gui / overview_table_row . cpp . o cmakefiles / opencv_cvv . dir / src / gui / rawview_group_subtable . cpp . o cmakefiles / opencv_cvv . dir / src / gui / rawview_table . cpp . o cmakefiles / opencv_cvv . dir / src / gui / rawview_table_row . cpp . o cmakefiles / opencv_cvv . dir / src / impl / call . cpp . o cmakefiles / opencv_cvv . dir / src / impl / data_controller . cpp . o cmakefiles / opencv_cvv . dir / src / impl / dmatch . cpp . o cmakefiles / opencv_cvv . dir / src / impl / filter . cpp . o cmakefiles / opencv_cvv . dir / src / impl / filter_call . cpp . o cmakefiles / opencv_cvv . dir / src / impl / final_show . cpp . o cmakefiles / opencv_cvv . dir / src / impl / init . cpp . o cmakefiles / opencv_cvv . dir / src / impl / match_call . cpp . o cmakefiles / opencv_cvv . dir / src / impl / show_image . cpp . o cmakefiles / opencv_cvv . dir / src / impl / single_image_call . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / accordion . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / collapsable . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / changed_pixels_widget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / channelreorderfilter . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / diffilterwidget <elongated> . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / grayfilterwidget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / overlayfilterwidget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / filter / sobelfilterwidget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / histogram . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / histogramoptpanel . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / cvvkeypoint . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / cvvmatch . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / cvvpointmatch . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / falsecolorkeypointpen . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / falsecolormatchpen . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointintervallselection . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointmanagement . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointportionselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointselectionselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointsettingsselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointshowsetting . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / keypointvaluechooser . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchintervallselection . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchmanagement . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchportionselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchscene . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchselectionselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchsettingsselector . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / matchshowsetting . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / rawview_window . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / showinrawviewwidget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / singlecolorkeypointpen . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / singlecolormatchpen . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / matchview / zoomableproxyobject . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / stfl_query_widget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / stfl_query_widget_lineedit . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / synczoomwidget . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / util . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / zoomableimage . cpp . o cmakefiles / opencv_cvv . dir / src / qtutil / zoomableimageoptpanel . cpp . o cmakefiles / opencv_cvv . dir / src / stfl / stringutils . cpp . o cmakefiles / opencv_cvv . dir / src / view / defaultfilterview . cpp . o cmakefiles / opencv_cvv . dir / src / view / dual_filter_view . cpp . o cmakefiles / opencv_cvv . dir / src / view / image_view . cpp . o cmakefiles / opencv_cvv . dir / src / view / linematchview . cpp . o cmakefiles / opencv_cvv . dir / src / view / pointmatchview . cpp . o cmakefiles / opencv_cvv . dir / src / view / rawview . cpp . o cmakefiles / opencv_cvv . dir / src / view / singlefilterview . cpp . o cmakefiles / opencv_cvv . dir / src / view / translationsmatchview . cpp . o - l / usr / local / cuda / lib64 - wl , - rpath , / usr / local / cuda / lib64 : . <repeated> / opencv / build / lib <annoyed> opt / qt / <number> / lib : . <repeated> / . <repeated> / lib / libopencv_features2d . so . <number> . <number> - ldl - lm - lpthread - lrt - lcudart_static - ldl - lrt - lnppc - lnppial - lnppicc - lnppidei - lnppif - lnppig - lnppim - lnppist - lnppisu - lnppitc - lnpps - lcublas - lcudnn - lcufft - l / usr / local / cuda / lib64 - l / usr / lib / x86_64 - linux - gnu / opt / qt / <number> / lib / libqt5widgets . so . <number> . <number> - lcudart_static - ldl - lrt - lnppc - lnppial - lnppicc - lnppidei - lnppif - lnppig - lnppim - lnppist - lnppisu - lnppitc - lnpps . <repeated> / . <repeated> / lib / libopencv_flann . so . <number> . <number> . <repeated> / . <repeated> / lib / libopencv_imgproc . so . <number> . <number> . <repeated> / . <repeated> / lib / libopencv_core . so . <number> . <number> . <repeated> / . <repeated> / lib / libopencv_cudev . so . <number> . <number> - lm - lpthread - lcublas - lcudnn - lcufft / opt / qt / <number> / lib / libqt5gui . so . <number> . <number> / opt / qt / <number> / lib / libqt5core . so . <number> . <number> make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_bioinspired_openexrimages_hdr_retina_tonemapping cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> / usr / local / include / google / protobuf / arena . h : <number> : more undefined references to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> / usr / local / include / google / protobuf / parse_context . h : <number> : more undefined references to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < google : : protobuf : : internal : : internalmetadata : : container < google : : protobuf : : unknownfieldset > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - caffe . pb . cc . <surprise> in function ` _global__sub_i_opencv_caffe . pb . cc ' : . <repeated> / opencv / build / modules / dnn / opencv - caffe . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' / usr / bin / ld : [ <percent> ] built target example_bioinspired_retinademo cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> / usr / local / include / google / protobuf / parse_context . h : <number> : more undefined references to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / opencv - onnx . pb . cc . <surprise> in function ` _global__sub_i_opencv_onnx . pb . cc ' : . <repeated> / opencv / build / modules / dnn / opencv - onnx . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' buildfile : . <repeated> / opencv / build / modules / java / jar / opencv / build . xml / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void google : : protobuf : : arena : : owndestructor < google : : protobuf : : internal : : wrappedmutex > ( google : : protobuf : : internal : : wrappedmutex <wink> ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : addcleanup ( void * , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void google : : protobuf : : arena : : owndestructor < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> >*) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : addcleanup ( void * , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void google : : protobuf : : arena : : owninternal < opencv_tensorflow : : nameattrlist_attrentry_donotuse > ( opencv_tensorflow : : nameattrlist_attrentry_donotuse * , std : : integral_constant < bool , true > <sad> / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : addcleanup ( void * , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` _global__sub_i_attr_value . pb . cc ' : . <repeated> / opencv / build / modules / dnn / attr_value . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < google : : protobuf : : repeatedptrfield < google : : protobuf : : message > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / attr_value . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` void google : : protobuf : : arena : : owninternal < opencv_tensorflow : : functiondef_node_attrentry_donotuse > ( opencv_tensorflow : : functiondef_node_attrentry_donotuse * , std : : integral_constant < bool , true > <sad> / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : addcleanup ( void * , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` _global__sub_i_function . pb . cc ' : . <repeated> / opencv / build / modules / dnn / function . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < google : : protobuf : : repeatedptrfield < google : : protobuf : : message > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / function . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_line_descriptor_knn_matching / usr / bin / ld : make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_line_descriptor_compute_descriptors cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` void google : : protobuf : : arena : : owninternal < opencv_tensorflow : : nodedef_attrentry_donotuse > ( opencv_tensorflow : : nodedef_attrentry_donotuse * , std : : integral_constant < bool , true > <sad> / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : addcleanup ( void * , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` _global__sub_i_graph . pb . cc ' : . <repeated> / opencv / build / modules / dnn / graph . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < google : : protobuf : : repeatedptrfield < google : : protobuf : : message > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` void * google : : protobuf : : arena : : allocateinternal < std : : map < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > , void * , std : : less < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > > , google : : protobuf : : internal : : mapallocator < std : : pair < std : : reference_wrapper < std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const > const , void *> > > > ( bool ) ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : internal : : arenaimpl : : allocatealignedandaddcleanup ( unsigned long , void (* ) ( void *)) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / graph . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / op_def . pb . cc . <surprise> in function ` _global__sub_i_op_def . pb . cc ' : . <repeated> / opencv / build / modules / dnn / op_def . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor . pb . cc . <surprise> in function ` _global__sub_i_tensor . pb . cc ' : . <repeated> / opencv / build / modules / dnn / tensor . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : verifyutf8 ( std : : __cxx11 : : basic_string < char , std : : char_traits <char> , std : : allocator <char> > const * , char const <wink> ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : verifyutf8 ( google : : protobuf : : stringpiece , char const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> / usr / local / include / google / protobuf / generated_message_util . h : <number> : more undefined references to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / tensor_shape . pb . cc . <surprise> in function ` _global__sub_i_tensor_shape . pb . cc ' : . <repeated> / opencv / build / modules / dnn / tensor_shape . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / versions . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / versions . pb . cc . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / versions . pb . cc . <surprise> in function ` google : : protobuf : : internal : : epscopyinputstream : : donewithcheck ( char const * * , int ) ' : / usr / local / include / google / protobuf / parse_context . h : <number> : undefined reference to ` google : : protobuf : : internal : : epscopyinputstream : : donefallback ( char const * , int ) ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / versions . pb . cc . <surprise> in function ` google : : protobuf : : internal : : initscc ( google : : protobuf : : internal : : sccinfobase <wink> ' : / usr / local / include / google / protobuf / generated_message_util . h : <number> : undefined reference to ` google : : protobuf : : internal : : initsccimpl ( google : : protobuf : : internal : : sccinfobase <wink> ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / versions . pb . cc . <surprise> in function ` _global__sub_i_versions . pb . cc ' : . <repeated> / opencv / build / modules / dnn / versions . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_line_descriptor_radius_matching jar : make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ javac ] compiling <number> source files to . <repeated> / opencv / build / modules / java / jar / opencv / build / classes [ <percent> ] built target example_line_descriptor_lines_extraction make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_line_descriptor_matching [ <percent> ] built target example_line_descriptor_lsd_lines_extraction make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target example_saliency_computesaliency / usr / bin / ld : cmakefiles / opencv_dnn . dir / src / tensorflow / tf_graph_simplifier . cpp . <surprise> in function ` google : : protobuf : : arena : : allochook ( std : : type_info const * , unsigned long ) const ' : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : / usr / local / include / google / protobuf / arena . h : <number> : undefined reference to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' / usr / bin / ld : cmakefiles / opencv_dnn . dir / src / tensorflow / tf_importer . cpp . <surprise> / usr / local / include / google / protobuf / arena . h : <number> : more undefined references to ` google : : protobuf : : arena : : onarenaallocation ( std : : type_info const * , unsigned long ) const ' follow / usr / bin / ld : cmakefiles / opencv_dnn . dir / types . pb . cc . <surprise> in function ` _global__sub_i_types . pb . cc ' : . <repeated> / opencv / build / modules / dnn / types . pb . cc : <number> : undefined reference to ` google : : protobuf : : internal : : addescriptors <elongated> ( google : : protobuf : : internal : : descriptortable const <wink> ' cd . <repeated> / opencv / build / modules / cvv & & / usr / local / bin / cmake - e cmake_symlink_library . <repeated> / . <repeated> / lib / libopencv_cvv . so . <number> . <number> . <repeated> / . <repeated> / lib / libopencv_cvv . so . <number> . <repeated> / . <repeated> / lib / libopencv_cvv . so make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' [ <percent> ] built target opencv_cvv make - f modules / cvv / cmakefiles / example_cvv_cvv_demo_autogen . dir / build . make modules / cvv / cmakefiles / example_cvv_cvv_demo_autogen . dir / depend make [ <number> <sad> entering directory ' . <repeated> / opencv / build ' cd . <repeated> / opencv / build & & / usr / local / bin / cmake - e cmake_depends "" unix makefiles "" . <repeated> / opencv . <repeated> / opencv_contrib / modules / cvv . <repeated> / opencv / build . <repeated> / opencv / build / modules / cvv . <repeated> / opencv / build / modules / cvv / cmakefiles / example_cvv_cvv_demo_autogen . dir / dependinfo . cmake - - color = make [ <number> <sad> leaving directory ' . <repeated> / opencv / build ' make - f modules / cvv / cmakefiles / example_cvv_cvv_demo_autogen . dir / build . make modules / cvv / cmakefiles / example_cvv_cvv_demo_autogen . dir / build make [ <number> ] directory ' . <repeated> / opencv / build ' ` ` ` </details> cheers",2
opencv/opencv,"cannot write files in prores codecs i am attempting to write a video file using opencv in the prores <number> codec . i have tried other codecs , such as xvid , and the code works perfectly , however no prores codec of any kind seems to be working . relavent code : / / above code specifies clipname , which increments for each clip created in the loop below subclip = cv . videowriter ( clipname , cv . videowriter_fourcc ( * ' apch ' ) , <number> , size ) / / adds frames to subclip f = <number> / / cliprange contains new frames every time this code is run while f < len ( cliprange ) : subclip . write ( cliprange [ f ] ) f + = <number> print ( "" clip created "" ) subclip . release ( ) when i run this code i get a runtime error for every attempted file - write : ` [ prores @ 0x 7 fb8e4986600 ] specified pixel format yuv420p is invalid or not supported [ error : <number> ] global / private / var / folders / <number> / 8 k48jl6d249_n_qfxwsl6xvm0000gn / t / pip - req - build - xxsyexfp / opencv / modules / videoio / src / cap_ffmpeg_impl . hpp ( <number> ) open could not open codec prores , error : unspecified error [ error : <number> ] global / private / var / folders / <number> / 8 k48jl6d249_n_qfxwsl6xvm0000gn / t / pip - req - build - xxsyexfp / opencv / modules / videoio / src / cap_ffmpeg_impl . hpp ( <number> ) open videoio / ffmpeg : failed to initialize videowriter ` i am guessing that , by default , opencv or ffmpeg are using the format yuv420p when trying to write these files . upon checking the file path listed "" / private / var / folders / <number> / 8 k48jl6d249_n_qfxwsl6xvm0000gn / t / pip - req - build - xxsyexfp / opencv / modules / videoio / src / cap_ffmpeg_impl . hpp "" is invalid . in fact , the folder "" / private / var / folders "" has no directory named "" <number> "" , and the file "" cap_ffmpeg_impl . hpp "" does not appear to be anywhere on my computer . do i need to download this format ? or update some software ? or possibly change some settings to enable it ? or even update some setting to have prores encoded with a different pixel format ? settings : - opencv <number> . <number> - macos <number> . <number> on intel ( not apple silicon ) - python <number> . <number> - ffmpeg <number> . <number> built with clang version <number> . <number> ( tags / release_401 / final ) configuration - - cc =x 8 6 _64 - apple - darwin13 . <number> - clang - - disable - doc - - enable - avresample - - enable - gmp - - enable - hardcoded - tables - - enable - libfreetype - - enable - libvpx - - enable - pthreads - - enable - libopus - - enable - postproc - - enable - pic - - enable - pthreads - - enable - shared - - enable - static - - enable - version3 - - enable - zlib - - enable - libmp3lame - - disable - nonfree - - enable - gpl - - enable - gnutls - - disable - openssl - - enable - libopenh264 - - enable - libx264 libavutil <number> . <number> / <number> . <number> libavcodec <number> . <number> / <number> . <number> libavformat <number> . <number> / <number> . <number> libavdevice <number> . <number> / <number> . <number> libavfilter <number> . <number> / <number> . <number> libavresample <number> . <number> . <number> / <number> . <number> . <number> libswscale <number> . <number> / <number> . <number> libswresample <number> . <number> / <number> . <number> libpostproc <number> . <number> / <number> . <number>",2
opencv/opencv,"opencv error with gpu backend < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue that can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) - opencv => <number> . <number> - dev - operating system / platform => ubuntu <number> - compiler => gcc # # # # # detailed description i cannot run anymore my cv2 . dnn on gpu . i tried to run the code in cpu and its workings . i also tested with the older version of opencv and is working . i think the new commit merged in february changed the dnn output is an np . array [ ] and is not compatible with <number> . <number> release version . could you please give an example of how to use the new output ? # # # # # steps to reproduce ` ` ` . python / / with cuda enabled net = cv2 . dnn . readnetfromonnx ( model_yolo_v5 ) ) net . setinput ( _image ) results = net . forward ( ) ` ` ` # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"warning : libmediandk . so , needed by . <repeated> / . <repeated> / lib / arm64 - v8a / libopencv_videoio . so , not found ( try using - rpath or - rpath - link ) hi i was trying to cross compile opencv for android . my ndk version is <number> and api level is <number> . the make is crashing stating warning : libmediandk . so , was not found . any guidance on rectifying the issue will be useful . [ image ] ( <url> i used the instructions here for the same : <url> the cmake command i used is . <repeated> - dcmake_toolchain_file <annoyed> home / ubuntu / android / android - ndk - r19c / build / cmake / android . toolchain . cmake - dandroid_ndk <annoyed> home / ubuntu / android / android - ndk - r19c - dandroid_native_api_level = android - <number> - dbuild_java = off - dbuild_android_examples = off - dbuild_android_projects = off - dandroid_stl =c + + _shared - dbuild_shared_libs = on - dcmake_install_prefix : path <annoyed> home / ubuntu / opencv / android_build / out - dandroid_abi = arm64 - v8a - dcmake_cxx_flags = "" - llog "" `",2
opencv/opencv,"unable to compile gapi code returning contours from cv : : gapi : : findcontours # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => windows <number> <number> bit - compiler => visual studio <number> c + + <number> & <number> c + + <number> , <number> # # # # # detailed description i have compiled version <number> . <number> on windows with visual studio . i am unable to compile code dealing with gapi that calls findcontours and returns the contours . in my example i have a conditional compilation that returns a gmat instead of contours , the code compiles . compiler error from vs ( error list window ) error c2338 type not found testgapi opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp <number> ' ' ' compiler warning from vs ( output window ) <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> error c2338 : type not found <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : gmatp , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : detail : : type_list_index_helper < <number> , target , cv : : gmat , cv : : gmatp , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > with <number> > [ <number> > target = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ util \ \ variant . hpp ( <number> <sad> message : see reference to class template instantiation ' cv : : util : : type_list_index < int , cv : : gmat , cv : : gmatp , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > ' being compiled <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ gproto . hpp ( <number> <sad> message : see reference to function template instantiation ' cv : : util : : variant < cv : : gmat , cv : : gmatp , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > : : variant < _ty , void > ( t & & ) ' being compiled <number> > with <number> > [ <number> > _ty = int , <number> > t = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ gproto . hpp ( <number> <sad> message : see reference to function template instantiation ' cv : : util : : variant < cv : : gmat , cv : : gmatp , cv : : gframe , cv : : gscalar , cv : : detail : : garrayu , cv : : detail : : gopaqueu > : : variant < _ty , void > ( t & & ) ' being compiled <number> > with <number> > [ <number> > _ty = int , <number> > t = int <number> > ] <number> > opencv \ \ include \ \ opencv2 \ \ gapi \ \ gproto . hpp ( <number> <sad> message : see reference to function template instantiation ' cv : : gprotoargs cv : : detail : : packargs < cv : : gmat , int , int > ( cv : : gmat , int , int ) ' being compiled <number> > testgapi \ \ testgapi . cpp ( <number> <sad> message reference to function template instantiation ' cv : : gprotoinputargs cv : : gin < cv : : gmat & , int & , int & > ( cv : : gmat & , int & , int & ) ' being compiled <number> > done building project "" testgapi . vcxproj "" - - failed . ' ' ' # # # # # steps to reproduce ` ` ` <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> < opencv2 / gapi . hpp > <hashtag> include </hashtag> < opencv2 / gapi / core . hpp > <hashtag> include </hashtag> < opencv2 / gapi / imgproc . hpp > <hashtag> define </hashtag> does_not_compile <number> / / comment this line to compile successfully cv : : gcomputation mgcomputation ( [ ] ( ) { cv : : gmat in ; <hashtag> if def </hashtag> does_not_compile int numdilation ; int numerosions ; <hashtag> else </hashtag> int numdilation ( <number> ); int numerosions ( <number> ); <hashtag> end if </hashtag> cv : : gmat mask ( cv : : gapi : : gaussianblur ( in , cv : : size ( <number> , <number> ) , <number> )); mask = cv : : gapi : : erode3x3 ( mask , numerosions ) ; mask = cv : : gapi : : dilate3x3 ( mask , numdilation ) ; <hashtag> if def </hashtag> does_not_compile cv : : garray < cv : : garray < cv : : point > > contours = cv : : gapi : : findcontours ( mask , cv : : retrievalmodes : : retr_external , cv : : contourapproximationmodes : : chain_approx_simple ) ; return cv : : gcomputation ( cv : : gin ( in , numdilation , numerosions ) , cv : : gout ( contours ) ); <hashtag> else </hashtag> return cv : : gcomputation ( in , mask ) ; <hashtag> end if </hashtag> }); const cv : : scalar white ( cv : : scalar ( <number> )); int main ( int argc , char * * argv ) { int ndilation ( <number> ) , nerosions ( <number> ) , rw ( <number> ) , rh ( <number> ) , ofs ( <number> ) , lw ( <number> ); int x( ofs ) , y ( ofs ) , i ; cv : : mat input ( cv : : mat : : zeros ( <number> , <number> , cv_8uc1 ) ); for ( i = <number> ; i < <number> ; i + + ) { cv : : rectangle ( input , cv : : rect ( x , y , rw , rh ) , white , lw , cv : : filled ) ; x + = rw + ofs ; y + = rh + ofs ; } <hashtag> if def </hashtag> does_not_compile std : : vector < std : : vector < cv : : point > > contours ; mgcomputation . apply ( cv : : gin ( input , ndilation , nerosions ) , cv : : gout ( contours ) ); <hashtag> else </hashtag> cv : : mat output ; mgcomputation . apply ( input , output ) ; <hashtag> end if </hashtag> <hashtag> if def </hashtag> does_not_compile if ( contours . empty ( ) ) { std : : cout < < "" no contours found . "" < < std : : endl ; } else { std : : cout < < "" found "" < < contours . size ( ) < < "" contours . "" < < std : : endl ; } <hashtag> end if </hashtag> return <number> ; } ` ` `",2
opencv/opencv,"how to confirm the version of the opencv 3 rdparty libs < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) - opencv = <number> . <number> - operating system / platform => linux - compiler => gcc version <number> . <number> # # # # # detailed description i need to obtain each software version in the 3 rdparty directory in the source code of opencv <number> . <number> to analyze whether opencv <number> . <number> has dependency vulnerabilities . but the third - party software version is not found in the code repository or in the document . for example , the versions of open - source software such as carotene , cpufeatures , ippicv , and ittnotify cannot be confirmed . ! [ image ] ( <url> # # # # # steps to reproduce # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with questions without a real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to the latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only the latest release for each branch . the ticket is closed if the problem is not reproduced with the modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as an archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"relocation r_x86_64_pc32 against symbol ` ff_pw_9 ' can not be used when making a shared object ; recompile with - fpic # # # # # system information ( version ) ` ` ` detected processor : x86_64 could not find pythoninterp ( missing : python_executable ) ( required is at least version "" <number> "" ) looking for ccache - not found found zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( found suitable version "" <date> "" , minimum required is "" <number> . <number> "" ) could not find openjpeg ( minimal suitable version : <number> , recommended version >= <number> . <number> ) could not find jasper ( missing : jasper_libraries jasper_include_dir ) found zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( found version "" <date> "" ) checking for module ' gtk + - <number> ' no package ' gtk + - <number> ' found cuda detected : <number> cuda : using cuda_arch_bin = <number> ; <number> ; <number> ; <number> ; <number> ; <number> ; <number> ; <number> ; <number> cuda nvcc target flags : - gencode ; arch = compute_35 , code = sm_35 ; - gencode ; arch = compute_37 , code = sm_37 ; - gencode ; arch = compute_50 , code = sm_50 ; - gencode ; arch = compute_52 , code = sm_52 ; - gencode ; arch = compute_60 , code = sm_60 ; - gencode ; arch = compute_61 , code = sm_61 ; - gencode ; arch = compute_70 , code = sm_70 ; - gencode ; arch = compute_75 , code = sm_75 ; - gencode ; arch = compute_80 , code = sm_80 ; - d_force_inlines could not find openblas include . turning openblas_found off could not find openblas lib . turning openblas_found off could not find atlas ( missing : atlas_clapack_include_dir ) a library with lapack api found . vtk is not found . please set - dvtk_dir in cmake to vtk build directory , or to vtk install subdirectory with vtkconfig . cmake file opencv python : during development append to pythonpath : / home / mosaic / downloads / opencv4 / build / python_loader checking for module ' libavresample ' no package ' libavresample ' found checking for module ' gstreamer - base - <number> ' no package ' gstreamer - base - <number> ' found checking for module ' gstreamer - app - <number> ' no package ' gstreamer - app - <number> ' found checking for module ' gstreamer - riff - <number> ' no package ' gstreamer - riff - <number> ' found checking for module ' gstreamer - pbutils - <number> ' no package ' gstreamer - pbutils - <number> ' found checking for module ' libdc1394 - <number> ' no package ' libdc1394 - <number> ' found caffe : no protobuf : no glog : no freetype2 : yes ( ver <date> ) harfbuzz : yes ( ver <number> . <number> ) could not find hdf5 ( missing : hdf5_libraries hdf5_include_dirs ) ( found version "" "" ) julia not found . not compiling julia bindings . module opencv_ovis disabled because ogre3d was not found no preference for use of exported gflags cmake configuration set , and no hints for include / library directories provided . defaulting to preferring an installed / exported gflags cmake configuration if available . failed to find installed gflags cmake configuration , searching for gflags build directories exported with cmake . failed to find gflags - failed to find an installed / exported cmake configuration for gflags , will perform search for installed gflags components . failed to find gflags - could not find gflags include directory , set gflags_include_dir to directory containing gflags / gflags . h failed to find glog - could not find glog include directory , set glog_include_dir to directory containing glog / logging . h module opencv_sfm disabled because the following dependencies are not found : glog / gflags checking for module ' tesseract ' no package ' tesseract ' found tesseract : no allocator metrics storage type : ' long long ' registering hook ' init_module_sources_opencv_dnn ' : / home / mosaic / downloads / opencv4 / opencv - <number> . <number> / modules / dnn / cmake / hooks / init_module_sources_opencv_dnn . cmake general configuration for opencv <number> . <number> = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : unknown extra modules : location ( extra ) : / home / mosaic / downloads / opencv4 / opencv_contrib - <number> . <number> / modules version control ( extra ) : unknown platform : timestamp : <number> - <number> - 1 7 t <time> z host : linux <number> . <number> - <number> - generic x86_64 cmake : <number> . <number> cmake generator : unix makefiles cmake build tool : / usr / bin / make configuration : release cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : / usr / bin / c + + ( ver <number> . <number> ) c + + flags ( release ) : - fsigned - char - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winit - self - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - o3 - dndebug - dndebug c + + flags ( debug ) : - fsigned - char - w - wall - werror = return - type - werror = non - virtual - dtor - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wundef - winit - self - wpointer - arith - wshadow - wsign - promo - wuninitialized - winit - self - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - g - o0 - ddebug - d_debug c compiler : / usr / bin / cc c flags ( release ) : - fsigned - char - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - winit - self - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - o3 - dndebug - dndebug c flags ( debug ) : - fsigned - char - w - wall - werror = return - type - werror = address - werror = sequence - point - wformat - werror = format - security - wmissing - declarations - wmissing - prototypes - wstrict - prototypes - wundef - winit - self - wpointer - arith - wshadow - wuninitialized - winit - self - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - g - o0 - ddebug - d_debug linker flags ( release ) : - wl , - - gc - sections - wl , - - as - needed linker flags ( debug ) : - wl , - - gc - sections - wl , - - as - needed ccache : no precompiled headers : no extra dependencies : dl m pthread rt cudart nppc nppial nppicc nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cudnn cufft - l / usr / local / cuda / lib64 3 rdparty dependencies : opencv modules : to be built : alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm features2d flann freetype fuzzy hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor ml objdetect optflow phase_unwrapping photo plot quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab xfeatures2d ximgproc xobjdetect xphoto disabled : face java_bindings_generator python_bindings_generator python_tests world disabled by dependency : - unavailable : cnn_3dobj cvv gapi hdf java js julia matlab ovis python2 python3 sfm viz applications : apps documentation : no non - free algorithms : yes gui : gtk + : yes ( ver <date> ) gthread : yes ( ver <number> . <number> ) gtkglext : no vtk support : no media i / <surprise> zlib : / usr / lib / x86_64 - linux - gnu / libz . so ( ver <date> ) jpeg : / usr / lib / x86_64 - linux - gnu / libjpeg . so ( ver <number> ) webp : build ( ver encoder : 0x0 2 0 f ) png : / usr / lib / x86_64 - linux - gnu / libpng . so ( ver <date> ) tiff : build ( ver <number> - <date> ) jpeg <number> : build jasper ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : no gstreamer : no v4l / v4l2 : yes ( linux / videodev2 . h ) parallel framework : pthreads trace : yes ( with intel itt ) other third - party libraries : lapack : no eigen : yes ( ver <number> . <number> ) custom hal : no protobuf : build ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas ) nvidia gpu arch : <number> <number> <number> <number> <number> <number> <number> <number> <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( no extra features ) include path : / home / mosaic / downloads / opencv4 / opencv - <number> . <number> / 3 rdparty / include / opencl / <number> link libraries : dynamic load python ( for build ) : / usr / bin / python3 install to : / usr / local - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - configuring done ` ` ` # # # # # detailed description ` ` ` / usr / bin / ld : / usr / local / lib / libavcodec . a ( vc1dsp_mmx . o ) : relocation r_x86_64_pc32 against symbol ` ff_pw_9 ' can not be used when making a shared object ; recompile with - fpic / usr / bin / ld : 最后的链结失败 : bad value collect2 : error : ld returned <number> exit status make [ <number> <sad> * * * [ modules / videoio / cmakefiles / opencv_videoio . dir / build . make : <number> ： lib / libopencv_videoio . so . <number> . <number> ] 错误 <number> make [ <number> <sad> * * * [ cmakefiles / makefile2 : <number> ： modules / videoio / cmakefiles / opencv_videoio . dir / all ] 错误 <number> make [ <number> <sad> * * * 正在等待未完成的任务 . <repeated> ` ` ` + i do not know whether this error is caused by ffmpeg or opencv # # # # # steps to reproduce ` ` ` + enable cuda 、 cudnn 、 ffmpeg ( refer to system information ( version ) ) ` ` ` # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with question without real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ ] i updated to latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only latest release for each branch . the ticket is closed , if the problem is not reproduced with modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,compile opencv with inference - engine hello when i use cmake to compile opencv with inferenceengine i get problem next how to solve it,2
opencv/opencv,"android ndk 2 3 b lts compile error hey please advise changes required i can pr fixes following error for arm64 / x86 / x86_64 when targeting ndk 2 3 b ` ` ` error at / opt / homebrew / cellar / cmake / <number> . <number> / share / cmake / modules / platform / android - determine . cmake : <number> ( message ) : android : cmake_android_arm_mode is set but is valid only for ' armeabi ' ` ` ` all is working with ndk target 2 1 b in all architectures with the following build script # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => macos arm <number> bit m1 - compiler => clang + + # # # # # detailed description ` ` ` android_configure : cmake config ndk_vesion_major : <number> ndk_platform : android - <number> android_ndk_home : / users / one / library / android / sdk / ndk / <number> . <number> sysroot : / users / one / library / android / sdk / ndk / <number> . <number> / toolchains / llvm / prebuilt / darwin - x86_64 / sysroot toolchain : / users / one / library / android / sdk / ndk / <number> . <number> / toolchains / llvm / prebuilt / darwin - x86_64 / sysroot / usr / include ar : / users / one / library / android / sdk / ndk / <number> . <number> / toolchains / llvm / prebuilt / darwin - x86_64 / bin / llvm - ar / users / one / library / android / sdk / ndk / <number> . <number> / users / one / source / apothecary / apothecary / build / opencv / build_android_arm64 cmake error at / opt / homebrew / cellar / cmake / <number> . <number> / share / cmake / modules / platform / android - determine . cmake : <number> ( message ) : android : cmake_android_arm_mode is set but is valid only for ' armeabi ' architectures . call stack ( most recent call first ) : / opt / homebrew / cellar / cmake / <number> . <number> / share / cmake / modules / cmakedeterminesystem . cmake : <number> ( include ) cmakelists . txt : <number> ( enable_language ) cmake error : cmake was unable to find a build program corresponding to "" unix makefiles "" . cmake_make_program is not set . you probably need to select a different build tool . - - configuring incomplete , errors occurred ! ^ received error ^ ` ` ` compilation successful in ndk 2 3 b : - arm - v7a / armv7 * * failures in : * * - x86 - x86_64 - arm - v8a / arm64 # # # # # steps to reproduce cmake compile : ` ` ` rm - rf $ build_folder mkdir $ build_folder cd $ build_folder if [ "" $ abi "" = "" armeabi - v7a "" ]; then export arm_mode = "" - dandroid_force_arm_build = true "" elif [ $ abi = "" arm64 - v8a "" ]; then export arm_mode = "" - dandroid_force_arm_build = true "" elif [ "" $ abi "" = "" x86_64 "" ]; then export arm_mode = "" - dandroid_force_arm_build = false "" elif [ "" $ abi "" = "" x86 "" ]; then export arm_mode = "" - dandroid_force_arm_build = false "" fi android_ndk =${ ndk_root } export android_native_api_level = <number> echo ${ android_ndk } pwd cmake \ \ - dandroid_toolchain = clang + + \ \ - dcmake_toolchain_file =${ ndk_root } / build / cmake / android . toolchain . cmake \ \ - dcmake_cxx_compiler_ranlib =${ ranlib } \ \ - dcmake_c_compiler =${ cc } \ \ - dcmake_cxx_compiler =${ cxx } \ \ - dcmake_cxx_flags = "" - fvisibility - inlines - hidden - stdlib = libc + + - o3 - fpic - wno - implicit - function - declaration "" \ \ - dcmake_c_flags = "" - fvisibility - inlines - hidden - stdlib = libc + + - o3 - fpic - wno - implicit - function - declaration "" \ \ ${ arm_mode } \ \ - d android_platform =${ android_platform } \ \ - dandroid_abi =${ abi } \ \ - dbuild_android_projects = off \ \ - d build_android_examples = off \ \ - d build_opencv_objdetect = off \ \ - d build_opencv_video = off \ \ - d build_opencv_videoio = off \ \ - d build_opencv_features2d = off \ \ - d build_opencv_flann = off \ \ - d build_opencv_highgui = on \ \ - d build_opencv_ml = on \ \ - d build_opencv_photo = off \ \ - d build_opencv_python = off \ \ - d build_opencv_shape = off \ \ - d build_opencv_stitching = off \ \ - d build_opencv_superres = off \ \ - d build_opencv_ts = off \ \ - d build_opencv_videostab = off \ \ - d with_matlab = off \ \ - d with_cuda = off \ \ - dbuild_shared_libs = off \ \ - dbuild_docs = off \ \ - dbuild_examples = off \ \ - dbuild_fat_java_lib = off \ \ - dbuild_jasper = off \ \ - dbuild_package = off \ \ - dbuild_opencv_java = off \ \ - dbuild_opencv_apps = off \ \ - dbuild_jpeg = off \ \ - dbuild_png = off \ \ - dhave_opencv_androidcamera = off \ \ - dwith_carotene = off \ \ - dwith_cpufeatures = off \ \ - dwith_tiff = off \ \ - dwith_openexr = off \ \ - dwith_1394 = off \ \ - dwith_jpeg = off \ \ - dwith_png = off \ \ - dwith_ffmpeg = off \ \ - dwith_opencl = off \ \ - dwith_gigeapi = off \ \ - dwith_cuda = off \ \ - dwith_cufft = off \ \ - dwith_jasper = off \ \ - dwith_imageio = off \ \ - dwith_ipp = off \ \ - dwith_openni = off \ \ - dwith_qt = off \ \ - dwith_v4l = off \ \ - dwith_pvapi = off \ \ - dwith_eigen = off \ \ - dbuild_tests = off \ \ - dandroid_ndk =${ ndk_root } \ \ - dcmake_build_type = release \ \ - dandroid_abi =$ abi \ \ - dandroid_stl =c + + _static \ \ - dandroid_platform =$ android_platform \ \ - dcmake_toolchain_file =$ android_cmake_toolchain \ \ - dbuild_perf_tests = off . <repeated> make - j ${ parallel_make } make install ` ` ` # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with question without real issue statement will be closed . - - > - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [x ] i updated to latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only latest release for each branch . the ticket is closed , if the problem is not reproduced with modern version . - - > - [x ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"haarcascade_frontalcatface muslim women detection < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) < ! - - example - opencv => <number> - operating system / platform => windows <number> bit - compiler => visual studio <number> - - > - opencv => : grey_question : - operating system / platform => : grey_question : - compiler => : grey_question : # # # # # detailed description < ! - - your description - - > the haarcascade_frontalcatface can not detect faces of mulsim women who wear hijab . also , it did not work in detecting far faces , some children faces although it is clear . like this image <url> # # # # # steps to reproduce < ! - - to add code example fence it with triple backticks and optional file extension ` ` ` . cpp / / c + + code example ` ` ` or attach as . txt or . zip file - - > # # # # # issue submission checklist - [ ] i report the issue , it ' s not a question < ! - - opencv team works with forum . opencv . org , stack overflow and other communities to discuss problems . tickets with question without real issue statement will be closed . - - > - [ ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found solution < ! - - places to check : * opencv documentation : <url> * faq page : <url> * opencv forum : <url> * opencv issue tracker : <url> * stack overflow branch : <url> - - > - [ ] i updated to latest opencv version and the issue is still there < ! - - master branch for opencv <number> . x and <number> branch for opencv <number> . x releases . opencv team supports only latest release for each branch . the ticket is closed , if the problem is not reproduced with modern version . - - > - [ ] there is reproducer code and related data files : videos , images , onnx , etc < ! - - the best reproducer - - test case for opencv that we can add to the library . recommendations for media files and binary files try to reproduce the issue with images and videos in opencv_extra repository to reduce attachment size * use png for images , if you report some cv related bug , but not image reader issue * attach the image as archive to the ticket , if you report some reader issue . image hosting services compress images and it breaks the repro code . * provide onnx file for some public model or onnx file with with random weights , if you report onnx parsing or handling issue . architecture details diagram from netron tool can be very useful too . see <url> - - >",2
opencv/opencv,"[ onnx ] inference different result between python and c + + < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # system information - opencv => <number> - operating system / platform => macos - compiler => gcc - <number> # # # detailed description with same model , same image , onnx inference diffrent result between python and c + + ; model download [ link ] ( <url> image download [ link ] ( <url> # # # steps to reproduce with python code below : ` ` ` python import cv2 import numpy as np img = cv2 . imread ( "" data / test . jpg "" ) img = cv2 . resize ( img , none , none , <number> , <number> , cv2 . inter_linear ) ; img = np . float32 ( img ) net = cv2 . dnn . readnetfromonnx ( "" faceboxes . onnx "" ) net . setpreferablebackend ( cv2 . dnn . dnn_backend_opencv ) net . setpreferablebackend ( cv2 . dnn . dnn_target_cpu ) blobimage = cv2 . dnn . blobfromimage ( img , <number> , ( img . shape [ <number> ] , img . shape [ <number> ] ) , ( <number> , <number> , <number> ) , false , false ) outnames = net . getunconnectedoutlayersnames ( ) net . setinput ( blobimage ) outs = net . forward ( outnames ) loc , conf = outs boxes = loc [ <number> ] . tolist ( ) scores = conf [ <number> ] . tolist ( ) # loc => 4 5 2 7 9 x4 conf => 4 5 2 7 9 x2 with open ( ' pyout . csv ' , ' w ' ) as f : for i in range ( len ( boxes ) <sad> if ( scores [ i ] [ <number> ] < <number> <sad> continue print ( scores [ i ] [ <number> ] ) line = f ' { i } , { boxes [ i ] [ <number> <sad> . 4 f } , { boxes [ i ] [ <number> <sad> . 4 f } , { boxes [ i ] [ <number> <sad> . 4 f } , { boxes [ i ] [ <number> <sad> . 4 f } , { scores [ i ] [ <number> <sad> . 6 f } \ \ n ' f . write ( line ) ` ` ` i get <number> locs ( score > = <number> ) , and this is python eval [ result ] ( <url> top <number> lines : ` ` ` pyout . csv <number> , - <number> , <number> , - <date> , - <number> , <number> <number> . <number> , <date> , - <number> , <number> , <number> <number> . <number> , - <number> , - <number> , <number> , <number> <number> , - <number> , <date> , - <number> , <number> , <number> <number> , - <number> , - <date> , - <number> , <number> , <number> <number> . <number> . <number> , - <number> , - <number> , <number> <number> . <number> , - <number> , - <number> , - <number> , <number> . <repeated> <number> lines in total ` ` ` while c + + code only get <number> locs ( score > = <number> ) ` ` ` c + + <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> <string> <hashtag> include </hashtag> < opencv2 / opencv . hpp > <hashtag> include </hashtag> < opencv2 / dnn . hpp > <hashtag> include </hashtag> <fstream> using namespace std ; using namespace cv ; int main ( ) { string model_path = "" / users / mix / faceboxes . pytorch / faceboxes . onnx "" ; string image_path = "" data / test . jpg "" ; mat clr = imread ( image_path , imread_color ) ; mat image , blob , conf , loc ; resize ( clr , image , size ( ) , <number> , <number> , inter_linear ) ; dnn : : blobfromimage ( image , blob , <number> , size ( image . cols , image . rows ) , scalar ( <number> , <number> , <number> ) , false , false ) ; auto net = dnn : : readnetfromonnx ( "" faceboxes . onnx "" ); net . setpreferablebackend ( dnn : : dnn_backend_opencv ) ; net . setpreferabletarget ( dnn : : dnn_target_cpu ) ; net . setinput ( blob ) ; std : : vector <string> outlayernames = net . getunconnectedoutlayersnames ( ); std : : vector <mat> outs ; net . forward ( outs , outlayernames ) ; mat ( outs [ <number> ] . size [ <number> ] , outs [ <number> ] . size [ <number> ] , cv_32f , outs [ <number> ] . data ) . copyto ( loc ) ; mat ( outs [ <number> ] . size [ <number> ] , outs [ <number> ] . size [ <number> ] , cv_32f , outs [ <number> ] . data ) . copyto ( conf ) ; ofstream ofs ( "" out . csv "" ); for ( int i = <number> ; i < conf . rows ; i + + ) { if ( conf . at <float> ( i , <number> ) > <number> ) { ofs < < i < < "" , "" < < loc . at <float> ( i , <number> ) < < "" , "" < < loc . at <float> ( i , <number> ) < < "" , "" < < loc . at <float> ( i , <number> ) < < "" , "" < < loc . at <float> ( i , <number> ) < < "" , "" < < conf . at <float> ( i , <number> ) < < endl ; } } ofs . close ( ); } ` ` ` and this is c + + eval [ result ] ( <url> top <number> lines out . csv <number> . <number> . <number> , - <number> , <number> , <number> <number> . <number> , - <number> , - <number> , <number> , <number> <number> , - <number> , - <number> , - <number> , - <number> , <number> <number> . <number> . <number> , - <number> , - <number> , <number> <number> , - <number> , <number> , - <number> , - <number> , <number> <number> . <number> , - <number> , - <number> , - <number> , <number> <number> , - <number> , - <number> , - <number> , - <number> , <number> . <repeated> <number> lines in total ` ` `",2
opencv/opencv,"add intel ® oneapi dpc + + / c + + compiler ( icx ) intel ® c + + compiler classic ( icc ) is deprecated and will be removed in a oneapi release in the second half of <number> ( [ deprecation notice ] ( <url> this commit is intended to add support for the next - generation compiler , intel ® oneapi dpc + + / c + + compiler ( icx ) ( the documentation for the compiler is available on the [ link ] ( <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"cuda compilation is not using ccache # # # system information opencv version : <number> . <number> operating system / platform : ubuntu <number> . <number> lts compiler & compiler version : gcc <number> . <number> cuda : <number> # # # detailed description using ccache <number> . <number> ( ubuntu version ) or latest version ( <number> . <number> ) , ccache is found by opencv ' s cmake but used only to compile c / c + + # # # steps to reproduce ` ` ` cd opencv mkdir build & & cd build cmake - dwith_cuda = on - dbuild_list = core , cudev , cudaimgproc \ \ - dcuda_arch_bin = "" <number> "" \ \ - dopencv_extra_modules_path = . <repeated> / . <repeated> / opencv_contrib / modules . <repeated> verbose = <number> make opencv_core verbose = <number> make opencv_cudaimgproc ` ` ` we could see that ` ccache ` is used for c + + files ` ` ` [ <percent> ] building cxx object modules / core / cmakefiles / opencv_core . dir / src / alloc . cpp . o cd / home / dgeld / src / opencv / build / modules / core & & / usr / bin / ccache / usr / bin / c + + - dcvapi_exports - dopencv_allocator_stats_counter_type = "" long long "" - dopencv_with_itt = <number> - d_use_math_defines - d__opencv_build = <number> - d__stdc_constant_macros - d__stdc_format_macros - d__stdc_limit_macros - dhave_malloc_h = <number> - dhave_memalign = <number> - dhave_posix_memalign = <number> - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / icv / include - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / iw / include - i / home / dgeld / src / opencv / build - i / home / dgeld / src / opencv / modules / core / include - i / home / dgeld / src / opencv / build / modules / core - i / home / dgeld / src / opencv_contrib / modules / cudev / include - i / home / dgeld / src / opencv / 3 rdparty / include / opencl / <number> - i / home / dgeld / src / opencv / 3 rdparty / ittnotify / include - isystem / usr / local / cuda - <number> / include - fsigned - char - w - wall - wreturn - type - wnon - virtual - dtor - waddress - wsequence - point - wformat - wformat - security - wmissing - declarations - winit - self - wpointer - arith - wsign - promo - wuninitialized - wsuggest - override - wno - delete - non - virtual - dtor - wno - comment - wimplicit - fallthrough = <number> - wno - strict - overflow - fdiagnostics - show - option - wno - long - long - pthread - fomit - frame - pointer - ffunction - sections - fdata - sections - msse - msse2 - msse3 - fvisibility = hidden - fvisibility - inlines - hidden - wno - undef - wno - enum - compare - wno - unused - function - wno - shadow - o3 - dndebug - dndebug - std =c + + <number> - fpic - md - mt modules / core / cmakefiles / opencv_core . dir / src / alloc . cpp . o - mf cmakefiles / opencv_core . dir / src / alloc . cpp . o . d - o cmakefiles / opencv_core . dir / src / alloc . cpp . o - c / home / dgeld / src / opencv / modules / core / src / alloc . cpp ` ` ` but cuda files are not cached : ` ` ` [ <percent> ] building nvcc ( device ) object modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o cd / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda & & / snap / cmake / <number> / bin / cmake - e make_directory / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . cd / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda & & / snap / cmake / <number> / bin / cmake - d verbose : bool = <number> - d build_configuration : string = release - d generated_file : string <annoyed> home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o - d generated_cubin_file : string <annoyed> home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o . cubin . txt - p / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . release . cmake - - removing / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o / snap / cmake / <number> / bin / cmake - e rm - f / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o - - generating dependency file : / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . nvcc - depend / usr / local / cuda - <number> / bin / nvcc - m - d__cudacc__ / home / dgeld / src / opencv / modules / core / src / cuda / gpu_mat . cu - o / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . nvcc - depend - ccbin / usr / bin / cc - m64 - d_use_math_defines - d__stdc_constant_macros - d__stdc_limit_macros - d__stdc_format_macros - dopencv_with_itt = <number> "" - dopencv_allocator_stats_counter_type = long long "" - d__opencv_build = <number> - xcompiler , \ \ "" - fsigned - char \ \ "" , \ \ "" - w \ \ "" , \ \ "" - wall \ \ "" , \ \ "" - wreturn - type \ \ "" , \ \ "" - wnon - virtual - dtor \ \ "" , \ \ "" - waddress \ \ "" , \ \ "" - wsequence - point \ \ "" , \ \ "" - wformat \ \ "" , \ \ "" - wformat - security \ \ "" , \ \ "" - wmissing - declarations \ \ "" , \ \ "" - winit - self \ \ "" , \ \ "" - wpointer - arith \ \ "" , \ \ "" - wuninitialized \ \ "" , \ \ "" - wno - comment \ \ "" , \ \ "" - wno - strict - overflow \ \ "" , \ \ "" - fdiagnostics - show - option \ \ "" , \ \ "" - wno - long - long \ \ "" , \ \ "" - pthread \ \ "" , \ \ "" - fomit - frame - pointer \ \ "" , \ \ "" - ffunction - sections \ \ "" , \ \ "" - fdata - sections \ \ "" , \ \ "" - msse \ \ "" , \ \ "" - msse2 \ \ "" , \ \ "" - msse3 \ \ "" , \ \ "" - fvisibility = hidden \ \ "" , \ \ "" - wno - undef \ \ "" , \ \ "" - wno - enum - compare \ \ "" , \ \ "" - wno - unused - function \ \ "" , \ \ "" - wno - shadow \ \ "" , \ \ "" - wno - unused - but - set - variable \ \ "" , \ \ "" - o3 \ \ "" , \ \ "" - dndebug \ \ "" , \ \ "" - dndebug \ \ "" - gencode arch = compute_72 , code = sm_72 - d_force_inlines - xcompiler - dcvapi_exports - xcompiler - fpic - - std =c + + <number> - dnvcc - i / usr / local / cuda - <number> / include - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / icv / include - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / iw / include - i / home / dgeld / src / opencv / build - i / home / dgeld / src / opencv / modules / core / include - i / home / dgeld / src / opencv / build / modules / core - i / home / dgeld / src / opencv_contrib / modules / cudev / include - i / home / dgeld / src / opencv / 3 rdparty / include / opencl / <number> - i / home / dgeld / src / opencv / 3 rdparty / ittnotify / include - - generating temporary cmake readable file / snap / cmake / <number> / bin / cmake - d input_file : filepath <annoyed> home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . nvcc - depend - d output_file : filepath <annoyed> home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend . tmp - d verbose = <number> - p / snap / cmake / <number> / share / cmake - <number> / modules / findcuda / make2cmake . cmake - - copy if different / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend . tmp to / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend / snap / cmake / <number> / bin / cmake - e copy_if_different / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend . tmp / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend - - removing / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend . tmp and / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . nvcc - depend / snap / cmake / <number> / bin / cmake - e rm - f / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . depend . tmp / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / cuda_compile_1_generated_gpu_mat . cu . o . nvcc - depend - - generating / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o / usr / local / cuda - <number> / bin / nvcc / home / dgeld / src / opencv / modules / core / src / cuda / gpu_mat . cu - c - o / home / dgeld / src / opencv / build / modules / core / cmakefiles / cuda_compile_1 . dir / src / cuda / . / cuda_compile_1_generated_gpu_mat . cu . o - ccbin / usr / bin / cc - m64 - d_use_math_defines - d__stdc_constant_macros - d__stdc_limit_macros - d__stdc_format_macros - dopencv_with_itt = <number> "" - dopencv_allocator_stats_counter_type = long long "" - d__opencv_build = <number> - xcompiler , \ \ "" - fsigned - char \ \ "" , \ \ "" - w \ \ "" , \ \ "" - wall \ \ "" , \ \ "" - wreturn - type \ \ "" , \ \ "" - wnon - virtual - dtor \ \ "" , \ \ "" - waddress \ \ "" , \ \ "" - wsequence - point \ \ "" , \ \ "" - wformat \ \ "" , \ \ "" - wformat - security \ \ "" , \ \ "" - wmissing - declarations \ \ "" , \ \ "" - winit - self \ \ "" , \ \ "" - wpointer - arith \ \ "" , \ \ "" - wuninitialized \ \ "" , \ \ "" - wno - comment \ \ "" , \ \ "" - wno - strict - overflow \ \ "" , \ \ "" - fdiagnostics - show - option \ \ "" , \ \ "" - wno - long - long \ \ "" , \ \ "" - pthread \ \ "" , \ \ "" - fomit - frame - pointer \ \ "" , \ \ "" - ffunction - sections \ \ "" , \ \ "" - fdata - sections \ \ "" , \ \ "" - msse \ \ "" , \ \ "" - msse2 \ \ "" , \ \ "" - msse3 \ \ "" , \ \ "" - fvisibility = hidden \ \ "" , \ \ "" - wno - undef \ \ "" , \ \ "" - wno - enum - compare \ \ "" , \ \ "" - wno - unused - function \ \ "" , \ \ "" - wno - shadow \ \ "" , \ \ "" - wno - unused - but - set - variable \ \ "" , \ \ "" - o3 \ \ "" , \ \ "" - dndebug \ \ "" , \ \ "" - dndebug \ \ "" - gencode arch = compute_72 , code = sm_72 - d_force_inlines - xcompiler - dcvapi_exports - xcompiler - fpic - - std =c + + <number> - dnvcc - i / usr / local / cuda - <number> / include - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / icv / include - i / home / dgeld / src / opencv / build / 3 rdparty / ippicv / ippicv_lnx / iw / include - i / home / dgeld / src / opencv / build - i / home / dgeld / src / opencv / modules / core / include - i / home / dgeld / src / opencv / build / modules / core - i / home / dgeld / src / opencv_contrib / modules / cudev / include - i / home / dgeld / src / opencv / 3 rdparty / include / opencl / <number> - i / home / dgeld / src / opencv / 3 rdparty / ittnotify / include ` ` ` i would expect the last line ` / usr / local / cuda - <number> / bin / nvcc / home / dgeld / src / opencv / modules / core / src / cuda / gpu_mat . cu ` to use ccache . # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"convert bgr to rgb now ! <repeated> # # # describe the feature and motivation "" the reason why the early developers at opencv chose bgr color format is probably that back then bgr color format was popular among camera manufacturers and software providers . e . g . in windows , when specifying color value using colorref they use the bgr format 0x0 0 bbggrr . bgr was a choice made for historical reasons and now we have to live with it . in other words , bgr is the horse ’ s ass in opencv . "" rgb revolution . give a like if you support the cause . # # # additional context _no response_",1
opencv/opencv,"g - api directml execution provider for onnxrt backend # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"google summer of code support onnx operator gather elements # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake this is a draft pull request for gsoc review purposes . edits need to be made .",1
opencv/opencv,"feat : update numpy type to mat type fail message output string representation of numpy array type if it is not convertible to opencv mat type example output from the test : ` ` ` cv2 . error : opencv ( <number> . <number> - dev ) : - <number> : error : ( - <number> : bad argument ) in function ' dumpinputarray ' > overload resolution failed : > - argument data type = object is not supported > - expected ptr < cv : : umat > for argument ' argument ' ` ` ` resolves # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"expose list of videocapture devices # # # describe the feature and motivation there is no way currently , within opencv , to list existing devices to let the user select them . this means that they must be obtained in a different manner ( either by re - implementing manually , or using a third - party library ) , which may not match opencv ' s indexes and may not work on all platforms . there is a workaround to try every indexes individually , but this has <number> main issues some videocapture devices will freeze if opened and closed quickly or if it was already in use . - the popular sd capture card ` gv - usb2 ` ( confirmed myself ) - i have had reports that an ` avermedia ` capture card is affected as well <number> . some devices take a long time to boot , making this technique quite slow ( like my ` logitech c920 ` webcam ) <number> . the total amount of devices is unknown , and usable devices may not be sequential . so we have to guess how many devices maximum the user may have , trying to balance between time spent or risking missing some <number> . even if you get all the ids after this , you do not have access to the names to present to the user . ( see # <number> ) # # # additional context _no response_",1
opencv/opencv,"add single image support to videocapture # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work <url> - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"added aruco - based qr code detection method to python sample # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"weird behaviour when i use image2blobparams # # # describe the feature and motivation full post is <url> my code in python paramtf2cedn = cv . dnn . image2blobparams ( ) paramtf2cedn . datalayout = cv . dnn . dnn_layout_nhwc ; paramtf2cedn . ddepth = cv . cv_32f ; paramtf2cedn . mean = ( <number> , <number> , <number> ) paramtf2cedn . scalefactor = <number> / <number> . paramtf2cedn . size = ( <number> , <number> ) paramtf2cedn . swaprb = false ; paramtf2cedn . paddingmode = cv . dnn . dnn_pmode_null print ( "" paramtf2cedn . scalefactor = "" , paramtf2cedn . scalefactor ) ` ` ` result is ` paramtf2cedn . scalefactor = ( <number> , <number> , <number> , <number> ) ` in c + + ` ` ` image2blobparams paramtf2cedn ; paramtf2cedn . datalayout = dnn_layout_nhwc ; paramtf2cedn . ddepth = cv_32f ; paramtf2cedn . mean = ( <number> , <number> , <number> ); paramtf2cedn . scalefactor = <number> / <number> . ; paramtf2cedn . size = size ( <number> , <number> ); paramtf2cedn . swaprb = false ; paramtf2cedn . paddingmode = dnn_pmode_null ; cout < < "" paramtf2cedn . scalefactor = "" < < paramtf2cedn . scalefactor < < endl ; ` ` ` result is paramtf2cedn . scalefactor = [ <number> , <number> , <number> , <number> ] difference is in constructor ( thanks to <user> ) <url> i think something must be write in doc or constructor change # # # additional context _no response_",1
opencv/opencv,"opencv ( <number> . <number> - dev ) error : assertion failed ( interp_mode = "" tf_half_pixel_for_nn "" ) # # # system information ` ` ` general configuration for opencv <number> . <number> - dev = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = version control : <number> . <number> - <number> - g9fa014edcd extra modules : location ( extra ) : c <annoyed> lib / opencv_contrib / modules version control ( extra ) : <number> . <number> - <number> - g8dfeed73 platform : timestamp : <number> - <number> - 0 5 t <time> z host : windows <number> . <number> amd64 cmake : <number> . <number> cmake generator : visual studio <number> <number> cmake build tool : c <annoyed> program files / microsoft visual studio / <number> / community / msbuild / current / bin / amd64 / msbuild . exe msvc : <number> configuration : debug release cpu / hw features : baseline : sse sse2 sse3 requested : sse3 dispatched code generation : sse4_1 sse4_2 fp16 avx avx2 avx512_skx requested : sse4_1 sse4_2 avx fp16 avx2 avx512_skx sse4_1 ( <number> files ) : + ssse3 sse4_1 sse4_2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 avx avx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 avx avx2 ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx512_skx ( <number> files ) : + ssse3 sse4_1 popcnt sse4_2 fp16 fma3 avx avx2 avx_512f avx512_common avx512_skx c / c + + : built as dynamic libs ? : yes c + + standard : <number> c + + compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe ( ver <number> . <number> ) c + + flags ( release ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / md / o2 / ob2 / dndebug c + + flags ( debug ) : / dwin32 / d_windows / w4 / gr / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / eha / wd4127 / wd4251 / wd4324 / wd4275 / wd4512 / wd4589 / wd4819 / mp / mdd / zi / ob0 / od / rtc1 c compiler : c <annoyed> program files / microsoft visual studio / <number> / community / vc / tools / msvc / <number> . <number> / bin / hostx64 / x64 / cl . exe c flags ( release ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / md / o2 / ob2 / dndebug c flags ( debug ) : / dwin32 / d_windows / w3 / d _crt_secure_no_deprecate / d _crt_nonstdc_no_deprecate / d _scl_secure_no_warnings / gy / bigobj / oi / fp : precise / mp / mdd / zi / ob0 / od / rtc1 linker flags ( release ) : / machine <kiss> 6 4 / incremental : no linker flags ( debug ) : / machine <kiss> 6 4 / debug / incremental ccache : no precompiled headers : yes extra dependencies : cudart_static . lib nppc . lib nppial . lib nppicc . lib nppidei . lib nppif . lib nppig . lib nppim . lib nppist . lib nppisu . lib nppitc . lib npps . lib cublas . lib cudnn . lib cufft . lib - libpath <sad> <annoyed> program files / nvidia gpu computing toolkit / cuda / v12 . <number> / lib / x64 3 rdparty dependencies : opencv modules : to be built : alphamat aruco barcode bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto disabled : world disabled by dependency : - unavailable : cvv freetype hdf julia matlab ovis python2 applications : tests perf_tests examples apps documentation : doxygen python javadoc non - free algorithms : yes windows rt support : no gui : win32ui win32 ui : yes opengl support : yes ( opengl32 glu32 ) vtk support : yes ( ver <number> . <number> ) media i / <surprise> zlib : optimized c <annoyed> install / zlib / lib / zlib . lib debug c <annoyed> install / zlib / lib / zlibd . lib ( ver <date> ) jpeg : build - libjpeg - turbo ( ver <number> . <number> - <number> ) simd support request : yes simd support : no webp : build ( ver encoder : 0x0 2 0 f ) png : optimized c <annoyed> install / libpng / lib / libpng16 . lib debug c <annoyed> install / libpng / lib / libpng16d . lib ( ver <date> ) tiff : build ( ver <number> - <number> . <number> ) jpeg <number> : build ( ver <number> . <number> ) openexr : build ( ver <number> . <number> ) hdr : yes sunraster : yes pxm : yes pfm : yes video i / <surprise> dc1394 : no ffmpeg : yes ( prebuilt binaries ) avcodec : yes ( <number> . <number> ) avformat : yes ( <number> . <number> ) avutil : yes ( <number> . <number> ) swscale : yes ( <date> ) avresample : yes ( <number> . <number> ) gstreamer : no directshow : yes media foundation : yes dxva : yes parallel framework : concurrency other third - party libraries : intel ipp : <number> [ <number> . <number> ] at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / icv intel ipp iw : sources ( <number> . <number> ) at : c <annoyed> lib / build / opencv / 3 rdparty / ippicv / ippicv_win / iw lapack : yes ( c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_intel_lp64 . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_sequential . lib c <annoyed> program files (x 8 6 ) / intel / oneapi / mkl / <number> . <number> / lib / intel64 / mkl_core . lib ) openvino : yes ( <number> . <number> ) eigen : yes ( ver . <repeated> ) custom hal : no protobuf : build ( <number> . <number> ) flatbuffers : builtin / 3 rdparty ( <number> . <number> ) nvidia cuda : yes ( ver <number> , cufft cublas ) nvidia gpu arch : <number> nvidia ptx archs : cudnn : yes ( ver <number> . <number> ) opencl : yes ( nvd3d11 ) include path : c <annoyed> lib / opencv / 3 rdparty / include / opencl / <number> link libraries : dynamic load python <number> : interpreter : c <annoyed> program files / python310 / python . exe ( ver <date> ) libraries : optimized c <annoyed> program files / python310 / libs / python310 . lib debug c <annoyed> program files / python310 / libs / python310_d . lib ( ver <date> ) numpy : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / numpy / core / include ( ver <number> . <number> ) install path : c <annoyed> users / laurent / appdata / roaming / python / python310 / site - packages / cv2 / python - <number> python ( for build ) : c <annoyed> program files / python310 / python . exe java : ant : c <annoyed> apache - ant - <date> / bin / ant . bat ( ver <date> ) jni : c <annoyed> program files / java / jdk - <number> / include c <annoyed> program files / java / jdk - <number> / include / win32 c <annoyed> program files / java / jdk - <number> / include java wrappers : yes java tests : yes install to : c <annoyed> install / opencv - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ` ` ` # # # detailed description i train my own cedn network using tensorflow <number> . <number> i used this script to freeze and convert . pb to . onnx ( <url> ` ` ` # freeze model import tensorflow as tf from tensorflow import keras from tensorflow . python . framework . convert_to_constants import convert_variables_to_constants_v2 import numpy as np from tensorflow . python . util import compat from tf2onnx import tf_loader from tf2onnx . tfonnx import process_tf_graph from tf2onnx . optimizer import optimize_graph from tf2onnx import utils , constants from tf2onnx . handler import tf_op class perte ( tf . keras . losses . loss ) : def __init__ ( self ) : super ( ) . __init__ ( ) def call ( self , y_vrai , y_pred ) : mse1 = tf . reduce_sum ( tf . square ( tf . subtract ( y_vrai , y_pred ) ) , [ <number> , <number> , <number> ] ) mse2 = tf . reduce_sum ( y_vrai * tf . square ( y_vrai - y_pred ) , [ <number> , <number> , <number> ] ) return <number> * mse2 + mse1 mon_modele = tf . keras . models . load_model ( "" c <annoyed> tmp / log_dirtocd / cedn_pb "" , custom_objects ={ ' perte ' : perte ( ) } , compile = false ) infer = mon_modele . signatures [ "" serving_default "" ] f = tf . function ( infer ) . get_concrete_function ( tf . tensorspec ( shape = mon_modele . inputs [ <number> ] . shape , dtype = mon_modele . inputs [ <number> ] . dtype ) ) frozen_func = convert_variables_to_constants_v2 ( f ) graph_def = frozen_func . graph . as_graph_def ( ) output_names = [ out . name for out in frozen_func . outputs ] input_names = [ inp . name for inp in frozen_func . inputs ] extra_opset = [ utils . make_opsetid ( constants . contrib_ops_domain , <number> ) ] with tf . graph ( ) . as_default ( ) as tf_graph : tf . import_graph_def ( frozen_func . graph . as_graph_def ( ) , name = ' ' ) with tf_loader . tf_session ( graph = tf_graph ) : g = process_tf_graph ( tf_graph , input_names = input_names , output_names = output_names , extra_opset = extra_opset ) onnx_graph = optimize_graph ( g ) model_proto = onnx_graph . make_model ( "" converted "" ) utils . save_protobuf ( "" model2b . onnx "" , model_proto ) print ( "" conversion onnx complete ! "" ) ` ` ` i tried too python - m tf2onnx . convert - - saved - model c <annoyed> tmp / log_dirtocd / cedn_pb - - output c <annoyed> tmp / cedn . onnx - - opset <number> but results does not change . opset must be >= <number> my c + + code is ` ` ` dnn : : net my_net ; utils : : logging : : setloglevel ( utils : : logging : : loglevel : : log_level_verbose ) ; my_net = dnn : : readnet ( "" c <annoyed> tmp / cedn / model2b . onnx "" ); ` ` ` i run program and i ve got an execption : ` ` ` [ debug : <number> <user> . <number> ] global system . cpp : <number> cv : : details : : setfpdenormalsignorehint core : update fp mxcsr flags = 0x0 0 0 0 9 fe0 [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : onnximporter dnn / onnx : processing onnx model from file : c <annoyed> tmp / cedn / model2b . onnx [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : populatenet dnn / onnx : loading onnx v8 model produced by ' tf2onnx ' : <number> . <number> 8 f8d49 . number of nodes = <number> , initializers = <number> , inputs = <number> , outputs = <number> [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseoperatorset dnn / onnx : using non - standard onnx opset [ <number> <sad> domain = ' ai . onnx . ml ' version = <number> [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseoperatorset dnn / onnx : using non - standard onnx opset [ <number> <sad> domain = ' ai . onnx . contrib ' version = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseoperatorset dnn / onnx : onnx opset version = <number> [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseoperatorset dnn / onnx : unknown domain = ' ai . onnx . contrib ' version = <number> . no dispatch map , you may need to register ' custom ' layers . [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseoperatorset dnn / onnx : unknown domain = ' ai . onnx . ml ' version = <number> . no dispatch map , you may need to register ' custom ' layers . [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : populatenet dnn / onnx : graph simplified to <number> nodes global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' scales__146 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' roi__133 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' new_shape__187 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _9__12 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _8__33 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _7__32 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _6__30 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _5__26 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _4__6 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _42__22 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _41__17 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _40__11 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _3__42 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _39__27 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _38__31 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _37__29 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _36__25 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _35__21 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _34__16 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _33__10 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _32__47 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _31__24 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _30__46 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _2__19 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _29__9 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _28__39 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _27__37 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _26__20 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _25__15 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _24__35 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _23__8 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _22__41 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _21__38 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _20__36 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _1__43 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _19__14 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _18__45 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _17__23 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _16__44 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _15__28 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _14__40 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _13__18 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _12__7 ' ] shape =[ <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _11__34 ' ] shape =[ <number> <number> <number> <number> ] data_type = <number> global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumptensorproto dnn / onnx : initializer [ <number> as ' func / statefulpartitionedcall / input / _10__13 ' ] shape =[ <number> ] data_type = <number> [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : populatenet dnn / onnx : input [ <number> ] dim [ <number> ] = <unk__188> ( dynamic ) [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : populatenet dnn / onnx : input [ <number> as ' args_0 : <number> ' ] shape =[ <number> <number> <number> <number> ] [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumpvalueinfoproto dnn / onnx : output [ <number> ] dim [ <number> ] = <unk__189> ( dynamic ) [ debug : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : dumpvalueinfoproto dnn / onnx : output [ <number> as ' identity : <number> ' ] shape =[ <number> <number> <number> <number> ] [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ transpose ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv1a / biasadd__53 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv1a / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv1a / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv1b / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv1b / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ maxpool ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / mpconv1 / maxpool ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv2a / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv2a / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv2b / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv2b / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ maxpool ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / mpconv2 / maxpool ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3a / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3a / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3b / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3b / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3c / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv3c / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ maxpool ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / mpconv3 / maxpool ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4a / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4a / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4b / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4b / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4c / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv4c / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ maxpool ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / mpconv4 / maxpool ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5a / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5a / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5b / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5b / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5c / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv5c / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ maxpool ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / mpconv5 / maxpool ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv6 / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / conv6 / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ conv ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / deconv6 / biasadd ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ relu ] <sad> onnx_node ! statefulpartitionedcall / statefulpartitionedcall / model / deconv6 / relu ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : processing node with <number> inputs and <number> outputs : [ resize ] <sad> onnx_node ! resize__137 ) from domain = ' ai . onnx ' opencv ( <number> . <number> - dev ) error : assertion failed ( interp_mode ! = "" tf_half_pixel_for_nn "" ) in cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseresize , file c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp , line <number> [ error : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ resize ] <sad> onnx_node ! resize__137 ) from domain = ' ai . onnx ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode input [ <number> ] = ' statefulpartitionedcall / statefulpartitionedcall / model / deconv6 / relu : <number> ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode input [ <number> ] = ' roi__133 ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode input [ <number> ] = ' scales__146 ' [ info : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode output [ <number> ] = ' resize__137 : <number> ' opencv ( <number> . <number> - dev ) error : unspecified error ( > node [ <email> <sad> ( onnx_node ! resize__137 ) parse error : opencv ( <number> . <number> - dev ) c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error failed ) interp_mode ! = "" tf_half_pixel_for_nn "" in function ' cv : : dnn : : dnn4_v20221220 : : onnximporter : : parseresize ' > ) in cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode , file c :\\ lib \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp , line <number> ` ` ` # # # steps to reproduce all data can be downloaded [ tf2 model ] ( <url> [ onnx model ] ( <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"build java without ant # # # pull request readiness checklist enables a path of building java bindings without ant * able to build opencv jar and docs without ant ` ` ` - - java : - - ant : no - - jni : / usr / lib / jvm / default - java / include / usr / lib / jvm / default - java / include / linux / usr / lib / jvm / default - java / include - - java wrappers : yes - - java tests : no ` ` ` * possible to build opencv jar without ant but tests still require ant * * merge with * * : <url> notes use ` opencv_java_ignore_ant = <number> ` to force "" java "" flow for building java bindings - java tests still require apache ant - jar does not include ` . java ` source code files . see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"moved barcode from opencv_contrib merge with <url> # # # # # todo - [x ] documentation ( bib ) - [x ] tutorial ( references ) - [x ] sample app ( refactored ) - [x ] java ( test passes ) - [x ] python ( test passes ) - [x ] build without dnn ` ` ` allow_multiple_commits = <number> force_builders = custom build_image : docs = docs - js : <number> build_image : custom = javascript buildworker : custom = linux - <number> , linux - <number> ` ` `",1
opencv/opencv,"exposing cv_maketype ( depth , cn ) and cv_8uc ( n ) , cv_8sc ( n ) etc . , macros to the python binding for arbitrary channel number handling in cv2 . cuda . gpumat # # # describe the feature and motivation it would be great to expose the following macros to the python binding : ` ` ` c <hashtag> define </hashtag> cv_maketype ( depth , cn ) ( cv_mat_depth ( depth ) + ( ( ( cn ) - <number> ) < < cv_cn_shift ) ) <hashtag> define </hashtag> cv_8uc ( n ) cv_maketype ( cv_8u , ( n ) ) <hashtag> define </hashtag> cv_8sc ( n ) cv_maketype ( cv_8s , ( n ) ) <hashtag> define </hashtag> cv_16uc ( n ) cv_maketype ( cv_16u , ( n ) ) <hashtag> define </hashtag> cv_16sc ( n ) cv_maketype ( cv_16s , ( n ) ) <hashtag> define </hashtag> cv_32sc ( n ) cv_maketype ( cv_32s , ( n ) ) <hashtag> define </hashtag> cv_32fc ( n ) cv_maketype ( cv_32f , ( n ) ) <hashtag> define </hashtag> cv_64fc ( n ) cv_maketype ( cv_64f , ( n ) ) ` ` ` while they are not of much use for the general python interface ( which uses numpy arrays with ` shape ` and ` dtype ` attributes readily available ) , they can be useful for cuda ` gpumat ` handling . consider the following code to initialize a ` gpumat ` from cuda memory from a cupy array ( it uses ` creategpumatfromcudamemory ` from # <number> <sad> ` ` ` python import cv2 import cupy as cp def cv_cuda_gpumat_from_cp_array ( arr : cp . ndarray ) - > cv2 . cuda . gpumat : assert len ( arr . shape ) in ( <number> , <number> ) , "" cupy array must have <number> or <number> dimensions to be a valid gpumat "" type_map = { cp . dtype ( ' uint8 ' <sad> cv2 . cv_8u , cp . dtype ( ' int8 ' <sad> cv2 . cv_8s , cp . dtype ( ' uint16 ' <sad> cv2 . cv_16u , cp . dtype ( ' int16 ' <sad> cv2 . cv_16s , cp . dtype ( ' int32 ' <sad> cv2 . cv_32s , cp . dtype ( ' float32 ' <sad> cv2 . cv_32f , cp . dtype ( ' float64 ' <sad> cv2 . cv_64f } depth = type_map . get ( arr . dtype ) assert depth is not none , "" unsupported cupy array dtype "" channels = <number> if len ( arr . shape ) = = <number> else arr . shape [ <number> ] # equivalent to unexposed opencv c + + macro cv_maketype ( depth , channels ) ( depth & <number> ) + ( ( channels - <number> ) < < <number> ) mat_type = depth + ( ( channels - <number> ) < < <number> ) mat = cv2 . cuda . creategpumatfromcudamemory ( arr . __cuda_array_interface__ [ ' shape ' ] [ <number> : : - <number> ] , mat_type , arr . __cuda_array_interface__ [ ' data ' ] [ <number> ] ) return mat ` ` ` the ` mat_type = . <repeated> ` line would be more readable with ` cv_maketype ` available , e . g . , ` ` ` python mat_type = cv2 . cv_maketype ( depth , channels ) ` ` ` thanks in advance # # # additional context _no response_",1
opencv/opencv,"videoio / ffmpeg : increased packet read attempt limit , allow configuring it resolves # <number> related # <number> * use different counters for wrong packets recieved by demuxer and errors from decoder * allow modifying these counters via environment variables ` opencv_ffmpeg_read_attempts ` / ` opencv_ffmpeg_decode_attempts ` * added logging when reading breaks at one of error limits notes i have been able to reproduce original issue with a video file with <number> total streams ( video + audio + subtitles ) , at some point in the video only packets from the last stream are being sent by the demuxer , thus exceeding our limit . for my specific video total number of packets from wrong stream was about <number> . i have chosen <number> as default value . * default limit of decoding attempts is quite low , because i am not sure in which cases it can be exceeded ( network stream ? ) . i tried to read 8 k video from the disk , but it did not cause break at decode point .",1
opencv/opencv,"add avif support through libavif . this is to fix <url> extra # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add int64 , uint64 , bool , uint data type . since we only have three bits for the data type , this pr wants to expand the number of bits of the data type to <number> bits . at present , there are many aspects of this pr that have not been considered . any suggestions are welcome . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,unable to call lmsolver in python . # # # descripe the feature and motivation i want to solve a levenberg - marquardt problem with lmsolver or cvlevmarq in opencv . but i find i can not call lmsolver or cvlevmarq with python . are these two interfaces not available in python ? # # # additional context _no response_,1
opencv/opencv,meta ' s segment anything with dnn # # # descripe the feature and motivation a novel model with published trained weights <url> - [x ] export model to onnx . need to export carefully . see [ below ] ( <url> for details . - [x ] model can be imported with dnn . requires a fix for expand to support broadcast axis > <number> . - [x ] model can be inferred - [ ] compare accuracy with pytorch - [ ] create a sample # # # additional context _no response_,1
opencv/opencv,"newton pnp solver # # # newton - pnp : real - time visual navigation for autonomous toy - drones * * ibrahim jubran , fares fares , yuval alfassi , firas ayoub , dan feldman * * link for the article with information and proof of working <url> the perspective - n - point problem aims to estimate the relative pose between a calibrated monocular camera and a known 3 d model , by aligning pairs of 2 d captured image points to their corresponding 3 d points in the model . we suggest an algorithm that runs on weak iot devices in real - time but still provides provable theoretical guarantees for both running time and correctness . existing solvers provide only one of these requirements . our main motivation was to turn the popular dji ' s tello drone ( < 9 0 gr , < <money> ) into an autonomous drone that navigates in an indoor environment with no external human / laptop / sensor , by simply attaching a raspberry pi zero ( < 9 gr , < <money> ) to it . this tiny micro - processor takes as input a real - time video from a tiny rgb camera , and runs our pnp solver on - board . extensive experimental results , open source code , and a demonstration video are included . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"request emplace_back ( ) in class mat defined in mat . hpp # # # descripe the feature and motivation c + + <number> introduce emplace_back into stl , which can in - place constructe that can improve performance . i am trying to working on it ( add a similar one to ` mat ` ) but i am new to this project and i dont think i can do it very well , so i issued this issue . if anyone want to work on this , i may help something . # # # additional context _no response_",1
opencv/opencv,"add scrollwheel in cocoa # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,imshow速度太慢 ， 如何能加快imshow # # # descripe the feature and motivation imshow速度太慢 ， 如何能加快imshow # # # additional context _no response_,1
opencv/opencv,"opencv cannot import onnx model : inconsistent shape for concatlayer in function ' cv : : dnn : : concatlayerimpl : : getmemoryshapes ' # # # system information opencv = python opencv - python - rolling - <number> . <number> operating system / platform windows <number> <number> bit python = <number> . <number> # # # detailed description i converted the modnet - mobilenetv2 model from paddleseg <url> to onnx . opencv cannot load the model ( i do not get any errors or warnings from netron ) . find the offending model here : <url> ` ` ` [ error : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ concat ] <sad> onnx_nodep2o . concat . <number> ) from domain = ' ai . onnx ' traceback ( most recent call last ) : file "" d :\\ local \ \ devel \ \ python \ \ opencv \ \ dnn_matting_modnet - mobilenetv2 \ \ inference . py "" , line <number> , in <module> model = cv2 . dnn . readnet ( model_path ) cv2 . error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecified error ) in function ' cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode ' > node [ <email> <sad> ( onnx_node ! p2o . concat . <number> ) parse error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ layers \ \ concat_layer . cpp : <number> : error size of input array ) inconsistent shape for concatlayer in function ' cv : : dnn : : concatlayerimpl : : getmemoryshapes ' ` ` ` # # # steps to reproduce load model with the following code ` ` ` python model = cv2 . dnn . readnet ( model_path ) ` ` ` # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"dnn ( tflite ) : add 3 rdparty flatbuffers with pre - generated schema relates # <number> <url> ( apache <number> ) todo [x ] fix build issues ( <number> - bit and static libraries ) - [x ] fix tests - [x ] readme and licensing information <cut/> ` ` ` force_builders = custom , custom win , linux avx2 , linux opencl , linux , windows , docs , win32 , linux32 build_image : custom = ubuntu : <number> build_shared : custom = off build_examples : custom = off build_shared : custom win = off xbuild_shared : linux avx2 = off ` ` `",1
opencv/opencv,"videoio : add support for orbbec femto mega rgb - d camera # # # videoio support for orbbec femto mega rgb - d camera [ about femto mega ] ( <url> # # # new feature be able to open the depth and color stream of the orbbec femto mega camera # # # bug fix obsensor ： fix the problem that takes too long to turn off the stream on windows platform - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch os | compiler | camera | result - - | - - | - - | - - windows11 | ( vs2022 ) msvc17 . <number> | femto mega | pass ubuntu22 . <number> | gcc11 . <number> | femto mega | pass",1
opencv/opencv,"feat arguments handling in python interface named arguments handling back - port from [ pr # <number> ] ( <url> to <number> . x branch . different arguments and code generation logic is used for easier integration with pr # <number> . named arguments mimic normal arguments and reveal their nature only during function call inter - opt . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"opencv <number> . <number> huawei cann support error : cann graph check failed in function ‘ compilecanngraph ’ # # # system information opencv version : <number> . <number> operating system / platform : ubuntu <number> compiler & compiler version : gcc <number> . <number> # # # detailed description i ’ m testing huawei ascend cann hardware . the example code is as in [ huawei cann backend · opencv / opencv wiki · github ] ( <url> but the sample code result in error : ` ` ` [ error : <number> <user> . <number> ] global net_cann . cpp : <number> compilecanngraph cann graph check failed , ret = <phone> terminate called after throwing an instance of ‘ cv : : exception ’ what ( <sad> opencv ( <number> . <number> ) / usr / huaweitest1 / opencv_4 . <number> / opencv - <number> . <number> / modules / dnn / src / net_cann . cpp : <number> : error : ( - <number> : unspecified error ) cann graph check failed in function ‘ compilecanngraph ’ [ error : <number> <user> . <number> ] global net_cann . cpp : <number> compilecanngraph cann graph check failed , ret = <phone> terminate called after throwing an instance of ‘ cv : : exception ’ what ( <sad> opencv ( <number> . <number> ) / usr / huaweitest1 / opencv_4 . <number> / opencv - <number> . <number> / modules / dnn / src / net_cann . cpp : <number> : error : ( - <number> : unspecified error ) cann graph check failed in function ‘ compilecanngraph ’ [ error : <number> <user> . <number> ] global op_cann . cpp : <number> loadtodevice cann check failed , ret = <number> terminate called after throwing an instance of ‘ cv : : exception ’ what ( <sad> opencv ( <number> . <number> ) / usr / huaweitest1 / opencv_4 . <number> / opencv - <number> . <number> / modules / dnn / src / op_cann . cpp : <number> : error error ) cann check failed in function ‘ loadtodevice ’ aborted ( core dumped ) ` ` ` i checked opencv <number> and cann installation , seems fine . cann ' s own sample code could run as well . # # # steps to reproduce ` ` ` <hashtag> include </hashtag> <iostream> <hashtag> include </hashtag> <vector> <hashtag> include </hashtag> "" opencv2 / opencv . hpp "" void preprocess ( const cv : : mat & src , cv : : mat & dst ) { src . convertto ( dst , cv_32fc3 ) ; cv : : cvtcolor ( dst , dst , cv : : color_bgr2rgb ) ; / / center crop cv : : resize ( dst , dst , cv : : size ( <number> , <number> )); cv : : rect roi ( <number> , <number> , <number> , <number> ); dst = dst ( roi ) ; dst = cv : : dnn : : blobfromimage ( dst , <number> / <number> , cv : : size ( ) , cv : : scalar ( <number> , <number> , <number> )); cv : : divide ( dst , cv : : scalar ( <number> , <number> , <number> ) , dst ) ; } void softmax ( const cv : : mat & src , cv : : mat & dst , int axis = <number> ) { using namespace cv : : dnn ; layerparams lp ; net netsoftmax ; netsoftmax . addlayertoprev ( "" softmaxlayer "" , "" softmax "" , lp ) ; netsoftmax . setpreferablebackend ( dnn_backend_opencv ) ; netsoftmax . setinput ( src ) ; cv : : mat out = netsoftmax . forward ( ); out . copyto ( dst ) ; } int main ( int argc , char * * argv ) { using namespace cv ; mat image = imread ( "" / path / to / image "" ); / / replace with the path to your image mat input_blob ; preprocess ( image , input_blob ) ; dnn : : net net = dnn : : readnet ( "" / path / to / image_classification_ppresnet50_2022jan . onnx "" ); / / replace with the path to the model net . setpreferablebackend ( dnn : : dnn_backend_cann ) ; net . setpreferabletarget ( dnn : : dnn_target_npu ) ; net . setinput ( input_blob ) ; mat out = net . forward ( "" save_infer_model / scale_0 . tmp_0 "" ); mat prob ; softmax ( out , prob , <number> ); double min_val , max_val ; point min_loc , max_loc ; minmaxloc ( prob , & min_val , & max_val , & min_loc , & max_loc ) ; std : : cout < < cv : : format ( "" cls = % d , score = % . 4 f \ \ n "" , max_loc . x , max_val ) ; return <number> ; } ` ` ` the onnx model is as link <url> # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"cancel prints in roiselector . cpp # # # descripe the feature and motivation hello , is it possible to add a boolean argument to the cv2 . roiselector method to prevent the method to print the following messages a roi and then press space or enter button cancel the selection process by pressing c button ! thanks ! # # # additional context _no response_",1
opencv/opencv,"tflite models importer # # # pull request readiness checklist * * merge with : * * <url> resolves <url> # # # # how to build ( tested on ubuntu <number> ) ` ` ` git clone - b v23 . <number> <url> cmake - s flatbuffers - b fbs_build cmake - - build fbs_build - j4 cmake - - install fbs_build - - prefix fbs_install ` ` ` ` ` ` export flatbuffers_dir =$ home / fbs_install / lib / cmake / flatbuffers / cmake \ \ - dcmake_build_type = release \ \ - dbuild_list = ts , dnn , python3 \ \ - dwith_flatbuffers = on \ \ - s opencv - b opencv_build ` ` ` # # # # how to build for opencv . js emcmake python3 . / opencv / platforms / js / build_js . py build_wasm \ \ - - build_wasm \ \ - - cmake_option = "" - dwith_flatbuffers = on "" \ \ - - cmake_option = "" - dflatbuffers_dir =$ home / fbs_install / lib / cmake / flatbuffers / "" ` ` ` <cut/> see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake ` ` ` force_builders = linux opencl , linux avx2 buildworker : linux opencl = linux - <number> ` ` `",1
opencv/opencv,"opencv cannot import onnx model : can not create layer "" . <repeated> "" of type "" exp "" # # # system information opencv => python opencv - python <number> . <number> operating system / platform windows <number> <number> bit python => <date> # # # detailed description i have a darknet yolov4 model that i can use with cv : : dnn : : readnetfromdarknet . i converted the model to an onnx file using [ this ] ( <url> codebase . i load the onnx file with cv : : dnn : : readnetfromonnx and ran my application , but got the following error . ` ` ` [ error : <number> ] global c :\\ ocv_bri_build \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) cv : : dnn : : dnn4_v20201117 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ exp ] <sad> <number> ) traceback ( most recent call last ) : file "" run . py "" , line <number> , in <module> aias . run ( proj_config ) runtimeerror : opencv ( <number> . <number> ) c :\\ ocv_bri_build \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecified error ) in function ' cv : : dnn : : dnn4_v20201117 : : onnximporter : : handlenode ' > node [ exp ] <sad> <number> ) parse error : opencv ( <number> . <number> ) c :\\ ocv_bri_build \ \ opencv - <number> . <number> \ \ modules \ \ dnn \ \ src \ \ dnn . cpp : <number> : error error ) can not create layer "" <number> "" of type "" exp "" in function ' cv : : dnn : : dnn4_v20201117 : : layerdata : : getlayerinstance ' ` ` ` issue submission checklist - [ ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [ ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,the einsum layer will be supported in the future # # # descripe the feature and motivation [ error : <number> <user> . <number> ] global onnx_importer . cpp : <number> cv : : dnn : : dnn4_v20221220 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs from domain = ' ai . onnx # # # additional context _no response_,1
opencv/opencv,support graphcutseamfinder in stitching_detailed . py # # # descripe the feature and motivation with the release of [ opencv <number> ] ( <url> this [ pr ] ( <url> becomes available so we can add ` ` ` seam_find_choices [ ' gc_color ' ] = cv . detail_graphcutseamfinder ( ' cost_color ' ) seam_find_choices [ ' gc_colorgrad ' ] = cv . detail_graphcutseamfinder ( ' cost_color_grad ' ) ` ` ` to <url> previously it was [ removed ] ( <url> since python bindings were corrupt . thanks to <user> for fixing this . # # # additional context _no response_,1
opencv/opencv,"a redundant logic in ` distancetransform ` . # # version opencv <number> . <number> . # # issue <url> ` ` ` c + + if ( need_labels ) { … … masksize = cv_dist_mask_5 ; } ` ` ` the ` masksize ` is set to ` cv_dist_mask_5 ` ` if ( need_labels ) ` . meaning , the follow - up code ` ` ` c + + if ( disttype = = cv_dist_c || disttype = = cv_dist_l1 ) masksize = need_labels ? cv_dist_mask_3 else if ( disttype = = cv_dist_l2 & & need_labels ) masksize = cv_dist_mask_5 ; ` ` ` does the same as ` ` ` c + + if ( ( disttype = = cv_dist_c || disttype = = cv_dist_l1 ) & & ! need_labels ) masksize = cv_dist_mask_3 ; ` ` ` . - - - - - - the redundant code should be removed . - _the ` masksize ` being set to ` cv_dist_mask_5 ` ` if ( need_labels ) ` , no matter what ` masksize ` is provided_ , is undocumented behavior , and should be properly documented .",1
opencv/opencv,"replace all instances of cuda texture references with texture objects see <url> and <url> cuda texture references have now been completley removed in the latest sdk ( cuda <number> ) . if <url> is merged this code block which is causing builds to fail against cuda <number> ( <url> will become redundant . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"vcpkg feature for intel ' s inference engine # # # descripe the feature and motivation hi i think actually there is no "" feature "" to build opencv with intel ' s inference engine support ( openvino , i guess ) . i do not know if is it there any fast solution from compile opencv from source code with the - dwith_openvino = on flag and that stuff . so i am requesting if it is possible to have a “ feature ” like the others for cuda support , dnn modules , contrib , etc . but for the intel ' s inference engine , thank you so much ! # # # additional context _no response_",1
opencv/opencv,"videoio orbbec gemini <number> and astra <number> camera support # # # pull request readiness checklist - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] the feature is well documented and sample code can be built with the project cmake # # # test result | os | compiler | camera | result | | - - - - - | - - - - - - - - - - - | - - - - - - - - - | - - - - - - - - | | windows11 | ( vs2022 ) msvc17 . <number> | orbbec gemini <number> | pass | | windows11 | ( vs2022 ) msvc17 . <number> | orbbec astra <number> | pass | | ubuntu22 . <number> | gcc9 . <number> | orbbec gemini <number> | pass | | ubuntu22 . <number> | gcc9 . <number> | orbbec astra <number> | pass |",1
opencv/opencv,"core ( logger ) opencv ' s modules base path - strip path from logger , keep file name only . - reverted ` cv_error ( ) ` changes .",1
opencv/opencv,"dnn : support onnx slice with negative steps by adding and using cv : : flipnd fixes <url> merge with <url> current workaround for negative steps is computing the forward indexing range for slice and flip at axis with negative step x of shape [ <number> , <number> ] , x[ <number> : <number> : - <number> , <number> : <number> : - <number> ] <=> np . flip ( x [ <number> : <number> : <number> , <time> : <number> ] , aixs =( <number> , <number> ) ) we can also extend ` range ` to support backward indexing and steps like python ` list ` , which needs more effort to do so i think . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"use qr code alignment markers merge with <url> added detect qr code alignment marker ( <number> marker for <number> - <number> versions , <number> markers for <number> - <number> versions ) . alignment marker uses to find the correct position of the fourth corner . to check the correctness , a benchmark [ was launched ] ( <url> [ qr . py . txt ] ( <url> - + <percent> total decode ( ~ + <percent> against detected qr codes ) - + <number> pixels total accuracy for corner detect logging from benchmark [ metric_corner_dist_new . txt ] ( <url> no decode regression now . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"depth encoding ffmpeg 1 6 bit file support # # # descripe the feature and motivation depth images typically come as type uint16 . i have been looking online , and there is pretty lackluster support for depth video compression , which i think is an extremely important problem . for example , i am a part of a group collecting a large robot dataset , and we need to compress our depth data . some existing approaches include using : <number> ) ffv1 , but opencv ffmpeg does not support 1 6 bit datatypes as of now <number> ) converting depth images to color , and then encoding normally it would be super helpful if cv2 could provide some built in functionality for this <happy> # # # additional context _no response_",1
opencv/opencv,"now there is no way to control gstreamer drop timeout when ip - camera drops # # # descripe the feature and motivation i am using opencv with gstreamer , and sometimes my camera can disconnect . and when its happened , i just have program freezing for <number> s . and ( maybe i did not find ) there is no way to change that timeout . similar thing - is cap_prop_read_timeout_msec for ffmpeg , but it didnt work for gstreamer . # # # additional context _no response_",1
opencv/opencv,"batched nmsboxes # # # descripe the feature and motivation batched nms is especially useful for modern generic object detection postprocessing . generic object detection has multiple classes , and it is important to perform nms on each class to give correct results . however , the existing api for nms in dnn module ` nmsboxes ` does not support multiple classes . we can use the same strategy from <url> to implement batched nms get the maximum coordinate from boxes , <number> . calculate the offset based on the maximum coordinate and class ids , <number> . add offsets to boxes , <number> . run ` nmsboxes ` with boxes with offsets , scores and thresholds . although this is quite simple to implement for experienced developers , it would still be better if we put it in the dnn module as a new api ( let us call it ` batchednmsboxes ` ) , and it can be done by calling a single api instead of wasting time implementing every time we need it . # # # additional context _no response_",1
opencv/opencv,"dnn matmul support 3 d or 4 d with broadcast * * merge with extra : * * <url> this pr follows the <url> the main purpose of this pr is making ` matmul ` support the broadcast that the second input has less dimention than the first one . and let the operation support simd and multi - thread . beacuse it does not support 1 d mat , only support matmul like ` ` ` 2 x3 x4 mul 4 x5 - > 2 x3 x5 2 x3 x4x5 mul 3 x5x6 - > 2 x3 x4x6 ` ` ` - * * <number> const inputs : * * create a virtual layer for the first input - * * <number> const input with cpu ( whether or not using broadcast ) : * * use the simd and multi - thread flow which for ` innerproduct ` - * * <number> const input with cuda : * * broadcast inputs will fallback to cpu . inputs with the same shape will use the cuda . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"opencv cannot import onnx model : ( bool ) layer in function ' cv : : dnn : : dnn4_v20220524 : : runlayer ' # # # system information opencv => python opencv - python - rolling <number> . <number> operating system / platform windows <number> <number> bit python => <number> . <number> # # # detailed description i converted the decoupledseg from paddleseg to onnx ( the onnx file opens fine in netron ) : <url> this fails to load in opencv <number> pre - release : ` ` ` [ error : <number> <user> . <number> ] global d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) cv : : dnn : : dnn4_v20220524 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ tile ] <sad> onnx_nodep2o . tile . <number> ) from domain = ' ai . onnx ' traceback ( most recent call last ) : file "" d :\\ local \ \ devel \ \ python \ \ opencv \ \ dnn_segmentation_paddle_emanet_voc \ \ inference . py "" , line <number> , in <module> model = cv2 . dnn . readnet ( model_path ) cv2 . error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecified error ) in function ' cv : : dnn : : dnn4_v20220524 : : onnximporter : : handlenode ' > node [ <email> <sad> ( onnx_node ! p2o . tile . <number> ) parse error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : assertion failed ) ( bool ) layer in function ' cv : : dnn : : dnn4_v20220524 : : runlayer ' ` ` ` # # # steps to reproduce ` ` ` model = cv2 . dnn . readnet ( model_path ) ` ` ` find the model here # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"core : support cv_check *() macros with ' bool ' parameters added support for : - ` cv_check { eq , ne } ( bool , bool , msg ) ` ( lt , ge , etc variants does not makes sense with ` bool ` and should not be used ) - ` cv_checktrue ( bool_expr_must_be_true , msg ) ` - ` cv_checkfalse ( bool_expr_must_be_false , msg ) ` also eliminates related build warning ` ` ` force_builders = linux32 build_image : custom win = msvs2019 ` ` `",1
opencv/opencv,"support nanotrack in video module [ teset data in opencv_extra ] ( <url> nanotrack is an extremely lightweight and fast object - tracking model . the total size is * * <number> mb * * . and the fps on m1 chip is * * <number> * * , on raspberry pi <number> is about * * <number> * * . ( float32 cpu only ) with this model , many users can run object tracking on the edge device . the author of nanotrack is <user> . the original repo is <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add support for cap_prop_orientation_auto to avfoundation backend this is an extension to # <number> . i pulled some of the code up , so now it would be somewhat easier to add support for orientation metadata into the other backends as well . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"opencv cannot import onnx model : can not create layer "" . <repeated> "" of type "" tile "" # # # system information opencv => python opencv - python - rolling <number> . <number> operating system / platform windows <number> <number> bit python => <number> . <number> # # # detailed description i converted the danets from paddleseg to onnx : <url> this fails to load in opencv <number> pre - release : ` ` ` [ error : <number> <user> . <number> ] global d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp ( <number> ) cv : : dnn : : dnn4_v20220524 : : onnximporter : : handlenode dnn / onnx : error during processing node with <number> inputs and <number> outputs : [ tile ] <sad> onnx_nodep2o . tile . <number> ) from domain = ' ai . onnx ' traceback ( most recent call last ) : file "" d :\\ local \ \ devel \ \ python \ \ opencv \ \ dnn_segmentation_paddle_danet_resnet50_os8_voc12aug \ \ inference . py "" , line <number> , in <module> model = cv2 . dnn . readnet ( model_path ) cv2 . error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecified error ) in function ' cv : : dnn : : dnn4_v20220524 : : onnximporter : : handlenode ' > node [ <email> <sad> ( onnx_node ! p2o . tile . <number> ) parse error : opencv ( <number> . <number> - dev ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ net_impl . hpp : <number> : error : ( - <number> : unspecified error ) can not create layer "" onnx_node ! p2o . tile . <number> "" of type "" tile "" in function ' cv : : dnn : : dnn4_v20220524 : : net : : impl : : getlayerinstance ' ` ` ` # # # steps to reproduce ` ` ` model = cv2 . dnn . readnet ( model_path ) ` ` ` find one of the models here # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files ( videos , images , onnx , etc )",1
opencv/opencv,"create _inputarray / _outputarray from fixed size array # # # descripe the feature and motivation i think we could add an extra constructor to ` _inputarray ` / ` _outputarray ` so that it can convert directly from a c + + fixed size array , like this : ` ` ` c + + template < typename _tp , std : : size_t _nm > _inputarray ( const _tp ( & vec ) [ _nm ] ); ` ` ` traditionally , we need to explicitly convert it to a combination of pointer + size , like : ` ` ` c + + auto arr = _inputarray ( vec , std : : size ( vec ) ); ` ` ` it would be more convenient to have something like auto arr = _inputarray ( vec ) ; ` ` ` # # # additional context _no response_",1
opencv/opencv,"cv . lut is missing in opencv <number> . <number> # # # descripe the feature and motivation hi , thank - you for this awesome js library . there was a use case which involved useing cv . lut however it is not available as of now . this function is available in opencv c + + and python versions . requesting you to please add this function in opencv js library with a proper documentation which will help developers like me kind regards , aditya # # # additional context _no response_",1
opencv/opencv,"stitching_detailed . py : it can not be spliced well , request help # # # descripe the feature and motivation stitching_detailed . py can not be spliced well , request help ` ` ` direction transverse <number> . jpg <number> * <number> <number> . jpg <number> * <number> <number> . jpg <number> * <number> ` ` ` i need to splice , but i can not complete the requirements very well . i need help pictures can be sent to email # # # additional context _no response_",1
opencv/opencv,pass the avdictionary generated from opencv_ffmpeg_capture_options and opencv_ffmpeg_writer_options to avformat_find_stream_info as well to gain access to more ffmpeg options # # # describe the feature and motivation while working on <url> i realized that the environment variables ` ` ` opencv_ffmpeg_capture_options ` ` ` and ` ` ` opencv_ffmpeg_writer_options ` ` ` can only configure a very small subset of the ffmpeg options ( the options listed in <url> that prevented me from passing all necessary options to fully specify the stream ( <url> anyway i suggest passing the avdictionary not just to ` ` ` avformat_open_input ` ` ` but to ` ` ` avformat_find_stream_info ` ` ` as well <url> and maybe even to ` ` ` avcodec_open2 ` ` ` <url> # # # additional context _no response_,1
opencv/opencv,print a debug message if the environment variables opencv_ffmpeg_capture_options and opencv_ffmpeg_writer_options are set # # # descripe the feature and motivation the environment variables ` ` ` opencv_ffmpeg_capture_options ` ` ` and ` ` ` opencv_ffmpeg_writer_options ` ` ` potentially have great impact on the internals of the ffmpeg video backend . but when they are used the log files do not show it . i ' d like to print a debug message containing the value of those variables ( in case they are set ) . # # # additional context this issue <url> contains examples of how those variables can be used .,1
opencv/opencv,"dnn : let quant and dequant of onnx_importer support the constant input . merge with extra for zeropoint and scale can be <number> - d tensor , only exists in ` quant ` , ` deaunt ` and weight of ` qlinearconv ` . we have supported <number> - d tensor case in ` qlinearconv ` . for ` quant ` and ` deaunt ` , we can support the constant input case , which is very common situation in per - channel qdq quantized format . model examples can be found at pr of extra . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn : more stable db text detection api * * merge with extra * * : <url> the followings are the purpose of the pr let ` blobfromimage ` support ` scalar ` scale to feed the requirement of standard deviation value . <number> . more stable db text detection api , the old post - processing will get assert errors when ` length = = <number> . ; ` . [ the related code part ] ( <url> <number> . output the correct confidence or score instead of always being * * <number> . * * . <number> . support pp - ocr - v2 and pp - ocr - v3 db models . the original inference code can be found [ here ] ( <url> [ related regression test model ] ( <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add cvgetpropvisible_cocoa naive fix for the # <number> issue . tested ' not visible ' behaviour via ` cv : : waitkey ( <number> ); ` and minimising the window during that wait . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add bindings to cuda photo denoising functions python bindings were missing from the cuda photo denoising functions . the function signatures have been updated and python test added to verify their existence . are these supposed to be in the main repository ? # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn : supports scatter and scatternd from onnx fixes # <number> merge with : <url> checklist : - [x ] scatter impl - [x ] scatter impl for negative indices - [x ] test data for scatter - [x ] scatternd impl - [x ] scatternd impl for negative indices - [x ] test data for scatternd - [x ] disable the perf test to save ci server resources before merge onnx operator doc : <url> # # benchmark benchmark was done on m1 macbook air ( 1 6 g mem ) . time is in milliseconds . * : shapes of input , indices , updates are all [ <number> , <number> , <number> , <number> ] . | version | operation | mean | median | min | | - | - | - | - | - | | intial | scatter | <number> | <number> | <number> | | initial | scatter - add | <number> | <number> | <number> | | + passing_mat | scatter | <number> | <number> | <number> | | + passing_mat | scatter - add | <number> | <number> | <number> | | current impl : + optm + pm | scatter | <number> | <number> | <number> | | current impl : + optm + pm | scatter - add | <number> | <number> | <number> | * : shapes of input , indices , updates are [ <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> , <number> ] , [ <number> , <number> , <number> , <number> ] respectively . | version | operation | mean | median | min | | - | - | - | - | - | | intial | scatternd | <number> | <number> | <number> | | initial | scatternd - add | <number> | <number> | <number> | | + passing_mat | scatternd | <number> | <number> | <number> | | + passing_mat | scatternd - add | <number> | <number> | <number> | | + pm + ng index | scatternd | <number> | <number> | <number> | | + pm + ng index | scatternd - add | <number> | <number> | <number> | | + optm1 + pm + ng index | scatternd | <number> | <number> | <number> | | + optm1 + pm + ng index | scatternd - add | <number> | <number> | <number> | | current impl : + optm2 + optm1 + pm + ng index | scatternd | <number> | <number> | <number> | | current impl : + optm2 + optm1 + pm + ng index | scatternd - add | <number> | <number> | <number> | # # potential issue scatternd should support duplicate indices if reduction is not none . also scatternd permits negative indices . see [ here ] ( <url> for more details . * * notice <emphasis> * * impl does not check for duplicate indices if reduction is none . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"color conversion : rgb => nv12 / nv21 add color conversion for rgb / rgba / bgr / bgra to nv12 / nv21 . # # # # # system information ( version ) - opencv => <number> . x - operating system / platform => all # # # # # detailed description there are conversions from nv12 / nv21 to rgb / rgba / bgr / bgra / gray but not the reverse . this can be useful if we need to pass the an image to a video encoder that expect that . <url> # # # # # steps to reproduce # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [ ] there is reproducer code and related data files images , onnx , etc",1
opencv/opencv,"add h264 / h265 writter support for android # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"g - api expose all core operations to python # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake # # # motivation python bindings are available since a long time , but only a few operations ( ` operation : : on ( . <repeated> ) ` wrappers ) were exposed . there was a great plan to implement feature in python parser that could automatically detect ` g - api ` operation ( via ` gapi_typed_kernel ` macro or so ) and expose it into python , but this functionality is more about giving an opportunity to user to implement python ` kernels ` for already existing in g - api operations . this pr is going to expose just ` operation : : on ` wrappers to python in order to give user everything that is available from c + + , because now , for developers who do not build from source and change the code only available a small amount of functionality . since a lot of developers use ` opencv ` from ` pip ` let us expose it once and forever . great example of g - api usage : <url> todo list [x ] expose ` core ` - [ ] expose ` imgproc ` - [ ] expose ` video ` - [ ] expose ` stereo ` - [ ] other stuff ( e . g constant initialization for ` g - type ` ' s & some compiler args )",1
opencv/opencv,add support for v4l2_pix_fmt_y16_be opencv currently only has support for ` v4l2_pix_fmt_y16 ` but not for the big endian version of this format would it be possible to add it to [ ` cap_v4l . cpp ` ] ( <url> the handling should be a lot similar to the little endian version of the format . but can opencv handle big endian data ? or are there any functions to swap the endianess ?,1
opencv/opencv,"<number> - bit_palette_color # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn silu activation in darknet related issue # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add multiview calibration [ gsoc <number> ] # # # pull request readiness checklist - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake the usage tutorial is on google docs following this link",1
opencv/opencv,"[ gsoc ] add more universal intrinsic implementations for rvv . this is a patch of my gsoc project that the goal is to make the existing universal intrinsic compatible with scalable ( variable - length ) backends . in # <number> , we have already introduce a new framework of universal intrinsic for risc - v vector backend and few implementations and test cases are also added . in this patch , we are going to add more universal intrinsic implementations for rvv . tested with qemu for rvv backend in various vlen qemu - riscv64 - cpu rv64 , x - v = true , vlen = <number> . / bin / opencv_test_core - - gtest_filter = "" hal * "" qemu - riscv64 - cpu rv64 , x - v = true , vlen = <number> . / bin / opencv_test_core - - gtest_filter = "" hal * "" ` ` ` also tested for avx and sse backend on linux . # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn qgemm and squeeze op13 supported on onnximporter [ squeeze node of onnx ] ( <url> has moved the axes from attribute to input . [ test data ] ( <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"videocapture ffmpeg rtsp low fps fixes when using ` videocapture ` with the ffmpeg backend to stream from an rtsp source at a low frame rate on a machine with a large number of cpu cores , ` videocapture : : read / grab ` can fail due to the interrupt timer exceeding ` timeout_after_ms ` . to try and fix this , this pr adds the ` cap_prop_n_threads ` which can be set on open to reduce the number of threads in this case . this is related to <url> but i cannot tell if it fixes the issue as i am unable to re - create it on a machine with reading from a file with only <number> cpu cores . in addition to this ` videocapture : : open ` can get stuck and therefore interupted on the call to ` err = avformat_find_stream_info ( ic , null ) ; ` when streaming from an rtsp source at a low frame rate regardless of the number of cpu cores . the call to ` open ` still succeeds and the frame can be read but because the flag ` interrupt_metadata . timeout ` has been set in ` open ` the call to ` grab / read ` fails . to fix this the ` interrupt_metadata . timeout ` flag is reset on entry to ` grab / read ` . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"v4l2 multi - planar capture support devices which only support multi - planar capture cannot be processed as single - planar . add multi - planar support to v4l driver . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn layer ( add dynamic batch and reducesum support ) related reducesum issue # <number> related dynamic batch of reduce layer issue # <number> in this pr , we supported two input of ` reducesum layer ` and dynamic batch size in ` reduce layer ` of ` onnx_importer . cpp ` . [ regression test ] ( <url> the [ pervious pr ] ( <url> on dynamic batch of reduce layer has been closed . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"videoio : add support for obsensor ( orbbec rgb - d camera ) # videocapture for obsensor obsensor is orbbec ' s new generation rgb - d camera brand , based on uvc protocol . the purpose is to let opencv read rgb and depth data from orbbec rgb - d camera via uvc protocol , rather than relying on the 3 rdparty library ( like openni ) . this will greatly facilitate users to directly read , use and process the depth camera in opencv . supported orbbec device * orbbec astra + : [ detail ] ( <url> * depth stream ( 6 4 0 x480x30 fps cv_16uc1 ) * rgb stream ( 6 4 0 x480x30fps bgr ) * orbbec femto : [ detail ] ( <url> * depth stream ( 6 4 0 x480x30 fps cv_16uc1 ) * rgb stream ( 6 4 0 x480x30fps bgr ) * orbbec future device : * . <repeated> supported os : windows and linux . ( macox will be supported in near future . ) supported hw : x86 , x64 , arm . since the new orbbec devices rely on uvc , while old devices still rely on openni . this will cause obsensor does not support some old orbbec cameras , and the following is a list of specific unsupported devices orbbec astra - orbbec astra pro for these unspported devices , we still need the support of ` openni ` , please refer to the [ detailed tutorial of using orbbec astra 3 d cameras in opencv ] ( <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] the feature is well documented and sample code can be built with the project cmake - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch",1
opencv/opencv,"cuda : add support for orin gpu # # # pull request readiness checklist reference see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add zoom factor to interactive calibration tool can be useful when the screen is smaller than the image from camera . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"hogdescriptor resolves <url> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"highgui wayland xdg_shell - enable using - dwith_wayland = on - adapted from <url> - using stable protocol - overrides have_qt if have_wayland and with_wayland are set signed - off - by winarske < <email> > # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake <user>",1
opencv/opencv,"save frames option for interactive calibration tool the option to save all frames that contribute to final calibration result . useful for dataset collection and further offline tuning . # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"imgcodecs : jpeg imwrite_jpeg_sampling_factor parameter fix <url> this merge request contains sample and test program . # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake ` ` ` force_builders = custom build_image : custom = centos : <number> buildworker : custom = linux - f1 ` ` `",1
opencv/opencv,"consider video meta on gstreamer video capture some gstreamer elements may produce buffers with very non standard strides , offsets and / or even transport each plane in different , non - contiguous pointers . this non - standard layout is communicated via gstvideometa structures attached to the buffers . given this , when a gstvideometa is available , one should parse the layout from it instead of generating a generic one from the caps . the gstvideoframe utility does precisely this the buffer contains a video meta , it uses that to fill the format and memory layout . if there is no meta available , the layout is inferred from the caps . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"opencv fails to import onnx model # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => windows <number> <number> bit - compiler => python <number> . <number> - installed packages : > numpy <number> . <number> > opencv - contrib - python <number> . <number> > pip <number> . <number> > setuptools <number> . <number> # # # # # detailed description when running the demo . py for [ opencv_zoo ] ( <url> the models fails to import . all the other models i tried to import did work well . i wrote a python script in a different venv to check the model with onnx and infer with the model with onnxruntime . the model imports and executes fine . this is not a problem with the models , but with the importer . error message : ` ` ` [ error : <number> <user> . <number> ] global d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ o nnx \ \ onnx_importer . cpp ( <number> ) cv : : dnn : : dnn4_v20211220 : : onnximporter : : handlenode dn n / onnx : error during processing node with <number> inputs and <number> outputs : [ clip ] <sad> <number> ) f rom domain = ' ai . onnx ' traceback ( most recent call last ) : file "" d :\\ local \ \ devel \ \ python \ \ opencv \ \ image_classification_mobilenet \ \ demo . py "" , li ne <number> , in <module> ' v2 ' : mobilenetv2 ( modelpath ='. / image_classification_mobilenetv2_2022apr . onnx ' , labelpath = args . label , backendid = args . backend , targetid = args . target ) , file "" d :\\ local \ \ devel \ \ python \ \ opencv \ \ image_classification_mobilenet \ \ mobilenet_v2 . py "" , line <number> , in __init__ self . model = cv . dnn . readnet ( self . model_path ) cv2 . error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecified error ) in function ' cv : : dnn : : dnn4_v20211220 : : onnximporter : : handlenode ' > node [ <email> <sad> ( <number> ) parse error : opencv ( <number> . <number> ) d :\\ a \ \ opencv - python \ \ opencv - python \ \ opencv \ \ modules \ \ dnn \ \ src \ \ onnx \ \ onnx_importer . cpp : <number> : error : ( - <number> : unspecifi ed error ) in function ' void __cdecl cv : : dnn : : dnn4_v20211220 : : onnximporter : : parse clip ( class cv : : dnn : : dnn4_v20211220 : : layerparams & , const class opencv_onnx : : nodep roto & ) ' > > ( expected : ' node_proto . input_size ( ) = = <number> ' ) , where > > ' node_proto . input_size ( ) ' is <number> > > must be equal to > > ' <number> ' is <number> > ` ` ` # # # # # steps to reproduce - reproducible : yes - enforceable : yes run the provided demo . py in [ opencv_zoo ] ( <url> # # # # # issue submission checklist - [x ] i report the issue , it ' s not a question - [x ] i checked the problem with documentation , faq , open issues , forum . opencv . org , stack overflow , etc and have not found any solution - [x ] i updated to the latest opencv version and the issue is still there - [x ] there is reproducer code and related data files images , onnx , etc",1
opencv/opencv,"support use memory buffer as input to read multi - page image # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"added blob contours to blob detector this pr adds an option to collect blob contours during blob detection ( simpleblobdetector ) . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"reimplementation of element - wise layers with broadcasting support this version supports element - wise n - ary operations on k - dimensional tensors with broadcast - compatible shapes . = = = * * update by * * <user> = = = onnx operators that needs broadcasting : <url> benchmarks on apple m1 : input tensor a of shape [ <number> , <number> , <number> , <number> ] , input tensor b of shape [ <number> , <number> , <number> , <number> ] | operation | eltwise layers ( ms ) | this pr ( ms ) | | - - - - - - - - - | - - - - - - - - - - - - - - - - - - - | - - - - - - - - - - - - - - - | | add | not supported | <number> | | and | not supported | - | | div | <number> | * * <number> * * | | equal | not supported | <number> | | greater | not supported | <number> | | less | not supported | <number> | | max | <number> | * * <number> * * ( nary ) | | mean | not supported | <number> ( nary ) | | min | <number> | * * <number> * * ( nary ) | | mul | <number> | * * <number> * * | | or | not supported | - | | pow | not supported | <number> | | sub | not supported | <number> | | sum | <number> | * * <number> * * ( nary ) | | xor | not supported | - | = = = * * end of updates * * = = = benchmarks : sum of two tensors with shape =( <number> , <number> ) eltwise layer takes 2 4 ms this implementation takes 4 9 ms note does not speed up anything right now , so it ' ll probably get removed . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"dnn sign , shrink and reciprocal for onnx_impoter add ` sign ` , ` shrink ` and ` reciprocal ` for ` onnx_importer . cpp ` . implemented with opencv , cuda , and opencl backend . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"python binding for matches and inliers_mask attributes of cv2 . detail_matchesinfo class in respect to the issue # <number> , the pull request makes * matches <emphasis> * and * inliers_mask <emphasis> * attributes of the * * cv2 . detail_matchesinfo * * class accessible from python interface . by this way , developers could be able to make changes on the computed matches . # # # pull request readiness checklist - [ yes ] i agree to contribute to the project under apache <number> license . - [ yes ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ yes ] the pr is proposed to the proper branch - [ yes ] there is a reference to the original bug report and related work",1
opencv/opencv,"videoio : initial ffmpeg <number> support resolves # <number> closes # <number> <cut/> _ffmpeg <user> . 0 _ ( debug ) ` ` ` . sh . / configure \ \ - - enable - shared - - disable - static - - enable - pic - - enable - gpl - - enable - nonfree \ \ - - enable - libxvid - - enable - libx264 - - enable - swresample \ \ - - prefix <annoyed> home / user / ffmpeg / install \ \ - - enable - debug = <number> - - disable - stripping - - extra - cflags = "" - gstabs + - og - g - fno - inline - fno - omit - frame - pointer "" - - disable - optimizations ` ` ` todo [x ] check hw acceleration support ( va ) * [x ] check and fix compatibility with older ffmpeg versions ( up to <number> [ releases ] ( <url> * [ ] ~ backport to opencv <number> ( ? ) ~ ` ` ` build_contrib : custom = off build_contrib : custom win = off build_shared : custom = off build_examples : custom = off build_image : linux avx2 = ubuntu : <number> xbuild_image : win64 = msvs2019 force_builders = custom , custom win , linux avx2 xbuild_image : custom = centos : <number> xbuild_image : custom = ubuntu : <number> build_image : custom = gstreamer : <number> buildworker : custom = linux - <number> , linux - <number> , linux - <number> test_opencl : custom = on build_image : custom win = ffmpeg xbuild_image : custom win = ffmpeg - plugin buildworker : custom win = windows - <number> test_modules : custom win = videoio test_opencl : custom win = on ` ` `",1
opencv/opencv,"add gather implementation * * merge with extra : * * <url> should resolve # <number> , resolve # <number> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add n - dimensional transpose to core # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"tiffdecoder does not support <number> bits ( integer ) opencv <number> as of opencv <number> , the tiffdecoder does not support <number> bits ( integer ) . it looks like a deliberate choice since decoding <number> and <number> are supported with optimized icv conversion functions . would it be accepted to add a "" slow "" unpacking step for non - standard bit depths ? there are at least two problems would be logical to automatically scale the resulting cv : : mat ( here by <number> < < ( <number> - <number> ) ) , but the caller would not be aware that such a conversion was done - tiff is so complex that it ' s hard to ensure that all and every packing scheme is supported",1
opencv/opencv,"g - api preprocessing giebackend integration integration of vpp preprocessing into giebackend . this pr had done in term of <url> pr . inferroi is supported only # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake # # # build configuration ` ` ` force_builders = xcustom , custom win , custom mac build_gapi_standalone : linux x64 = ade - <number> . 1 f build_gapi_standalone : win64 = ade - <number> . 1 f build_gapi_standalone : mac = ade - <number> . 1 f build_gapi_standalone : linux x64 debug = ade - <number> . 1 f xbuild_image : custom = centos : <number> xbuildworker : custom = linux - <number> build_gapi_standalone : custom = ade - <number> . 1 f build_image : custom = ubuntu - openvino - <number> . <time> . <number> xbuild_image : custom win = openvino - <number> . <number> build_image : custom mac = openvino - <number> . <number> test_modules : custom = gapi , python2 , python3 , java test_modules : custom win = gapi , python2 , python3 , java test_modules : custom mac = gapi , python2 , python3 , java buildworker : custom = linux - <number> test_opencl : custom = off test_bigdata : custom = <number> test_filter : custom =* build_image : custom win = gapi - onevpl - <number> . <number> buildworker : custom win = windows - <number> build_contrib : custom win = off ` ` `",1
opencv/opencv,"g - api : added reshape ( ) functionality to cpu backend # # # summary * added reshape functionality to cpu backend * fixed fluid heterogeneous reshape # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake # # # build configuration ` ` ` force_builders = custom , custom win , custom mac build_gapi_standalone : linux x64 = ade - <number> . 1 f build_gapi_standalone : win64 = ade - <number> . 1 f build_gapi_standalone : mac = ade - <number> . 1 f build_gapi_standalone : linux x64 debug = ade - <number> . 1 f xbuild_image : custom = centos : <number> xbuildworker : custom = linux - <number> build_gapi_standalone : custom = ade - <number> . 1 f build_image : custom = ubuntu - openvino - <number> . <time> . <number> build_image : custom win = openvino - <number> . <number> build_image : custom mac = openvino - <number> . <number> test_modules : custom = gapi , python2 , python3 , java test_modules : custom win = gapi , python2 , python3 , java test_modules : custom mac = gapi , python2 , python3 , java buildworker : custom = linux - <number> # disabled due high memory usage test_opencl : custom = off test_bigdata : custom = <number> test_filter : custom =* ` ` `",1
opencv/opencv,"added neon support in builds for windows on arm since ~ visual studio <number> ~ visual studio <number> ( <number> ) , the data types ( e . g . ` int32x4_t ` ) used by arm neon instructions are now defined , so the blocking of neon support in builds for windows on arm has been resolved . - ` _arm64_distinct_neon_types ` preprocessor - [ arm64 vector intrinsics typedefs float32x4_t and int32x4_t to the same type - visual studio feedback ] ( <url> - # <number> todo [x ] (* alalek <emphasis> <wink> fix ` build_samples = on ` mode * ` find_package ( opencl ) ` founds opencl from cuda sdk which can not work with arm64 cross - compialtion * added ` - dwith_opencl = off - dcmake_disable_find_package_opencl = on ` - [x ] (* alalek <emphasis> <wink> configure msvs <number> build image <cut/> # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [ ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake ` ` ` force_builders = custom win xbuild_image : custom win = msvs2019 - arm64 build_image : custom win = msvs2022 - arm64 buildworker : custom win = windows - <number> build_contrib : custom win = off xbuild_examples : custom win = off ` ` `",1
opencv/opencv,"issues <number> the patch will try to use <number> bits / channel to generate <number> bits / channel panoramas . <cut/> example of input files : [ <number> ] ( <url> ! [ <number> ] ( <url> example of the panorama using the <number> . <number> ( from standard linuxmint packages ) : ! [ panorama - <number> - bit - ref ] ( <url> example of the panorama using the branch <number> . x with the patch : ! [ panorama - <number> - bit ] ( <url> example of the panorama using the branch <number> . x with the patch with the images converted to <number> bits / channel ( cv_16uc3 and multiply by <number> <sad> ! [ panorama - <number> - bit ] ( <url> somehow the result seems to keep more details when the images was converted to <number> bits / channel , see the attached crops input image | panorama using <number> . <number> ( <number> bits / channel ) | panorama using the patch and <number> bits / channel | panorama using the patch and <number> bits / channel - - - | - - - | - - - | - - - ! [ 1 _zoom ] ( <url> | ! [ panorama - <number> - bit - ref - zoom ] ( <url> | ! [ panorama - <number> - bit - zoom ] ( <url> | ! [ panorama - <number> - bit - zoom ] ( <url> # # # pull request readiness checklist see details at <url> - [ x ] i agree to contribute to the project under apache <number> license . - [ x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [ x ] the pr is proposed to the proper branch - [ x ] there is a reference to the original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"cv : : va_intel : : convertfromvasurface always fails on amd devices using mesa < - - if you have a question rather than reporting a bug please go to <url> where you get much faster responses . if you need further assistance please read [ how to contribute ] ( <url> this is a template helping you to create an issue which can be processed as quickly as possible . this is the bug reporting section for the opencv library . - - > # # # # # system information ( version ) - opencv => <number> . <number> - operating system / platform => slackware - current # # # # # detailed description cv : : va_intel : : convertfromvasurface always fails on amd devices using mesa # # # # # steps to reproduce i am trying to convert a vaapi image from ffmpeg in an avframe to a umat , so my code is like this : ` ` ` bool motionopencv : : process_frame ( const avframe * frame , bool video ) { / / check if the incoming frame is vaapi if ( video & & frame - > format = = av_pix_fmt_vaapi & & frame - > hw_frames_ctx ) { avhwframescontext * hw_frames = ( avhwframescontext <wink> frame - > hw_frames_ctx - > data ; avhwdevicecontext * device_ctx = hw_frames - > device_ctx ; avvaapidevicecontext * hwctx ; if ( device_ctx - > type ! = av_hwdevice_type_vaapi ) { lerror ( "" vaapi frame does not have vaapi device "" ); return false ; } hwctx = static_cast < avvaapidevicecontext *>( device_ctx - > hwctx ) ; cv : : umat tmp ; const vasurfaceid surf = reinterpret_cast < uintptr_t const > ( frame - > data [ <number> ]); try { cv : : va_intel : : convertfromvasurface ( hwctx - > display , surf , cv : : size ( frame - > width , frame - > height ) , tmp ) ; } catch ( cv : : exception & e ) { lwarn ( "" error converting image from va - api to opencv + e . msg ) ; return false ; } ` ` ` digging into it , modules / core / src / va_intel . cpp calls ` vaderiveimage ` which if using mesa , calls ` vlvaderiveimage ` . but per the comment in mesa , that always fails on amd devices unless your program name is "" vlc "" , "" h264encode "" , or "" hevcencode "" ` ` ` /* this function is used by some programs to test for hardware decoding , but on * amd devices , the buffers default to interlaced , which causes this function to fail . * some programs expect this function to fail , while others , assume this means * hardware acceleration is not available and give up without trying the fall - back * vacreateimage + vaputimage */ ` ` ` <url> the solution then , is modules / core / src / va_intel . cpp should have the vacreateimage + vaputimage fallback that mesa requires .",1
opencv/opencv,dnn hint to ignore denormals processing reworks # <number> continues # <number> resolves # <number>,1
opencv/opencv,core : fp denormals support relates # <number> - support x86 sse ftz + daz flags info ` ` ` force_builders = custom build_image : custom = openmp : <number> buildworker : custom = linux - <number> allow_multiple_commits = <number> ` ` `,1
opencv/opencv,"feature : submodule or a class scope for exported classes all classes are registered in the scope that corresponds to c + + namespace or exported class . example : ` cv : : ml : : boost ` is exported as ` cv . ml . boost ` ` cv : : simpleblobdetector : : params ` is exported as ` cv . simpleblobdetector . params ` for backward compatibility all classes are registered in the global module with their mangling name containing scope information . example : ` cv : : ml : : boost ` has ` cv . ml_boost ` alias to ` cv . ml . boost ` type closes # <number> backport for <number> . x is required to handle [ gapi aliases ] ( <url> in the right way cv . gapi . wip . draw . rect = cv . gapi_wip_draw_rect cv . gapi . wip . draw . text = cv . gapi_wip_draw_text cv . gapi . wip . draw . circle = cv . gapi_wip_draw_circle cv . gapi . wip . draw . line = cv . gapi_wip_draw_line cv . gapi . wip . draw . mosaic = cv . gapi_wip_draw_mosaic cv . gapi . wip . draw . image = cv . gapi_wip_draw_image cv . gapi . wip . draw . poly = cv . gapi_wip_draw_poly cv . gapi . streaming . queue_capacity = cv . ` ` ` # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or another license that is incompatible with opencv - [x ] the pr is proposed to the proper branch - [x ] there is a reference to the original bug report and related work - [x ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [x ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"reading bigtiff images * * merge with extra resolves # <number> # # # pull request readiness checklist see details at <url> - [ ] i agree to contribute to the project under apache <number> license . - [ ] to the best of my knowledge , the proposed patch is not based on a code under gpl or other license that is incompatible with opencv - [ ] the pr is proposed to proper branch - [ ] there is reference to original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake",1
opencv/opencv,"add general broadcasting layer performance details ( broadcasting 1 x1 to 1 6 x2048x2048 mat of cv_32fc1 ~ 2 6 0 mb ) : ocl copyto : for dims <= <number> , there is ` clenqueuecopybufferrect ` , that runs in 1 ms and can work on strided data , but higher dimensions are not supported . ocl memcpy : 2 2 ms cpu copyto : 3 6 ms cpu memcpy : 2 0 ms cuda memcpy : 1 1 ms tests does not enable any of the conformance tests , since the only two we could use - ` add_bcast ` and ` sub_bcast ` - use <number> - d inputs and we convert them to 2 d , inserting <number> in the wrong spot , rendering the shape layouts non - compatible . other tests with broadcasting ( and , or , xor , less , equal , greater , and such ) require bool output . # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or other license that is incompatible with opencv - [x ] the pr is proposed to proper branch - [ ] there is reference to original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake ` ` ` force_builders = custom , custom win buildworker : custom = linux - <number> , linux - <number> build_image : custom = ubuntu - cuda : <number> build_image : custom win = openvino - <number> . <number> buildworker : custom win = windows - <number> test_bigdata : custom win = <number> test_filter : custom win =* test_modules : custom win = dnn , gapi , python2 , python3 , java test_opencl : custom win = on build_contrib : custom win = off ` ` `",1
opencv/opencv,"adapt remote inference to operate with nv12 blobs # # # pull request readiness checklist see details at <url> - [x ] i agree to contribute to the project under apache <number> license . - [x ] to the best of my knowledge , the proposed patch is not based on a code under gpl or other license that is incompatible with opencv - [x ] the pr is proposed to proper branch - [ ] there is reference to original bug report and related work - [ ] there is accuracy test , performance test and test data in opencv_extra repository , if applicable patch to opencv_extra has the same branch name . - [ ] the feature is well documented and sample code can be built with the project cmake ` ` ` force_builders = custom , custom win , custom mac build_gapi_standalone : linux x64 = ade - <number> . 1 f build_gapi_standalone : win64 = ade - <number> . 1 f build_gapi_standalone : mac = ade - <number> . 1 f build_gapi_standalone : linux x64 debug = ade - <number> . 1 f xbuild_image : custom = centos : <number> xbuildworker : custom = linux - <number> build_gapi_standalone : custom = ade - <number> . 1 f build_image : custom = ubuntu - openvino - <number> . <time> . <number> build_image : custom win = openvino - <number> . <number> build_image : custom mac = openvino - <number> . <number> test_modules : custom = gapi , python2 , python3 , java test_modules : custom win = gapi , python2 , python3 , java test_modules : custom mac = gapi , python2 , python3 , java buildworker : custom = linux - <number> # disabled due high memory usage test_opencl : custom = off test_bigdata : custom = <number> test_filter : custom =* ` ` `",1
