repo,created_at,label,title,body
facebook/react,2023-08-02 02:26:00,bug,Bug: [18.3.0-canary] renderToString hoists some tags to top(working in 18.2),"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18.3.0-canary-493f72b0a-20230727

## Steps To Reproduce

1.  run following code.
```js
import * as ReactDOMServer from ""react-dom/server"";
const element = (
  <html>
    <head>
      {/* meta and title are hoisted */}
      <meta charSet=""utf-8"" />
      <title>title</title>
      {/* the script tag is not hoisted */}
      <script src=""foo""></script>
      {/* but this is hoisted */}
      <script src=""foo"" async></script>
    </head>
  </html>
);

console.log(ReactDOMServer.renderToString(element));
```

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:
https://codesandbox.io/s/react1830-canary-493f72b0a-20230727-ssr-hoist-bug-lvhj45?file=/src/index.js
<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior
console.log outputs `<meta charSet=""utf-8""/><script src=""foo"" async=""""></script><title>title</title><html><head><script src=""foo""></script></head></html>`

## The expected behavior
console.log outputs `<html><head><meta charSet=""utf-8""/><title>title</title><script src=""foo""></script><script src=""foo"" async=""""></script></head></html>`"
facebook/react,2023-07-17 22:43:05,bug,[DevTools Bug]: Chrome extension gets disconnected from the page after 30sec of inactivity,"### Website or app

https://react.dev/

### Repro steps

Steps:
1. go to a react page like https://react.dev/
2. open the devtools Components tab, everything works correctly.
3. change tab (a non react one) and wait 30 sec - 5 min (not super exact)
4. go back to the tab that has the react page you're debugging
5. the Components does not work anywore: you can't select and view components on the page.

![Screenshot 2023-07-17 at 3 40 10 PM](https://github.com/facebook/react/assets/6637867/6e04b8e9-c219-4a2e-a2c3-d6f2f3e61b2f)

My guess is that it's related to Chrome killing the service worker after inactivity on the page. See https://bugs.chromium.org/p/chromium/issues/detail?id=1152255#c185

![Screenshot 2023-07-17 at 3 41 13 PM](https://github.com/facebook/react/assets/6637867/c59e359a-f3ee-4af3-a331-fd6afca8dd46)

Going back to the page doesn't seem to wake the serviceworker up.

Chrome: Version 114.0.5735.198 (Official Build) (x86_64)
React Extension: 4.28.0
macOS: 13.4.1


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-07-13 19:01:47,bug,[DevTools Bug]: Deprecated __REACT_DEVTOOLS_GLOBAL_HOOK__ ????,"### Website or app

N/A

### Repro steps

Hi, I have heard that the new versions of React will not support the REACT_DEVTOOLS_GLOBAL_HOOK. If there any information about this update that you can share. Is there a new way to achieve the same result of using the REACT_DEVTOOLS_GLOBAL_HOOK but with a different method? What is the future of React without the REACT_DEVTOOLS_GLOBAL_HOOK?

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-06-07 17:26:43,bug,"[DevTools Bug]: React devtools stuck at Loading React Element Tree, troubleshooting instructions are Chrome-specific","### Website or app

corporate project (private)

### Repro steps

Load page, then open React DevTools. Reloading or closing and reopening the tab does not fix the problem. Quitting and reopening Firefox sometimes fixes the problem.
<img width=""837"" alt=""image"" src=""https://github.com/facebook/react/assets/2292782/0fbcaf7c-533f-4afd-819c-b9f2749aa765"">

The [linked troubleshooting instructions](https://github.com/facebook/react/blob/main/packages/react-devtools/README.md#the-react-tab-shows-no-components) provide no guidance for users of browsers other than Chrome; I am running Firefox v114 (on macOS 13.2.1).

### How often does this bug happen?

Often

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-05-31 15:17:41,bug,Bug: Radio button onChange not called in current React Canary,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18.3.0-canary-a1f97589f-20230526


## Steps To Reproduce

1. Create radio buttons that toggle `disabled` in `onChange`
2. After selecting each radio button, `onChange` is no longer called

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

The following CodeSandbox demonstrates the issue with the current react canary version. The issue is not present when react & react-dom versions are changed to stable 18.2.0

https://codesandbox.io/s/react-canary-radio-buttons-deiqb3?file=/src/App.js

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior
`<input type=""radio"" />`'s `onChange` prop is not called on subsequent clicks of the input

## The expected behavior
`<input type=""radio"" />`'s `onChange` prop should be called on subsequent clicks of the input
"
facebook/react,2023-05-16 18:27:09,bug,[DevTools Bug]: Strict mode badge points to the old docs,"### Website or app

https://fb.me/devtools-strict-mode

### Repro steps

The Strict mode warning badge points to https://fb.me/devtools-strict-mode which points to the strict mode section in [the old docs](https://legacy.reactjs.org/docs/strict-mode.html) instead of [the new docs](https://react.dev/reference/react/StrictMode).

Badge:
<img width=""273"" alt=""Screenshot 2023-05-16 at 20 20 59"" src=""https://github.com/facebook/react/assets/37460589/2b3cbcf0-43b7-4f71-a420-02d31258ef92"">

Code:
https://github.com/facebook/react/blob/4cd7065665ea2cf33c306265c8d817904bb401ca/packages/react-devtools-shared/src/devtools/views/Components/InspectedElement.js#L240

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-05-08 16:35:24,bug,[DevTools Bug]: Regression - profiling doesn't store props value ,"### Website or app

Doesn't apply

### Repro steps

Old version of DevTools provided ability to see changes in props / state between commits.
https://legacy.reactjs.org/cc2a8b37bbce52c49a11c2f8e55dccbc/see-which-props-changed.gif

Current version provide information about the reason to re-render, but lack of ability to see exactly how props / state variables are changing between re-renders is a huge regression for utility of Profiler.

Components tab keeps only the latest values for the props / state.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-05-06 23:03:50,bug,"[DevTools Bug] Cannot add node ""1751"" because a node with that id is already in the Store.","### Website or app

local repo

### Repro steps

Loading a React component with the React profiler recording enabled

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.6-7f8c501f6

### Error message (automated)

Cannot add node ""1751"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:28581:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26606:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26775:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:57029:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2023-05-02 03:53:41,bug,[DevTools Bug]: React pages not being detected as using React in Incognito mode,"### Website or app

https://opensource.fb.com

### Repro steps

It seems that with the latest update of Chrome and React DevTools, it cannot detect pages as using React on incognito. Screenshot attached below:

<img width=""518"" alt=""image"" src=""https://user-images.githubusercontent.com/34370238/235575519-8eeaa3c5-80fa-4cc4-a33d-3d2f0541f0d9.png"">

* _Chrome version: 112.0.5615.137 (arm64)_
* _React DevTools version: 4.27.6 (4/20/2023)_

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-04-24 07:00:55,bug,[DevTools Bug]: components tree not loaded in Microsoft Edge,"### Website or app

https://react.dev/

### Repro steps

1. Load react app in Microsoft Edge
2. Open dev tools and go to components tree 
3. The tree is not loaded: 
4. This is the message I get: Loading React Element Tree... If this seems stuck, please follow the [troubleshooting instructions](https://github.com/facebook/react/blob/main/packages/react-devtools/README.md#the-react-tab-shows-no-components).

Edge version - latest: Microsoft Edge
Version 112.0.1722.58 (Official build) (64-bit)

![image](https://user-images.githubusercontent.com/16520016/233919983-66f889b5-4f53-416f-8609-e6f8b9cc205c.png)


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-04-07 09:03:43,bug,"[DevTools Bug] Cannot add node ""108084"" because a node with that id is already in the Store.","### Website or app

local react development 

### Repro steps

Open React Dev Tools

### How often does this bug happen?

Often

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.3-28ce1c171

### Error message (automated)

Cannot add node ""108084"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:28167:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26196:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26365:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56618:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2023-04-04 12:09:08,bug,[DevTools Bug]: DevTools settings are not being saved in the Edge browser,"### Website or app

https://react.dev/

### Repro steps

1. Open DevTools
2. Click View Settings
3. Change one of the options
4. Refresh page
5. No option is being saved

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-03-22 18:12:35,bug,[DevTools Bug]: 'Unable to find React on the page' in incognito on Firefox,"### Website or app

https://react.dev

### Repro steps

1. Open an incognito tab
2. visit a react 18 website
3. try and use dev tools (I'm on `4.27.1`)

<img width=""1728"" alt=""CleanShot 2023-03-22 at 11 10 16@2x"" src=""https://user-images.githubusercontent.com/8675906/226998803-79f8e57b-ab7f-46e1-9cad-8744fe92296a.png"">


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-03-14 19:55:26,bug,"[DevTools Bug] Cannot add node ""8891"" because a node with that id is already in the Store.","### Website or app

localhost

### Repro steps

Tried to use the react dev tools Components tab inspector tool

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.2-1a88fbb67

### Error message (automated)

Cannot add node ""8891"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:27863:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25892:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26061:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56323:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2023-03-03 19:05:34,bug,Bug: Error occurs when returning empty fragment '<></>' on a page,"In was working when i comment a code and see this error
Then i start to test possible ways of reproduce the bug
Notes: 
1.using <>{null}</> dont give the error
2.using <>{undefined}</> give the error

React version: 18.2.0

## Steps To Reproduce

1.import { Fragment } from ""react"";

export default async function Companies() {
  return ( 
<Fragment ></Fragment > {/* Or => <></>*/}
 );
}

2.Start the localHost (dev server, etc)

## The current behavior

<img width=""739"" alt=""image"" src=""https://user-images.githubusercontent.com/101187684/222805443-ecd39b24-461e-404c-a2fb-b02199837278.png"">


## The expected behavior

<img width=""960"" alt=""image"" src=""https://user-images.githubusercontent.com/101187684/222805607-870260bb-7643-4502-89cc-24189fe952f2.png"">
"
facebook/react,2023-02-23 04:42:56,bug,why not?[DevTools Bug]: ,"### Website or app

Website

### Repro steps

Loading React Element Tree...

If this seems stuck, please follow the [troubleshooting instructions](https://github.com/facebook/react/tree/main/packages/react-devtools#the-issue-with-chrome-v101-and-earlier-versions).

### How often does this bug happen?

Every time

### DevTools package (automated)

Loading React Element Tree...  If this seems stuck, please follow the troubleshooting instructions.

### DevTools version (automated)

Loading React Element Tree...  If this seems stuck, please follow the troubleshooting instructions.

### Error message (automated)

Loading React Element Tree...  If this seems stuck, please follow the troubleshooting instructions.

### Error call stack (automated)

```text
Loading React Element Tree...

If this seems stuck, please follow the troubleshooting instructions.
```


### Error component stack (automated)

```text
Loading React Element Tree...

If this seems stuck, please follow the troubleshooting instructions.
```


### GitHub query string (automated)

```text
Loading React Element Tree...

If this seems stuck, please follow the troubleshooting instructions.
```
"
facebook/react,2023-02-10 17:36:03,bug,Unable to establish connection with the sandpack bundler.[DevTools Bug]: ,"### Website or app

https://beta.reactjs.org/learn/sharing-state-between-components

### Repro steps

https://beta.reactjs.org/learn/sharing-state-between-components
Unable to establish connection with the sandpack bundler.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-02-03 16:17:32,bug,[DevTools Bug]: React Dev Tools breaks craigslist?,"### Website or app

https://newyork.craigslist.org/search/hhh

### Repro steps

1. Enable React Dev Tools
2. Visit https://newyork.craigslist.org/search/hhh
3. 50% of the time you will see broken page https://imgur.com/a/yJPeAvA
4. Disable React Dev Tools
5. Visit https://newyork.craigslist.org/search/hhh
6. 100% of the time you will see a working page

### How often does this bug happen?

Often

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

```text
TypeError: svs-boot-setManifest-exception:Cannot read properties of null (reading 'insertBefore')
    at cl.injectCss (bbe8a523a089547e594fa2f101021699a377645c.js:6:12097)
    at cl.injectResource (bbe8a523a089547e594fa2f101021699a377645c.js:6:14293)
    at bbe8a523a089547e594fa2f101021699a377645c.js:6:20658
    at Array.forEach (<anonymous>)
    at injectResourceSet (bbe8a523a089547e594fa2f101021699a377645c.js:6:20634)
    at bigBang (bbe8a523a089547e594fa2f101021699a377645c.js:6:17286)
    at cl.setManifest (bbe8a523a089547e594fa2f101021699a377645c.js:6:18123)
    at manifest.js:1:4
cl.unexpected @ bbe8a523a089547e594fa2f101021699a377645c.js:6
cl.setManifest @ bbe8a523a089547e594fa2f101021699a377645c.js:6
(anonymous) @ manifest.js:1
VM1218:1 Uncaught SyntaxError: Unexpected end of JSON input
    at JSON.parse (<anonymous>)
    at localStorage-092e9f9e2f09450529e744902aa7cdb3a5cc868d.html:38:37
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-01-30 03:38:53,bug,[DevTools Bug]: Expected static flag was missing,"### Website or app

https://github.com/Contrick64/scryfall-random

### Repro steps

This error reproduces on initial page load. I can't seem to find what caused it to start showing up, as it points to a part of my code that has existed since well before I first got the error.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-01-25 11:39:56,bug,"ERROR TypeError: Cannot read property 'createElement' of undefined, js engine: hermes","### Website or app

I'm using flipper to debug react-native app

### Repro steps

migrate to current version of RN-0.71.1
using flipper
enable hermes engine
run the APP

[see](https://github.com/facebook/react/issues/26042#issue-1556179961)
[here ](https://github.com/facebook/react-native/issues/35958#issue-1556258460)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

```text
ERROR TypeError: Cannot read property 'createElement' of undefined, js engine: hermes
ERROR TypeError: Cannot read property 'createElement' of undefined, js engine: hermes
ERROR TypeError: Cannot read property 'createElement' of undefined, js engine: hermes
ERROR TypeError: Cannot read property 'createElement' of undefined, js engine: hermes
```


### Error component stack (automated)

```text
this is related to --->> path: node_modules/react-devtools-core/dist/backend.js
function initialize() {
canvas = window.document.createElement('canvas');
canvas.style.cssText = ""\\n xx-background-color: red;\\n xx-opacity: 0.5;\\n bottom: 0;\\n left: 0;\\n pointer-events: none;\\n position: fixed;\\n right: 0;\\n top: 0;\\n z-index: 1000000000;\\n "";
var root = window.document.documentElement;
root.insertBefore(canvas, root.firstChild);
}
```


### GitHub query string (automated)

_No response_"
facebook/react,2023-01-19 23:00:02,bug,[DevTools Bug]: This page doesn't appear to be using react,"### Website or app

reactjs.org

### Repro steps

Go to website.
Click on the react DevTools icon in the extensions.
after reaload, hover the extension.
![image](https://user-images.githubusercontent.com/7319949/213579759-5a8dcd59-6492-4c67-b8bd-0a09f7017a92.png)
![image](https://user-images.githubusercontent.com/7319949/213581014-fe55022f-783d-4e53-9c9d-3e804e62498a.png)


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2023-01-05 08:46:14,bug,Bug: Suspense | client component should have a queue,"React version: 18.2.0
Next version: 13.1.1

## Steps To Reproduce

1. Suspend using a (fake) promise in a client component in app dir (next)
2. Try to useState after the use() call

````ts
'use client';

import { use, useState } from 'react';

const testPromise = new Promise((resolve) => {
  setTimeout(() => {
    resolve('use test promise');
  });
});

export default function Page() {
  use(testPromise);
  useState(0);

  return <h1>Test</h1>;
}

````

Link to code example:
https://codesandbox.io/s/github/xiel/app-playground/tree/suspense-error/?from-embed=&file=/app/page.tsx

## The current behavior

Suspending for a promise in a client component and using state/reducer after results in errors during hydration:

```
react-dom.development.js?9d87:94 Warning: An error occurred during hydration. The server HTML was replaced with client 

react-dom.development.js?9d87:94 Warning: You are accessing ""digest"" from the errorInfo object passed to onRecoverableError. 

on-recoverable-error.js?eb92:17 Uncaught Error: Should have a queue. This is likely a bug in React. Please file an issue.
```

## The expected behavior

No error during hydration.
"
facebook/react,2022-12-13 11:39:55,bug,[DevTools Bug]: Too Much Recursion - Firefox & Appsync,"### Website or app

https://eu-west-1.console.aws.amazon.com/appsync/

### Repro steps

1. Use Firefox with React Dev Tools added
2. Log into AWS, go to Appsync console and select an API
3. The app will then freeze and you should get a `too much recursion` error in the console

This only seems to happen in Firefox. Not confident on whether this is a DevTools, Firefox or Appsync issue but it only seems to happen when DevTools is enabled.
![Screenshot 2022-12-13 at 11 29 14](https://user-images.githubusercontent.com/44685146/207308231-68dcbe56-8221-4d46-b644-c3a8f62595b3.png)




### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-12-11 00:02:00,bug,"[DevTools Bug] Element ""5"" not found","### Website or app

local dev environment

### Repro steps

Occurs on app launch

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""5"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-06 11:31:26,bug,"[DevTools Bug] Element ""65"" not found","### Website or app

https://sh0ny-it.github.io/hw-master/#/junior

### Repro steps

E

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""65"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-05 15:47:21,bug,"[DevTools Bug] Element ""24"" not found","### Website or app

http://localhost:3000

### Repro steps

Someone know solution? I saw a peoples that have same problem, but no one helped :/

### How often does this bug happen?

Only once

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""24"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-05 10:48:01,bug,[DevTools Bug]: Components Tab does not show up,"### Website or app

https://beta.reactjs.org/

### Repro steps

1. Visit website
2. Open dev tools

This happens on https://beta.reactjs.org/ but I first noticed in on a personal project (localhost).

When I open the dev tools, the CPU goes up. At first, the Components tab does not show up. After a loooooong time, it does show up, however when I click on it nothing renders inside.

![Screenshot 2022-12-05 at 12 42 04](https://user-images.githubusercontent.com/58694408/205619186-85451e15-9b2f-43d9-b7f6-3ffaba6e7346.png)


I don't know if it's the newest Chrome version or the newest extension version that's causing it.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-12-04 16:36:01,bug,"[DevTools Bug] Element ""35"" not found","### Website or app

.

### Repro steps

.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""35"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-04 06:45:46,bug,"[DevTools Bug] Element ""7"" not found","### Website or app

The error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15

### Repro steps

The error occurred at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""7"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-02 23:53:37,bug,"[DevTools Bug] Element ""717"" not found","### Website or app

http://localhost:3000/managerCr

### Repro steps

Al darle un nuevo key al form, para que este se resetee, me salta este error

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""717"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-02 09:38:22,bug,"[DevTools Bug] Element ""34"" not found","### Website or app

test

### Repro steps

React devtool send me this error, how i can fix it

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""34"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-01 15:31:57,bug,"[DevTools Bug] Element ""6"" not found","### Website or app

localhost

### Repro steps


If click to the App > State, browser drop me a error ""Element ""6"" not found"", and more red line, i don't understand what is it. Browser updating 10 minutes later to 108.0.5359.72 version. Help me


### How often does this bug happen?

Only once

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""6"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-12-01 04:52:51,bug,"[DevTools Bug] Element ""21"" not found","### Website or app

https://inquisitive-haupia-f14ecb.netlify.app/

### Repro steps

map list

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.0-bd2ad89a4

### Error message (automated)

Element ""21"" not found

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39558:15
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40933:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39771:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42429:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35080:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37705:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44505:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39237:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39439:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39409:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44686:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44115:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31940:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32584:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39834:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56039:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-11-11 12:15:45,bug,[DevTools Bug]: react-devtools depends on vulnerable version of electron,"### Website or app

https://github.com/facebook/react/blob/main/packages/react-devtools/package.json

### Repro steps

### Issue
electron package versions <18.3.7 suffer from a security vulnerability: ""Exfiltration of hashed SMB credentials on Windows via file:// redirect"".
See https://github.com/advisories/GHSA-p2jh-44qj-pf2v

### Solution
Upgrade electron dependency in react-devtools to >18.3.7

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-10-17 08:20:47,bug,"[DevTools Bug] Could not find ID for Fiber ""MiddleSectionContainer""","### Website or app

///

### Repro steps

log in 
inspect element 

### How often does this bug happen?

Only once

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.25.0-336ac8ceb

### Error message (automated)

Could not find ID for Fiber ""MiddleSectionContainer""

### Error call stack (automated)

```text
at getFiberIDThrows (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:7007:11)
    at fiberToSerializedElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8765:11)
    at inspectElementRaw (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8934:21)
    at Object.inspectElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:9237:38)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:11584:56
    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4192:18)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4838:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:13163:9)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39612:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37920:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38454:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:41105:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:33778:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36399:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43155:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37920:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38092:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38122:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38092:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43336:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42781:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30676:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31302:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38517:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54684:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find ID for Fiber ""MiddleSectionContainer"" in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-10-06 05:33:48,bug,"[DevTools Bug] Cannot add node ""5370"" because a node with that id is already in the Store.","### Website or app

----

### Repro steps

----

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.25.0-336ac8ceb

### Error message (automated)

Cannot add node ""5370"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26596:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24626:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24795:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54959:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-09-09 22:42:43,bug,[DevTools Bug]: React extension tab in Edge DevTools doesn't have emoji prefix in title.,"### Website or app

https://reactjs.org/

### Repro steps

1. Open Developer Tools with React extension on any website that using React in Edge.
2. Check the react extension tab (Profiler and Components), it doesn't have emoji prefix in the title like Chrome does.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-09-05 03:06:52,bug,[DevTools Bug]: DevTools shouldn't skip over keyed Fragments in the tree,"### Website or app

https://github.com/reactjs/reactjs.org/pull/4981

### Repro steps

1. Wrap something into `<Fragment key=""stuff"">`
2. It doesn't show up in DevTools

We filter out fragments because they tend to be useless. But this one is important! Keys are crucial and we should show anything with a key in the tree.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-08-23 10:14:02,bug,Bug: out of order application of useState changes,"After updating my app to React 18 I had problem with inconsistent state from useState

I created code example with the problem:
* I have 2 states `const [done, setDone] = useState(false);` (inside hook) and `const [ids, setIds] = React.useState([])`
* I call `setIds([1,2,4])` (inside await, but it's executed immediately, as we see in console) and then `setDone(true)`
* then component is rerendered with updated `done` but original `ids`
* then component is rerendered, but with both states updated

React version: 18.2.0, 18.3.0-next
Link to code example: <https://codesandbox.io/s/purple-dust-ww9hjm?file=/src/index.jsx>
Smaller repro: <https://codesandbox.io/s/cocky-fermat-qoiel3> (from eps1lon's comment)

## The current behavior
In the example after clicking run:
`Inner` is rerendered with `done: true`, but without updated `ids`.
`FormikLike` is created with empty `ids`.
`[]` is displayed under button.


## The expected behavior
First `Inner` rerender should have updated `ids` state.
`FormikLike` should be created with non-empty `ids`.
`[1,2,4]` is displayed under button.

It workied this way in React 17.

In https://stackoverflow.com/a/48610973 @gaearon wrote:
> > But can you trust React to update the state in the same order as setState is called for the same component?
>
> Yes.

Answer was for class components, but I hope the same is true for multiple useState hooks in single function component."
facebook/react,2022-08-11 13:08:10,bug,"[DevTools Bug] Cannot add node ""7448"" because a node with that id is already in the Store.","### Website or app

https://github.com/digita-webshop/digita-webshop-frontend/tree/develop

### Repro steps

1) npm install
2) npm start
3) inspect element
4) components tab


### How often does this bug happen?

Sometimes

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.25.0-336ac8ceb

### Error message (automated)

Cannot add node ""7448"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
emit@moz-extension://9908b50a-48f2-40b1-9532-79813e5a6947/build/main.js:24626:22
bridge_Bridge/this._wallUnlisten<@moz-extension://9908b50a-48f2-40b1-9532-79813e5a6947/build/main.js:24795:14
listener@moz-extension://9908b50a-48f2-40b1-9532-79813e5a6947/build/main.js:54959:41
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-07-27 06:25:25,bug,"[DevTools Bug]: ""open in editor"" not working for vscode remote files","### Website or app

empty

### Repro steps

/data/home/xxxx/src/test.tsx

1. Inspect component
2. User clicks ""open in editor""
3. file not found

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-07-20 15:32:03,bug,[DevTools Bug]: window.bundle.js:2 TypeError: Cannot read properties of undefined (reading 'action'),"### Website or app

https://onepiece-cardgame.dev/builder?f=%24R+%28%22zoro%22%29

### Repro steps

1. Load webpage
2. Tools doesnt work

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-07-15 06:54:26,bug,"[DevTools Bug] Could not inspect element with id ""69""","### Website or app

Internal company application  

### Repro steps

Sometimes I got this error, I solve it be restart my device 

### How often does this bug happen?

Sometimes

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.14.0-d0ec283819

### Error message (automated)

Could not inspect element with id ""69""

### Error call stack (automated)

```text
Uncaught Error: Could not inspect element with id ""69""

The error occurred at Ni (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:261874)
    at Suspense
    at yl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:248667)
    at div
    at Tl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:252578)
    at Ji (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:278469)
    at div
    at div
    at Oa (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:229816)
    at Va (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:236032)
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:305187
    at yl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:248667)
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:249909
    at div
    at div
    at Ns (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:299485)
    at vn (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:180478)
    at Un (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:194071)
    at Pl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:253263)
    at kc (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:339874)
```


### Error component stack (automated)

```text
at Ni (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:261874)
    at Suspense
    at yl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:248667)
    at div
    at Tl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:252578)
    at Ji (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:278469)
    at div
    at div
    at Oa (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:229816)
    at Va (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:236032)
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:305187
    at yl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:248667)
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:249909
    at div
    at div
    at Ns (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:299485)
    at vn (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:180478)
    at Un (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:194071)
    at Pl (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:253263)
    at kc (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:339874)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not inspect element with id  in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-07-07 17:02:35,bug,"[DevTools Bug] Cannot remove node ""0"" because no matching node was found in the Store.","### Website or app

Bitbucket repo

### Repro steps

just npm react-devtools and run

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.24.7-7f673317f

### Error message (automated)

Cannot remove node ""0"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at /Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:333971
    at f.emit (/Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:279464)
    at /Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:281005
    at /Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:667650
    at Array.forEach (<anonymous>)
    at A.e.onmessage (/Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:48:667634)
    at A.t (/Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:39:2838)
    at A.emit (events.js:315:20)
    at e.exports.L (/Users/matt/.config/yarn/global/node_modules/react-devtools-core/dist/standalone.js:3:58322)
    at e.exports.emit (events.js:315:20)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-07-05 03:46:47,bug,[DevTools Bug]: Uncaught TypeError: hook.sub is not a function,"### Website or app

New project created by CRA

### Repro steps

1. create a new project by CRA on mac os Monterey v12.4;
2. import 'react-devtools' at first line of `src/index.tsx`;
3. sudo npm i --location=global react-devtools;
4. npx react-devtools;
5. in my raect app console run `t=document.createElement('script'); t.type='text/javascript'; t.src='http://localhost:8097'; document.head.prepend(t)`;
6. Uncaught TypeError: hook.sub is not a function, below are some screenshots.
<img width=""659"" alt=""error"" src=""https://user-images.githubusercontent.com/13808186/177245359-f55c0017-d833-4043-8a0a-43cc3c4eb5b1.png"">
<img width=""647"" alt=""location"" src=""https://user-images.githubusercontent.com/13808186/177245574-4fb78d7d-2abc-423b-9ef5-006007c70081.png"">



### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-06-29 02:11:00,bug,"[DevTools Bug] Cannot remove node ""25"" because no matching node was found in the Store.","### Website or app

localhost

### Repro steps

This error occurs every time I rebuild my React app

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.24.7-7f673317f

### Error message (automated)

Cannot remove node ""25"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26516:43
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24434:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24603:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54566:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-06-08 13:57:18,bug,[DevTools Bug]: devtool show every page use `React`,"### Website or app

https://vuejs.org/guide/introduction.html

### Repro steps

Open the [Vue3](https://vuejs.org/guide/introduction.html) doc site, then `react` devtool show this page use  procution build of `React`
<img width=""1141"" alt=""Screen Shot 2022-06-08 at 9 53 07 PM"" src=""https://user-images.githubusercontent.com/20318608/172634699-e12ac63f-da07-49ea-8181-3f569d8abddc.png"">

And Github too
<img width=""1200"" alt=""Screen Shot 2022-06-08 at 9 56 19 PM"" src=""https://user-images.githubusercontent.com/20318608/172634864-3e505afc-aa2b-4781-b7c1-ffce9df78635.png"">

I'm using Crome
Version 102.0.5005.61 (Official Build) (x86_64)

<img width=""424"" alt=""Screen Shot 2022-06-08 at 10 00 58 PM"" src=""https://user-images.githubusercontent.com/20318608/172636242-0944aecf-c532-4271-b59b-cda3ceccf553.png"">



### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-06-05 03:26:15,bug,[DevTools Bug]: Warning: Internal React error: Expected static flag was missing. Please notify the React team.,"### Website or app

https://codepen.io/alejozarate/pen/zYRLKww

### Repro steps

The component is successfully rendered with all the interactions working properly.

As far as I can tell, the error is only shown in the console. The traceback point to the line 15 of the codepen:

const _ahr = await SContract.methods.rewardPerHour().call();

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-05-26 02:08:32,bug,"[DevTools Bug] When inspecting, hook values after `useDeferredValue` are offset","### Website or app

https://github.com/Alduino/React-useDeferredValue-DevTools-Reprod

### Repro steps

1. Start the app in either dev or production mode.
2. Open the React dev tools
3. Click on the ""App"" component
4. The first Memo has the value `3.14` (the value that the second Memo should have) instead of `1.41`

If you want it to throw an error instead of just looking at the values, you can set `window.throwIfIncorrect = true` before DevTools inspects the component.

I tried in CodeSandbox's embedded React dev tools as well - the issue happens in React 18.0 (though the first hook has no value and the second has the first hook's value) but it doesn't happen in 18.1.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.24.0-82762bea5

### Error message (automated)

First is 3.14 when it should be 1.41

### Error call stack (automated)

```text
I@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:14022:6
exports.inspectHooksOfFiber@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:14090:12
inspectElementRaw@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:8847:65
inspectElement@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:9130:38
agent_Agent/<@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:11002:56
emit@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:4137:18
Bridge/this._wallUnlisten<@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:4780:14
listener@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:12986:11
EventListener.handleEvent*listen@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:12989:14
Bridge@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:4778:31
setup@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:12979:18
welcome@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:12958:8
EventListener.handleEvent*@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:12961:8
__webpack_require__@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:20:30
@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:84:18
@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/react_devtools_backend.js:87:10
```


### Error component stack (automated)

```text
InspectedElementContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:39159:43
Suspense
ErrorBoundary_ErrorBoundary@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:37513:5
div
InspectedElementErrorBoundaryWrapper@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:38001:46
NativeStyleContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:40650:38
div
div
OwnersListContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:33491:37
SettingsModalContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:36112:40
Components_Components@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:42701:52
ErrorBoundary_ErrorBoundary@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:37513:5
div
div
ThemeProvider@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:37655:23
PortaledContent@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:37685:34
div
div
div
ThemeProvider@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:37655:23
TimelineContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:42881:35
ProfilerContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:42326:35
TreeContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:30393:31
SettingsContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:31015:35
ModalDialogContextController@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:38064:38
DevTools_DevTools@moz-extension://38b0497f-37d9-49e7-9147-393888469493/build/main.js:54073:27
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=First is 3.14 when it should be 1.41 in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-05-19 06:17:14,bug,"[DevTools Bug] Cannot add node ""1"" because a node with that id is already in the Store.","### Website or app

http://localhost:3000/typeofFood

### Repro steps

.

### How often does this bug happen?

Often

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.24.6-ca7a38ae4

### Error message (automated)

Cannot add node ""1"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26389:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24436:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24605:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54547:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-05-12 18:21:01,bug,[DevTools Bug]: Chrome Devtools missing from Chrome extensions Webstore - 404 response,"### Website or app

https://chrome.google.com/webstore/detail/react-developer-tools/

### Repro steps

The download page for the Chrome React Devtools extension returns 404. I checked the Firefox developer tools download page and that's still up.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-05-12 09:55:13,bug,[DevTools Bug]: console.log crashes when I enable DevTools on Chrome,"### Website or app

https://github.com/coolwind0202/web/blob/23877f77e23837a007aada5be363d65290571c88/web/src/components/page/members/IndexPage/IndexPage.tsx#L27

### Repro steps

For checking whenever my project will work correctly, when I run a command `next dev`, errors which are shown below occurred.

![image](https://user-images.githubusercontent.com/51913600/168043187-bf77a943-39c4-45d3-9a1d-2cdf97c904df.png)

When I pass a first argument which is an **object** to `console.log()`, `console.error()`, `console.warn()`, I can repro the errors.
for example, 
```
console.log([]);
console.error({});
```

Before a few days, the errors had never occured.
When I disable DevTools, the errors calm down.

I think, the errors may be caused by the codes which are shown below.
https://github.com/facebook/react/blob/7d9e17a9826595dacbf9e19e8f85b07837adf276/packages/react-devtools-shared/src/backend/utils.js#L193

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-05-11 08:52:21,bug,[DevTools Bug]: Component shown twice in flame graph when using React.memo,"### Website or app

I cannot provide. Private.

### Versions

- React Developer Tools 4.24.3 (3/30/2022)
-  ""react"": ""^17.0.2"",
- ""react-dom"": ""^17.0.2"",

### Repro steps

I am using react-devtools to profile my app.
When a component is wrapped in React.memo and a custom comparison function is provided, that component shows up twice in the Flamegraph. 
I tried with different components in the app and the behaviour is always the same. 
I tried with both the chrome extension and the stand-alone npm package.

Shows only one instance in Flamegraph:
`export default React.memo(SurfacesSelector)`

Shows up twice in Flamegraph:
`export default React.memo(SurfacesSelector, customShallowEqual)`

OR

```
export default React.memo(SurfacesSelector, (prevProps: SurfacesSelectorProps, nextProps: SurfacesSelectorProps) => {
      if (deepEqual(prevProps.enabledSurfaces, nextProps.enabledSurfaces)) {
      return false;
    }
    if (deepEqual(prevProps.surfaces, nextProps.surfaces)) {
      return false;
    }
    return shallowEqual(prevProps, nextProps);
})
```

See the following screenshots for the result.
![Screen Shot 2022-05-11 at 10 35 11](https://user-images.githubusercontent.com/12138301/167808653-32c04c1f-eec5-44d0-ba03-6cc1be26547b.jpg)
![Screen Shot 2022-05-11 at 10 34 56](https://user-images.githubusercontent.com/12138301/167808672-98dbd9c1-2e27-4dc7-a9a1-b0d2e063410a.jpg)



### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-04-28 14:39:31,bug,Bug: setState is not flushed if an iframe is added in the same tick in Safari,"Batched state changes are not flushed if the promise code changes the DOM (= attaching an iframe). Reproduceable with Safari.

React version: 18.1.0

## Steps To Reproduce

1. Open [codepen](https://codepen.io/kh-viewar/pen/wvpVLxM) with Safari.
2. Click ""Upload""
3. => ""Spinner"" text does not render while Promise is in pending

Link to code example:

https://codepen.io/kh-viewar/pen/wvpVLxM

## The current behavior

State changes are not executed.

## The expected behavior

State should change to true and render spinner component. After promise is resolved (2 seconds timeout), state should switch back to false and spinner component should unmount.

## Original discussion

https://github.com/facebook/react/issues/24365"
facebook/react,2022-04-25 19:40:15,bug,"[DevTools Bug] Cannot add node ""1"" because a node with that id is already in the Store.","### Website or app

https://github.com/edavetisyan/Recipes

### Repro steps

running react-native debugger


### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.14.0-d0ec283819

### Error message (automated)

Cannot add node ""1"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at /usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:48:140545
    at c.emit (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:48:89515)
    at /usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:48:90986
    at /usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:48:347787
    at Array.forEach (<anonymous>)
    at S.Gc.e.onmessage (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:48:347771)
    at S.n (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:40:3009)
    at S.emit (events.js:315:20)
    at e.exports.P (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:8:9318)
    at e.exports.emit (events.js:315:20)
    at e.exports.dataMessage (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:8:15409)
    at e.exports.getData (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:8:14651)
    at e.exports.startLoop (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:8:12066)
    at e.exports._write (/usr/lib/react-native-debugger/node_modules/react-devtools-core/dist/standalone.js:8:11421)
    at doWrite (_stream_writable.js:403:12)
    at writeOrBuffer (_stream_writable.js:387:5)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-04-21 14:08:39,bug,[DevTools Bug]: Error in event handler: Error: Attempting to use a disconnected port object,"### Website or app

https://codesandbox.io/s/blissful-raman-2on7k2

### Repro steps

1. Create a react app 
```
yarn create react-app test-react
cd test-react
yarn start
```
2. Create `.env.development` file in root.
```
HTTPS=true
PORT=4100
BROWSER=none
```
3. Visit https://localhost:4100/ in Chrome v100.0.4896.127
4. Open React Devtools by inspecting page, some times it shows `Components` tab but in large application it does not show the `Components` tab. If it shows the tab the error message is sent to dev tools every second.
5. See error message in [chrome://extensions/](chrome://extensions/)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-04-15 16:28:29,bug,"Bug: Incorrect Hydration Mismatch Detection during Suspense - ""Hydration failed because the initial UI does not match what was rendered on the server.""","React version: 18.0.0

## Steps To Reproduce

1. Add a Suspense Boundary
2. Add a **component that will suspend** to load some data (faked).
3. Render at least one **sibling component** _after_ the suspending component.
3. Render server-side using renderToPipeableStream()
4. Render client-side using hydrateRoot()

__Reproductions in CodeSandbox:__

- [Reproduction 1](https://yt3148.sse.codesandbox.io) with Next.js [Edit](https://codesandbox.io/s/github/xiel/SuspenseHydrationErrorNext)
- [Reproduction 2](https://codesandbox.io/s/brave-euclid-lx3et7?file=/server/render.js) using renderToPipeableStream (based on [example demo](https://codesandbox.io/s/kind-sammet-j56ro?file=/server/render.js:1054-1614))

````jsx
<>
  <h4>This headline hydrates fine. </h4>
  <SomethingA />{/* Will suspend on client and server */}
  <h3>💥 This element after the suspending Component triggers an error (only in development).</h3>
</>
````

## The current behavior

- The sibling component following the suspending component is incorrectly seen as a **hydration mismatch**.
- Console will show errors (and Next.js will show big error overlay):

````
Warning: Expected server HTML to contain a matching <h3> in <div>.
    at h3
    at IndexPage
    at Suspense
    at App (webpack-internal:///./pages/_app.js:42:27)
    at ErrorBoundary (webpack-internal:///./node_modules/next/dist/compiled/@next/react-dev-overlay/client.js:8:20638)
    at ReactDevOverlay (webpack-internal:///./node_modules/next/dist/compiled/@next/react-dev-overlay/client.js:8:23179)
    at Container (webpack-internal:///./node_modules/next/dist/client/index.js:323:9)
    at AppContainer (webpack-internal:///./node_modules/next/dist/client/index.js:825:26)
    at Root (webpack-internal:///./node_modules/next/dist/client/index.js:949:27)
````

````
Uncaught Error: Hydration failed because the initial UI does not match what was rendered on the server.
    at throwOnHydrationMismatch (react-dom.development.js?ac89:14344:1)
    at tryToClaimNextHydratableInstance (react-dom.development.js?ac89:14372:1)
    at updateHostComponent$1 (react-dom.development.js?ac89:20636:1)
    at beginWork (react-dom.development.js?ac89:22373:1)
    at HTMLUnknownElement.callCallback (react-dom.development.js?ac89:4157:1)
    at Object.invokeGuardedCallbackDev (react-dom.development.js?ac89:4206:1)
    at invokeGuardedCallback (react-dom.development.js?ac89:4270:1)
````

- Depending on the location of the suspending component and their siblings, there are errors or no errors at all. When the suspending component is the last component, there are no errors logged.
- Error can be suppressed by wrapping the suspending component in a Suspense boundary directly

## The expected behavior

- I expect to be able to use the same fetching mechanism using suspense on server and client in [React v18](https://reactjs.org/blog/2022/03/29/react-v18.html#suspense-in-data-frameworks).
_OR_
- If this is unsupported usage, the errors should be more clear and consistent."
facebook/react,2022-04-12 15:27:31,bug,Bug: Some transition updates haven't been rendered,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18.0.0

## Steps To Reproduce

1. Write a react component with the following code:

```jsx
import React, { useState, startTransition } from 'react';

export default function App() {
  const arr = Array(9999).fill(1);
  const [value, setValue] = useState(0);
  const handleInputChange = (e) => {
    console.log(e.target.value);
    startTransition(() => {
      setValue(e.target.value);
    });
  };
  const getValues = () => {
    return arr.map((item, index) => {
      return (
        <div key={index}>
          {value}-{index}
        </div>
      );
    });
  };
  return (
    <div>
      <input type=""text"" onChange={handleInputChange} />
      <div>{getValues()}</div>
    </div>
  );
}
```
2. input content in the `<input />` at a very fast speed.
3. As shown in the figure below, all the contents I entered for the first time have been rendered. But the second time I input `216948261894`, only `216948` is rendered, and the following characters are not rendered.
![img](https://s1.ax1x.com/2022/04/12/LnoNvQ.gif)

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

https://codesandbox.io/s/react18-starttransition-5u1en2?file=/src/App.js

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior

As shown in the figure below, all the contents I input for the first time have been rendered. But the second time I input `216948261894`, only `216948` is rendered, and the following characters are not rendered.

## The expected behavior

Everything I input should be rendered."
facebook/react,2022-04-05 14:21:22,bug,Bug: componentWillUnmount is called twice,"React version: 18.0.0

## Steps To Reproduce

`componentWillUnmount` is called twice upon toggling the rendered component. Even when **StrictMode** is disabled

Link to code example: https://codesandbox.io/s/componentwillunmount-called-twice-hrpzy5?file=/src/App.js

## The current behavior

After upgrading to react 18 we've seen some different behavior in a conditionally rendered, lazy class component. 

In the provided code example the class component is rendered first. After the first toggle, the class component's componentWillUnmount is called twice. 

Subsequent toggle calls correctly lead to a single componentWillUnmount invocation.

This does only seem to affect the class component when its rendered first. If the condition is changed to initially show the other function component the class component unmounts just fine

## The expected behavior

The class component's componentWillUnmount is only called once"
facebook/react,2022-04-04 00:59:24,bug,Bug: `suppressHydrationWarning` is not taken into account in production builds in React 18,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18 

## Steps To Reproduce

1. Clone https://github.com/Avansai/next-multilingual
2. run: npm install
3. run: npm build
4. run: npm run start-example-build
5. Go to the following URL: http://localhost:3000/fr-ca/tests/routes-dynamiques/123
6. Open the console log: no errors
7. Kill the app, and update the react and react-dom package to version 18.0
8. Re-run steps 2 to 5
9. Open the console log: lots of hydration errors #[425](https://reactjs.org/docs/error-decoder.html?invariant=425), #[418](https://reactjs.org/docs/error-decoder.html?invariant=418) and #[423](https://reactjs.org/docs/error-decoder.html?invariant=423).

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

https://github.com/Avansai/next-multilingual

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior

Using React 18, the tests are failing because of the errors being thrown in the console. 

<img width=""935"" alt=""image"" src=""https://user-images.githubusercontent.com/6453322/161457567-c0777ffa-a3e5-4e21-80bc-51619942299f.png"">

I confirmed this was related to `suppressHydrationWarning` because as soon as I removed the following code from: https://github.com/Avansai/next-multilingual/blob/main/example/pages/tests/dynamic-routes/%5Bid%5D.tsx#L37

```jsx
<tr>
  <td>{messages.format('rowLocalizedWithAsPath')}</td>
  {/* Adding `suppressHydrationWarning` until
  https://github.com/vercel/next.js/issues/32772 is resolved */}
  <td suppressHydrationWarning={true}>{asPath}</td>
</tr>
```

The errors stop. However, I could not find any mention that `suppressHydrationWarning` was no longer supported with React 18 and these errors are only triggered on builds, not in dev mode (which is why I am opening this issue)

## The expected behavior

`suppressHydrationWarning`  should prevent these errors from being thrown."
facebook/react,2022-04-03 15:03:00,bug,"[DevTools Bug] Cannot add child ""2"" to parent ""1"" because parent node was not found in the Store.","### Website or app

https://github.com/Sam-Apostel/3d-browser/tree/f124285a55930bb78e42099e33047827919cb800

### Repro steps

clone the repo at the linked commit
- npm install
- npm run dev
- open localhost:3000
- open react devtools
- refresh the page

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.23.0-e28a0db22

### Error message (automated)

Cannot add child ""2"" to parent ""1"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at /usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:53:330974
    at c.emit (/usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:53:277732)
    at /usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:53:279273
    at /usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:53:659742
    at Array.forEach (<anonymous>)
    at A.e.onmessage (/usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:53:659726)
    at A.t (/usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:44:3009)
    at A.emit (events.js:315:20)
    at e.exports.L (/usr/local/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:8:13567)
    at e.exports.emit (events.js:315:20)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-03-30 15:10:06,bug,"NPM ""version"" export format changed (now includes SHA and date)","React version: 18.0.0

## Steps To Reproduce

1. `import { version } from ""react""`
2. `console.log(version)`

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

https://codesandbox.io/s/new-platform-dvqpp8?file=/src/App.js

## The current behavior

Version is: `18.0.0-fc46dba67-20220329`

## The expected behavior

Version is: `18.0.0` (or `18.0.1` 😉 )"
facebook/react,2022-03-27 21:59:08,bug,[DevTools Bug]: useEffect hook incorrectly labelled as reason for component rendering,"### Website or app

https://codesandbox.io/s/react-devtools-useeffect-incorrect-render-reason-x3inwg

### Repro steps

1. Start profiling
2. Click the button
3. Stop profiling
4. Check the reasons for rendering

<img width=""435"" alt=""Screen Shot 2022-03-28 at 10 48 57 AM"" src=""https://user-images.githubusercontent.com/4659562/160302578-1650b71f-d78b-48fe-b413-17f941595632.png"">

<img width=""241"" alt=""Screen Shot 2022-03-28 at 10 54 17 AM"" src=""https://user-images.githubusercontent.com/4659562/160302601-e92d48e3-801f-414a-8b9a-acc080f488f6.png"">

It is incorrect to say that hook 2 is why this rendered. Hook 1 is why it rendered. Hook 2 was rescheduled during that render. These really need to be separated as they are completely different things. In a component with a lot of effects it can be tedious to find which hook was the one that caused it to render (especially with transpilling, minifying etc where hooks are not always shown nicely).

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-03-22 06:19:18,bug,"[DevTools Bug] Unsupported Bridge operation ""0""","### Website or app

React Native Init App

### Repro steps

Just run the React-DevTools and then forward to port 8087 to debug in real device


### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.24.1-ac574d688

### Error message (automated)

Unsupported Bridge operation ""0""

### Error call stack (automated)

```text
at /Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:48:335334
    at f.emit (/Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:48:278775)
    at /Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:48:280316
    at /Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:48:664831
    at Array.forEach (<anonymous>)
    at A.Zh.e.onmessage (/Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:48:664815)
    at A.t (/Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:39:2836)
    at A.emit (events.js:315:20)
    at e.exports.L (/Users/tb921t/.npm-global/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:3:58322)
    at e.exports.emit (events.js:315:20)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Unsupported Bridge operation  in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-03-14 17:50:04,bug,[DevTools Bug]: Components + Profile tabs not showing up in Chrome 99,"### Website or app

https://codesandbox.io/s/new

### Repro steps

1. Open up Chrome inspector
2. Expect Components + Profile tabs to appear

Computer:
2019 MacBook Pro (Intel)

Browser:
Google Chrome
Version 99.0.4844.51 (Official Build) (x86_64)

DevTools:
4.24.0 (3/10/2022)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-03-04 20:49:02,bug,renderToReadableStream Passes Reusable Chunks,"https://github.com/cloudflare/miniflare/issues/203

We switched to using ""bytes"" streams but, by spec, chunks get transferred in that format. Totally reasonable. However, we pass reusable chunks which then get detached. I'm not sure why the polyfill or fixtures didn't catch this.

We really should be using the byob model instead and copy into a larger chunk for perf anyway so this just makes that more urgent."
facebook/react,2022-02-17 11:33:33,bug,"[DevTools Bug] Cannot add node ""1"" because a node with that id is already in the Store.","### Website or app

local running intranet web app

### Repro steps

1 . start intranet web app
2. call in DevTools from React DevTools ""Components""

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.23.0-e28a0db22

### Error message (automated)

Cannot add node ""1"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26229:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24415:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24581:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54033:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-02-15 20:08:08,bug,[DevTools Bug] Cannot read properties of undefined (reading 'skin'),"### Website or app

https://coorpacademy.github.io/components/components/?path=/story/moleculequestionsfreetext--withdefaultvalue

### Repro steps

Using the React dev tools, example: search for moleculequestionsfreetext in the story book, then the context is undefined hence the error (the provider shows that the skin is ok but the context doesn't reach the component - at least for the dev tools, because it actually reaches as the context is used), the patch is easy to get a default value in case it is undefined but it should not be needed because it would only for the devtools' sake.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.23.0-e28a0db22

### Error message (automated)

Cannot read properties of undefined (reading 'skin')

### Error call stack (automated)

```text
at FreeText (https://coorpacademy.github.io/components/components/main.e80d0d6a3191ede6a2b1.bundle.js:1:322446)
    at H (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:13603:5)
    at exports.inspectHooksOfFiber (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:13671:12)
    at inspectElementRaw (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8426:65)
    at Object.inspectElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8709:38)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:10542:56
    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4225:18)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4868:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:12535:9)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39022:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37375:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37864:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40513:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:33354:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35975:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42520:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37375:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37548:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42700:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42146:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30256:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30878:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37927:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53807:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot read properties of undefined (reading 'skin') in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-02-12 23:29:52,bug,"[DevTools Bug] Could not find node with id ""18"" in commit tree","### Website or app

https://github.com/ModelSaber/ModelSaber.Main

### Repro steps

Just try and load the 2/5 and 3/5 report from a reload and start profiling instance.

Change `REACT_APP_API_URL` to `https://apimodelsaber.rainemods.io` in order to launch the app without needing the full .NET 6 environment and the corresponding data in the postgres database.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.23.0-e28a0db22

### Error message (automated)

Could not find node with id ""18"" in commit tree

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25574:13
    at Map.forEach (<anonymous>)
    at RankedChartBuilder_getChartData (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25570:24)
    at ProfilingCache_ProfilingCache.getRankedChartData (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25686:11)
    at CommitRankedAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43680:32)
    at gi (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:15400:7)
    at zj (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:16741:7)
    at jl (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19201:86)
    at il (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:18756:11)
    at hl (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:18748:23)
```


### Error component stack (automated)

```text
at CommitRankedAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43659:34)
    at div
    at div
    at div
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35975:3)
    at Profiler_Profiler (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53276:34)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37375:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37548:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42700:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42146:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30256:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30878:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37927:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53807:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find node with id  in commit tree in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-02-02 10:58:47,bug,"[DevTools Bug] Could not find ID for Fiber ""App""","### Website or app

https://codesandbox.io/s/react-devtools-bug-z2vjo

### Repro steps

Run the Next app, open up react dev tools and inspect the Box component, rendered by @react-three/fiber.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.23.0-e28a0db22

### Error message (automated)

Could not find ID for Fiber ""App""

### Error call stack (automated)

```text
at getFiberIDThrows (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:6493:11)
    at fiberToSerializedElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8237:11)
    at inspectElementRaw (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8406:21)
    at Object.inspectElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8709:38)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:10542:56
    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4225:18)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4868:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:12535:9)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39022:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37375:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37864:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40513:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:33354:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35975:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42520:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37375:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37548:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37518:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42700:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42146:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30256:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30878:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37927:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53807:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find ID for Fiber ""App"" in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-01-20 18:17:36,bug,[DevTools Bug]: Different results in chrome and firefox,"### Website or app

https://www.youtube.com/, https://developer.mozilla.org/en-US/docs/Learn/CSS/CSS_layout/Flexbox,https://docs.github.com/en

### Repro steps

I was using the YouTube website. I have a habit of checking if the website i have visited was using react or not. As always I checked for YouTube website too. But the react developer tools extension was showing the youtube website is using react. 

Just to make sure it was right I have installed the firefox browser then installed the react developer tools extension and opened the youtube website, But in firefox, the extension was saying the youtube is not using react. 

For confirming with other websites, I have visited mdn and GitHub docs in both chrome and firefox. The chrome react developer tools extension says that the mdn is using react and the firefox react developer tools extension says that the mdn is not using react. For the GitHub docs website, the chrome extension says that the GitHub docs website was using the react whereas the firefox react developer tools extension shows that the Github docs website doesn't use the react. 

I am attaching the screen recording. To differentiate what websites I have visited in chrome and firefox I previously opened the websites in chrome and showed the results, for firefox I have typed the query and showed the results. Can you tell why it was happening like this?


https://user-images.githubusercontent.com/59245935/150397327-1cae5d73-0e63-4a32-b54b-5aa3da522db4.mov



Repro steps
1. Login to the website
2. Scrolling  the website
3. Noticed the bug 

Versions:

Chrome: 97.0.4692.99
Firefox: 96.0.2
React Developer Tools:4.22.0


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-01-16 14:57:01,bug,[DevTools Bug]: not sure if bug or  not:  devtools marks Youtube as built with react.,"### Website or app

https://www.youtube.com/

### Repro steps

I was watching a video on youtube when I noticed the new video hover features which were not there on the 15th of January 2022. 
This then led me to the react dev tools which I noticed had turned blue, never had before. wanted to know of this is a bug or not. 
I have restarted my computer and browser severally.

[](url
![tube](https://user-images.githubusercontent.com/65865227/149665223-b3243940-c1b0-418f-bfba-a8ca9eebb885.png)
)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-01-14 23:20:34,bug,"[DevTools]: Confusing ""Why did this render?"" with hooks","### Website or app

https://codesandbox.io/s/elegant-paper-5lhxd?file=/src/App.js

### Repro steps

1. visit https://5lhxd.csb.app/
2. start recording profile
3. click a button (to update the context value)
4. check the report:
<img width=""252"" alt=""Screenshot 2022-01-15 at 00 14 52"" src=""https://user-images.githubusercontent.com/9800850/149597139-a6ef2564-0254-4f36-a128-d2de2792d12e.png"">

### Problem

The list of hooks that were changed is **correct**. However, this list is not really the reason why this component has been rendered. The only thing that changed and led to this rerender is the context value.

I think the list of changed hooks is useful but this should be listed separately. For the ""Why did this render?"" only context and state (and maybe useSyncExternalStore snapshots) should be listed. As only those actually cause rerenders.

On a separate note - it's a bummer that contexts are not counted within the changed hooks list and the only report we get is that ""context has changed"". If I use multiple contexts in a component this information is often not enough because I can't easily check which context has been updated.


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-01-12 07:40:24,bug,"[DevTools Bug] Cannot add node ""6194"" because a node with that id is already in the Store.","### Website or app

Code Sandbox app

### Repro steps

1. Clcik inspect from Google Chrome
2. Click components 
3. it will show up this message

### How often does this bug happen?

Sometimes

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.22.0-0229baee2

### Error message (automated)

Cannot add node ""6194"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26181:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24367:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24533:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53958:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2022-01-05 11:49:06,bug,"[DevTools Bug] Cannot add child ""34"" to parent ""33"" because parent node was not found in the Store.","### Website or app

localhost

### Repro steps

Just started profiler and reloaded the page

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.22.0-0229baee2

### Error message (automated)

Cannot add child ""34"" to parent ""33"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26248:43
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24367:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24533:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53958:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-12-27 20:33:10,bug,"[DevTools Bug] Cannot add node ""1"" because a node with that id is already in the Store.","### Website or app

It's empty project creating with 'react-native init'

### Repro steps

1. Start rn app in debug mode

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.14.0-d0ec283819

### Error message (automated)

Cannot add node ""1"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:140545
    at c.emit (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:89515)
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:90986
    at /Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347787
    at Array.forEach (<anonymous>)
    at S.Gc.e.onmessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:48:347771)
    at S.n (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:40:3009)
    at S.emit (events.js:315:20)
    at e.exports.P (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:9318)
    at e.exports.emit (events.js:315:20)
    at e.exports.dataMessage (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:15409)
    at e.exports.getData (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:14651)
    at e.exports.startLoop (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:12066)
    at e.exports._write (/Applications/React Native Debugger.app/Contents/Resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:8:11421)
    at doWrite (_stream_writable.js:403:12)
    at writeOrBuffer (_stream_writable.js:387:5)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-12-21 09:06:07,bug,"[DevTools Bug] Could not find ID for Fiber ""Route""","### Website or app

null

### Repro steps

I wrote a component using dhtmlxgantt:

```
<div ref={ganttContent} />
```

initialized in this way: 
```
useEffect(() => {
  gantt.init(ganttContent.current);
  gantt.parse(data);
}, []);
```

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

Could not find ID for Fiber ""Route""

### Error call stack (automated)

```text
at getFiberIDThrows (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:5836:11)
    at fiberToSerializedElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:7543:11)
    at inspectElementRaw (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:7712:21)
    at Object.inspectElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8004:38)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:9837:56
    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4257:18)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:10500:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:11737:9)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38726:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37092:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37572:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40146:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35254:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35695:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42085:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37092:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37222:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37256:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37222:3)
    at SchedulingProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43423:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:41711:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30116:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30727:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37635:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53004:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find ID for Fiber ""Route"" in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-12-16 00:54:41,bug,"[DevTools Bug] Cannot add node ""7184"" because a node with that id is already in the Store.","### Website or app

just in my local machine

### Repro steps

1. Load page.
2. Open react components.

### How often does this bug happen?

Only once

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

Cannot add node ""7184"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26134:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24349:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24509:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53230:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-12-15 20:20:58,bug,"[DevTools Bug] Could not find ID for Fiber ""App""","### Website or app

not public

### Repro steps

I have two code bases in a yarn workspaces linked monorepo. One is using react-three-fiber (the lib), and the other one is really thin wrapper around it with some simple UI, just couple of buttons. Both are using multiple (3) zustand stores.

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

Could not find ID for Fiber ""App""

### Error call stack (automated)

```text
at getFiberIDThrows (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:5836:11)
    at fiberToSerializedElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:7543:11)
    at inspectElementRaw (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:7712:21)
    at Object.inspectElement (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:8004:38)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:9837:56
    at Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:4257:18)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:10500:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/react_devtools_backend.js:11737:9)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38726:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37092:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37572:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40146:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35254:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:35695:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42085:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37092:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37222:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37256:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37222:3)
    at SchedulingProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:43423:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:41711:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30116:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:30727:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37635:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53004:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find ID for Fiber ""App"" in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-12-15 04:44:55,bug,Revert changes to react-devtools-inline Webpack config from PR #22760,Resolves #22958
facebook/react,2021-12-15 00:54:52,bug,[react-devtools-inline][4.22.0]: broken published package,"## Steps To Reproduce

1. install `react-devtools-inline` in a project that has `react` and `react-is`
2. open node
3. `require('react-devtools-inline')`
4. See error:
```
Uncaught:
Error: Cannot find module '/Users/jstejada/code/jstejada-react/build/oss-experimental/react-is'
Require stack:
- /home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js
- <repl>
    at Function.Module._resolveFilename (node:internal/modules/cjs/loader:933:15)
    at Function.Module._load (node:internal/modules/cjs/loader:778:27)
    at Module.require (node:internal/modules/cjs/loader:1005:19)
    at require (node:internal/modules/cjs/helpers:102:18)
    at Object.14 (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:1344:18)
    at __webpack_require__ (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:21:30)
    at Object.3 (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:5710:17)
    at __webpack_require__ (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:21:30)
    at Object.10 (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:495:64)
    at __webpack_require__ (/home/avi/projects/temp/inlinetest/node_modules/react-devtools-inline/dist/backend.js:21:30) 
```

## The current behavior

Published package contains the following code:

```
module.exports = require(""/Users/jstejada/code/jstejada-react/build/oss-experimental/react-is"");
...
module.exports = require(""/Users/jstejada/code/jstejada-react/build/oss-experimental/react"");
```

## The expected behavior

No absolute paths in bundle. successful evaluation."
facebook/react,2021-12-05 23:35:15,bug,"[DevTools Bug] Could not find node with id ""99"" in commit tree","### Website or app

https://consumption.coffee/

### Repro steps

1. Visit homepage
2. Start Profiler
3. Visit Coffee Page via navbar
4. Visit Home Page via navbar
5. Boom Error

### How often does this bug happen?

Sometimes

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

Could not find node with id ""99"" in commit tree

### Error call stack (automated)

_No response_

### Error component stack (automated)

```text
CommitRankedAutoSizer@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:44375:34
div
div
div
SettingsModalContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:35694:40
Profiler_Profiler@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:52477:34
ErrorBoundary_ErrorBoundary@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:37092:5
div
div
ThemeProvider@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:37221:23
PortaledContent@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:37255:34
div
div
div
ThemeProvider@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:37221:23
SchedulingProfilerContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:43422:45
ProfilerContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:41710:35
TreeContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:30115:31
SettingsContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:30726:35
ModalDialogContextController@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:37634:38
DevTools_DevTools@moz-extension://84945e53-7ac8-3442-9da2-ed4bcec5b8a4/build/main.js:53003:27
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not find node with id  in commit tree in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-11-26 00:52:17,bug,[DevTools Bug]: CDN-based site not working,"### Website or app

https://lcdev.shaped.ca

### Repro steps

Dev tools not working in FF or Chrome, says ""This page doesn't appear to be using react"".

React is included via CDN as shown on react website https://reactjs.org/docs/cdn-links.html:
``
	<script crossorigin=""anonymous"" src=""https://unpkg.com/react@17/umd/react.development.js""></script>
``

Web Console on said page says:
``
Download the React DevTools for a better development experience: https://reactjs.org/link/react-devtools
``

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2021-11-21 13:24:29,bug,[DevTools Bug]: Scheduling Profiler Tab Doesn't Show Up In DevTools,"### Website or app

https://codesandbox.io/s/react-18-sandbox-0w6uk

### Repro steps

1. Open [this codesandbox](https://codesandbox.io/s/react-18-sandbox-0w6uk)
2. Open the app in new window
3. Open Profiler tab of the React DevTools _(ver 4.21.0-2f8f60ca8)_
4. See no Scheduling Profiler icon
<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/16290753/142763716-2c814adb-1ee7-40b6-aea5-a2e35c5c3d02.png"">


### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2021-11-17 10:35:28,bug,React 18 Bug: Hydration mismatch if empty string is rendered,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18.0.0-beta-4ff5f5719-20211115

## Steps To Reproduce

1. Render empty string  `ReactDOMServer.renderToString("""")`

Link to code example: https://codesandbox.io/s/react-18-emptry-string-hydration-mismatch-xz4w9
Original issue: [`f9d729e` (#1541)](https://github.com/OperationCode/front-end/pull/1541/commits/f9d729e4070ce5ae178d1746fe470646652eaf3f#diff-887fe6affa5f7a2804ac12d95865452237617ee98d7342f1df8352cf641a70d4L90-R93)

Did some spelunking in the codebase and it seems to me that the reconciler is looking for a hydrateable instance (https://github.com/facebook/react/blob/00ced1e2b7610543a519329a76ad0bfd12cd1c32/packages/react-reconciler/src/ReactFiberBeginWork.new.js#L1413-L1415). But since an empty string won't appear as a text node (if we just set `innerHTML = string`), the reconciler thinks there's a mismatch. In legacy roots we didn't throw but warn **unless we had an empty string**:  https://github.com/facebook/react/blob/75f3ddebfa0d9885ce8df42571cf0c09ad6c0a3b/packages/react-dom/src/client/ReactDOMComponent.js#L1202-L1208

## The current behavior

Console error is logged with ""An error occurred during hydration. The server HTML was replaced with client content""

## The expected behavior

No hydration mismatch just like in React 17: https://codesandbox.io/s/react-17-emptry-string-no-hydration-mismatch-forked-5tgmw

"
facebook/react,2021-11-15 10:16:20,bug,"[DevTools Bug] Cannot add node ""378218"" because a node with that id is already in the Store.","### Website or app

private localhost

### Repro steps

not sure ..... it happened when I recorded a new profile 

### How often does this bug happen?

Sometimes

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.20.0-e5f486b5a

### Error message (automated)

Cannot add node ""378218"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25742:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:23957:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24117:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:52741:41)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-11-10 02:20:05,bug,"[DevTools Bug] Cannot add child ""44"" to parent ""42"" because parent node was not found in the Store.","### Website or app

https://github.com/I7RANK/holbertonschool-web_react

### Repro steps

Hello, I hope you are great.

1. Make a git clone and go to `0x03-react_props/task_2/dashboard/` directory
2. Install the node_modules using `npm install`
3. Then execute `npm run start`
4. And go to http://localhost:8080/
5. You need to install the React Developer Tools extension for Google Chrome (link [here](https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi))
6. Open the Chrome DevTools by pressing `F12`
7. Now in the tabs of the new window opened by pressing `F12` choose the tab `Profiler`
8. And click on the `Reload and start profiling` button to see the error.

See an example here and thanks in advance:
![React_developer_tools_Error](https://user-images.githubusercontent.com/65993425/141037229-fda4531e-822e-4b16-8e03-410f84321b8d.jpg)

Also, I have other extensions just in case you need
![image](https://user-images.githubusercontent.com/65993425/141038103-37fa2313-b360-4cce-8208-d6b9f5866beb.png)

and in another Google chrome profile that I tried, has these extensions
![image](https://user-images.githubusercontent.com/65993425/141038275-f8456a76-2890-4c01-86d4-da22bbe0a6fa.png)



### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.21.0-2f8f60ca8

### Error message (automated)

Cannot add child ""44"" to parent ""42"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26191:43
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24349:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24509:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:53230:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-11-03 15:37:15,bug,Named hooks parsing fails for certain Code Sandbox examples,"Noticed some parsing bugs when writing some example code for React conf. Taking this Sandbox...
https://codesandbox.io/s/keen-bartik-udlpn?file=/src/App.js:58-341

---
This fails by loading neither hook:
```js
function Example({ defaultName = ""-"", defaultLocation = ""-"" }) {
  const [name, setName] = useState(""-"");
  const [location, setLocation] = useState(""-"");

  return (
    <div style={STYLE}>
      <div>Name: {name}</div>
      <div>Location: {location}</div>
    </div>
  );
}
```
---

Even worse, this fails by only loading ""location"" and showing it first (instead of state)
```js
function Example({ defaultName = ""-"", defaultLocation = ""-"" }) {
  //asd
  const [name, setName] = useState(""-"");
  //asd
  const [location, setLocation] = useState(""-"");

  return (
    <div style={STYLE}>
      <div>Name: {name}</div>
      <div>Location: {location}</div>
    </div>
  );
}
```

---
This ails by only loading the location hook, but at least the name shows beside the right hook:
```js
function Example() {
  const [name] = useState(""-"");
  const [location] = useState(""-"");

  return (
    <div style={STYLE}>
      <div>Name: {name}</div>
      <div>Location: {location}</div>
    </div>
  );
}
```
Interestingly, changing the order of the hooks _still_ only lets it load `location`, although it also still shows it as the second hook (bug).
```js
function Example() {
  const [location] = useState(""-"");
  const [name] = useState(""-"");

  return (
    <div style={STYLE}>
      <div>Name: {name}</div>
      <div>Location: {location}</div>
    </div>
  );
}
```

---

This format also only loads ""location"":
```js
function Example(props) {
  const stateA = useState(null);
  const name = stateA[0];

  const stateB = useState(null);
  const location = stateB[0];

  return (
    <div style={STYLE}>
      <div>Name: {name}</div>
      <div>Location: {location}</div>
    </div>
  );
```"
facebook/react,2021-10-26 16:59:53,bug,"[DevTools Bug]: ""Reload and start profiling"" button is missing on Microsoft Edge","### Website or app

Any development URL are ok

### Repro steps

1. Open the React Developer Tools extension on Chrome and navigate to the Profiler tab. Note the presence of the ""Reload and start profiling"" button.
2. Open the React Developer Tools extension on Microsoft Edge and navigate to the Profiler tab. Note the absence of this button.

I have checked #21384, and `document.featurePolicy.allowsFeature('sync-xhr')` returns `true`.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

4.20.2

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2021-10-21 02:09:18,bug,"[DevTools Bug] Cannot remove node ""264"" because no matching node was found in the Store.","### Website or app

https://codesandbox.io/

### Repro steps

i open chrome developer tools width react-developer-tool to view layout page and :v while i scroll page :v

### How often does this bug happen?

Often

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.20.0-e5f486b5a

### Error message (automated)

Cannot remove node ""264"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25851:43
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:23957:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24117:12
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:52741:41)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-10-19 22:09:48,bug,[DevTools Bug]: Loading / parsing hook names is failing on v4.20,"### Website or app

reactjs.org

### Repro steps

1. Open a website that uses React.
2. Inspect an element that uses Hooks.
3. Attempt to load hook names.
4. Loading hook names always fails:

![image](https://user-images.githubusercontent.com/1271509/137997605-a3f601a9-59e5-4370-b307-3ff50af22cae.png)
"
facebook/react,2021-10-15 20:09:41,bug,[DevTools Bug]: Firefox and Edge show error in console about unrecognized installation on v4.20.0,"### Website or app

reactjs.org

### Repro steps

1. Install React DevTools v4.20.0 in Firefox
2. Load reactjs.org in Firefox
3. Open Firefox DevTools
4. Observe error in console

![image](https://user-images.githubusercontent.com/1271509/137547605-e6ad3045-c20a-4828-9895-af46b8bb4db1.png)


### How often does this bug happen?

Every time
"
facebook/react,2021-10-14 14:11:25,bug,Standalone Devtools splash page unresponsive after client disconnects ,"### Website or app

Any client that connects to the standalone DevTools

### Repro steps

While using the standalone DevTools app:
1. Connect a client to the DevTools
2. Disconnect that client and you should be redirected to the introduction page

The event listeners are not re-attached to the splash page elements after the client disconnects. The <script> tags that can be copied on click and the ""Profiler tab"" link stop working.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2021-10-02 13:53:08,bug,"[DevTools Bug] Could not inspect element with id ""2"". Error thrown:Cached data for element ""2"" not found","### Website or app

https://public-deploy3.test-eu.tankionline.com/browser-public/index.html?config-template=https://c{server}.public-deploy3.test-eu.tankionline.com/config.xml&resources=../resources&balancer=https://balancer.public-deploy3.test-eu.tankionline.com/balancer

### Repro steps

attempting to change the line of the code which locks the store in an online game, this usually works but this time it didn't

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.18.0-f58bbcf9a

### Error message (automated)

Could not inspect element with id ""2"". Error thrown:Cached data for element ""2"" not found

### Error call stack (automated)

_No response_

### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37563:3)
    at Suspense
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36097:5)
    at div
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36542:3)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:38972:3)
    at div
    at div
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:34323:3)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:34764:3)
    at Components_Components (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40911:52)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36097:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36215:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36249:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36215:3)
    at SchedulingProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:42093:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:40537:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:29185:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:29796:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:36605:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:51354:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Could not inspect element with id . Error thrown:
Cached data for element  not found in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
"
facebook/react,2021-09-29 08:43:19,bug,"Bug: setState updater called but not rendered, in Safari, in concurrent mode","<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 18.0.0-alpha-9175f4d15-20210928

## Steps To Reproduce

Minimal reproduction in [this codesandbox](https://codesandbox.io/s/black-moon-ymw10?file=/src/index.js).

This issue only appears in Safari, including mobile Safari. It works fine in Chrome and Firefox.

In order to reproduce, you'll need to:

0. Render the app with `createRoot` in the latest 18.0 alpha
1. Within an element's `ref` function, append an iframe to that element using `appendChild`

*The following steps have been removed after simplifying the reproduction:*

~~1. Make a state change originating in a message from a YouTube embed iframe. I assume the same issue would occur with messages from other cross domain iframes, although I have not tested this. The issue does *not* appear when making an async state change through `setTimeout`.~~
~~2. Render a tooltip using the ""tippy.js"" library. I haven't dug into what inside of Tippy is causing this, but commenting out the tippy element resolves the issue. I have confirmed that rendering a simple portal does *not* reproduce the issue.~~


Link to code example:

https://codesandbox.io/s/black-moon-ymw10?file=/src/index.js

## The current behavior

In the example in safari, the updater function passed to `setState` is run (confirmed with the ""run state updater"" console.log)... but the new state is never passed to the component function, leaving the app hanging with no error message, and displaying ""not done"".

## The expected behavior

In Chrome and Firefox, the App component updates after the state updater function is called, as expected, displaying ""done"".

---

Thanks for all the wonderful work on React, btw! It's been a lot of fun playing with the new concurrent React features. Can't wait to see them hit a stable release!
"
facebook/react,2021-09-24 10:11:29,bug,Bug: Components inside typescript namespaces cause ReferenceError,"Forwarded from https://github.com/vitejs/vite/issues/3900 by @not-rusty based on the recommendation of @sodatea, which stated that this should be considered as a bug in the `react-refresh` package.
&nbsp;  

----

### Describe the bug
Apparently components inside namespaces is not supported. I don't exactly know if the error comes from esbuild or something, but it would nice.

The bug is that is not shown as a compilation error.

### Reproduction
Simply start a `react-ts` project with `$ npm init @vitejs/app my-vue-app --template react-ts` and write the following code:

```ts
namespace Lol {
  export const Lol = () ={
    return (
      <div>
        <p>Some component</p>
      </div>
    );
  };
}

function App() {
  return (
    <div className=""App"">
      <header className=""App-header"">
        <Lol.Lol />
        <p>Hello Vite + React!</p>
      </header>
    </div>
  );
}

export default App;
```

### System Info
```js
  System:
    OS: Linux 5.8 Ubuntu 20.04.2 LTS (Focal Fossa)
    CPU: (8) x64 Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz
    Memory: 440.54 MB / 7.47 GB
    Container: Yes
    Shell: 5.8 - /usr/bin/zsh
  Binaries:
    Node: 14.16.0 - /usr/local/bin/node
    npm: 6.14.13 - /usr/local/bin/npm
  Browsers:
    Chrome: 90.0.4430.93
    Firefox: 89.0.1
```

Used package manager: npm

### Logs
I get the following runtime error:

```js
App.tsx:13 Uncaught ReferenceError: _c is not defined
    at App.tsx:13
    at App.tsx:14
```

I suppose it should be a compilation error?"
facebook/react,2021-09-16 05:45:36,bug,"Error: ""Cannot read property 'length' of undefined""","Describe what you were doing when the bug occurred:
1. Just trying to profile an React Native app

![Screenshot_2021-09-16_07-44-35](https://user-images.githubusercontent.com/1957563/133556382-73ddabab-f985-4fd2-9d60-1a1438d8bd8b.png)


---------------------------------------------
Please do not remove the text below this line
---------------------------------------------

DevTools version: 4.10.1-3a8c04e3b2

Call stack: at Jc (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:342921)
    at ii (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:59391)
    at Kl (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:114564)
    at Ns (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:101189)
    at Is (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:101117)
    at Os (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:100980)
    at gs (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:24:96690)
    at w (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:40:1749)
    at MessagePort.A.port1.onmessage (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:40:2251)

Component stack: at Jc (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:341665)
    at div
    at div
    at fi (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:235808)
    at /nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:347533
    at Hi (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:249715)
    at /nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:251980
    at div
    at div
    at lu (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:305412)
    at En (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:173743)
    at jn (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:184811)
    at Ji (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:253490)
    at Nd (/nix/store/dhc43b6q83i2d842ry36ydnwck64wlyc-react-native-debugger-0.11.7/share/resources/app.asar/node_modules/react-devtools-core/dist/standalone.js:56:353738)"
facebook/react,2021-09-15 19:14:01,bug,[DevTools Bug]: hook resolution throws fetch failure,"### Website or app

n/a

### Repro steps

have a webpack project that uses a domain mapped to your local IP such as (appx.whenidev.net) in my case that's served with https
try to resolve hook names
check console and observe the million errors

https://p181.p1.n0.cdn.getcloudapp.com/items/4gulW8Wo/3bba0881-ee74-4478-8cb8-68370b878855.jpg?v=ab371b6e8fca2cee905d1b9f828ac0d8 shows my console

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2021-09-10 21:18:32,bug,Bug: Maximum call stack size exceeded (React Devtools),"I encountered the same issue as #20640 but using `react-devtools` as a stand-alone app instead of from the the browser.

The bottom line is that the profiler becomes unresponsive after any interaction with a heavily loaded react page.

React version:

* React: 17.0.2
* ReactDOM: 17.0.2
* React Devtools: 4.18.0

## Steps To Reproduce

1. Attach a react-devtools stand-alone (`yarn run react-devtools`) to a session
2. Start profiling
3. Do something on your app that would cause a huge number of updates
4. The profiler stops functioning

## Diagnostics

I actually went through and found out the origin of the issue, however I am not sure what would be the best approach to fix it.

It seems the the origin is a (very) big incoming message:

<img width=""516"" alt=""image"" src=""https://user-images.githubusercontent.com/883486/132918559-c6a3dee1-1ddd-4647-ab06-2f19892b67b0.png"">

That eventually causes a maximum call stack exceeded error:

<img width=""522"" alt=""image"" src=""https://user-images.githubusercontent.com/883486/132918578-f02f3031-b84a-4a89-84cf-4dd4d70cef7f.png"">

Coming from `util.js:128`:

```ts
export function utfDecodeString(array: Array<number>): string {
  return String.fromCodePoint(...array);
}
```

And interestingly enough, doing spread operator with an array with 235124 elements actually causes a `Maximum call stack size exceeded` error 😄 

I tried replacing the code above with the following replacement and it seems to work:

```ts
export function utfDecodeString(array: Array<number>): string {
  return array.map(c => String.fromCodePoint(c)).join("""")
}
``` "
facebook/react,2021-09-05 12:52:10,bug,"[DevTools] Support providing the number of components above selected or ""depth""","### Website or app

https://github.com/appium/appium/issues/14825

### Context

While implementing Appium e2e tests we found this limitation
https://github.com/appium/appium/issues/14825

I'm trying to figure out how to solve this and understand if my changes are reducing the number of layers above a certain component 

### How often does this bug happen?

Sometimes

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2022-05-09 12:37:52,feature,[DevTools] Manifest version 2 is deprecated,"### Website or app

https://developer.chrome.com/blog/mv2-transition/

### Repro steps

Use latest React DevTools with Electron (Chromium) (18.2.0 / Chromium 100)

```
  (node:80082) ExtensionLoadWarning: Warnings loading extension at ./node_modules/electron-devtools-vendor/extensions/react-developer-tools:
    Manifest version 2 is deprecated, and support will be removed in 2023. See https://developer.chrome.com/blog/mv2-transition/ for more details.
```

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_"
facebook/react,2020-12-13 19:26:42,feature,Bug: react-devtools not working inside react based chrome extensions,"react-devtools not working inside react based chrome extensions

React version: 17.0.1

## Steps To Reproduce

1. install react-devtools extension in chrome
2. git clone https://github.com/lxieyang/chrome-extension-boilerplate-react
3. cd into directory
4. yarn install
5. yarn build 
6. open chrome extensions page
7. change to developer mode
8. load unpacked, point to the directory build within chrome-extension-boilerplate-react directory
9. click on react dev tools extension
10. The following text is displayed ""This is a restricted browser page. React devtools cannot access this page.""
11. When inspecting the page, the tabs components and profiler are NOT shown

Link to code example: https://github.com/lxieyang/chrome-extension-boilerplate-react

## The current behavior
1. When clicking on the react devtools chrome extension: ""This is a restricted browser page. React devtools cannot access this page.""
2. Also when inspecting the pages of the chrome extension, the tabs components and profiler are NOT shown

## The expected behavior
1. When clicking on the react devtools chrome extension: The devtools recognize that the page is using react
2. when inspecting the pages of the chrome extension, the tabs components and profiler are shown and populated"
facebook/react,2020-07-06 10:17:15,feature,Suggestion: show HOC names in profiler,"(Deleted template as this is a suggestion, not a bug.)

The dev tools helpfully extracts HOC names and shows them in the components tree. [Example](https://react-devtools-tutorial.now.sh/higher-order-components):

![image](https://user-images.githubusercontent.com/921609/86582724-03ddbc00-bf7a-11ea-83fb-f9d0a3902e5f.png)

However, it doesn't give the same treatment to components in the profiler:

![image](https://user-images.githubusercontent.com/921609/86582766-135d0500-bf7a-11ea-8e2a-520597150db6.png)

In large trees, it is very confusing to see two components with the same name, so it would be useful to show the HOC name here as well.

As a workaround for now, users can click through to the ""components"" tab from the profiler, when a component is selected in the profiler flamegraph, to see this extra information."
facebook/react,2020-06-10 04:45:34,feature,Add https support to standalone DevTools,"I notice that the standalone react-devtools use http instead https like `""<script src=""http://192.1.2.3:8097""></script>""` ([src code](https://github.com/facebook/react/blob/4c7036e807fa18a3e21a5182983c7c0f05c5936e/packages/react-devtools/app.html#L186-L194))，but my website is always https, so it will get broken because I cant change it to http if it's in `iframe`(it's diffcult to change the host environment/website protocol).

I'm not familar with the react-devtools implementations, maybe something cause it can only use http to open the server, happy to hear the details, thanks!

React-devtools: 3.6.3

"
facebook/react,2020-04-22 15:00:02,feature,Improve UX of finding full `key` value,"## The current behavior

The full value of the `key` is very difficult / impossible to find and use in the interface of the React Devtools.

![Kapture 2020-04-22 at 11 47 47](https://user-images.githubusercontent.com/1935696/79997715-25222680-84ba-11ea-97ba-51f1679a8c91.gif)

Only managed to find it by accident :(

## The expected behavior

The `key` is visible in the props list to the right.

### Detailed Proposal

As mentioned below in https://github.com/facebook/react/issues/18702#issuecomment-617924196

Add a light divider and new section in the props panel to the right.

Potentially also add a question mark that shows an explanation about the fact that things in this section are not really props.

Ref (Original implementation): https://github.com/facebook/react-devtools/pull/328"
facebook/react,2020-04-06 12:08:53,feature,Expose API like `createStyles` for converting style object to CSS string,"## Feature Request

provide API on react-dom to convert style object to CSS string. the API could be used to build dynamic CSS easily.

### Approach 1

```js
import {createCSS} from 'react-dom'

const inlineStyleString = createCSS({
  overflow: 'hidden', 
  display: '-webkit-box',
  WebkitLineClamp: 2,
})

return <style>{`.clamp-text { ${inlineStyleString} }`}</style>
```

### Approach 2

Another approach is to only map the key and value from the original style object, and let user play with it.

```js
import {createStyles} from 'react-dom'

// return a object with key-value pairs of css rules
const cssStyleObject = createCSS({
  WebkitTransform: 'scale(2)',
})
// return { '-webkit-transform': 'scale(2)'  }

const inlineStyleString = Object.keys(cssStyleObject).reduce((serialized, key) => {
  serialized += `${key}: ${cssStyleObject[key]};`
  return serialized
}, '')

return <style>{`.clamp-text { ${inlineStyleString} }`}</style>
```


## Why

See other react styling library like **radium**, the way to build css is quite similar with react inline style if they need anything aligned with react like browser prefix such as `Webkit` or detect the unit less number for some special rule such as `line-height`, they have to re-implement the logic.

the style object in react is quite convenient, but it's can only used for inline style. hope react team could consider to expose it in the react-dom. might not the origin function name, but the same functionality."
facebook/react,2020-01-29 21:21:56,feature,Profiler should highlight host components (e.g. DOM elements) on mouseover,"Feature request from a DevTools user at Faceook:
> If I mouse over a node in the flame graph if you can highlight it in the view like the inspector that would be really amazing!"
facebook/react,2019-11-07 01:41:48,feature,Adding visible state to Suspense Fallback component to enhance CSS transitions.,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Feature I suppose.

**What is the current behavior?**
Suspense fallback component flashes on and immediately unmounts (flashes off) when fetching is complete. See similar discussions [here](https://stackoverflow.com/questions/57404653/react-suspense-prevent-flashing-of-fallback-spinner) and [here](https://stackoverflow.com/questions/54158994/react-suspense-lazy-delay).

**What is the expected behavior?**
It would be ideal to add better transitions to the fallback component when it mounts and unmounts, but doing so requires using something like [TransitionGroup](https://reactcommunity.org/react-transition-group/transition-group) or [Framer Motion](https://www.framer.com/api/motion/animate-presence/), which require a prop to listen to know when to mount and unmount. If we could somehow have the fallback component receive some kind of state from Suspense on when it is mounting and unmounting the fallback component, that would be great.
"
facebook/react,2019-10-10 05:08:30,feature,Make it easier to debug (undefined components) in production,"**Do you want to request a *feature* or report a *bug*?** Feature

**What is the current behavior?**

So for whatever reason, terser/minification causes a bug where one of my components is undefined during render. What I get is the standard production mode minified error.

For some reason this time, I can't seem to do *any* of the following:

1. See any stack information for the component (React just says ""something somewhere is undefined"")
2. Set a breakpoint on the error point (for some reason with webpack + devtool sourcemap, chrome isn't letting me do a mid-line breakpoint at any place above the error)
3. Disable Reacts error catching temporarily so I can pause on the actual error
4. Use a development version of React with any ease but with the prod settings (I tried turning off both process.env.NODE_ENV checks but then you get an error `It is not supported to run the profiling version of a renderer (for example, react-dom/profiling) without also replacing the scheduler/tracing)`

A big upgrade here would be to fix all of these (except 2, which is either a Webpack or Chrome bug). Can we get better stacks in production mode? That's the ideal. That with number 3 would be the most helpful: a query like `?disableNiceErrors=true` that prevents React from catching/re-throwing the error later would make it so much easier.

As it is now, it's incredibly painful to debug (already an hour into it and without the breakpoints working on minified React it's hard to really even figure out where besides manual code commenting).

Edit: A fifth would be source maps for react itself in production bundles which may work."
facebook/react,2019-10-08 07:34:07,feature,"Add ""search"" functionality to Profiler graphs","<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
feature
**What is the current behavior?**
it‘s hard for me to find a component in the profiler tab
**What is the expected behavior?**
add a search function,just like components tab

"
facebook/react,2019-09-27 12:02:35,feature,react-devtools: tiny feature request (copy to clipboard related),"Hey guys, great job with the new devtools 💯 perhaps this request can be put in the backlog for a future release as I think it could be quite useful and could save a few steps for developers. 

**What is the current behavior?**
- Copying data to clipboard stringifies all key-value pairs of an object
- When the keys' values happen to be objects or arrays, the values are given in constructor form rather than seeing the contents of that object/array

This is what was copied:
<img width=""369"" alt=""Screen Shot 2019-09-27 at 12 24 19"" src=""https://user-images.githubusercontent.com/30755017/65767498-f4496100-e125-11e9-8f8b-3faa4516d0c2.png"">

This is a pasted version in VSCode:
<img width=""399"" alt=""Screen Shot 2019-09-27 at 12 24 48"" src=""https://user-images.githubusercontent.com/30755017/65767486-ee538000-e125-11e9-9886-7dcd165c19db.png"">

**What is the desired behavior?**
- When I click on ""Copy to clipboard"" in react-devtools, my copied object's data _keys_ and _values_ are in an unstringified format, or at least, I am presented the option to have this copied in an unstringified format
- The _values_ of each key is readable, e.g: if the value is an object, I can see the expanded object and all its key-value pairs clearly as shown here (this is logged into Chrome console from react-devtools)
<img width=""311"" alt=""Screen Shot 2019-09-27 at 12 29 11"" src=""https://user-images.githubusercontent.com/30755017/65767365-99176e80-e125-11e9-98d3-98fd9c4889f9.png"">

"
facebook/react,2019-08-22 16:46:40,feature,Devtools: Impossible to debug firefox webextension moz-extension: pages due to strict CSP,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

* feature request
* Original issue is: https://github.com/facebook/react-devtools/issues/922
   * There are more details.
* Mozilla's Bugzilla: https://bugzilla.mozilla.org/show_bug.cgi?id=1573027

**What is the current behavior?**

The react-devtools toolbar button does not light up and clicking on it says ""This page does not appear to be using react"". This is probably since extensions are not allowed to inject scripts into other extensions' pages.

**What is the expected behavior?**

The react-devtools toolbar button should light up and the addon should be able to debug the page

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

This is still reproducible

* Firefox v68~
* react devtools v4.0.5
* react v16.9
"
facebook/react,2019-08-22 12:29:14,feature,DevTools: Ability to save inspected values as global variable (as it was in previous version),"Hi, I really like new dev tools (4.0.5), but I would like to request one useful feature.

In previous version it was possible to save inspected value (prop/state/context) and its parts as a global variable using context menu->store as global variable, so it can be easily be accessed through console using `$tmp{n}`. In current version this is behaviour was replaced be creating new ""bug"" button which will just print all values in console. Unfortunately it's hard to navigate through this object, because $_ in console will return undefined. And in order to access it you need to expand group, find desired property and open context menu->store as global variable. 
It would be perfect if you combine these 2 approaches so it would be possible to both print values using ""bug"" button and opening context menu directly in react dev tools panel wihtout need of intermediate step. 

Thanks! 

"
facebook/react,2019-08-16 20:16:56,feature,New React Developer Tools does not clearly indicate empty object or array,"**Do you want to request a *feature* or report a *bug*?**

Bug/unexpected behavior.

**What is the current behavior?**

When an object or array is empty, there's no arrow to expand and see that it's empty, nor is there an `(empty)` indication. Initially, I was concerned that I couldn't expand any object or array from the new React DevTools due to this.

![Screen Shot 2019-08-16 at 3 11 35 PM](https://user-images.githubusercontent.com/11951801/63195539-7aa75900-c038-11e9-95fe-4754f7d14693.png)

**What is the expected behavior?**

I would expect to either be able to expand the empty object, or to see `(empty)` next to the non-expandable object.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Chrome version: 76.0.3809.100 (Official Build) (64-bit)
React Developer Tools Version: 4.0.2 (8/15/2019)

[Reference discussion on Twitter](https://twitter.com/taniarascia/status/1162441422496325633)
"
facebook/react,2019-08-16 15:42:33,feature,New React DevTools can't access immutable.js objects?,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
When the state or props are formed by Immutable.js objects, react devtools cannot expand it nor copy to temporal variable anymore.
![Screenshot from 2019-08-16 16-18-09](https://user-images.githubusercontent.com/28344917/63174215-c90d2580-c041-11e9-847c-7bbd153399f9.png)

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

https://codesandbox.io/s/withered-cherry-h3dfh

**What is the expected behavior?**
Be able to inspect the value of the immutable object or at least, copy it into a temporal variable.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16.8.6
Chrome Version 70.0.3538.77 (Official Build) (64-bit)"
facebook/react,2019-05-29 17:56:31,feature,DevTools in production environment ,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->
![2019-05-29_13-00](https://user-images.githubusercontent.com/33522235/58580030-d3254200-8211-11e9-882d-85da2664a3b2.png)


**Do you want to request a *feature* or report a *bug*?**
I think the production environment, React Devtools you should not show any information about the state or components. similar to redux Devtools. 
**What is the current behavior?**
Just now I can Edit some information with React DevTools in the production environment

"
facebook/react,2019-04-18 09:59:04,feature,act() should warn in testing frameworks besides jest ,"**Do you want to request a *feature* or report a *bug*?**

feature request

**What is the current behavior?**

The warnings for missing act() warnings around updates only happen in jest. 

**What is the expected behavior?**

We should support other test runners/frameworks as well (like jasmine, karma, etc)

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.8.0+"
facebook/react,2019-04-09 15:36:48,feature,allow to disable style hydration missmatch warning,"some background:

when dealing with inline styles (e.g. radium), SSR and caching of SSR results, you might run into problems because of differences in vendor prefixes. Best tradeof is to render always with all vendor prefixes on the server. But this will lead to style missmatches on hydration. You can use `suppressHydrationWarning`, but then you have to add this property to every element that receives these styles, which is not practical.

**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

style missmatch causes a warning on development. 

**What is the expected behavior?**

you can set a global flag to supress style missmatch warnings

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

React 16.8"
facebook/react,2019-04-01 04:57:23,feature,useRef eslint rule proposal,"Proposal: Ensure that all reads from a `ref` use `.current`.

```js
export function MyComponent() {
  const isActiveRef = useRef<boolean>(false);

  // Proposal: this should be a linting violation
  if (isActiveRef) {
    console.log('will always be true');
  }

  // Reads need to be done from .current
  if (isActiveRef.current) {
    console.log('correct usage');
  }
}
```

I often find myself doing boolean checks based on the `.current` value of a `ref`. I am paranoid that if I leave the `.current` off then I am creating a bug"
facebook/react,2019-03-27 02:29:09,feature,Hook equivalent for `getSnapshotBeforeUpdate`,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

There is no hook based equivalent for `getSnapshotBeforeUpdate`. The docs state:
> Our goal is for Hooks to cover all use cases for classes as soon as possible. There are no Hook equivalents to the uncommon getSnapshotBeforeUpdate and componentDidCatch lifecycles yet, but we plan to add them soon.

**What is the expected behavior?**

There is a hook based equivalent for `getSnapshotBeforeUpdate`, maybe something like:

```js
function ScrollingList(props) {
  const lengthRef = React.useRef(0);
  const listRef = React.useRef(null);

  const prevHeight = React.useSnapshot(() => {
    if (lengthRef.current < props.list.length) {
      const list = listRef.current;
      return list.scrollHeight - list.scrollTop;
    }
  }); 

  React.useEffect(() => {
    lengthRef.current = props.list.length;
  }, [props.list.length]);

  React.useEffect(() => {
    if (prevHeight != null) {
      const list = listRef.current;
      list.scrollTop = list.scrollHeight - prevHeight;
    }
  }, [prevHeight]);
  return (
    <div ref={listRef}>{/* ... */}</div>
  );
}
```

This code probably is bug-ridden and not the best use of hooks but you get the idea.

I’d like to know:
1. If this feature is planned or on the roadmap.
2. What the proposed API will be.
3. If anyone is working on this.

Sorry, if this is being tracked somewhere and I haven’t seen it. I’m planning an intense component which will use `getSnapshotBeforeUpdate` and I’d love some guidance about the future of this lifecycle method.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

React v16.8.0 and later. All browsers and OSes.

"
facebook/react,2019-03-21 04:21:58,feature,[eslint-plugin-react-hooks] Add option to require functions from core hooks in dependencies,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

The `react-hooks/exhaustive-deps` will currently accept either (and fix neither) of the following:

```javascript
// Scenario A: Function returned by core hook IS NOT specified as a dependency.
const [value, setValue] = useState(initialValue);
const toggle = useCallback(() => setValue(v => !v), []);
```

```javascript
// Scenario B: Function returned by core hook IS specified as a dependency.
const [value, setValue] = useState(initialValue);
const toggle = useCallback(() => setValue(v => !v), [setValue]);
```

**What is the expected behavior?**

If we add the following to our `.eslintrc.js`

```
// Introduces a `requireCoreFunctions` config option.
'react-hooks/exhaustive-deps': ['error', { requireCoreFunctions: true }]
```
then the rule should fail in scenario A, and fixing should result in scenario B.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

n/a"
facebook/react,2019-03-11 02:26:13,feature,eslint-react-hooks: should we enforce to use React.useMemo if there is no state hooks?,"1. Regarding the performance optimization, should we always use React.useMemo in case there is no state hooks inside a function component?
2. If the answer is yes, can we use some eslint rules to make sure everyone do it?"
facebook/react,2019-02-25 22:20:17,feature,Collapse forwardRef and other wrappers in React error stacks,"**Feature**

**What is the current behavior?**

With a UI kit that uses forwardRefs, I get error messages like this:

![image](https://user-images.githubusercontent.com/12100/53372321-61ceae00-3907-11e9-9d20-190315cc7f06.png)

**What is the expected behavior?**

It would look a lot better if forwardRefs were a bit less obtrusive, and if they picked up the functions displayName rather than just the name of the function they wrap.

In our UI Kit we have a pattern like:

```
const View = forwardRef(function UIView(){ return <div /> })
View.displayName = 'SomeDisplayName'
```

For some reason it's not showing that, just showing the inner one. Further the ""bigness"" of ForwardRef() makes it hard to see visually when scanning. Perhaps something more like this would help:

```
Warning: Encountered two children with the same key, `confluence`. Keys should be unique so that components maintain their identity across updates. Non-unique keys may cause children to be duplicated and/or omitted — the behavior is unsupported and could change in a future version.
    in div (created by ForwardRef(Gloss))
    in Gloss (ForwardRef)
    in Gloss (ForwardRef) (created by OnboardMain)
    in div (created by Gloss (ForwardRef))
    in Gloss (ForwardRef)
    in Gloss (ForwardRef) (created by SliderPane)
```

Further, the `created by` information is often more useful to me, but it's never aligned nicely. Could do something like:

```
Warning: Encountered two children with the same key, `confluence`. Keys should be unique so that components maintain their identity across updates. Non-unique keys may cause children to be duplicated and/or omitted — the behavior is unsupported and could change in a future version.
      in div (created by ForwardRef(Gloss))
      in Gloss (ForwardRef)
    from OnboardMain:
      in Gloss (ForwardRef)
   from Gloss (ForwardRef):
      in div
      in Gloss (ForwardRef)
   from SliderPane:
      in Gloss (ForwardRef)
```

All together, if it would pick up displayNames, the stack would be far more readable for me:


```
Warning: Encountered two children with the same key, `confluence`. Keys should be unique so that components maintain their identity across updates. Non-unique keys may cause children to be duplicated and/or omitted — the behavior is unsupported and could change in a future version.
      in div (created by Row)
      in View (forwardRef)
    from OnboardMain:
      in Row (forwardRef)
   from Col (forwardRef):
      in div
      in Grid (forwardRef)
   from SliderPane:
      in View (forwardRef)
```
"
facebook/react,2019-02-09 08:44:57,feature,Shallow renderer does not support React.memo,"**Do you want to request a *feature* or report a *bug*?**
bug

**What is the current behavior?**
In normal rendering, you can memoize a class-based component: https://jsfiddle.net/586ea3cx/

With the shallow renderer, it seems like you can't: https://jsfiddle.net/odj217Lv/1/

This might be blocking https://github.com/airbnb/enzyme/pull/1914, in which tests are failing with `Cannot call a class as a function`. (it's tough to repro stuff with the shallow renderer)"
facebook/react,2019-02-03 02:57:23,feature,Simultaneous key events in effect handled out of order,"**Do you want to request a *feature* or report a *bug*?**

Report a bug.

**What is the current behavior?**

I have an app that's registering event listeners for `window`'s key events (via `useEffect`). Those event listeners are triggering state updates (via `useState`). I think I have found a bug where simultaneous key events occurring in the same frame (whether down or up) will be handled out of order, causing state to becoming out of sync.

Take the following simple app (https://codesandbox.io/s/1z3v9zrk4j). I've kept this as keyup only for simplicity.

```
function App() {
  const [keys, setKeys] = useState([]);

  console.log('App', keys);

  const onKeyUp = function (event) {
    console.log('onKeyUp', event.key, keys);

    setKeys([...keys, event.key]);
  };

  useEffect(function () {
    console.log('effect', keys);

    window.addEventListener('keyup', onKeyUp);

    return function () {
      console.log('removing event listener', keys);

      window.removeEventListener('keyup', onKeyUp);
    };
  });

  return <p>{keys.join(', ')}</p>;
}
```

If I press down any two keys, e.g. the ""q"" and ""w"" keys, and then release them at precisely the same time, the following happens:

- The `keyup` event listener for `w` is called, which in turn calls `setKeys` with `['w']`
- `App` is re-rendered with `keys === ['w']`
- The `keyup` event listener for `q` is called, which in turn calls `setKeys` with `['q']`
- The effect's cleanup function is called, removing the event listener with `keys === []`
- The effect is run again, the event listener being added with `keys === ['w']`
- `App` is re-rendered with `keys === ['q']`
- The effect's cleanup function is called, removing the event listener with `keys ===['w']`
- The effect is run again, the event listener being added with `keys === ['q']`

This results in `keys === ['q']`. The render with `w` has been lost.

With three keys, only two keys are reliably shown. Four keys - only two are reliably shown.

If I add another `useState` call, the first `useState` has no issues - all keys are reliably detected. See https://codesandbox.io/s/0yo51n5wv:

```
function App() {
  const [keys, setKeys] = useState([]); 
  const [dummy, setDummy] = useState('foo');

  console.log(""rendering App"", keys);

  const onKeyUp = function(event) {
    console.log(""onKeyUp event received"", event.key, keys);

    setKeys([...keys, event.key]);
    setDummy('foo');
  };

  useEffect(function() {
    console.log(""adding event listener"", keys);

    window.addEventListener(""keyup"", onKeyUp);

    return function() {
      console.log(""removing event listener"", keys);

      window.removeEventListener(""keyup"", onKeyUp);
    };
  });

  return (
    <div>
      <p>Keyups received:</p>
      <p>{keys.join("", "")}</p>
      <button onClick={() => setKeys([])}>Reset</button>
    </div>
  );
}
```

**What is the expected behavior?**

I would expect the final state array to contain all keys released, in order. There are a few workarounds for this issue (e.g. passing a function to `setState` to retrieve the current value instead of using the rendered value), but from the documentation it seems that is an escape hatch for use when the effect's callback is not renewed on each state change, and should not be necessary in this case (unless I've misunderstood).

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

It happens on both versions that support hooks - `16.8.0-alpha.0` and `16.8.0-alpha.1`. This is on Chrome/Safari/Firefox on MacOS Mojave.
"
facebook/react,2019-01-30 10:43:37,feature,Allow the same DOM node to use both a callback and a RefObject in its ref prop,"**Do you want to request a *feature* or report a *bug*?** Feature

**What is the current behavior?**

The `ref` attribute passed to a DOM node can be either a callback or a `RefObject`, but not both. Sometimes, that's exactly what's required: for example, a library like [react-pose](https://popmotion.io/pose/api/posed/#posed-usage-create-a-posed-component-existing-components) demands ref forwarding to work with a React Component, but you'd also like to retain a reference to the _same_ parent DOM node within that component itself for a different reason. It's often not possible to nest DOM nodes to achieve a similar thing using two different ref attributes as that breaks layout.

Here's a link to a naive attempt to achieve this: https://codesandbox.io/s/4jyw3q3v57

I'm not surprised this doesn't work as there's no reason for the parent ref callback to fire, but I don't know how else to go about it.

**What is the expected behavior?**

The callback provides the component with its own reference to the parent DOM node, whilst also providing it to the parent component via the passed `RefObject`.
"
facebook/react,2019-01-05 22:14:39,feature,React.Suspense provide a lifecycle so components can handle the `display:none` removal,"**Do you want to request a *feature* or report a *bug*?**

It's a feature. 

**What is the current behavior?**

React.Suspense mounts its children with a `display: none` style if a promise is thrown. Once the thrown promise is resolved, React removes the `display: none` style.

**What is the expected behavior?**

The children components have no easy way to know when the `display: none` style is removed by React. This is problematic when one child component needs to read from the DOM layout to correctly display its elements. Most people wait for the `componentDidMount` callback to trigger, but because the element is `display: none`, it can't read any value from the DOM layout.

The issue was discovered in https://github.com/mui-org/material-ui/issues/14077. I believe that React should provide a lifecycle so the children components know when they are visible, that it's safe to do layout computations. 

The best workaround I'm aware of it to use the [Intersection Observer API](https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API) but it requires a polyfill on IE 11 and Safari.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Version: 16.7.0-alpha.2"
facebook/react,2018-12-15 20:58:16,feature,[Hooks] Proposal: expose info about current component for custom hooks,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
The core built-in hooks – `useRef`, `useMemo`, etc. – rely on internal React state which is not exposed, namely the current rendering component. This means that these hooks cannot be implemented in user-land. Similar hooks cannot be implemented either.

I'm proposing adding a built-in hook which would provide information about the current rendering component. This would enable more advanced hooks to be implementable in user-land.

I have a use case which is a perfect example. I need a hook similar to `useMemo`, but where the computed value is shared across all components of the same type. I’m calling it `useSharedMemo`.

The ideal API would look something like this:

```typescript
const Component = ({cacheKey}) => {
  const value1 = useSharedMemo(() => /* expensive computation 1 */, [cacheKey]);
  const value2 = useSharedMemo(() => /* expensive computation 2 */, [cacheKey]);
  return <>{value1} {value2}</>;
};
```

In the code above, every instance of `Component` would get the same values for `value1` and `value2`, provided `cacheKey` is the same. When `cacheKey` changes, the two values would recompute once and the new values would be returned to all instances as they re-render. (The actual use case in my app is styles that need to update only when the theme changes.)

I have a [hacky implementation](https://codesandbox.io/s/jplyz2pkr9) of this hook that works, but it requires changing the API to the following:

```typescript
const useSharedMemo = createUseSharedMemo();

const Component = ({cacheKey}) => {
  const ref = React.useRef();
  const value1 = useSharedMemo(ref, () => /* expensive computation 1 */, [cacheKey]);
  const value2 = useSharedMemo(ref, () => /* expensive computation 2 */, [cacheKey]);
  return <>{value1} {value2}</>;
};
```

For this to work, the [implementation](https://codesandbox.io/s/jplyz2pkr9) has to keep a counter of calls that resets every time a component’s render call starts or ends. This would be trivial if there were a way to know which component is currently rendering. Since there is no way, the implementation has to make up for it by requiring a `ref` be passed in. Since it has no information about the type of the current component, it also requires that a `useSharedMemo` “instance” be created in the component definition’s enclosing scope.

Worse, the implementation uses `useLayoutEffect` to detect when the render is done, which might break with concurrent mode or with future React changes.

**What is the expected behavior?**

If React provided information about the current rendering component, the implementation of `useSharedMemo` would be much easier and less brittle.

A possible solution is a hook like the following:

```typescript
const [currentType, currentRef] = React.useCurrentComponent();
```

With this information, we can implement the ideal API above and we do not have to rely on `useLayoutEffect`:

```typescript
let values = new WeakMap();
let cacheKeys = new WeakMap();
let lastRef = null;
let callIndex = 0;

function useSharedMemo(fn, keys) {
  const [currentType, currentRef] = React.useCurrentComponent();
  if (currentRef !== lastRef) {
    callIndex = 0;
  }
  const index = callIndex;
  callIndex++;
  const typeValues = values.get(currentType) || [];
  const typeCacheKeys = cacheKeys.get(currentType) || [];
  if (!typeValues[index] || !compareKeys(keys, typeCacheKeys[index])) {
    typeValues[index] = fn();
    typeCacheKeys[index] = keys;
    values.set(currentType, typeValues);
    cacheKeys.set(currentType, typeCacheKeys);
  }
  return typeValues[index];
}
```

(Note that I’m treating `currentType` and `currentRef` as opaque values, so for my purposes it doesn’t matter if they are the actual type and an actual ref to the component instance. I imagine having them be accurate would be a more powerful API, but the implementation might require them to be opaque values.)

P.S. - A common use case that would benefit from `useSharedMemo` is `useCallback`. 99% of the time callbacks are identical across components of the same type. It’s wasteful not to share the cache.

**Demo**
https://codesandbox.io/s/jplyz2pkr9
"
facebook/react,2018-12-03 01:04:57,feature,[Scheduler] Add support for delayed scheduling of callbacks.,"Has the React team considered adding the ability to specify a time delay when scheduling callbacks on Scheduler. This would be useful to enable using Scheduler as a general scheduling solution in a JS environment, removing the need to use and manage setTimeouts/setInterval calls. "
facebook/react,2018-11-20 13:47:08,feature,Provide a way to pass context to renderToStaticMarkup on the client,See https://github.com/facebook/react/issues/14287#issuecomment-440277999 and https://github.com/facebook/react/pull/14182#issuecomment-440125029. This accidentally worked for a few releases but was a bug. However we might want to consider actually supporting this with an opt-in API.
facebook/react,2018-11-16 18:02:00,feature,React.lazy does not allow retrying a rejected promise,"**Do you want to request a *feature* or report a *bug*?**

It can be seen as a feature or a bug, depending on angle. Let's say it's an enhancement to how `lazy` works.

**What is the current behavior?**

When using `React.lazy`, if the given promise rejects while trying to asynchronously load a component, it's no longer possible to retry loading the component chunk because `lazy` internally caches the promise rejection.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

This does not seem to work great in CodeSandbox because it's using service workers, which get in the way when simulating offline mode, yet this small app illustrates the issue: https://codesandbox.io/s/v8921j642l

**What is the expected behavior?**

A promise rejection should not be cached by `lazy` and another attempt to render the component should call the function again, giving it the chance to return a new promise.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

AFAIK all version of React that include `lazy`.
"
facebook/react,2018-11-13 17:57:45,feature,setState/dispatch 2nd arg callback or emitEffect() use case,"# Feature Request

## Current Behavior

useState/useReducers Hook's updater/dispatch functions do not expose a way to execute code _after_ the update has been made.

## Expected Behavior

useState or useReducer Hooks expose some way to locally execute the equivalent of inline useEffect/didUpdate etc. (to allow for overriding default side effect behavior) (e.g. emitEffect(() => ...)).

## Use Case

Formik exposes 2 props which control when form validation occurs: `validateOnChange` and `validateOnBlur`. When `validateOnChange` set to `true`, form validation will run whenever `handleChange`, `setFieldValue`, or `setValues` are called (these fns all update form `values`). Similarly, when `validateOnBlur` is `true`, validation will also run whenever `handleBlur`, `setFieldTouched`, and `setTouched` are called (these fns all update the `touched` state of the form). The reason that Formik doesn't centralize orchestrating validation logic into `componentDidUpdate` (i.e. run validation whenever either `this.state.values` or `this.state.touched` changes) is to allow for local overrides in custom input components.

For example, often times in a 3rd party input component (e.g. like Airbnb's Rheostat (https://github.com/airbnb/rheostat)) the value and touched state need to update together because there isn't a real ""blur"" event for the component or there isn't a prop for it. With Formik, you get around this by updating the field's value and touched state imperatively but override the validation behavior to only run once. To do this, Formik's current non-hooks API exposes an extra parameter to the `setFieldValue` and `setFieldTouched` methods which allow you to opt out of running validation after the update is made. Internally this looks like:

```js
setFieldValue(name, value, shouldValidate = true) {
   this.setState(prevState => setIn(prevState.values, name, value), () => {
      if (this.props.validateOnChange && shouldValidate) {
        this.validateForm(this.state.values)
      }
    })
  }

setFieldTouched(name, touch = true, shouldValidate = true) {
   this.setState(prevState => setIn(prevState.touched, name, touch), () => {
      if (this.props.validateOnBlur && shouldValidate) {
        this.validateForm(this.state.values)
      }
    })
  }
```

If a callback was supported by either `useReducer`'s `dispatch` or `useState` update fn, Formik could maintain its current API footprint and allow for the following code to work as expected:

```js
const CustomRangedInput = (props) => {
  const formik = useFormikContext();
  function handleChangeValue(value) {
    // Set the value
    formik.setFieldValue(props.name, value, false /* avoid normal validation logic */)
    // Mark the field as touched
    formik.setFieldTouched(props.name, true)
  }

   return <Rheostat
     onValueChange={handleChangeValue}
      min={1}
      max={100}
      values={[1, 100]}
   />;
}
``` 

However, with the current hooks API's there is not a way for me to expose this to users, since I have to lift update the validateOnChange/validateOnBlur validation logic to `useEffect`. In my current Formik x Hooks PR, this looks like

```js
 React.useEffect(
    () => {
      if (!!didMount.current && !!validateOnChange && !state.isSubmitting) {
          validateForm(state.values);
      }
    },
    [state.values, validateOnChange, state.isSubmitting]
  );

 React.useEffect(
    () => {
      if (!!didMount.current && !!validateOnBlur && !state.isSubmitting) {
          validateForm(state.values);
      }
    },
    [state.touched, validateOnBlur, state.isSubmitting]
  );

```

This will execute whenever `state.values` or `state.touched` change, which is usually what people want 98% of the time. However, I don't know/see a way to make this compatible with the old API (where you can locally override validation).


**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.7.0-alpha
"
facebook/react,2018-11-02 08:27:15,feature,<option> and <textarea> elements should be able to contain Components that return strings and render their output correctly,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Feature request

**What is the current behavior?**

If you use a Component that returns a string inside an option or textarea element, the Component will be rendered as '[object Object]'

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

https://jsfiddle.net/38tz0ym2/

**What is the expected behavior?**

The string returned by the component should be rendered as the elements innerHTML.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

This was working accidentally in react v16.0.0 - v16.4.2 but was apparently buggy and crash prone. It no longer works at all in react v16.6.0"
facebook/react,2018-10-26 03:24:35,feature,"More helpful interaction for ""React does not recognize the 'propName' prop on a DOM Element""","**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

When you pass a unknown prop to a DOM element - common when you pass `{...props}` to something that turns out to be a div/span/any other DOM element rather than a composite component - you get a warning like so:

```Warning: React does not recognize the `propName` prop on a DOM element. ....```

If the tree is pretty complicated, especially if you're using HOCs, it can be very hard to find where this prop has been passed and to which DOM element. 

**What is the expected behavior?**

We had a quick muck around with react-dom and logging the DOM element that triggers this warning allows you to see the element in the DOM and makes it much easier to work out where the prop is being accidentally passed. You can even use the react dev tools to work out exactly which line the component is defined in the code.

Simply logging the element is obviously not the most elegant way of showing the user where the mistake is, but would it be possible to do _something_ in order to make it quicker to fix mistaken prop passing like this?

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

All."
facebook/react,2018-10-11 13:14:18,feature,Support for classList,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

Only `className` exists right now.

**What is the expected behavior?**

I think adding `classList` property for DOM elements would be useful. As far as I understand React Fire will drop support for IE11 but even in IE11, you can add or remove classList from elements.

Another idea might be adding support for for array type for `className` (or future `class`, which will make more sense because array). If array is passed, `classList` will be used for DOM elements. Otherwise, `className` is used as usual in the real DOM side.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

All versions. I hope this is added in React Fire."
facebook/react,2018-09-21 01:39:46,feature,Accessing ReactDebugCurrentFrame without using __SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,"**Do you want to request a *feature* or report a *bug*?**
Request a feature.

**What is the current behavior?**
I am working on writing a custom React renderer (https://github.com/toxicFork/react-three-renderer-fiber) and right now I have to use `React.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED` in order to access stack information for the current frame in order to display warnings for the developer (https://github.com/Methuselah96/react-three-renderer-fiber/blob/react-reconciler/src/core/customRenderer/descriptors/CustomDescriptor.ts#L102).

**What is the expected behavior?**
I would like to be able to access the stack information for the current frame without worrying for the safety of my employment. Is there a better way for me to be doing this?

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16.5.2"
facebook/react,2018-09-03 16:20:27,feature,Make possible to get component stack or at least its hash,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

*feature*

**What is the current behavior?**

There is no good way to get the a list of component parents. The only way to do it is to wrap the rendered component to an error boundary, throw a dummy error and pass `componentStack` to the rendered component without throwing the error again.

![image](https://user-images.githubusercontent.com/1082083/44995658-cb9b7a00-afac-11e8-83da-6b60f6edb6e6.png)

Unfortunately the idea of hiding thrown errors was refused (see https://github.com/facebook/react/issues/11098) so this experimental hack isn't useful. 

**What is the expected behavior?**

I'd like to deterministically identify components at DOM tree and use this data to assign persistent data to it without using any custom identifiers. For example store visibility state for a specific component at window.localStorage. 

Related to https://github.com/facebook/react/issues/1137
"
facebook/react,2018-08-16 01:41:06,feature,KeyboardEvent.repeat is not normalized,"**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
Documentation of SyntheticEvent claims: ""React normalizes events so that they have consistent properties across different browsers."" https://reactjs.org/docs/events.html#supported-events

Documentation of Keyboard Events lists `boolean repeat` as a supported field: https://reactjs.org/docs/events.html#keyboard-events

IE11/Edge do not natively support `repeat`, but React does not normalize the event to set `repeat: true` when a keyDown event repeats (i.e. when a key is held down). (Edge has an open bug on this but of course IE11 is abandonware.)

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem.**

Tab to to the only div in this repro and hold down a key:
https://jsfiddle.net/acsr4ofu/

Bug: 'repeat!' alert does not appear in IE11

**What is the expected behavior?**
An alert dialog showing 'repeat!' should appear in any browser that React supports.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
Current version of React, any browser which does not natively support `KeyboardEvent.repeat` but IE/Edge in particular (Chrome always supported; FF since 28; Safari since 10.1). Unknown if this worked in previous versions of React."
facebook/react,2018-08-13 12:34:23,feature,React.cloneElement cannot remove existing props,"**Do you want to request a *feature* or report a *bug*?**

A feature.

**What is the current behavior?**

When an element is cloned with [React.cloneElement](https://reactjs.org/docs/react-api.html#cloneelement), it's possible to add new props or modify existing ones, but not to remove existing props. 

[Relevant code](https://github.com/facebook/react/blob/v16.8.0-alpha.1/packages/react/src/ReactElement.js#L333).

Example of how it works right now:

```
const element = React.createElement(""a"", {href: ""http://github.com""});
const newElement = React.cloneElement(element, {href: undefined});
console.log(newElement.props); // {href: undefined}
```

**What is the desired behavior?**

It would be great to add some way to remove props (passing `undefined` as value?):

```
const element = React.createElement(""a"", {href: ""http://github.com""});
const newElement = React.cloneElement(element, {href: undefined});
console.log(newElement.props); // {}
```

I guess I could use directly `React.createElement` but, AFAIK, I'll have also to worry about special attributes like `key` and `ref`. I'd rather not mess with internals.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I think it has worked this way in all React versions.

**What's your use case?**

I am applying a map transformation of elements recursively and I need to remove some virtual props before passing the real elements for React to render.  Console shows `React does not recognize the [unknownProp] prop on a DOM element` for those props, I'd want to avoid that."
facebook/react,2018-07-30 03:24:48,feature,Setting rendering mode to <Surface> component of react-art,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
`<Surface>` of canvas mode and `<Surface>` of svg mode cannot be used in a document at the same time.

**What is the expected behavior?**
Two more `<Surface>` components with each other modes can be used in a document at the same time.


Currently developers can set only one rending mode globally in react-art, using `setCurrent()` of `art/modes/current` module. So there is no way to use `<Surface>` of canvas mode and `<Surface>` of svg mode in a document together. On the other hand, `art.js` internally used in `react-art` provides a way to use each other modes together as directly importing their corresponding modules.  (e.g. svg.js for svg mode, canvas.js for canvas mode in `art.js` lib). 

I would like to suggest a way to set own rendering mode per instance of `<Surface>` Component by new property `mode`. This feature makes `<Surface>` with canvas mode and `<Surface>` with svg mode to be used together in a document like the following.

```javascript
const {
  Surface,
  Group,
  Shape
} = require('react-art');

class TestComponent extends React.Component {
  render() {
    return (
      <React.Fragment>
        <Surface mode=""svg"" width={100} height={100}>
          <Group>
            <Shape width={10} height={10} />
          </Group>
        </Surface>
        <Surface mode=""canvas"" width={100} height={100}>
          <Group>
            <Shape width={10} height={10} />
          </Group>
        </Surface>
      </React.Fragment>
    );
  }
};
```

If `mode` property is not set, `<Surface>` works as canvas mode for compatibility.

I have made a PR for this feature to show how to work and use it ([#13249](https://github.com/facebook/react/pull/13249)).
And you can check this feature through `art` fixture of my PR.

Feel free to discuss about this feature.



"
facebook/react,2018-06-21 12:57:54,feature,Some means of determining component order in hierarchy,"I apologise for raising this issue again, I'm not intending to be annoying or disrespectful by this, but my previous issue #13034 was closed with a suggestion which I believe not does mitigate the feature request.  I'm simply not aware if I should take the hint and go away, if there's a bit of a backlog and I should hang in there, or if it's been missed.  I'm assuming the latter, but beg forgiveness if that is not the case.  Should you want me to go away I will do so :)  Thanks.

I'm trying to build a container component and child component, whereby the children can sit anywhere in the hierarchy beneath the container, but know their relative order/index within that hierarchy.

My use-case is to build a helper wrapper for CSS grids, allowing subcomponents to themselves render a ""row"" component which knows it must be the next index, and may or may not progress the row counter for the next ""row"" component found in order.

I've looked into two possible avenues - recursing using React.Children on the component, which stops when it hits a component without props.children (e.g. a redux-connect()-ed one), and passing some means of counting via context, which fails because it seems the render() methods of the child components isn't always called in ""DOM order"".

More background here - https://stackoverflow.com/questions/50776933/react-get-component-order-within-hierarchy

It would be great to have some feature in React which might allow for this.

@aweary had previously suggested in #13034 that context could do this with nesting, however I'd raised the thought that this would give an indication of recursion depth, not relative position."
facebook/react,2018-05-31 14:31:19,feature,Allow `ref` attribute on custom elements,"**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
Creating a custom element with a `ref` attribute currently is not possible, because `React.createElement('my-element', { ref: 'my-attribute' })` will interprete the `ref` attribute in a special sense.

**What is the expected behavior?**
Like for `htmlFor` it would be great if there were an alias that allows creating a `ref` attribute on a custom element.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
16.3"
facebook/react,2018-05-22 17:47:53,feature,ReactIs.typeOf for non-elements,"**What is the current behavior?**

`ReactIs.typeOf` currently only works for element types, it does not allow you to pass a raw `Component` or `ForwardRef` to know the type.

The use case for this is in `hoist-non-react-statics` I now need a [special cases for `ForwardRefs`](https://github.com/mridgway/hoist-non-react-statics/issues/48). To do this, I will need to know the type of the source and target components, but currently I would need to turn them into elements first.

All of the `ReactIs.is*` functions also have this issue since they use the `typeOf` function internally.

```js
const ForwardComponent = React.forwardRef(() => {});

ReactIs.typeOf(ForwardComponent); // undefined
ReactIs.typeOf(React.createElement(ForwardComponent)); // Symbol(react.forward_ref)
```

**What is the expected behavior?**

Ideally I could pass in just the Component and get the type of it: 
```js
ReactIs.typeOf(ForwardComponent) // Symbol(react.forward_ref)
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

ReactIs@16.3.2
"
facebook/react,2018-05-16 03:52:24,feature,Callback in react-test-renderer for component changes,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
Many react components render multiple times automatically.  Take a graphql component.  First it will render a loading spinner, then the data after it has been fetched.  In react-test-renderer, it's difficult to get a snapshot to consistently render for these types of components.

Currently, we set an interval timer and check the component tree to see if the data has loaded and thus rendered.  This approach results in a bunch of unnecessary checks and slows down tests because the interval timer will run after the component has been rendered.

**What is the expected behavior?**
I would expect react-test-render to allow a callback that will be invoked each time any component in the component tree changes, basically after any component's `componentDidUpdate` ran.  I would expect an API like:
```
import TestRenderer from 'react-test-renderer';
const testRenderer = TestRenderer.create(
  <Todos />
);
testRenderer.onChange(() => {
// check TODOS for whether the data has loaded and do the snapshot
})
```

This would be useful in react-dom and react-native.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
It's a proposal for a new feature, doesn't affect any existing React versions

"
facebook/react,2018-05-07 05:18:39,feature,Expose a way of creating a ReactTestInstance for React DOM nodes,"**Do you want to request a *feature* or report a *bug*?**
I would like to request a new feature. 

**What is the current behavior?**
At the moment there is no way to create an instance of a `ReactTestInstance`. I would love to be able to do something like:

```js
const div = document.createElement('div');
const componentRef = ReactDOM.render(<MyComponentUnderTest />, div); // or renderIntoDocument
const testInstance = new ReactTestInstance(componentRef);
// now I can run queries using the API of ReactTestInstance
// against a component tree that has been fully DOM rendered
testInstance.findAll(..)
testInstance.children.forEach(() => {});
testInstance.parent
```

**Why?**
Libraries such as [Enzyme](https://github.com/airbnb/enzyme) can be used to test React components by [full DOM rendering](http://airbnb.io/enzyme/docs/api/mount.html) and providing an API to find components and get information about them.

As far as I know, there is no way of querying the component tree created by `ReactDOM.render` without relying on the internals of React nodes. This means that Enzyme, in order to support full DOM rendering and it's querying API, it has to interact with React nodes directly. This reliance causes problems in Enzyme whenever React adds a new node type (forwardRef, ContextProvider/Consumer for example).

I have started [this RFC](https://github.com/airbnb/enzyme/issues/1648) that proposes that Enzyme uses `ReactTestWrapper` from `react-test-renderer` as a layer on top of React node objects. This allows the library to be decoupled from the internals of React. The solution proposed in the RFC relies on the being able to create a `ReactTestInstance` from a `ReactDOM.render` component tree.

Please let me know if this is something you would consider 😄 "
facebook/react,2018-04-25 11:23:02,feature,Show culprit in `Cannot update during an existing state transition...` ?,"**Do you want to request a *feature* or report a *bug*?**
*Feature.*

**What is the current behavior?**
We are probably all familiar with the following message.

```
bundle.js:1

Warning: Cannot update during an existing state transition (such as within `render`
or another component's constructor). Render methods should be a pure function
of props and state; constructor side-effects are an anti-pattern, but can be moved
to `componentWillMount`.
```

For those who never came across this message, it can be created simply by mounting the following component.

```js
class UpdateDuringRender extends Component {
  constructor(props) {
    super(props);
    this.state = {
      text: 'initial',
    };
  }

  updateState(text) {
    this.setState({ text });
  }

  render() {
    this.updateState('updated!');

    return (
      <div>
        {this.state.text}
      </div>
    );
  }
}
```

**What is the expected behavior?**
Can we be more specific which component triggers this warning (I think it could be even treated as an optional error)? When this suddenly appears in the app, sometimes it's tough to find the culprit. Some stack trace or at least the offending component would be helpful in the warning message. Not sure how it's solved internally, but yeah, would be a nice hint to help remove anti-patterns.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
`""react"": ""^16.3.1""`"
facebook/react,2018-04-15 23:54:01,feature,Support partial hydration for static content,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

Hydrating some server-rendered content can be difficult, inefficient or impossible. For example, in the process of rendering on the server, significant work or additional data may be required for data processing and conversion, such as custom templating or localization. The content can be large too, such as product information or a news article.
 
When the resulting content is highly dynamic and changes with state, there is no choice but recreate it within React paradigm and recreate it on client. However, complicated server-generated content is often (if not typically) static. Delivering a redundant copy of static content to client just to compare and ignore it during hydration seems a waste of resources and can be prohibitively expensive. 

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

https://codesandbox.io/s/zx38ow3z8x

**What is the expected behavior?**

Instead of additional complications of recreating it on the client with hydration, wouldn’t it be much easier to just accept the content from server as-is and tell hydrate() to leave it alone?

There may be a few options for non-hydrating SSR:

1. Add a new callback `shouldComponentHydrate()` to disable hydration of component content
```javascript
export default class NoHydrate extends Component {
    // return false to avoid re-rendering of this component in hydrate()
    shouldComponentHydrate() {
        return false;
    }
    render() {
        // on server, simply render content
        // on client, this is never called and server content is accepted as-is
        return (
            <div>
                {this.props.children}
            </div>
        );
    }
}

```

2. Access SSR content from DOM in `render()`
_This is probably the worst option, though it is the only one that definitely works currently._
```javascript
export default class AutoHydrate extends Component {
    render() {
        // on server, simply render content
        // on client, find SSR in dom and re-render using dangerouslySetInnerHTML
        // ** requires a unique id, generated before or during server rendering **
        return (typeof window === 'undefined') ? (
            <div id={this.props.id}>
                {this.props.children}
            </div
        ) : (
            <div id={this.props.id}
                dangerouslySetInnerHTML={{
                    __html: document.getElementById(this.props.id).innerHTML
                }}
            />
        );
    }
}
```

3. Use `dangerouslySetInnerHTML` with empty content
_It actually works now, but it is not documented that it is supposed to._
```javascript
export default class Ssr extends Component {
    render() {
        // on server, simply render content
        // on client, render empty content using dangerouslySetInnerHTML,
        // which normally causes a warning of content mismatch and keeps the existing content
        // also add suppressHydrationWarning to turn off the warning.
        return (typeof window === 'undefined') ? (
            <div>
                {this.props.children}
            </div>
        ) : (
            <div
                dangerouslySetInnerHTML={{
                    __html: ''
                }}
                suppressHydrationWarning
            />
        );
    }
}
```

Considering that there is a way to make it work now, documenting (3) may be all that needs to happen. However if (1) could be added with same behavior, it would look cleaner.


**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Version 16.2. Not sure if the working option 3 has worked before or is supposed to work in future versions."
facebook/react,2018-03-30 22:23:24,feature,Uncaught Error: Unexpected object passed to ReactTestInstance constructor (tag: 13). This is probably a bug in React.,"Hello,

I'm testing v16.3.0 and I'm getting this warning:
```
Warning: Detected multiple renderers concurrently rendering the same context provider. This is currently unsupported 
```

 My use case is that I wanted to render React Test Renderer (for ex) from a Component in an another (ReactDOM) renderer but it does not seem to work - I wanted to use same Provider in both containers.

I thought `React.createContext` create objects with no state and could be shared across renderers / Fiber containers.. Is there workaround for this? Are there any thoughts on this, Is this a definite behaviour?

Following the warning, I eventually get the error message which I believe is related to the warning:

```
Uncaught Error: Unexpected object passed to ReactTestInstance constructor (tag: 13). This is probably a bug in React.
```

thank you for any guidance :)"
facebook/react,2018-03-29 08:35:33,feature,[SVG] Enable focusable to accept boolean values,"Moved discussion from https://github.com/facebook/react/issues/6212.

## Situation

The [`focusable` attribute](https://www.w3.org/TR/SVGTiny12/interact.html#focusable-attr) from the SVG specifications is an [enumerated attribute](https://html.spec.whatwg.org/#keywords-and-enumerated-attributes) accepting values `""true""`, `""false""` and `""auto""`. Because it is technically not a boolean attribute (although it certainly somehow looks like it), React expects the value to be passed as a string. See the following example:

```diff
-<svg focusable>I should be focusable</svg>
-<svg focusable={true}>I should be focusable</svg>
+<svg focusable='true'>I should be focusable</svg>
```

The thing is, the `focusable` attribute is often used in conjunction with elements from the ARIA specification, in which attributes are booleans and not enumerated attributes with `""true""` and `""false""` values. The [`aria-hidden` attribute](https://www.w3.org/TR/wai-aria-1.1/#aria-hidden) is a good example of that.

For instance, [following a good practice for icon-buttons](https://fvsch.com/code/svg-icons/#section-html):

```html
<button type=""button"">
  <svg aria-hidden=""true"" focusable=""false"">
    <use xlink:href=""#icon-play""></use>
  </svg>
  <span class=""access-label"">Start playback</span>
</button>
```

From an authoring perspective, the above snippet would likely be written like this in JSX:

```jsx
<button type='button'>
  <Icon icon='play' aria-hidden={true} focusable={false} />
  <span class='access-label'>Start playback</span>
</button>
```

The problem is that `focusable` **cannot** be authored as a boolean, otherwise it will **not** be printed out in the DOM. On the other hand, `aria-hidden` is perfectly fine being written as a boolean at it gets coerced by React.

## Proposal

Given the default value for the `focusable` attribute is `""auto""`, this is very likely this attribute gets authored to change its value to `true` or `false`. In that regard, it is confusing that it has to be specified as a string, when other attributes accepting booleans can be authored as such.

The suggestion would be to make it possible for `focusable` to be specified as either a boolean or a string, like other similar attributes. In other words, all the following should work:

```jsx
<svg focusable>I should be focusable</svg>
<svg focusable={true}>I should be focusable</svg>
<svg focusable='true'>I should be focusable</svg>
<svg focusable={false}>I should not be focusable</svg>
<svg focusable='false'>I should not be focusable</svg>
<svg focusable='auto'>I should be focusable</svg>
```

From an authoring perspective, I believe this would be the most straightforward and less confusing."
facebook/react,2018-03-26 05:23:57,feature,Elements loose focus when moving to or from a portal,"**Do you want to request a *feature* or report a *bug*?**

Perhaps a bug, but could also be seen as a feature

**What is the current behavior?**

When you move an element to a portal through `ReactDOM.createPortal` the element looses focus if it had focus. If the element gains focus while in a portal, when moving out of the portal it looses focus.

**What is the expected behavior?**

I would have expected React to maintain focus of the element when moving in or out of a portal

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Tested with

React version: 16.2
Browsers: Latest chrome and firefox. I did not test others but expect the behaviour to be the same
OS: Tested on Mac High Sierra
"
facebook/react,2018-02-28 16:13:56,feature,Expose the `onSelect` event for any focusable element?,"I was curious earlier if the `onSelect` event works on things other than form elements and `contentEditable` elements, and by disabling a couple of conditions in the code, it seems — at first brush, at least — that `onSelect` works splendidly on any element that is focusable (via the `tabIndex` attribute), to obtain things like:

![ezgif-2-326cf42421](https://user-images.githubusercontent.com/205375/36798178-8aa7fdda-1cb2-11e8-8838-46e04300a09a.gif)

Would there be any problem with relaxing the conditions for allowing `onSelect` to include elements that have a `tabIndex` attribute? (Other than the case where existing implementations in userland rely on the event being disabled when `contenteditable = true` is removed from an element)."
facebook/react,2018-02-19 09:59:43,feature,Support for changing a Portal's container without remounting children?,"(This is related to https://github.com/facebook/react/issues/3965.)

I'm working on a project with a lot of globally unique component instances. (In other words, their `key`s are essentially database IDs.) It also has DnD functionality. For reordering, everything is fine, but moving an instance from one parent to another causes a complete re-render of the instance.

Instead of moving the nodes around myself, I was thinking of using Portals. Each instance has a prop for which element ID they should render inside. Then, to reparent them, it'd be a simple case of passing a different element ID. (I'm using Redux, so it'd just be a `string` in the state.)

However, changing a Portal's container node also causes a complete re-render of everything passed to `ReactDOM.createPortal`. (See this [CodePen](https://codepen.io/anon/pen/PQQKRP).)

Would it be possible to skip this re-rendering and effectively move the Portal contents instead?"
facebook/react,2018-02-15 23:59:23,feature,Context Transform,"With the new context API it is really bulky to create a middle man that consumes one context value, transforms it and provides another one. You have to create many components and store an intermediate state to avoid rerendering the provider if the input is unchanged.

We could have a convenience API for this use case.

```js
function transform(inputValue) {
  return [...inputValue, extraData];
}

<Context.Middleware transform={transform}>{children}</Context.Middleware>
```

cc @acdlite "
facebook/react,2018-02-09 03:30:16,feature,[FEATURE] implement a `getState` method to get the sync state since `setState` is async.,"
**Do you want to request a *feature* or report a *bug*?**
<h2>FEATURE REQUEST</h2>



**What is the current behavior?**
```js
class MyComponent extends Component {
  state = { a: 1 }
  method1 = () => {
    this.setState({ a: 2 });
    this.method2();
  }
  method2 = () => {
    // It can not log 2 in the console.
    console.log(this.state.a);
  }
  // xxx...
}
```

**What is the expected behavior?**
```js
class MyComponent extends Component {
  state = { a: 1 }
  method1 = () => {
    this.setState({ a: 2 });
    this.method2();
  }
  method2 = () => {
    // With the `getState` method, it should log 2 in the console.
    console.log(this.getState().a);
  }
  // xxx...
}
```
"
facebook/react,2018-01-23 23:51:58,feature,Feature request: middleware,"This is an incomplete draft for a feature I think could be
really cool. It can replace higher order components
and context in a way I think is more in the component
spirit of React.

I do not know if this feature is feasible or desirable for
React, especially as it would lead to a bigger API surface.
The proposal is written as if it was documentation to
give a feel for how it would be to use it.

# About React middleware
A middleware is applied somewhere in the component tree
and are instantiated just after child components are
instantiated and just before they mount. In this context,
child components means child components at any depth.

Middleware is used just like normal components, but
it works slightly differently. When a middleware element
is used it added to the middleware stack. If it is
already on the middleware stack, it removed from
the stack and pushed to the end, with the most
innermost props.

## Simplified example
In addition to the actual classes, the stack also
includes its most recent props. But this is roughly
how it works.

```javascript
<MiddlewareA>
  {/* middleware stack for ""A"": [MiddlewareA] */}
  <A />
  <MiddlewareB>
    {/* middleware stack for ""B"": [MiddlewareA, MiddlewareB] */}
    <B>
      <MiddlewareA
        {/* middleware stack for ""C"": [MiddlewareB, MiddlewareA] */}
        <C />
      </MiddlewareA>
    </B>
  </MiddlewareB>
</MiddlewareB>
```

# Lifecycle methods

## Additions to the existing lifecycle methods

### Mounting
- new: Middleware.shouldMiddlewareMount
- new: Middleware.shouldMiddlewarePropagate
- Component#constructor
- new: Middleware#constructor
- new: Middleware#middlewareWillMount()
- Component#componentWillMount()
- Component#render()
- new: Middleware#interceptRender()
- Component#componentDidMount

### Unmounting
- new: Middleware#middlewareWillUnmount

## static shouldMiddlewareMount(ReactComponent)
Determine if the current middleware should apply for
a component. If the method isn't implemented, the
middleware will always be applied.

If a middleware is on the middleware stack, this method
is called every time a component is constructed.

### Example

```javascript
class TransformInlineStyles extends React.Middleware {

  /**
   * Only mount middleware when you set
   * transformInlineStyles to a truthy
   * value. Children of the given
   * component can still enable
   * the middleware
   */
  static shouldMiddlewareMount(Component) {
    return Component.transformInlineStyles
  }

  // ...
}

const A = props => (
  // ...
)

const B = props => (
  // ...
)
B.transformInlineStyles = true

const App = () => (
  <TransformInlineStyles>
    {/* Not applied to ""A"" */}
    <A>
      {/* Applied to ""B"" */}
      <B />
    </A>
  </TransformInlineStyles>
)
```

## static shouldMiddlewarePropagate(ReactComponent)
Determine whether the middleware should remain on
the middleware stack or be excluded for the subtree
below the given component. If not specified it returns
false, in other words: The default behavior for
middleware is to propagate.

This is useful if you want to limit middleware from
affecting deeply nested children. It is also useful
for only giving middleware access to its immediate
children.

### Example

```javascript
import React from 'react'

class ProvideTheme extends React.Middleware {
  static StopPropagation = props => props.children

  static shouldMiddlewarePropagate(Component) {
    return Component !== this.StopPropagation
  }

  // ...
}

const App = () => (
  <ProvideTheme>
    {/* middleware stack for ""A"": [ProvideTheme] */}
    <A>
      <ProvideTheme.StopPropagation>
        {/* middleware stack for ""B"": [] */}
        <B />
      </ProvideTheme.StopPropagation>
      {/* middleware stack for ""C"": [ProvideTheme] */}
      <C />
    </A>
  </ProvideTheme>
)
```

## middlewareWillMount(reactInstance)
Called before the child component calls componentWillMount.
This is a good place to initialize state for the middleware
instance.

## MiddlewareWillUnmount(reactInstance)
Called before the child component calls componentWillUnmount.

### Example
This is a naïve example of how it could be used to trigger
automatic updates with Mobx.

```javascript
class Observer extends React.Middleware {
  middlewareWillMount(reactInstance) {
    this.dispose = autorun(() => {

      // Let Mobx track the observables
      // used in the render method.
      reactInstance.render()

      // Force update the component instance
      // after Mobx has stopped tracking the
      // autorun function.
      //
      // .. yes, I know it's hacky.
      setTimeout(() => {
        reactInstance.forceUpdate()
      })
    })
  }

  middlewareWillUnmount(reactInstance) {
    // Stop listening for changes from Mobx
    this.dispose()
  }
}
```

## interceptRender(children)
interceptRender is called with the result from the render
function of the component. The resulting value is what is
used to render the DOM.

### Example
This is an example of a middleware that transforms object
classes into a string. The result works similarly to how
ng-class works in AngularJS.

```javascript
class ObjectClassNames extends React.Middleware {

  /**
   * This is a life cycle method.
   *
   * Intercept the render method and recursively
   * loop through all children, performing
   * this.transformProps() on their props.
   */
  interceptRender(children) {
    return React.Children.map(children, child => {
      if (! React.isValidElement(child)) {
        return child
      }
      return {
        ...child,
        props: this.transformProps(child.props),
        children: this.interceptRender(child.children)
      }
    })
  }

  /**
   * If a child has an object className, call
   * this.transformClassname() on it.
   */
  transformProps(props) {
    if (! props || ! props.className || typeof props.className !== 'object') {
      return props
    }

    return {
      ...props,
      className: this.transformClassname(props.className)
    }
  }

  /**
   * Concatenate the truthy keys of the className
   * object into a string.
   */
  transformClassname(className) {
    const result = []

    for (const key of Object.keys(className)) {
      if (className[key]) {
        result.push(key)
      }
    }

    return result.join(' ')
  }
}

const Widget = (props) => (
  <div className={{ 'Widget': true, 'Widget--active': props.active }}>
    Some widget
  </div>
)

const App = () => (
  <ObjectClassNames>
    <Widget active={true} />
  </ObjectClassNames>
)
```

# Why middleware?
React middleware can replace two problematic patterns used with React.

## Context
The h2 on context in the React docs says ""Why Not To Use
Context"". Context is however a very useful feature. And
people have been and will continue to use and abuse it
in the forseeable future. React Router has started
abusing context in its most recent version, which shows
that there is clearly a need here.

With middleware, as I propose it, you would be able to
inject props into an arbitrary subtree of your app. This
has performance implications, but would be an ideal
scenario for libraries such as React Router, as the
relevant props (or as it is now, context) rarely
changes. With middleware shouldComponentUpdate
will still function like you would expect.

## Higher order components
A primal rule of programming is DRY. When using Mobx
with React, you must use the observer decorator on
all reactive classes. This isn't a really big deal,
but not having to include that would reduce the size
of every single observer component by two lines and
most importantly, I wouldn't forget it.

When creating a higher order component static properties
are no longer available. The package hoist-non-react-static
is designed so that you should be able to access static
properties of higher order components transparently.
If a static property is initialized in the lifecycle
methods of a component, it will however not be proxied.

Creating higher order components is also a messy affair.

With middleware you could achive the same thing in a React
way. To replace connect from react-redux you could set
shouldMiddlewarePropagate to return false, and it would
affect only one component.

Alternatively you could use static properties for
mapStateToProps and mapDispatchToProps."
facebook/react,2018-01-18 08:02:52,feature,Add React.isFragment api for verifying Fragment,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

I want to request a feature.

**What is the current behavior?**

We have no API to verify a ReactNode is a React 16 Fragment. Though now we can use `React.isValidElement(instance) && typeof instance.type === 'symbol'` to distinguish it. It's verbose and seems uncertianly right.

**What is the expected behavior?**

Add api:

```
React.isFragment(object)
```
Verifies the object is a React.Fragment. Returns true or false.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

React 16.2"
facebook/react,2018-01-12 10:23:26,feature,React.Children.toArray and React.cloneElement do not work with portal elements,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

BUG or undefined behaviour

**What is the current behavior?**

Doing
```
React.Children.toArray(
  ReactDOM.createPortal(...)
)
```

fails with:
```
Objects are not valid as a React child (found: object with keys {$$typeof, key, children, containerInfo, implementation}). If you meant to render a collection of children, use an array instead.
```

Namely, the following complete snippet fails:

```jsx
import React from 'react';
import { render, createPortal } from 'react-dom';

const RenderChildren = ({ children }) => {
  children = React.Children.toArray(children)
  return <h1>Renders children with toArray: {children}</h1>
}


const App = () => ( 
  <RenderChildren name=""CodeSandbox"">
    {createPortal(<div>rendered in portal</div>, document.getElementById('portal'))}
  </RenderChildren>
);

render(<App />, document.getElementById('root'));
```

while the following one, which wraps the portal in another element works just fine

```jsx
import React from 'react';
import { render, createPortal } from 'react-dom';

const RenderChildren = ({ children }) => {
  children = React.Children.toArray(children)
  return <h1>Renders children with toArray: {children}</h1>
}


const App = () => ( 
  <RenderChildren name=""CodeSandbox"">
    <div>
        {createPortal(<div>rendered in portal</div>, document.getElementById('portal'))}
    </div>
  </RenderChildren>
);

render(<App />, document.getElementById('root'));

```
**What is the expected behavior?**
I am aware that `createPortal` is a new feature, but in the best case scenario it should be possible to use it everywhere other valid nodes are accepted.

The same thing is happening for `React.cloneElement(ReactDOM.createPortal(....))` - it's probably weird to try and clone a portal 😄  - but maybe we should specify in the `createPortal` documentation that it cannot be cloned, at least for now. Should I open a PR for that?

Let me know your thoughts

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I'm using React 16.*"
facebook/react,2018-01-08 02:13:55,feature,Lifecycle methods for ReactDOM.hydrate,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

There is no way to distinguish if a `componentDidMount` or `componentWillMount` lifecycle method was called in response to a call to `ReactDOM.hydrate`.

**What is the expected behavior?**

I have a component that scrolls to the top of the page in componentDidMount. This makes sense when the component is first created within the client. However, it doesn't make sense when the component has been hydrated, as `componentDidMount` is called after the content is already visible, and possibly after the user has already scrolled.

Would it be possible to add a `componentWillHydrate` lifecycle method? Then I could do something like this to achieve the desired behavior:

```js
componentWillHydrate() {
  this.hydrated = true 
}
componentDidMount() {
  if (!this.hydrated) {
    this.scrollToTop()
  }
}
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

All"
facebook/react,2017-12-20 04:38:28,feature,Include component props into the stack from componentDidCatch(),"Second argument of `componentDidCatch` contains componentStack like this:
```
{""componentStack"":""
 in _Header (created by onlyUpdateForKeys(_Header))
 in onlyUpdateForKeys(_Header) (created by TDSection)
 in TDSection (created by Container)
 in Container (created by AutoLookup)
 in AutoLookup (created by AutoLookup)
 in AutoLookup (created by GroupContainer)
 in GroupContainer (created by Container)
 in Container (created by AutoLookup)
 in AutoLookup (created by _Content)
 in div (created by _Content)
 in _Content (created by pure(_Content))
 in pure(_Content) (created by ContentList)
 in ContentList (created by TabContainer)
 in TabContainer (created by Container)
 in Container (created by AutoLookup)
 in AutoLookup (created by AutoLookup)
 in AutoLookup (created by CardBase)
 in div (created by SharedStorage)
 in SharedStorage (created by CardBase)
 in TDValidation (created by CardBase)
 in CardBase (created by Wrapper)
 in Wrapper (created by Connect(Wrapper))
 in Connect(Wrapper) (created by mapProps(Connect(Wrapper)))
 in mapProps(Connect(Wrapper)) (created by Connect(mapProps(Connect(Wrapper))))
 in Connect(mapProps(Connect(Wrapper))) (created by getContext(Connect(mapProps(Connect(Wrapper)))))
 in getContext(Connect(mapProps(Connect(Wrapper)))) (created by Card)
 in Card (created by branch(Card))
 in branch(Card) (created by Connect(branch(Card)))
 in Connect(branch(Card)) (created by getContext(Connect(branch(Card))))
 in getContext(Connect(branch(Card))) (created by Connect(getContext(Connect(branch(Card)))))
 in Connect(getContext(Connect(branch(Card)))) (created by getContext(Connect(getContext(Connect(branch(Card))))))
 in getContext(Connect(getContext(Connect(branch(Card))))) (created by mapProps(getContext(Connect(getContext(Connect(branch(Card)))))))
 in mapProps(getContext(Connect(getContext(Connect(branch(Card)))))) (created by Connect(mapProps(getContext(Connect(getContext(Connect(branch(Card))))))))
 in Connect(mapProps(getContext(Connect(getContext(Connect(branch(Card))))))) (created by getContext(Connect(mapProps(getContext(Connect(getContext(Connect(branch(Card)))))))))
 in getContext(Connect(mapProps(getContext(Connect(getContext(Connect(branch(Card)))))))) (created by _Tab)
 in div (created by LoadingWrapper)
 in div (created by LoadingWrapper)
 in LoadingWrapper (created by Connect(LoadingWrapper))
 in Connect(LoadingWrapper) (created by _Tab)
 in div (created by _Tab)
 in _Tab (created by Connect(_Tab))
 in Connect(_Tab) (created by withContext(Connect(_Tab)))
 in withContext(Connect(_Tab)) (created by Page)
 in Page (created by Connect(Page))
 in Connect(Page) (created by onlyUpdateForKeys(Connect(Page)))
 in onlyUpdateForKeys(Connect(Page)) (created by ContentRouter)
 in ContentRouter (created by _Content)
 in _Content (created by Connect(_Content))
 in Connect(_Content) (created by _SplitPane)
 in div (created by ContentWrapper)
 in div (created by ContentWrapper)
 in ContentWrapper (created by _SplitPane)
 in div (created by _SplitPane)
 in _SplitPane (created by Connect(_SplitPane))
 in Connect(_SplitPane) (created by _MasterLayout)
 in div (created by _MasterLayout)
 in div (created by _MasterLayout)
 in _MasterLayout (created by Connect(_MasterLayout))
 in Connect(_MasterLayout) (created by Root)
 in div (created by ThemeChanger)
 in ThemeChanger (created by Connect(ThemeChanger))
 in Connect(ThemeChanger) (created by Root)
 in StartupSync (created by Connect(StartupSync))
 in Connect(StartupSync) (created by Root)
 in _default (created by Root)
 in Provider (created by Root)
 in Root
 in AppContainer""}
```
This data don't contain any helpful error data if user send me ticket to bugtracker. Can info contain props of every component?
"
facebook/react,2017-12-08 19:45:41,feature,Enable synchronously toggling experimental addUserTimingListener,"Currently ReactInternals.addUserTimingListener enables logging to the User Timing API inside a `setTimeout`. 
However, for the use case of building a profiling tool where instrumentation is turned off most 
of the time to avoid overhead, then randomly turned on for a period to take a sample, it would be ideal to be able to enable this logging synchronously (eg. turn it on at the start of a callstack and turn it off again at the end of the synchronous stack).

This is complicated by the fact that the enabled state of the logging should not be toggled inside a React callstack. As the profiler would have control over when the toggling happens, I think it would be fine to throw an error/return false/something like that if the logging was toggled while inside a React stack, allowing the profiler to try again later.

In fact, if the toggling was synchronous the profiler cp avoid this error by doing the toggling at the start and end of an event handler/setTimeout/reqAnimFrame etc which would ensure that we are not inside synchronous call into React (though I'm not quite sure what the implications of Fiber are here; would React async rendering mean that we could re-enter from the event loop at a time when this toggling should not happen?). 

Alternatively the toggling could happen 'async but just later in the same React stack' (like setState). In this case the code requesting the toggling would probably need some kind of callback to know that toggling happened so it can keep track of whether React's logging is on or off. However this variant seems like it would be more complicated to implement, and for our use case I don't think it's any more useful than having the toggling just fail in a recoverable way.

cc @gaearon "
facebook/react,2017-12-05 02:11:42,feature,smarter autoFocus,"I believe autoFocus should be improved for the better DX. Browser-native autoFocus is nice and all but does not work when a component is updated. Sure, that's fine. But it's pretty common to need focus after a component has been updated. Yep, declaratively. How? Immutable data ftw. At least, it works for me.

I use this wrapper component https://github.com/este/este/blob/master/components/AutoFocus.js

Check componentDidUpdate method. I hope it makes sense. 
"
facebook/react,2017-11-17 16:51:33,feature,Add a <ReactDOM.Portal /> element,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

I’d like to request a feature.

**What is the current behavior?**

To create a portal, you currently have to use a function:

```jsx
function MyComponent(props) {
  return <Foo>
    ...
    {ReactDOM.createPortal(<Bar>
      ...
    </Bar>, myElement)}
  </Foo>
}
```

**What is the expected behavior?**

```jsx
function MyComponent(props) {
  return <Foo>
    ...
    <ReactDOM.Portal target={myElement}>
      <Bar>...</Bar>
    </ReactDOM.Portal>
  </Foo>
}
```
"
facebook/react,2017-11-13 03:54:44,feature,Make React resilient to DOM mutations from Google Translate,"## Coming from search? See workaround here: https://github.com/facebook/react/issues/11538#issuecomment-417504600. And star this issue: https://bugs.chromium.org/p/chromium/issues/detail?id=872770.

**Do you want to request a *feature* or report a *bug*?**

Bug, though there's a decent chance it's a Chrome/Google Translate one

**What is the current behavior?**

When using Google Translate on a page using React 16, a certain code pattern produces a Javascript error (`Failed to execute 'removeChild' on 'Node': The node to be removed is not a child of this node.`) when the rendered content changes.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem via https://jsfiddle.net or similar (template for React 16: https://jsfiddle.net/Luktwrdm/, template for React 15: https://jsfiddle.net/hmbg7e9w/).**

(This has only been checked on macOS 10.13.1)
1. Navigate to https://qq49kwjynj.codesandbox.io/ in a Chrome browser set to some language other than Japanese.
2. Right click the page and select ""Translate to English""
3. Click the checkbox, and the error will show.

The source of the example can be found at https://codesandbox.io/s/qq49kwjynj
The part of the code that seems to cause it is the following two lines:
```js
{this.state.checked && ""選択済み""}
{!this.state.checked && ""無選択""}
```
Changing this to the following fixes the behavior with Google Translate:
```js
{this.state.checked ? ""選択済み"" : ""無選択""}
```

**What is the expected behavior?**

It should not produce an error.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I created an identical example with React 15 at the following pages:
https://p93xxmr0rq.codesandbox.io/
https://codesandbox.io/s/p93xxmr0rq
When repeating the same steps outlined above, no error was produced.
It only seems to affect React 16.
As this is a Chrome-only feature, it only affects Chrome."
facebook/react,2017-10-23 15:35:57,feature,Support onEnd SVG event,"Hi,
I'm trying to use the \\<animateTransform\\> Element in a React project. But I'm not able to use the ""onend"" attribute:

> Warning: Unknown event handler property `onend`. It will be ignored.

Is there a workaround for this ?"
facebook/react,2017-10-22 12:09:58,feature,HTML5 Templates apart from JSX for Non Programmatic situations,"Just before I have my view, please take a look at these 2 examples

Hello World React => [App.jsx](https://git.io/vdNQb)
Hello World Vue => [App.vue](https://git.io/vdNQN)

Identical? Isn't it?
Yes these are two different **paradigm** but hey they are expressing same things and both are doing it in the same way by taking benefits of the Virtual DOM 

My Feature Request is that,
Just like Vue supports [JSX](https://vuejs.org/v2/guide/render-function.html) for Programmatic situations.
I think React should also support HTML5 Templates for Non Programmatic situations?? 

Also, **Imagine trying to create a website based on an existing theme that you purchased. Changing it over to JSX literally will feel like sticking a knife through your eyeballs. With Templates, existing HTML just works.**

Please also note that Programmatic situations have a low use case
and Non Programmatic situations have a high use case."
facebook/react,2017-10-17 23:57:26,feature,New way to bind event handler function,"Currently there is no way to use event handlers in functional components without performance degradation e.g - unnecessary function recreations through .bind or arrow functions 
```
function Node({node}){
  return (
    <div>
      <button onClick={()=>node.parent.removeChild(node)}>remove</button>
      <div>{node.text}</div>
   </div>
  )
}
```
In this case on each render a new handler will be created, and also react will need to perform some bookkeeping - remove previous handler from dom element and add new handler (ok, with event delegation system react will not touch dom elements and only replace handler somewhere in internal structure but what about events which don't bubble?) and all this take some noticeable time in my application. 
Sure I can change to class components and solve problem by bind handlers only once when component will be created but what if I want to use functional components? 
So in my application I came up with new and fastest method of binding handlers. Actually it does not preform binding at all). What is the reason of binding function in event handler? - we need to access current component props or current component state. Is there another way to get props or state of component? React does not describe this in docs but yes - we can access to props or state of current component without any unnecessary functions recreations on each render.
```
function Node({node}){
  return (
    <div>
      <button onClick={onClick}>remove</button>
      <div>{node.text}</div>
   </div>
  )
}

function onClick(e){
  const {node} = getProps(e);
  node.parent.removeChild(node)
}

function getProps(e){
  return e.target[Object.keys(e.target).filter(k => k.indexOf('__reactInternalInstance') !== -1)[0]]._debugOwner.memoizedProps;
} 
```
and this demo https://codesandbox.io/s/4r59w89lox

To perform event delegations in an efficient way react need to assign current vdom-element to each rendered dom element. And vdom-element has link to actual owner component where we can get our props and state. So why react concealed this from developers and made them suffer not only from performance degradation but also from choosing problem (how many articles and advices recommend or investigate this rule to not bind handlers in render function, and how many solutions exist - use bind in constructor ? - use class field properties with arrow functions ? - use autobind decorators?) ??? And my solution not need even to recreate handlers on each component instantiation like all current solutions does, so it definitely the fastest way. I highly suggest make this api public or make some helper to get current props and state from event.target"
facebook/react,2017-10-11 05:22:33,feature,Need a hook for hydration mismatch,"**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
In React 16, the `data-react-checksum` attribute was removed from the server rendered markup. In previous versions, we used this attribute to beacon checksum mismatches to our log servers to be notified of production issues. With the attribute removed, we have no mechanism to determine if a checksum mismatch occurred.

I'm aware that checksum issues no longer cause the entire DOM to re-render, however, it is still important that we know when they do occur. A typical use case is when we display ads or autoplay video. We want to know if an ad gets re-rendered (double counted) or an autoplay video is interrupted due to React re-rendering the DOM.

Other related bugs/requests:
- Ability to debug checksums in production - https://github.com/facebook/react/issues/10016

**What is the expected behavior?**
The solution does not necessarily need to re-introduce the checksum attribute again. It could be some other event, hook, or callback that applications can leverage to handle checksum issues.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
- React 16
- All browsers
- Worked in  <= React 15
"
facebook/react,2017-10-09 18:45:57,feature,Support srcObject attribute for video element,"**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

currently you cannot set the srcObject for a video.  You get an error:
```
Warning: React does not recognize the `srcObject` prop on a DOM element. If you intentionally want it to appear in the DOM as a custom attribute, spell it as lowercase `srcobject` instead. If you accidentally passed it from a parent component, remove it from the DOM element.
```

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem via https://jsfiddle.net or similar (template for React 16: https://jsfiddle.net/Luktwrdm/, template for React 15: https://jsfiddle.net/hmbg7e9w/).**

```
return (
      <video srcObject={this.props.stream}>
)
```

There is another issue that was closed but the issue was never resolved: https://github.com/facebook/react/pull/1474

Firefox has deprecated using `URL.createObjectURL()` and Safari doesn't support it.

**What is the expected behavior?**

The ability to set the `srcObject` on a video element.  This is common for WebRTC applications now.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

""react"": ""^16.0.0"""
facebook/react,2017-10-01 18:51:32,feature,Warn about unexpected HTML inside SVG,"react and react-dom versions: 16.0.0

```js
const { createElement: h } = require('react');
const { renderToStaticMarkup } = require('react-dom-server');

renderToStaticMarkup(h('div', null, h('svg', null, h('span')))) 
// '<div><svg><span></span></svg></div>'
```

But they shouldn't be mixed:

in browser console:

```js
new Range().createContextualFragment('<div><svg><span></span></svg></div>')
// ""<div><svg></svg><span></span></div>""
```
I guess it's programmer responsibility to avoid that case? I understand that for performance you don't check this"
facebook/react,2017-09-27 22:08:12,feature,Feature Request: Global didUpdate(),"Currently, its not easy to write global logic that executes after React has re-rendered. The `componentDidUpdate` lifecycle method works great when your logic is isolated to a component, but I've found myself more and more recently wanting a global `didUpdate` hook baked into React.

A simple example where this is useful is if you want an isolated function (perhaps a keyboard shortcut) that creates an element on the screen and then focuses it.

```js
const createNewTodo = async () => {
  const id = createTodo()
  await React.didUpdate()
  focusTodoItem(id)
}
```

At [Notion](www.notion.so), we've written custom logic for doing this, but it makes upgrading with React more difficult and unstable. I think this would be useful for others too, particularly those who use Redux and are building complicated UI interactions. "
facebook/react,2017-09-27 05:07:15,feature,React 16 does not lowercase HTML attributes in generated HTML,"**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

ReactDOMServer generates camelCased markup for the `cellSpacing` and `cellPadding` attributes:

`<table cellSpacing=""1"" cellPadding=""2""></table>`

(Here's an example pen: https://codepen.io/anon/pen/jGBLdP)

I believe these attributes are canonically lowercased. If I lowercase the attributes in JSX, React warns that I'm not using the right names:

```Warning: Invalid DOM property `cellpadding`. Did you mean `cellPadding`?```

**What is the expected behavior?**

The attribute names would be rendered lowercase:

`<table cellspacing=""1"" cellpadding=""2""></table>`

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

This is in 16.0.0. Prior versions of React stripped these attributes."
facebook/react,2017-09-14 12:57:40,feature,Support for string targets for isomorphic createPortal,"This is a proposal for the `createPortal` API to in addition to Nodes support strings for the second argument(container), which could pave a declarative way to achieve out of order server side rendering.

```js
#createPortal(..., container: Node|String, ...)
```

The server could render a portals children in-place and have the client-side hydration process move it to the right location client-side using `document.querySelector` if the container is a string."
facebook/react,2017-09-01 01:49:25,feature,"Make on/off, yes/no boolean attributes work","When you pass a boolean to some attributes (e.g. `autoSave`, `autoCorrect`) in 15, they don't work correctly because they actually want a specific string (`yes` and `no`). I think there were also some attributes that want `on` and `off`.

Let's just “make them work”? Could use a special flag/whitelist for that. There should be very few of these.

Similarly we should probably make `<script crossOrigin />` be valid and turn into `<script crossOrigin=""anonymous"" />`. Currently I don’t think this works on master."
facebook/react,2017-08-31 02:48:27,feature,Implement Silent Updates in the State Update Queue,"[Reason React](https://github.com/reasonml/reason-react) has silent updates. Meaning it's a normal update in the state queue that can be reverted. The only difference is, it doesn't need a rerender by itself. If props have changed, it does rerender.

Unclear if we expose this to the existing ClassComponent API or make it a feature of new APIs."
facebook/react,2017-08-01 06:44:46,feature,More permissive rehydration logic,"So for the last little while I've been pursuing an idea called [React Snapshot](https://github.com/geelen/react-snapshot), where instead of running your code in a Node environment to generate static HTML, you run it in a virtual browser (jsdom or chrome headless) and take a snapshot of the DOM at a particularly moment in time, then host the snapshots like any other static file (technique also known as pre-rendering).

I've been tossing around different API choices (https://github.com/geelen/react-snapshot/pull/30) in order to handle components that have async data fetching requirements, but I'm already starting to see real promise in this approach. Because the snapshot environment is so similar to the client one, far fewer changes are needed to get the performance & accessibility benefits of serving real HTML to your users. This is an example of the React Snapshot async API to make a component snapshottable:

```diff
+ import { snapshot } from 'react-snapshot'

class Home extends React.Component {
  state = { quotes: null }

  componentWillMount() {
+   snapshot(() => (
      fetch('/api/quotes')
        .then(response => response.json())
+   ))
    .then(quotes => {
      this.setState({ quotes })
    })
  }

  render() {
    const { quotes } = this.state
    return (
      <div className=""Quotes"">
        {
          quotes && quotes.map((quote, i) => <Quote key={i} quote={quote}/>)
        }
      </div>
    )
  }
}
```

The idea is that any async parts of your app can be wrapped in a `snapshot` call, which caches responses and rehydrates on the client. However, I've hit a few walls that I think means I'd need changes to React itself to take this to its logical conclusion. Hence, I wanted to start the discussion about whether such changes would be compatible with React's future direction.

### Rehydration

As far as I can tell from my experimentation and from reading the code, the two criteria for reusing the existing DOM elements in a pre-rendered HTML page is:

* the adler32 hash of the initial client-rendered markup has to match the `data-react-checksum` present on the `rootElement`.
* the `_domID` of each instance in the render tree needs to match the `data-react-id` on each DOM element

Between those two criteria, its enforced that the _structure_ and the _content_ of the DOM is the same. I can kinda see why both are needed—the checksum is the cheapest way to confirm the structure will be the same, but the ID of each element is needed to actually wire everything up. Also, `data-react-checksum` is just an attribute, and could be calculated off something that's no longer present in the HTML.

However, generating the exact right checksum in any other way than the existing SSR API turns out to be pretty difficult!

### HTML-escaping woes

I hit this problem where I was rendering the React app like normal, then taking the `innerHTML` of the root container, then passing it to [`addChecksumToMarkup`](https://github.com/facebook/react/blob/master/src/renderers/dom/stack/server/ReactMarkupChecksum.js#L26), and not getting the same checksum as `ReactDOMServer.renderToString`. I first realised I needed to add the `data-reactid` to each element along the way, which wasn't too hard, but still it wasn't working. I figured out it's due to [`escapeTextContentForBrowser`](https://github.com/facebook/react/blob/master/src/renderers/dom/shared/escapeTextContentForBrowser.js) converting things like `'` to `&#x27;` and `""` to `&quot;`, meaning that while the content _appears_ the same once rendered, the precise string is not, therefore the checksum is not, and no rehydration takes place.

From what I can understand, again by reading the code, React _always_ sanitises the HTML content before generating markup (on server or in client), it's just the fact that once its injected into the DOM, `innerHTML` doesn't re-sanitise things like quotes. They don't technically need to be, as discussed in issue https://github.com/facebook/react/issues/3879, and so if that were to be changed this particular problem would disappear, but there may well be more I just haven't hit yet. To me, the real issue is needing to have the content be byte-for-byte equivalent, rather than just functionally (and structurally) equivalent.

### My interim solution

At the moment, I've realised its easier to boot up the app in its entirety, wait for all async processes to take place, then effectively reboot the app using `ReactDOMServer.renderToString` and splice the markup in place. Any side-effects relying on `componentDidMount` (like CSS injection or meta tags in the HEAD) that affect the DOM _outside_ the React app are preserved, but the markup and checksum of the React-rendered HTML are guaranteed to be correct. It works, but its not ideal. You still have to understand that your components are running in two different ""modes"", they'll run different lifecycle methods in each, and only one generates the final snapshot. Which I think adds an unreasonable conceptual burden, much the same way server-rendering does.

That's really the problem I see with the status quo and why I started looking into this problem in the first place. If snapshot/server rendering requires too much overhead, most people won't do it, which is exactly where we're at. Create-react-app doesn't include any because none of the options are simple enough with a broad enough applicability. The official [React Router docs](https://reacttraining.com/react-router/web/guides/code-splitting/code-splitting-server-rendering) warn agains combining server-rendering and code-splitting. Server-rendering boilerplates include fairly specific webpack hacks to provide the same environment on server and client, etc.

The result is that most people only ever do client-rendering. They serve a blank page & render everything client-side. Code splitting and service worker caching offer useful advantages but imo it's not enough. Snapshot rendering _could_ be the solution, but only if it can offer big benefits for small changes to application code.

### My Dream Solution

Architecturally, what I'd like is for an arbitrary React app to be launched on one browser, executed until ready (async resources complete), snapshotted (serialised to HTML), then resumed on another browser. Those snapshots would be generated then cached at the edge of a CDN during deployment, or periodically depending on how often the content changes.

Practically, I think that would require two changes to React's architecture:

The first is for a weaker check for rehydration—some other fingerprint than a hash of the escaped HTML. Some other method for a snapshot to indicate to React to reuse as much of the existing DOM as possible.

The second would be for only parts of the tree to be rehydrated rather than the whole thing. If a component has some side-effect, say in a `componentDidMount`, then the snapshotted HTML would include the result of that side-effect. But when the app boots on the client side, the render method will generate the initial behaviour. At the moment React would replace what's there with what's just been rendered, but it might be preferable to leave the DOM unchanged on the first render, then wire things up later.

I don't know the exact specifics of a solution, nor do I know enough of the internals of React as it is now or as it will become, but I wanted to start the discussion and see if there was any interest from the React team & wider community in this use case and direction. I look forward to hearing your thoughts!"
facebook/react,2017-07-03 17:35:57,feature,Add default property for ES2015 modules,"**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

```js
import * as React from ""react""
```

The above code causes deprecation warnings because of how `_interopRequireWildcard` works in babel:

```
Warning: Accessing PropTypes via the main React package is deprecated, and will be removed in  React v16.0. Use the latest available v15.* prop-types package from npm instead. For info on usage, compatibility, migration and more, see https://fb.me/prop-types-docs
Warning: Accessing createClass via the main React package is deprecated, and will be removed in React v16.0. Use a plain JavaScript class instead. If you're not yet ready to migrate, create-react-class v15.* is available on npm as a temporary, drop-in replacement. For more info see https://fb.me/react-create-class
```

You can use this with babel:

```js
import React from ""react""
```

This works because of synthetic import support in babel, using `__esModule`.

If you use TypeScript the above does not work unless you set `allowSyntheticDefaultImports` to true in tsconfig.json.

Most packages work fine with `* as Module`, but React's deprecated warnings get tripped because babel copies the object with `_interopRequireWildcard`.

**What is the expected behavior?**

```js
import React from ""react""
```

The above should work without needing to set `allowSyntheticDefaultImports`. This can be done by providing a `default` property.

Possibly by adding this to `src/isomorphic/ReactEntry.js`:

```js
React.default = React
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

React 15.6.1
Chrome 59
OS X 10.12.5
Node 8.1.2

Does not work in previous versions since deprecation warning was added."
facebook/react,2017-06-29 09:26:15,feature,Feature request: Warnings hook,"**Do you want to request a *feature* or report a *bug*?**
I want to request a *feature*.

**What is the current behavior?**
- Warning code is hardcoded to call `fbjs/lib/warning`. This makes it hard to integrate warnings with tools.

- As a workaround we could use webpack’s `resolve.alias` to alias `fbjs/lib/warning` into our fork which displays it on-screen.

   - This use case is similar to https://github.com/facebook/react/pull/7360.

- Another use case is to integrate React warnings with our testing infrastructure. This helps us to better see which warning belongs to which test.

    ![image](https://user-images.githubusercontent.com/193136/27680560-587f934a-5ce6-11e7-985b-a1a2042d831a.png)

**What is the expected behavior?**
- React allows library user to override the default warnings behavior.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
- The workaround works in React 15 and should still work in React 16, as the flat bundles does `var warning = require('fbjs/lib/warning')` which means we can still hook into it for now.

- This probably breaks when the flat bundles stopped doing `var warning = require('fbjs/lib/warning')`."
facebook/react,2017-06-27 06:44:09,feature,Include canary tests of community packages as part of the release process,"(cc @gaearon, from https://github.com/yannickcr/eslint-plugin-react/issues/1258#issuecomment-311211399)

In general, there's lots of little utilities that are helpful to have as standalone packages. One of them is https://unpkg.com/airbnb-prop-types@2.7.0/build/helpers/getComponentName.js, for example.

It'd be ideal for the React team to maintain this package - primarily, so that it would be guaranteed to either not break when a new React version is released, or be updated to work with the upcoming version prior to release.

In the interests of the React team not signing up to maintain all the package requests that might come in, would it be possible to make ""part of the release process"" be ""ensure compatible versions of community packages exist""? I'd be happy to create the above package, for example, and I know the community would love a guarantee that enzyme would always work with any official React version release, prior to the release."
facebook/react,2017-06-16 20:15:20,feature,Set initial state to undefined,"**Do you want to request a *feature* or report a *bug*?**
Feature 

**What is the current behavior?**
Initial state is null.

**What is the expected behavior?**
Initial state is undefined. We can manually set the state to undefined in the class, but it would make sense to be out of the box as it would allow destructuring without errors.

**Version:**
React 15.6.1
React-Dom 15.6.1

Sorry if this has been suggested before. "
facebook/react,2017-05-31 07:36:59,feature,Children forEach filters functions,"**React v15.5.4**

Unexpected behaviour of `React.Children.forEach` – silently filters functions.

**Current behavior:**

```jsx
const element = (
  <div>
    {() => {}}
    {() => {}}
  </div>
);
console.log(React.Children.toArray(element.props.children))
// Output: []
```

**Expected behavior:**

Children.forEach should throw *Invariant* error then *function* type child is met.

or

Children.forEach shouldn't filter *function* type children.
"
facebook/react,2017-04-29 15:14:59,feature,Warn when PureComponent renders impure ones as its children,"**Do you want to request a *feature* or report a *bug*?**
feature

**What is the current behavior?**
When a PureComponent renders an impure one as its child, no warning in console or devtool

**What is the expected behavior?**
It could be better if we get a warning message for such cases

This may be related to https://github.com/facebook/react/pull/9240 since React now disallow PureComponent to have a custom `shouldComponentUpdate` implement, so a ""pure"" component with customized and more efficient `shouldComponentUpdate` method must inherit from `Component` base class and add a `isPureReactComponent` property themselves in order to prevent the warning message appear

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
15.5.4
"
facebook/react,2017-04-21 18:27:56,feature,Feature Request: dangerouslySetInnerHTML as a return value,"I haven’t fully thought through this yet, so hoping for some feedback on whether it is viable or breaks too many assumptions.

The goal is to create an escape hatch that would support rendering to conditional comments, alternative method for web components, and other unknown and probably bad ideas.

Something along the lines of:

```jsx
const DangerousComment = ({children}) => ({
  dangerouslySetInnerHTML: {__html: `<!-- ${children} -->` }
});

ReactDOMServer.renderToString(
  <head>
    <DangerousComment>
      {""[if lte IE 9]><script src='/public/media.match.js'></script><![endif]""}
    </DangerousComment>
  </head>
); // <head><!--[if lte IE 9]><script src='/public/media.match.js'></script><![endif]--></head>
```

```jsx
const WebComponent = ({children}) => ({
  dangerouslySetInnerHTML: {__html: `<!-- ${children} -->` }
});
ReactDOM.render(() => (
  <WebComponent>`
    <app-toolbar arbitary-prop=""sure-y-not"">
      <div main-title>Web Components!</div>
    </app-toolbar>
  `</WebComponent>
), document.body.firstChild);
```

The first example is the one I’m more interested at this point in time. The benefit here is this could work on both the DOM and server renderer without userland hacks like https://nemisj.com/conditional-ie-comments-in-react-js/ or `componentDidMount` ref replacement (client only, I believe?).

A couple initial problems I see with this are (may or may not be solvable/avoidable with slight tweaks):

* returning an arbitrary object `render` which isn’t supported
* how will unmounting this work?

Alternative proposal:

Introduce a new DOM-renderer primitive `<comment />` or `<dangerousComment />` (to follow a similar approach to `dangerouslySetInnerHTML` clearly indicating the caveats that come with rendering comments). This alternative solution only solves the conditional comment issue above."
facebook/react,2017-03-10 20:04:07,feature,"Serializing to ""HTML with only as much JS as necessary""","This is sort of a feature request, mostly because I haven't been able to make the current react tooling do what I need it to do.

I have a massively huge article up on https://pomax.github.io/bezierinfo that consists of a single page of React-managed content, of which the bulk is ""passive"" HTML content. Paragraphs, headings, that kind of stuff. However, there are also active components with JS interaction bindings, and so I was trying to figure out a way to turn this article into a ""react-managed by the client"" into a ""thin ui managed by the client, react-built offline"" solution. 

I had a look at ReactDOMServer.renderToStaticMarkup, but while this generates all the HTML nicely, it also loses any and all JS bindings that are necessary to keep the UI working. As this is an ~800kb article not counting static assets (there's some 1.6MB of images on top of that), the notion of first loading the plain markup content and then loading the react bundle on top of that, to hook into the preloaded markup would be a terrible experience for people in the slightly-less-wealthy parts of the world.

Is there some way to, or would there be a sense in developing a way to render React content to HTML ""as much as possible"" while preserving React's management of interactive UI components? Essentially, a solution in between `renderToString` and `renderToStaticMarkup`, to prevent as much data duplication between the pregenerated HTML and the react application itself? It seems that the heavy payload (in bytes, but certainly in client-side processing as well) incurred by using a fully react-managed bundle could be relieved significantly if ""content that is guaranteed passive, regular HTML"" could be serialized out as HTML with React hooking back into that code only for elements that require active management.

(I have no idea how much work that would be, but I'm pretty sure it would help bring down the average page size down again, which on a global scale would save quite a lot of time and money, while allowing content to be loaded by people for whom parts of the internet are currently inaccessible due to load time and byte cost)"
facebook/react,2017-01-15 17:26:32,feature,Expose more through __REACT_DEVTOOLS_GLOBAL_HOOK__.inject,"This feature request came out of a brief discussion on twitter with @gaearon: https://twitter.com/dan_abramov/status/820356665899945984

`__REACT_DEVTOOLS_GLOBAL_HOOK__.inject` exposes access to `getClosestInstanceFromNode`, `getNodeFromInstance`, `ReactMount` and `ReactReconciler`. This is great, and we use it to collect component level performance metrics for our customers in production, however, there are more things we'd love to have access to for the purpose of helping our users pinpoint performance issues in their applications.

This ""issue"" includes some initial observations, but I hope we can keep an open dialogue about this - in particular considering Fiber will probably change the landscape with regards to what the most common performance problems are and we'd instrument according to that.

I understand the urge to keep the API surface area as small as possible in order to create minimal commitment to APIs to allow for internals to move fast. Exposing additional internals with the explicit warning that you are not committed to their API is fine for us, as a tool vendor.

We've looked at using `batchedUpdates` to highlight the batched nature of the work performed by React. E.g. is the work in your application getting batched properly. If we only have access to the component updating methods on `ReactReconciler` (`mountComponent`, `receiveComponent`, `unmountComponent` etc) we can't really know when a batch of work starts and when it ends. `batchedUpdates` is available in ReactDOM with the `unstable_` prefix, but exposing it in the `inject` hook would be very useful: https://github.com/facebook/react/blob/master/src/renderers/dom/ReactDOM.js#L38

We've also (experimentally) hooked into `EventPluginUtils.executeDispatchesInOrder` in order to capture events and measure the work resulting from these, but i suspect there is a better place to do this (`ReactEventListener.dispatchEvent`?). In any case, having access to the event system would be very useful. Again, using the ""unstable_"" prefix to indicate non-committal would be fine for us.

I've not had much time to look into what we'd need access to for Fiber instrumentation, but so far it looks like a hook into `beginWork` would be very useful: https://github.com/facebook/react/blob/199db638c4d0fbc6e4d99534be0060deea518a32/src/renderers/shared/fiber/ReactFiberBeginWork.js#L640"
facebook/react,2016-12-08 07:12:28,feature,Feature Request: Support auxclick event (onAuxClick),"Version 15.4.1

As starting Chrome 55, there is a new event `auxclick` to handle middle click, and `click` doesn't trigger by middle click anymore
https://developers.google.com/web/updates/2016/10/auxclick"
facebook/react,2016-09-13 21:12:40,feature,Skip comparing known constant props during reconciliation,"**Do you want to request a _feature_ or report a _bug_?**

_Feature_

**What is the current behavior?**

Currently, whether a prop is expected to change or not, it is passed through props. There is no way for us as developers to mark a prop as fixed (should never change over the lifecycle of a component).

**What is the expected behavior?**

These would _not_ be static properties, i.e. they can change from instance to instance. But they are created at initial render and never change (`===`) from the original. Ideally, though I'm not sure on implementation, they would also always be `deepEqual` to the original (perhaps something like `Object.freeze(Object.assign({}, originalObject))`.

Advantages to allowing props to be explicitly marked as fixed:
1. shouldComponentUpdate would be better if a lot if certain props automatically don't / can't change
2. Allow for errors / warnings when a prop marked as fixed is changed after the initial render.
3. Possible compiler / render optimizations knowing that particular props will never change over the course of multiple renders. In particular, if a component only has fixed props (including children), we know the render output is fixed, and we can just inline that output into the parent render output.

One possible API would allow a separate fixedProps category.

`React.createElement(MyComponent, { name: 'Hello' }, children, { message: 'Always this one string' })` (maintain signature)

For JSX, one (very mediocre) idea would be something like:
`<MyComponent name=""Hello"" $fixed.message=""Always this one string"" />`

Another would be a separate prop called $fixed or something similar.

`React.createElement(MyComponent, { name: 'Hello', $fixed: { message: 'Always this one string' } }, children)`
`<MyComponent name=""Hello"" $fixed={{ message: 'Always this one string' }} />`

It would be critical that `props` and `fixedProps` would be merged when the component is actually created. `this.props` would always contain all props, functional components still get one arg, and thus nobody has to rewrite render for any component.
"
facebook/react,2016-08-24 11:29:59,feature,Support Symbol keys for props,"**Do you want to request a _feature_ or report a _bug_?**
I want to report a bug

**What is the current behavior?**
`render()` doesn't receive props with Symbol keys (for example, `{[Symbol()]: 'lol'}`). I guess it is because of `hasOwnProperty` in [ReactElement.createElement](https://github.com/facebook/react/blob/a56e105081e27877a2ecbfdc8d591e3b151b2af1/src/isomorphic/classic/element/ReactElement.js#L223)

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem via jsfiddle or similar.**
https://jsfiddle.net/sh2xbm3x/1/

**What is the expected behavior?**
Symbol-keyed `props` passed to `render()` 

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
Every single one, as far as I know
"
facebook/react,2016-07-19 01:08:47,feature,Highlight element that failed checksum,"At the moment, when the server-rendered markup doesn't match the client-rendered markup, a big red error is shown, but it can be difficult/impossible to locate the element where this occurred.

Would something that logged the offending element to the console be considered as a pull request?

![react-diff](https://cloud.githubusercontent.com/assets/4443482/16935229/07cc7b20-4da1-11e6-9e30-f5f584d33182.gif)

https://github.com/facebook/react/compare/master...davidgilbertson:log-checksum-failed-element
"
facebook/react,2016-07-07 12:03:57,feature,"Having this.state in the constructor and this.setState everywhere else is violating the ""Uniform access principle""","ES6 style of creating components in React is clearly violating the [Uniform access principle](https://en.wikipedia.org/wiki/Uniform_access_principle). It's a general source of confusion (and bugs) to be able to set state in two different ways:
- You MUST do `this.state = {}` if you're in the constructor
- You MUST do `this.setState()` everywhere else.

I believe it'd be much better to have just 1 way to set state in general, and it should be universal. If we're already accessing state through `this.state`, and setting state in the constructor with `this.state`, why not do it everywhere else? The behavior of `this.setState` could be easily replicated with [ES6 Proxies](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy/handler/set).
"
facebook/react,2016-06-26 02:51:13,feature,Polyfill MouseEvent.buttons for Safari,"During mouse move event, 'e.buttons' returns 'undefined' in Safari.  Behaving correctly in Chrome.  While pressing the left mouse button and moving the mouse expect 'e.buttons' to return 1.  'e.nativeEvent.which' returns the correct result on Safari.

Sample Code:

``` javascript
import React from 'react';

export default class Canvas extends React.Component {
  constructor(props) {
    super(props);
  }

  mouseDown(e) {
    console.log(""mouse down"", e.buttons, e.nativeEvent.which);
  }

  mouseMove(e) {
    console.log(""mouse move"", e.buttons, e.nativeEvent.which);
  }

  mouseUp(e) {
    console.log(""mouse up"", e.buttons, e.nativeEvent.which);
  }

  render() {
    var canvasStyle = {
      backgroundColor: 'rgba(0, 0, 255, 0.5)',
      position: 'absolute',
      top: '0px',
      left: '0px',
      width: '100%',
      height: '100%'
    }

    return (
      <div>
        <canvas id=""canvas"" style={canvasStyle}
          onMouseDown={this.mouseDown.bind(this)}
          onMouseMove={this.mouseMove.bind(this)}
          onMouseUp={this.mouseUp.bind(this)}>
        </canvas>
      </div>
    );
  }
}
```

**Versions**
React: 15.1.0
Safari: 9.1.1 (11601.6.17)

**OS**
OS X El Capitan Version 10.11.5

**Computer**
Model Name: MacBook Air
Model Identifier:   MacBookAir6,2
Processor Name: Intel Core i7
Processor Speed:    1.7 GHz
Number of Processors:   1
Total Number of Cores:  2
L2 Cache (per Core):    256 KB
L3 Cache:   4 MB
Memory: 8 GB
"
facebook/react,2016-06-20 20:46:42,feature,Warn for string refs where owner != __self,"Sebastian wants to warn when `owner !== __self`, because this is the hard case to find when codemodding from string refs to callback refs.
"
facebook/react,2016-06-06 16:34:24,feature,RFC: Make Refs Opt-in,"**Note: This is my personal proposal.**
**Please don’t announce it anywhere as “React dropping refs!”** 😄 

Higher order components solve many problems of mixins, however they come with their own problems. The most painful one in my experience is that they hide the ref of the wrapped component, so they can’t be treated as transparent wrappers. This is well described in #4213.

As we prepare for de-emphasizing `createClass` and mixins I think it’s important that we **treat higher order components as first class pattern in React, and provide full support for it without clashes with existing features**. This means we need to fix refs to work well with higher order components.

In the community, I see that people embrace stateless functional components even though they don’t have public instances and don’t support refs pointed at them. I think that this is a good indication that refs are moving from being a commonly needed feature to an escape hatch, and so it can be further de-emphasized by becoming opt-in.
### What Doesn’t Change

`<div ref={...} />` works like before and provides node in a callback.
`<StatelessFunctionalComponent ref={...} />` works like before and provides `null` in a callback.
### Classes Opt Into Exposing a Ref

In the spirit of https://github.com/facebook/react/issues/4213#issuecomment-115019321, I propose that **`this.ref` becomes an opt-in API on every class component.** If you want your components to be “reffable” (that is, to expose their public instances as refs), you need to manually call it in your constructor:

``` js
class MyComponent extends Component {
  constructor(props) {
    super(props)
    this.ref(this) // I'm exposing my public instance!
  }

  ...
}

// Will print MyComponent instance
<MyComponent ref={instance => console.log(instance)} />
```
### By Default, Don’t Expose Refs

If you don’t call `this.ref(this)` during the constructor, React will automatically call it with `null`:

``` js
class MyComponent extends Component {
  ...
}

// Will print null
<MyComponent ref={instance => console.log(instance)} />
```

This means that by default, class components will act just like functional components. There is no access to the instance unless the class opts in.
### Automatic Cleanup

If the class opts in, it only needs to call `this.ref` in the constructor. React will take care of automatically calling it with `null` when the component unmounts.
### New! Forwarding a Ref to Another Component

**This is the new feature here.**
Since we opt into refs, we can cleanly support ref forwarding for higher order components:

``` js
function wrap(WrappedComponent) {
  return class extends Component {
    ...

    render() {
      return <WrappedComponent ref={this.ref} />
    }
  }
}
```

By passing `ref={this.ref}`, we let `WrappedComponent` supply its own instance, if available. This way the fact that it’s wrapped with a higher order component becomes unobservable.

This also works fine if you conditionally switch between different components or delay rendering:

``` js
function wrap(WrappedComponent) {
  return class extends Component {
    ...

    render() {
      return this.state.isReady ? <Spinner /> : <WrappedComponent ref={this.ref} />
    }
  }
}
```

Let’s say `isReady` is `false` initially. React would take care of calling `this.ref(null)` after the constructor ran (since it knows the constructor never called `this.ref(this)`). So initially the parent receives `null`, as expected.

When `WrappedComponent` mounts, it will call `this.ref(this)` with its instance, which make it available to the parent. When `WrappedComponent` unmounts, React will call `this.ref(null)` for its instance, cleaning it up again.

The same works if we alternate between `<WrappedComponent ref={this.ref} />` and `<SomeOtherComponent ref={this.ref} />`.
### Upsides
#### Higher Order Components are Unobservable

This removes a common pain point in that wrapping a component with a HOC changes its public API.
#### Refs are Further Discouraged

By making them opt-in, we better signal that you shouldn’t use them for data flow. The component can also be certain that changing or removing an imperative method is not a breaking change because by default it doesn’t expose the ref. If it exposes the ref, this is done intentionally.
#### Providing Explicit Imperative APIs

The component may also choose to provide a subset of methods as its public API:

``` js
class MyComponent extends Component {
  constructor(props) {
    super(props)
    // I'm exposing just some stuff!
    this.ref({
      focus: this.focus.bind(this)
    })
  }

  focus() { ... }
  privateMethodIDontWantAnybodyToCall() { ... }
}
```

This lets the component choose _which_ methods it wants to expose imperatively, and which are still considered implementation details.
### Downsides
#### Migration Cost

This would be an easy enough codemod for most components (just add `this.ref(this)` to any class component). But it’s still a cost considering some of those components are on npm and out of your control. Arguably most third party components don’t provide imperative methods anyway, but this will cause some trouble.
#### Potential for Misuse

I can imagine people doing `this.ref(this)` and then `<WrappedComponent ref={this.ref} />` in the `render` method. This could get confusing but I don’t see any easy way to prevent or warn about this.
#### Too Much Freedom

Technically you’d be able to pass `this.ref(42)`, `this.ref(findDOMNode(this))` or other weird things. Maybe we could limit possible values to React public instances and `null`. On the other hand, the ability to only provide a subset of methods as described in “Providing Explicit Imperative APIs” seems useful.
### Other Considerations
#### `this.props.ref`?

We could have provided `ref` inside `this.props` as `this.props.ref`. I would argue that we don’t want to do this for two reasons:
- We want React to still “manage” it partially by calling `this.ref(null)` when component unmounts. Otherwise it’s too easy to introduce memory leaks. Having magic behavior for one of the props would be unexpected.
- We don’t want `{...this.props}` to transfer the ref over to the child as that would be unexpected in most cases.

---

What do you think?

cc @facebook/react-core 
"
facebook/react,2016-05-12 14:04:17,feature,ReactPerf 15.1.0-alpha.1 expose isProfiling on the exported object?,"I've tested the new Perf tools on one of the screens in one app I'm working on and it works great :)

One thing I wanted also on the old one Perf tools was to be able to check whether the Perf tools are started or stopped, so basically exposing `ReactDebugTool`'s `isProfiling` variable. My use for that is that I have a keyboard shortcut (in development) bound to start/stop the Perf tools and currently I'm forced to maintain such a variable myself to know whether to start or stop (which gets out of sync if I start/stop the perf tools without the keyboard shortcut).

If you don't think it's worthwhile to expose `isProfiling` feel free to close this issue.
"
facebook/react,2016-05-07 20:53:16,feature,support for event.movementX/Y [feature-request],"currently it seems synthetic events don't have those properties on mousemove
"
facebook/react,2016-04-27 02:17:42,feature,Introduce __PROFILE__ build,"As discussed in #6015, we plan to add a new build configuration that does _not_ have the `__DEV__` overhead but that comes with the new `ReactPerf` enabled.

This means that developer warnings, etc, will need to be gated by `__DEV__`, but the component tree (#6549) and some other events (e.g. #6612) will need to be gated by `__PROFILE__`.

I’m curious how this could be implemented. Right now our system is simple:
### Current System
#### Variables
- `__DEV__ = (process.env.NODE_ENV !== 'production')`
#### Development Build (any `NODE_ENV` except `'production'`)
- `__DEV__` is `true`
#### Production Build (`NODE_ENV` is `'production'`)
- `__DEV__` is `false`

As you can see, if `process.env.NODE_ENV` is omitted, we assume `__DEV__` mode. This is a sensible assumption, and not the one we want to change, as most projects today don’t specify anything as `NODE_ENV` in development, and we don’t want them to suddenly lose all developer warnings.

Therefore, I propose the following new system:
### Proposed System
#### Variables
- `__DEV__ = (process.env.NODE_ENV !== 'profile' && process.env.NODE_ENV !== 'production')`
- `__PROFILE__ = (process.env.NODE_ENV === 'profile'`)
#### Development Build (any `NODE_ENV` except `'profile'` or `'production'`)
- `__DEV__` is `true`
- `__PROFILE__` is `true`
#### Profile Build (`NODE_ENV` is `'profile'`)
- `__DEV__` is `false`
- `__PROFILE__` is `true`
#### Production Build (`NODE_ENV` is `'production'`)
- `__DEV__` is `false`
- `__PROFILE__` is `false`

This would let us have three separate build configurations. We can use the same pattern in other `fbjs` projects as well, if desired. I would say it’s unlikely we’d ever want to add a separate fourth configuration so this should cover all our needs.

Any thoughts why this would be a bad idea? Should I implement this in `fbjs`?

cc @facebook/react-core 
"
facebook/react,2016-04-22 19:20:50,feature,Proposal: Replace NODE_ENV with REACT_ENV for __DEV__ replacement,"This has come up a couple times lately as being an issue (#6479, #6581, #6511), I think perhaps because we added the minification warning and people are ending up seeing they aren't getting prod code when they expected it. But there's also the argument that you want React to be production and still use NODE_ENV for other purposes.

There would be a few things to figure out to make sure envify works, and deciding what we do for other projects which currently also use the NODE_ENV pattern (eg, Relay, fbjs, third-party code, etc).

This might not be a good idea at all though and definitely isn't happening immediately, but wanted to start the discussion.
"
facebook/react,2016-04-11 09:22:06,feature,Support for asynchronous values (like Promises and Observables),"At the moment, it is quite cumbersome to work with promises in React. Either you write a lot of code to update the state inside the `then()` callbacks or you use a library like [react-promise](https://www.npmjs.com/package/react-promise) which only works for children (not properties) because it is based on components.

Another option is to traverse the whole virtual dom and replace promises before they are passed to React. (I have read once a blog post about this but cannot find it any longer.) This is obviously not ideal for the rendering performance.

Since promises are now a standard, I think there should be simpler way to use them directly in the view. One option would be to introduce an AsyncValue type with a Promise implementation. The AsyncValue type would then be supported for children as well as properties.

Promises could then simply be used like this:

``` javascript
<div>{promise(this.state.myPromise)}</div>
```

Opposed to supporting promises everywhere (directly without the promise function), this should not have unexpected side effects. Moreover, other types of asynchronous values (for example Observables) could be added later easily just by creating another AsyncType implementation.

Angular 2 takes a similar route with the [async pipe](https://angular.io/docs/ts/latest/guide/pipes.html#!#the-impure-asyncpipe-). There you can simply write 

``` javascript
{{ myPromise | async }}
```

in attributes as well as normal content.

This feature would also be very convinient for libraries like Flux because it allows to fetch properties directly inside the view and avoids the reptition which you have with Relay:

``` javascript
<div>{promise(this.state.model.get('property'))}</div>
```

What do you think about this proposal?
Has this been discussed before? I couldn't find a thread which covers this particular topic.
"
facebook/react,2016-04-06 15:18:13,feature,componentDidUnmount functionality (in addition to componentWillUnmount),"It would be great if it was possible to run some code after the component actually unmounted. This is useful where you consider the following:

```
component Parent {
    has service Service
    has child Child

    componentWillUnmount: {
         destroys Service
    }

    render: {
         Service gets passed to Child as prop
    }
}

component Child {
    componentWillMount: {
         starts listening to Service
    }

    componentWillUnmount: {
         stops listening to Service
    }
}

```

The above will throw exception during Child componentWillUnmount since Parent gets unmounted first so Service is already destroyed.

If there existed componentDidUnmount the Service can be destroyed after the children are unmounted, i.e.

```
component Parent {
    has service Service
    has child Child

    componentDidUnmount: {
         destroys Service
    }

    render: {
         Service gets passed to Child as prop
    }
}

component Child {
    componentWillMount: {
         starts listening to Service
    }

    componentWillUnmount: {
         stops listening to Service
    }
}

```
"
facebook/react,2016-04-05 04:54:27,feature,onFocusIn/onFocusOut events,"Like mouse enter/leave, these are almost always what you want, not the `onFocus` and `onBlur` events we currently expose. I run into this semi-frequently when actually doing product work. We should add them.
"
facebook/react,2016-03-12 18:55:40,feature,Feature Request: innerHTML alternative,"I think many people agree that the `dangerouslySetInnerHTML={{ __html: ... }}` api is gross even though there is great reasoning behind it.

I have a few issues with it beyond it's verbosity that I think could be added as static method on the react class.
#### My Issues with `dangerouslySetInnerHTML`:

1) Verbose and not always dangerous.
2) Can't mix safe and unsafe html.
3) Can't mix html with react elements.
4) Have to manually concat html strings.

---

A better solution would be to provide a way to mark html as ""safe"".

``` js
var React = require(""react"");

// Use #markSafe method to bypass reacts automatic html escaping.
var myReactEl = <div>{ React.markSafe(""<br/>"") }</div>;
```

This is still explicitly telling react that we trust the html but solves all of the problems above.

``` js
// Mixing safe and unsafe html.
(
    <div>
        { React.markSafe(""<br/>"") }
        { ""<br/>"" }
    </div>
);

// Mixing html with react elements.
(
    <div>
        { React.markSafe(""<br/>"") }
        <span>Hello!</span>
    </div>
);

// Multiple innerHTML sets.
(
    <div>
        { React.markSafe(""<br/>"") }
        { React.markSafe(""<hr/>"") }
    </div>
);
```

I think this api would be much friendlier than the current html api and probably wouldn't even require a major version bump.

Thoughts?
"
facebook/react,2022-03-24 09:40:53,question,Bug: infinite function component rendering with lazy loading,"Unexpected behavior (infinite function component rendering) when lazy loading component with string interpolation (reactive variable state)

React version: 18.0.0 rc3

## Steps To Reproduce

1. Not working:
```javascript
import { lazy } from 'react';

export default ({ langauge }) => {
  const Privacy = lazy(() => import(`./Privacy-${langauge}.jsx`));
  return <Privacy />;
};
```

2. working:
```javascript
import { lazy } from 'react';

const PrivacyPT = lazy(() => import(`./Privacy-pt.jsx`));
const PrivacyES = lazy(() => import(`./Privacy-es.jsx`));
const PrivacyEN = lazy(() => import(`./Privacy-en.jsx`));

export default ({ language }) => {
  
  return (
    <>
      {language === 'pt' && (
        <PrivacyPT />
      )}
      {language === 'es' && (
        <PrivacyES />
      )}
      {language === 'en' && (
        <PrivacyEN />
      )}
    </>
  );
};

```

Link to code example: https://codesandbox.io/s/spring-haze-ppyzky?file=/src/App.jsx

![Screenshot from 2022-03-24 09-35-58](https://user-images.githubusercontent.com/61196637/159887181-86a62ff3-b3c2-42f1-aa0a-b113d0cd52b6.png)
## The current behavior


## The expected behavior
"
facebook/react,2022-02-16 06:20:48,question,Question on reconciliation,"Let's say on first render, first datastore as below:

```json
[
  {id: 1, name: ""Duke""},
  {id: 1, name: ""Villanova""},
  {id: 1, name: ""Neclause""},
  {id: 2, name: ""Derk""}, 
  {id: 3, name: ""Lily""}
]
```
we render following:
<ul>
  <li key=""1"">Duke</li>
  <li key=""1"">Villanova</li>
  <li key=""1"">Neclause</li>
  <li key=""2"">Derk</li>
  <li key=""3"">Lily</li>
</ul>
I changed the datasource to `[{id: 2, name: ""Derk second time""}, {id: 3, name: ""Lily second time""}]` after a setTimeout, What I expect on next render is:

<ul>
  <li key=""2"">Derk second time</li>
  <li key=""3"">Lily second time</li>
</ul>

But it render the following result,  key=""1"" li remains in the dom tree, which I expect to be eliminated.

<ul>
  <li key=""1"">Duke</li>
  <li key=""1"">Villanova</li>
  <li key=""2"">Derk second time</li>
  <li key=""3"">Lily second time</li>
</ul>

Hower when I changed the datasource to `[]` after a setTimeout, What I expect on next render is:
<ul>
  <li key=""1"">Duke</li>
  <li key=""1"">Villanova</li>
  <li key=""2"">Derk second time</li>
  <li key=""3"">Lily second time</li>
</ul>

Out ot my expection, I got following a empty dom tree???
A online demo is accessiable on https://codesandbox.io/s/ji-chu-lie-biao-antd-4-18-7-forked-nm4sly?file=/index.js

Can someone explain the reconciliation for me this case, thanks!"
facebook/react,2021-07-12 13:59:15,question,React 18 : useState 'sharing' state value between components of the same type,"<!--
  Ask a question or share feedback about the React 18 release here.
-->

I created a custom table component that has pagination, search bar and export to excel button. The main usage of the component is as follows:

```jsx
<ClientsTable
   dataset={sellsInfo}
   Headers={sellsInfoHeaders}
   downloadFileName={`OrdenesCliente-${clientId}`}
   pageSize={2}
   identifier={'id'}
/>  
```

To enable pagination, global filter and dataset download, I used state variables using `useState` hook, as follows. 

```jsx
const ClientsTable = (props) => {
   let { dataset, Headers, pageSize, actions, identifier, downloadFileName } = props;
                          
   const [ displayDataset, setDisplayDataset ] = useState(dataset);
   const [ pageIndex, setPageIndex ] = useState(0);
   const numPages = Math.ceil(displayDataset.length / pageSize);
   const [ canNextPage, setCanNextPage ] = useState(true);
   const [ canPrevPage, setCanPrevPage ] = useState(false);
   const [ sortSmallToLarge, setSortSmallToLarge ] = useState(true);
   const [ sortingField, setSortingField ] = useState(null);

   // some logic for filtering, pagination and rendering
}
```

Rendering only one table component in an app view works just fine. However, rendering two or more implies that React uses the same state variables for all table components in the view. This is unintended behavior for me. In particular, I created a custom tab component that renders two different tables on tab selection. The two tables have the same state variables, different pros though, which causes conflicts in the view.

How can I render multiple table components in my app without having this behavior?

Thanks in advance. 
 "
facebook/react,2021-06-04 20:51:28,question,Bug: setState run twice  with own hooks,"React version: 17.0.2

## Steps To Reproduce

1. click .App
2. 

Link to code example:
```js
const useL = (service) => {
  const [l, setL] = useState([])
  const [ab, setAB] = useState({});
  const load = async () => {
    const res =  await service()

    setAB((t) => {
      // LOG twice
      console.log(""22222"");
      setL([...l, ...res.l]);
      return t;
    });
  }
  useEffect(() => {
    load()
  }, [])

  return {
    list: l,
    load
  }
}

export default function App() {
  const [a, setA] = useState([]);
  const {load} = useL(() => {
    return new Promise(res => {
      setTimeout(() => {
        setA(1)
        res({
          l: [1, 2]
        })
      }, 2000)
    })
  })  

  const handle = () => {
    load();
  };
  return (
    <div className=""App"" onClick={() => handle()}>
      <h1>Hello CodeSandbox</h1>
      <h2>Start editing to see some magic happen!</h2>
    </div>
  );
}
```

Why  log twice

**Edited** by @bvaughn to fix formatting."
facebook/react,2021-04-30 20:54:22,question,Bug: useEffect happens synchronously when useLayoutEffect was called,"

React version: 17.0.2 (also tested on 16.9.0, same result)

## Steps To Reproduce

1. Open this codesandbox: https://0gobm.csb.app/ ([Source](https://codesandbox.io/s/smoosh-sun-0gobm?file=/src/index.js))
2. Try to click on the ""Works"" button
3. Dropdown opens
4. Try to click on the ""Broken"" button
5. Dropdown does not open

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

https://0gobm.csb.app/


The issue is related to how `useEffect` calls are scheduled. We have this effect:


```js
  useEffect(() => {
    if (!open) {
      return;
    }
    const clickListener = (event) => {
      if (!dropdownRef.current.contains(event.target)) {
        onDropdownClose();
      }
    };

    window.addEventListener(""click"", clickListener);

    return () => {
      window.removeEventListener(""click"", clickListener);
    };
  }, [open, onDropdownClose]);
```

Normally, this effect happens asynchronously. However, sometimes, it is synchronous and we add listener to the `window` before the event fully bubbles and the dropdown gets closed by the same event.

This only happens if any underlying component contains `useLayoutEffect` with `setState` call (as shown on codesandbox). Looks like this causes forced render and this chain of events.

P.S. The report looks very similar to another one: https://github.com/facebook/react/issues/20074. However, I decided to open a new issue, because the circumstances are different (no portals in this case)"
facebook/react,2021-03-31 16:09:52,question,State not getting updated properly while using useReducer with Context API,"Due to proprietary code issue, can't share the code but will explain the issue elaborately. I have two functions, **function 1** and **function 2** . Inside **function 1**, I am updating my state twice by calling dispatch with two different action.type each time. After updating my state, I want to call **function 2** (from **function 1**), and read state's value. But my state is not getting updated properly. 

Can you please help me with this issue.

I am using useReducer with context api, for state management. "
facebook/react,2021-03-05 14:25:14,question,Bug: Inconsistent behavior between development and production builds,"I was trying to implement `useOn`, a hook lets you write the following code:

```js
useOn()
  .who(window)
  .when('resize')
  .what(() => {
    console.log('resize!');
  });
```

My approach worked in production, but not in development. I discovered some pretty unexpected behavior by React.

When running `useOn` in development, it seemed like the resize event listener isn't being removed when the ResizeComponent gets unmounted (turned out it was actually being added twice).

So I added a `console.log`, just above the call to `addEventListener`, but only one log appeared.

I then added a `debugger` statement just above the call to `console.log` and noticed that it is indeed being called twice, but in the second time it's called, `console.log` is overwritten by React's `disabledLog()`.

React version: 17.0.1

## Steps To Reproduce

(tip: you can skip checking out the repo (steps 1 to 3), as it can be reproduced in [the repo's CodeSandbox](https://codesandbox.io/s/github/dutzi/use-on))

1. Check out https://github.com/dutzi/use-on
2. `yarn` then `yarn start`
3. (Chrome will start up)
4. Open the DevTools' Console
5. Click on ""Mount Resize Component""
6. Resize window
7. (You should see ""resize!"" printed in the console)
8. Click on ""Unmount Resize Component""
9. ""resize!"" messages should stop showing up in the console, but they don't.

I did make some un-orthodox stuff in that hook. But I believe this is a bug since when trying this hook in a production build, it worked fine.

To test out a production build:

(tip: you can skip checking out the repo (steps 1 to 2), as it can be reproduced in [the repo's deployed version](https://use-on.vercel.app/))

1. Check out https://github.com/dutzi/use-on
2. `yarn` then `yarn start:prod`
3. Complete steps 4-9 from above

❤️"
facebook/react,2021-03-02 10:42:18,question,Bug: 17.0.1 npm packages were not built from 17.0.1 source code?,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 17.0.1

## Steps To Reproduce

1. Go at https://codesandbox.io/s/warning-issue-formik-v2-tutorial-start-forked-lwtxx?file=/src/index.js
2. Then reproduce the famous ""warning"" (**the issue is not about this warning**)
![image](https://user-images.githubusercontent.com/9853656/109627307-298f8500-7b4a-11eb-9330-5ef9d8840094.png)
(type something in the Tool ID)
(the codesandboxe may freeze a little bit, try to reload the page)
3. Open the browser console (chrome in my case) and you will see
![image](https://user-images.githubusercontent.com/9853656/109634512-ff41c580-7b51-11eb-847c-f7cb7ca3754a.png)
Then go to the code (red arrow on the image above)
4. Make a breakpoint here
![image](https://user-images.githubusercontent.com/9853656/109635453-0e754300-7b53-11eb-945b-f1848ac07a53.png)
reload the page and initiate the ""warning"" again (see 2)
4.  Go down the callstack and you can see the ""warning"" is written by this code
![image](https://user-images.githubusercontent.com/9853656/109627844-b6d2d980-7b4a-11eb-9812-440d8d3aa7c3.png)
5.  Now the question.. I'm using the 17.0.1 version and this code shouldn't exist there. Check this out https://github.com/facebook/react/releases/tag/v17.0.1 and you can find only this:
![image](https://user-images.githubusercontent.com/9853656/109628307-45475b00-7b4b-11eb-8a25-61dad589fa8b.png)
As you can see instead of  `error('A component is changing...` we have `console.error(
        'A component is changing...`.
The changes were made by this https://github.com/facebook/react/commit/0cf22a56a18790ef34c71bef14f64695c0498619#diff-e94b1d695a508a3bacbab157bbc3035d75e53174bdb82181077d1b374046f9a6 long time ago.
6. Does it mean that 17.0.1 version of react and react-dom packeges were not built from this source code https://github.com/facebook/react/releases/tag/v17.0.1 ? Seems like a big issue then.
7. It happens without Codesandbox as well.

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->
Link to code example:
https://codesandbox.io/s/warning-issue-formik-v2-tutorial-start-forked-lwtxx?file=/src/index.js
<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior
We keep seeing the word ""Warning"" (which is produced by `warning` function, though it's not a warning but an error)
![image](https://user-images.githubusercontent.com/9853656/109633773-2cda3f00-7b51-11eb-9ab8-93b33316e50b.png)

## The expected behavior
We shouldn't see the word ""Warning"" if the packages v.17.0.1 were bilt from https://github.com/facebook/react/releases/tag/v17.0.1 sourcecode (because it calls  `console.error(
        'A component is changing... `  instead of `error('A component is changing... `)
![image](https://user-images.githubusercontent.com/9853656/109636757-a45d9d80-7b54-11eb-8a35-b745b0fa168f.png)


"
facebook/react,2021-02-18 16:23:13,question,Mixin rewrite with React and Higher-order-functions,"@gaearon How would you re-write the following mixins?

https://github.com/gerobit/ivis-core/blob/a51154c8937d4d392f20303caeb8b450ecf9189a/client/src/lib/decorator-
helpers.js"
facebook/react,2020-12-29 17:35:10,question,Can I help?,"Hi guys,
I would really like to join you in order to help with the maintenance of React.

My change proposal is to apply the clean code rules to React codebase; for example:
- better name to variables
- single point of return
- have short functions with a single purpose
- ...

In order to avoid a long code review process, I would like to make 1 PR for each package that I'm going to edit.

I've already signed the CLA. 

Hope you'll be interested in this,

Matteo"
facebook/react,2020-09-18 04:46:12,question,React build files contain the version of react being used,"Is there any way to hide react version or is it designed to have react versions in the build files which are given in the production

"
facebook/react,2020-08-05 15:35:17,question,Suggestion: Add release date in the experimental (0.0.0) version,"https://samver.org/

It's hard to know which one is the latest experimental react version in the `yarn` or `npm`.

0.0.0-experimental-ede917064
0.0.0-experimental-4c8c98ab9
0.0.0-experimental-7f28234f8

Can you tell which one is newer without doing querying on the web?

I suggest adding a release date in the version.

`0.0.0-experimental-20200802-3d0895557`"
facebook/react,2020-08-03 13:15:54,question,Bug: can't inspect well react warnings for `componentWillReceiveProps has been renamed`,"Cf the screenshot bellow. I have no `componentWillReceiveProps` in my code, so the warning comes from a library.
I don't know how to quickly identify which library causes this warning and make a fix pull request on the repo of this library.

Any tips to also remove the warning ?

Thanks a lot 


![image](https://user-images.githubusercontent.com/25119847/89186329-ebbe8400-d59b-11ea-87f4-47d2f215be72.png)


"
facebook/react,2020-07-09 15:48:17,question,Which approach is better for getting data from state ?,"
Let's say our state is as follows:
```
state: {
    user: {
        name: 'Dave',
        email: 'Dave@gmail.com'
    }
}
```

so while getting the user property from the state which one of the following should I do?

**Case 1:**

```
const user = this.state.user

this way following is valid
user == this.state.user //true
user.name = 'Manny';
console.log(this.state.user.name) //Manny
```
though it won't cause re-render until we use setState() but basically the value of the state variable has changed.

**Case 2:**

```
const user = { ...this.state.user};

this way
user == this.state.user //false
user.name = 'Manny';
console.log(this.state.user.name) //Dave
```

Which approach is better or are there any side effects of any of them?"
facebook/react,2020-07-07 08:00:30,question,"When waiting for an async action to end, and in the meantime the state changes, there is no way to know about the change","<!--
In a functional component, when waiting for an async action to complete and meanwhile changing any state - at the end of the async action the component stays in the previous lifecycle, thus doesn't know about the state change.

-->

React version:
16.13
## Steps To Reproduce

1. create a functional component with 2 elements
2. the component will have an Integer - counter prop and is initialised with 0 value.
3. the first element has an ""onClick"" action, which increases the counter. 
4. the second element has an ""onClick"" action, which is asynced and waits for a timeout promise and only then increases the counter.
5. press the second element, and while waiting for the async action to complete press the firs element as well.

```
import React, { useState } from ""react"";

export default function App() {
  const [counter, setCounter] = useState(0);
  return (
    <div className=""App"">
      <div>
        <button
          onClick={async () => {
            await new Promise(resolve => {
              setTimeout(() => {
                resolve();
              }, 4000);
            });
             setCounter(counter + 1);
          }}
        >
          Async
        </button>
        <label>{counter}</label>
      </div>
      <div>
        <button
          onClick={() => {
            //increases the counter state
            setCounter(counter + 1);
          }}
        >
          Add 1
        </button>
        <label>{counter}</label>
      </div>
    </div>
  );
}

```

Link to code example:
https://codesandbox.io/s/red-bush-7w8yk?file=/src/App.js

-- A different version of the same issue
https://stackoverflow.com/questions/62424530/how-to-get-the-changed-state-after-an-async-action-using-react-functional-hooks

## The current behavior
current behaviour: the counter value at the end of the async action will equal to 1

## The expected behavior
expected behaviour: the counter value at the end of the async action will equal to 2"
facebook/react,2020-06-02 09:26:57,question,Bug: React Function getting called for no Reason,"<!--
 The problem is that a random function is getting called up, for no reason, I tried to trace it and looked like there is a problem with react-development. react-development is calling a long ago used function
-->

React version: ^16.13.1

## Steps To Reproduce

1. Click on Task 1
2. Check the dsdfdsf tab,and check it
3. Click on Create a new Task 
4. create a new task



Link to code example:

<!--
 code --> https://github.com/aman-singal/Recact_todoList
-->

## The current behavior
When adding a new task, The last used function of a different component gets called for no reason. The milestone of the list gets cleared when adding a new Task


##  The expected behavior
The Function shouldn't be called, the already existing List shouldn't be cleared
"
facebook/react,2020-05-27 05:01:13,question,"Click event and setTimeout update state, different times of rendering","When I try to batch update all the states with click events, the final render is once. When the state is updated in batch with setTimeout, the result is multiple times. May I ask what is the useStateHook running mechanism inside react? Is the same asynchronous function, but The Times of rendering is not the same?

React version: 16.12.0

## Steps To Reproduce

1. Bind a single click event to the DOM and handle multiple state updates in batches
2. After the component is mounted, create a setTimeout timer, which is also a batch update of state

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example: [https://codesandbox.io/s/charming-fermat-syi5g?file=/src/App.js](url)

## The current behavior
Click event batch update and timer batch update rendering times are different


## The expected behavior
Also triggers an update

What is the operation mechanism of useState hook? Why not the same number of asynchronous renderings?
"
facebook/react,2020-04-21 21:57:06,question,Bug: function component cannot have ref property,"When passing ref property to a function component, validateFunctionComponentInDev shows a warning about it being a bad thing.

React version: HEAD

While this might have been true for stateless function components (as many places refer to func components), probably since the introduction of hooks, they can have state, and I had the impression that one could rewrite any class component into a function based one.

```
function TestComponent({ref}) {
  const [state] = React.useState({
    stateField1: 42,
  });
  assignRef(ref, state);
  return (
    <div>
      { state.stateField1 }
    </div>
  );
}

This code wont work, caller would never be able to get The Answer. Caller is sad.
```

## The current behavior
React giving a warning and making the ref prop defunct.

## The expected behavior
Remove limitation and let ref on my function component live happily ever after."
facebook/react,2020-04-02 07:42:30,question,"{[styles.box, {           transform: [{ scale:this.state.scaleNum}]      }]}>当这样给scale赋值时，初始化生效，但是在一个事件中改变scaleNum的值，scaleNum的值改变了，页面貌似没有重新渲染？"," <View style={[styles.box, {
         transform: [{ scale:this.state.scaleNum}]}>当这样给scale赋值时，初始化生效，但是在一个事件中改变scaleNum的值，scaleNum的值改变了，页面貌似没有重新渲染？"
facebook/react,2020-02-17 13:33:08,question,Question: It's not problem just question in my mind,"Why all implements of hooks in `react-dom` and we have to import from `react`?
Just I want to know about it"
facebook/react,2020-02-13 11:57:53,question,"Question: How to use ""useRef"" when it is passed through Portal?","Hi. I create `ref` in root component in my app. When i bind `ref` to element, which is a child of another element that is render in the portal, ref ""current"" property is always  `undefined`.

Sandbox with example here:
https://codesandbox.io/s/refs-through-portals-test-o5lqr

How can i use refs with portals with expected behaviour?"
facebook/react,2020-02-06 12:06:08,question,Question: can I lie to useEffect about its dependencies in this case?,"You [frequently][1] [see][2] warnings that you shouldn't lie to `useEffect` and `useMemo` about their dependencies.


I'm wondering about this special case, which I see throughout the codebase at work:
```
useEffect(() => {
  // We want effect to run when a changes, but not b
  doStuff(a, b)
, [a])
```

Let's say `a` changes every 10 seconds, and `b` changes every second, and we want to `doStuff` whenever `a` changes.

This works, is simple and elegant. Yet all guides discourage this, suggesting to do this instead:
```
let bRef = useRef()
useEffect(() => {
  bRef.current = b
})

useEffect(() => {
  doStuff(a, bRef.current)
}, [a])
```
Why? Aren't those equivalent?

I could see a potential issue with `useMemo(() => compute(a, b), [a])`. It can be accessed in the entire component, and sometimes it will be stale, which contradicts the meaning of memoization.

But what about that useEffect case?

  [1]: https://overreacted.io/a-complete-guide-to-useeffect/#dont-lie-to-react-about-dependencies
  [2]: https://reactjs.org/docs/hooks-faq.html#is-it-safe-to-omit-functions-from-the-list-of-dependencies"
facebook/react,2020-02-03 14:42:12,question,Question: Can useMemo be used instead of useRef?,"Hi, just out of curiosity can `useMemo` be used instead of `useRef` when doing it as following:

Example:

```javascript
const ref = useRef(null);
const ref2 = useMemo(() => { current: null }, []);
```

It looks to me that both refs will be working just fine as DOM ref and as mutable value similar to instance fields in classes. Why then `useRef` is implemented differently comparing to `useMemo` considering [ReactFiberHooks.js](https://github.com/facebook/react/blob/master/packages/react-reconciler/src/ReactFiberHooks.js) code for `useRef` and `useMemo`?

Thanks!"
facebook/react,2020-02-03 13:54:42,question,Bug: react-devtools profiler doesn't show props,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

According to docs, it's possible to see props and state of a component at each commit.
https://reactjs.org/blog/2018/09/10/introducing-the-react-profiler.html#browsing-commits

Yet, in every project I've tried all I see is the names of changed props, but not their values, like the gif from docs claims.


React version: 16.12.0
extension: 4.4.0 (1/3/2020) ... Created from revision f749045a5 on 1/3/2020

## Steps To Reproduce

1. create counter app with CRA or another method
2. open profiler and change state
3. observe how only the name of the prop is visible, but not its value

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

```
function App() {
  let [a, setA] = useState(0);

  return (
    <div className=""App"">
      <button onClick={() => setA(v => v + 1)}>inc</button>
      <Kid a={a} />
    </div>
  );
}

function Kid() {
  return null;
}
```

## The current behavior
[current behavior](https://i.imgur.com/9I6rAIf.png)

## The expected behavior
[expected behavior](https://i.imgur.com/yVf36Ff.png)

Am I doing something wrong or has this feature been removed? I know you can see props in the inspector, but there it's only the most recent ones."
facebook/react,2020-01-26 12:30:09,question,Question: Can the useContext hook be used without a provider?,"Is it a correct form to use the useContext hook without the enclosing provider?

See this simple [sandbox](https://codesandbox.io/s/3q2x15l4rm) which uses  the JediContext.Provider.

In this modified [sandbox](https://codesandbox.io/s/react-hooks-usecontext-z2ryh) I removed the provider and the useContext hook still works.

What are the consequences of using the useContext hook without a provider?"
facebook/react,2020-01-17 12:17:34,question,Question: I have a lot of very nice and smart vuejs components. is there a way to use it in an react project? ,"🚨 This issue tracker is not for questions. 🚨

As it happens, support requests that are created as issues are likely to be closed. We want to make sure you are able to find the help you seek. Please take a look at the following resources.

## Coding Questions

If you have a coding question related to CLOSE THIS PAGE - NOW and CLOSE THIS PAGE - NOW DOM, it might be better suited for Stack Overflow. It's a great place to browse through frequent questions about using CLOSE THIS PAGE - NOW, as well as ask for help with specific questions.

https://stackoverflow.com/questions/tagged/CLOSE THIS PAGE - NOW

## Talk to other CLOSE THIS PAGE - NOW developers

There are many online forums which are a great place for discussion about best practices and application architecture as well as the future of CLOSE THIS PAGE - NOW.

https://CLOSE THIS PAGE - NOWjs.org/community/support.html#popular-discussion-forums

## Proposals

If you'd like to discuss topics related to the future of CLOSE THIS PAGE - NOW, or would like to propose a new feature or change before sending a pull request, please check out the discussions and proposals repository.

https://github.com/CLOSE THIS PAGE - NOWjs/rfcs
"
facebook/react,2020-01-16 10:02:14,question,Bug: useRef can not return a persist ref object,"<!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version:

16.12.0

## Steps To Reproduce

1. Click Button，View print data
2. Click Button，View print data

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:
https://codesandbox.io/s/kind-fog-mce7w

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior
Every time you click the button, the printed data will change

## The expected behavior
Every time I click the button, the printed data should be an empty object"
facebook/react,2020-01-15 00:31:27,question,the worst arabic translation ,"language and translation
i'm sure that you translated your react site by google 
the worst result at all 
i hope you correct it 

https://github.com/reactjs/rfcs
"
facebook/react,2020-01-09 13:19:55,question,"In React and react-dom 16 Upgrade, getting error: Uncaught TypeError: Cannot read property 'input' of undefined","I am trying to upgrade my Reactjs and react-dom versions from v15.6.2 to ^v16.0.0 and am getting the above error.

Clicking through the error in the console, I see that the _React$DOM is undefined. I have followed all the migration instructions on the React.org site and looked extensively, but cannot seem to get rid of this error on versions of react and react-dom of v16 and above.

Any ideas on what that issue is and how I can resolve it?

Below are my package.json and webpack.config. They are pre-upgrade to v16, but I have tried adding core-js per Reactjs.org.

```
var _React$DOM = _react2.default.DOM,
    input = _React$DOM.input, <<< Here is the highlighted error
    select = _React$DOM.select,
    textarea = _React$DOM.textarea;
My dependencies in package.json:
```


 ```
 ""devDependencies"": {
    ""babel-loader"": ""^7.1.5"",
    ""babel-preset-es2015"": ""^6.1.18"",
    ""babel-preset-react"": ""^6.1.18"",
    ""babel-preset-stage-0"": ""^6.24.1"",
    ""chai"": ""^4.1.2"",
    ""chai-jquery"": ""^2.0.0"",
    ""css-loader"": ""^1.0.0"",
    ""eslint-config-rallycoding"": ""^3.2.0"",
    ""mocha"": ""^5.2.0"",
    ""react-addons-test-utils"": ""^15.0.1"",
    ""style-loader"": ""^0.21.0"",
    ""webpack-cli"": ""^3.1.0"",
    ""webpack-dev-server"": ""^3.1.8""
  },
  ""dependencies"": {
    ""actioncable"": ""^5.2.4"",
    ""axios"": ""^0.18.0"",
    ""babel-core"": ""^6.2.1"",
    ""babel-polyfill"": ""^6.26.0"",
    ""babel-preset-stage-1"": ""^6.1.18"",
    ""cloudinary-core"": ""^2.5.0"",
    ""cloudinary-react"": ""^1.0.6"",
    ""dotenv-webpack"": ""^1.5.5"",
    ""file-loader"": ""^2.0.0"",
    ""jquery"": ""^3.3.1"",
    ""jsdom"": ""^12.0.0"",
    ""lodash"": ""^4.1.0"",
    ""prop-types"": ""^15.6.1"",
    ""react"": ""^15.6.2"",
    ""react-bootstrap"": ""^0.32.1"",
    ""react-day-picker"": ""^7.1.6"",
    ""react-dom"": ""^15.6.2"",
    ""react-dropzone"": ""^5.0.1"",
    ""react-helmet"": ""^5.2.0"",
    ""react-redux"": ""^5.0.7"",
    ""react-router-dom"": ""^4.3.1"",
    ""react-stripe-elements"": ""^2.0.1"",
    ""redux"": ""^4.0.0"",
    ""redux-form"": ""^7.4.2"",
    ""redux-thunk"": ""^2.2.0"",
    ""sha1"": ""^1.1.1"",
    ""webpack"": ""^4.17.2""
  }
```
My webpack.config:

```
const webpack = require('webpack');
const Dotenv = require('dotenv-webpack');
const raf = require('raf')

const path = require('path');


module.exports = {
  mode: 'none',
  watch: false,
  entry: [
    'babel-polyfill', './src/index.js'
  ],
  output: {
    path: __dirname,
    publicPath: '/',
    filename: 'bundle.js'
  },
  module: {
    rules: [
      {
        test: /\\.(jpg|png|svg)$/,
        use: [
          {
            loader: 'file-loader',
            options: {
              name: '[path][name].[hash].[ext]'
            }
          }
        ]
      },
      {
        test: /\\.js?$/,
        exclude: /node_modules/,
        use:
          { loader: 'babel-loader',
        options: {
         presets: ['react', 'es2015', 'stage-0']
       }
      }
    }

    ] // end of loaders

  devServer: {
    historyApiFallback: true,
    contentBase: './'
  },
  plugins: [
    new webpack.DefinePlugin({
      'process.env.NODE_ENV': JSON.stringify(process.env.NODE_ENV)
    }),
    new Dotenv()
  ]
};
```"
facebook/react,2020-01-01 22:36:31,question,My Navbar work fine until next day,"Hi, I'm Newbie for Web Dev. I'm starting learning React and next.js and i got issue now i have to do a Navbar so I use Navbar from reactstrap to my next.js project it work fine but nextday I open my project and then my project kaboom.

==

Error: Invalid hook call. Hooks can only be called inside of the body of a function component. This could happen for one of the following reasons:
1. You might have mismatching versions of React and the renderer (such as React DOM)
2. You might be breaking the Rules of Hooks
3. You might have more than one copy of React in the same app
See https://fb.me/react-invalid-hook-call for tips about how to debug and fix this problem.
▶ 2 stack frames were collapsed.
Example
./components/MainNav.js:19
  16 | import 'bootstrap/dist/css/bootstrap.min.css'
  17 | 
  18 | const Example = (props) => {
> 19 |   const [isOpen, setIsOpen] = useState(false);
  20 | 
  21 |   const toggle = () => setIsOpen(!isOpen);
  22 | 
View compiled
▶ 11 stack frames were collapsed.

==

This is my Navbar Code

==
```
import React, { useState } from 'react';
import {
  Collapse,
  Navbar,
  NavbarToggler,
  NavbarBrand,
  Nav,
  NavItem,
  NavLink,
  UncontrolledDropdown,
  DropdownToggle,
  DropdownMenu,
  DropdownItem,
  NavbarText
} from 'reactstrap';
import 'bootstrap/dist/css/bootstrap.min.css'

const Example = (props) => {
  const [isOpen, setIsOpen] = useState(false);

  const toggle = () => setIsOpen(!isOpen);

  return (
    <div>
      <Navbar color=""light"" light expand=""md"">
        <NavbarBrand href=""/"">reactstrap</NavbarBrand>
        <NavbarToggler onClick={toggle} />
        <Collapse isOpen={isOpen} navbar>
          <Nav className=""mr-auto"" navbar>
            <NavItem>
              <NavLink href=""/"">HOME</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href=""/"">WHAT</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">WHO</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">BRANCH</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">WHERE</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">WHEN</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">FAQS</NavLink>
            </NavItem>
            <NavItem>
              <NavLink href =""/"">GAME</NavLink>
            </NavItem>
          </Nav>
          <NavbarText>Simple Text</NavbarText>
        </Collapse>
      </Navbar>
    </div>
  );
}

export default Example;
```
==

What wrong is it and what should i do to resolve this. Thank.
"
facebook/react,2019-12-27 09:06:55,question,Function passed as parameter and saved in Context API store takes old store from Context API,"When I get 401 status code from backend I run refreshToken method with passing the function where expired token occurred. In refreshToken method I get new token and set in refreshTokenLastFunc property function from parameter.

Then I watch when refreshTokenLastFunc was updated using React useEffect and run once again the function where expired token occurred.

The problem is while I run store.refreshTokenLastFunc() in useEffect, the function in refreshTokenLastFunc property uses old Context API store(so it uses old token not the new one). You can read my comment in useEffect for store.refreshTokenLastFunc.

**Do you want to request a *feature* or report a *bug*?**
I don't actually know if it is a React bug or I don't understand JS properly

**What is the current behavior?**
If I ran **store.refreshTokenLastFunc**  function in useEffect it takes old store, not the new one

**What is the expected behavior?**
I want **store.refreshTokenLastFunc**  function to run with new store, because store was modified before this function run

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React verison: 16.12.0
Browser: Google Chrome: 79

My code:
```js
export const StoreProvider = props => {
  const getToken = () => localStorage.getItem(""token"");

  const initState = () => ({
    token: getToken(),
    isAuth: false,
    userRole: ""old role"",
    mainUrl: MainUrl,
    apiUrl: ApiUrl,
    refreshTokenLastFunc: () => {}
  });

  const [store, setStore] = useState(initState());

  const getUserInfo = async () => {
    if (getToken()) {
      try {
        const apiConfig = {
          method: ""GET"",
          headers: {
            ""Content-Type"": ""application/json"",
            Authorization: `Bearer ${store.token}`
          }
        };

        const response = await fetch(`${store.apiUrl}get-me`, apiConfig);
        const responseJson = await response.json();

        if (response.ok) {
          // Update Context API
          setStore({
            ...store,
            userRole: responseJson.role,
            userName: responseJson.name,
            userGroupId: responseJson.group_id,
            isAuth: true
          });
        } else if (response.status === 401) {
          refreshToken(getUserInfo);
        } else {
          throw new Error(`Some error occurred`);
        }
      } catch (error) {
        console.log(error);
      }
    }
  };

  const refreshToken = async func => {
    try {
      const apiConfig = {
        method: ""GET"",
        headers: {
          Accept: ""application/json"",
          Authorization: `Bearer ${store.token}`
        }
      };

      const response = await fetch(`${store.mainUrl}refresh-token`, apiConfig);
      const responseJson = await response.json();

      if (response.ok) {
        // Update token in local storage
        localStorage.setItem(""token"", responseJson.token);
        // Update Context API
        setStore({
          ...store,
          userRole: ""new role"",
          token: responseJson.token,
          refreshTokenLastFunc: func
        });
      } else {
        throw new Error(`Some error...`);
      }
    } catch (error) {
      throw error;
    }
  };

  useEffect(() => {
    getUserInfo();
  }, []);

  useEffect(() => {
    // If I console log my store before calling function, store is correctly updated, but the function uses old store.
    console.log(""store from useEffect: "", store); // store.userRole = 'new role' which is correct
    store.refreshTokenLastFunc(); // store.userRole = 'old role' which should be 'new role'
  }, [store.refreshTokenLastFunc]);

  return (
    <StoreContext.Provider value={[store, setStore, logout, getUserInfo]}>
      {props.children}
    </StoreContext.Provider>
  );
};
```"
facebook/react,2019-12-25 02:09:17,question,How can i change react umd version  in node_modules? ,"i saw react package have `cjs` and `umd` folder
and `index.js` in react output `cjs` version 

so if i want to use `umd` how can i change?

![image](https://user-images.githubusercontent.com/20965813/71428891-b0eb9800-26fe-11ea-8887-691e69203855.png)
"
facebook/react,2019-12-23 17:38:13,question,Opinions on Ionic Reac,"Hello, 

Feasibility of making a project in ionic reaction.

I am interested in knowing more about Ionic react.

And I would like to know if it is advisable to do a project with Ionic React.

How scalable the project would be.

And you can use all the tools that have both ionic and rea, example: camera, maps, notifications, routes, among others.

I appreciate if you can advise me.

Thank you"
facebook/react,2019-12-20 03:11:53,question,Unexpected function component call using useState,"**Do you want to request a *feature* or report a *bug*?**

*question*

**What is the current behavior?**

``` typescript
import React, { useState, useEffect } from ""react"";
import ReactDOM from ""react-dom"";

import ""./styles.css"";

function Child() {
  console.log(""Child render"");
  return null;
}

function App() {
  const [count, setCount] = useState(0);

  console.log(""Render"");

  useEffect(() => {
    console.log(""count changed"", count);
  }, [count]);

  return (
    <div>
      <h2>UseState</h2>
      <p>clicked: {count}</p>
      <button
        onClick={() => {
          setCount(count + 1);
        }}
      >
        +1
      </button>
      <button
        onClick={() => {
          setCount(count);
        }}
      >
        +0
      </button>
      <Child />
    </div>
  );
}

const rootElement = document.getElementById(""root"");
ReactDOM.render(<App />, rootElement);

```

There are two button in this count example. Clicking one makes count +1，another makes count no change. If clicking the no change button first, ""Render"" will not be logged. But if clicking the +1 button first and then clicking anthor，""Render"" will show twice, but """"Child render"" will only show once.

I found the explain in document.

> If you update a State Hook to the same value as the current state, React will bail out without rendering the children or firing effects. (React uses the Object.is comparison algorithm.)

Does React only promise that Children component will not be rendered in this situation? The current component may still be rendered?

This is an example in sandbox.

https://codesandbox.io/s/long-firefly-nz5px?fontsize=14&hidenavigation=1&theme=dark

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

""react"": ""^16.12.0"",
""react-dom"": ""^16.12.0"",




"
facebook/react,2019-12-02 06:48:29,question,question about this.state and this.setState,"Here is my question:
There is a multiselect child component, the  value is maintained in its parent component [this.state.values] like this:

Parent {
 state={
  values:[]
 }
  render(){
    return(
     Multiselect value={this.state.values}
    )
  }
}

in the multiselect 's [onChange] function I control the multiselect value in two ways and two different render results:

First:
  onChange= ()=>{
    this.setState({ values:this.handleValue(this.state.values)});
  }

  handleValues = (values)=>{
    //here to add or splice, eg:
    values.push('treeNode1');
    return values
  }

  when I print this.state.values in the render() function, the 'treeNode1' is added, however ths multiselect component's 'treeNode1' option wasn,t selected;

but if change the onChange() function like this, it works right:
  onChange= ()=>{
    this.setState({ values:this.handleValue(Object.assign([],this.state.values))});
  }

I pass the copy of the [this.state.values] rather than [this.state.values].

I,m confused.

I know that we can,t change varibles in the state directly, however in the first way, [this.state.values] in the render() function is added by 'treeNode1', it looks correctly .

Here is my guess :  React maintains a real state.  When React renders it can get the 'real' state of [this.state.values] which works actually rather than the [this.state.values] that I changed directly in the onChange function by adding 'treeNode1'. The state value[this.state.values] printed in the render() function looks correct ,because the directly change in the handleValues () function rather than changed by this.setState().

I wonder how and why this happens and is my guess right? :)
"
facebook/react,2019-11-25 23:35:19,question,Why useEffect's default behavior is to run on every render?,"**Do you want to request a *feature* or report a *bug*?**
API design question about `useEffect`

**What is the current behavior?**
Currently `useEffect` runs on every render. This default behavior can be dangerous in situations like dealing with HTTP requests when you forget to pass the second argument. This seems to be a common mistake especially for newcomers like myself. I can't think of many (any) patterns where you want to run `useEffect` on every render. What was the reasoning behind not defaulting to run once? 
"
facebook/react,2019-11-04 15:21:26,question,Static rendering strategy doesn't work on codesplitted routes,"I'm not really sure if it is a bug or not but I'm facing problems trying to use static rendering strategy + injecting DOM elements manually to a non hydrated react component. 

**What is the expected behavior?**
The thing that I expect to do is the next:
1 - Render a component on server side and server it to client
2 - On the head of my app having a JS that will inject some DOM elements (ads) on non hydratable components.
3 - On client avoid hydratation of that component using the hacky thing of empty dangerousHTML

**What is the current behavior?**
The thing that I expect to do is the next:
1 - Render a component on server side and server it to client
2 - On the head of my app having a JS that will inject some DOM elements (ads) on non hydratable components.
3 - On client hydratation the code is throwing a ssr vs csr missmatch and is re-building everything.

I've tested it on pages without code-splitting and them seem to work like a charm, only have this behavior on code splitted routes.

What I'm doing wrong?
"
facebook/react,2019-10-31 20:13:50,question,Missing React tab in chrome dev tools,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

**What is the current behavior?**

There is no React tab in my chrome dev toolbar anymore.  I have deleted and reinstalled the react extension but the issue was not resolved.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

**What is the expected behavior?**

There use to be a React tab in the chrome dev console toolbar

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I'm using ios on google chrome."
facebook/react,2019-10-29 23:51:32,question,"""DevTools v4 is incompatible with this version of React"" with React Native & latest version of React","<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Bug
**What is the current behavior?**
After starting react-devtools, I get the following error message:
```
DevTools v4 is incompatible with this version of React
Either upgrade React or install React DevTools v3:
```

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**
* Use expo cli to initialize an expo app
* run `yarn add react@latest` to ensure the most recent version of React is installed (currently v16.11.0)
* run `yarn add react-devtools` to install react-devtools
* run `yarn start` to start the app
* run `npx react-devtools` to open react-devtools

**What is the expected behavior?**
react-devtools should work with React Native and the latest version of React

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16.11.0
React Native 0.59.3
Fedora 30"
facebook/react,2019-10-28 07:31:07,question,"""de-opting to synchronous mode"" in use-subscription README","<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Outdated README (maybe?)

**What do the docs currently say?**

> use-subscription is safe to use in concurrent mode. However, [it achieves correctness by sometimes de-opting to synchronous mode](https://github.com/facebook/react/issues/13186#issuecomment-403959161), obviating the benefits of concurrent rendering.

In the linked issue, @bvaughn [explains](https://github.com/facebook/react/issues/13186#issuecomment-403959161) that this is referring to chains of synchronous updates using `componentDidUpdate`. However, the `useSubscription` hook now uses a passive `useEffect()`, as opposed to a synchronous `componentDidUpdate()`.

Would this mean that it's no longer ""de-opting to sync mode"", and the warning could be removed from the README?"
facebook/react,2019-10-14 19:57:20,question,Can't get the profiler screenshot feature working in the React DevTools,"I've recently come across [this tweet](https://twitter.com/brian_d_vaughn/status/1113200027835310080) and saw that the new DevTools profiler apparently would be able to capture images of the DOM after each commit and display them on the right side (below the commit information). 

Was this feature indeed added to DevTools 4.2.0? Or is it coming out in a future version?

Just asking because I didn't manage to get it working while trying with my projects.

**React**: 16.10.2
**Browser:** Google Chrome 77.0.3865.90
**DevTools:** 4.2.0-a8b8ffb89"
facebook/react,2019-10-08 08:07:27,question,Components not correctly displayed,"**Do you want to request a *feature* or report a *bug*?**

bug

**What is the current behavior?**

elements not reconized

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

only use components tab in console of chrome.

**What is the expected behavior?**

see any elements react

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Window 10 64bit
react ^16.8.1


![image](https://user-images.githubusercontent.com/13313/66378070-068f8e80-e9b3-11e9-9956-cd8b98525efb.png)

This is my console component visualizzation, the same page visualizated with linux&chrome not have any issue"
facebook/react,2019-09-19 08:14:31,question,Plans for handling `hidden` differently,"In one of talks about react Suspense [dan mentioned](https://youtu.be/6g3g0Q_XVb4?t=1650) that `<div hidden={true} />` would be deprioritized by react but still rendered if it has the time.

In his example he used it to prerender content that was seemingly part of another page. However in another talk by andrew it was used to [prerender tabs](https://youtu.be/ByBPyMBTzM0?t=1742). The second showcase is incorrect according to the [living standard for this attribute](https://html.spec.whatwg.org/multipage/interaction.html#the-hidden-attribute):

> The hidden attribute must not be used to hide content that could legitimately be shown in another presentation. For example, it is incorrect to use hidden to hide panels in a tabbed dialog, because the tabbed interface is merely a kind of overflow presentation — one could equally well just show all the form controls in one big page with a scrollbar.

I don't necessarily agree with the reasoning given in the spec but I'm more interested if the core team is aware of this conflict and if there are plans to resolve this somehow or simply ignore it."
facebook/react,2019-09-12 14:45:25,question,Add third parameter to useState to get current value,"**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

If you set a callback on something, e.g. inside useEffect, your callback captures the current value of that state and can never update it. To know the most up-to-date value, you currently have to call the setter with a function containing the new value, even if you only return the value you receive from it. Cf. https://stackoverflow.com/questions/57847594/react-hooks-accessing-up-to-date-state-from-within-a-callback#comment102122412_57847594

**What is the expected behavior?**

There should be a way to access the current state via a getter for these situations. Adding this as a third return value from useState would be non-intrusive and backwards compatible.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

n/a"
facebook/react,2019-09-05 21:54:48,question,React DevTools downgrade not working for Chrome.,"**Do you want to request a *feature* or report a *bug*?**
Bug
**What is the current behavior?**
The following command fails
```
yarn run test:chrome 
```
error message:
```                                                                                
yarn run v1.17.3
$ node ./shells/chrome/test
internal/validators.js:112
    throw new ERR_INVALID_ARG_TYPE(name, 'string', value);
    ^

TypeError [ERR_INVALID_ARG_TYPE]: The ""file"" argument must be of type string. Received type object
    at validateString (internal/validators.js:112:11)
    at normalizeSpawnArguments (child_process.js:398:3)
    at spawn (child_process.js:534:16)
    at launchChrome (/home/andrei/src/react-devtools/node_modules/chrome-launch/index.js:27:12)
    at Object.<anonymous> (/home/andrei/src/react-devtools/shells/chrome/test.js:9:1)
    at Module._compile (internal/modules/cjs/loader.js:936:30)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:947:10)
    at Module.load (internal/modules/cjs/loader.js:790:32)
    at Function.Module._load (internal/modules/cjs/loader.js:703:12)
    at Function.Module.runMain (internal/modules/cjs/loader.js:999:10)
error Command failed with exit code 1.
```

**What is the expected behavior?**
Launch a new browser window. The following command works
```
yarn run test:firefox 
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
Using Version 76.0.3809.100 (Official Build) (64-bit), /usr/bin/google-chrome-stable is aliased to chrome.

Any help is appreciated, we're running a old version of React at work and upgrading it would be nontrivial. Hence I need the old version of the React DevTools."
facebook/react,2019-09-03 21:37:55,question,"[Dev Tools] Chrome component console errors get output from ""backend.js""","**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

When I get a component stack trace, the log trace comes from the dev tools extension:

![image](https://user-images.githubusercontent.com/12100/64210499-4d401500-ce58-11e9-87f5-b0c9115fe62f.png)

**What is the expected behavior?**

I use that trace usually to click and set a debugger so I can debug some React internals.
"
facebook/react,2019-09-02 18:49:15,question,Inline setTimeout within the render causes a random number to appear on screen.,"Reporting a bug:

The current behaviour is that setTimeout, inside a render method, is producing a random number, for no explained reason. The number persist after the setTimeout has gone off, and the setTimeout still produces the desired effect, for example, if it was to log something the console, it would still do that.
```jsx
// sample code

(not the original code in which the problem occurred, but I was able to reproduce the problem in codesandbox.io):

** start of code example **
(styles.css = 
    .App {
        font-family: sans-serif;
        text-align: center;
    }
)

import React from ""react"";
import ReactDOM from ""react-dom"";

import ""./styles.css"";

function App() {
  return (
    <div className=""App"">
      <h1>Hello CodeSandbox</h1>
      <h2>Start editing to see some magic happen!</h2>
      {setTimeout(() => console.log(""hello world""), 3000)}
    </div>
  );
}

const rootElement = document.getElementById(""root"");
ReactDOM.render(<App />, rootElement);
```

/* (output of webpage) */

Hello CodeSandbox
Start editing to see some magic happen!
5

** end of code **

The expected behaviour was that the setTimeout function would produce no trace of any text, whilst still working.

It is present in react version 16.9.0"
facebook/react,2019-08-31 14:31:51,question,Can't use instance values without refs,"I am experimenting with migrating some of my class components to Hooks. My use case is explained in detail [here](https://stackoverflow.com/questions/57722818/idiomatic-react-with-heavy-dom-manipulation-mathjax), but I can quickly explain the idea.

I have a class component `<MJX>` which exposes a `.ready` Promise. Since `<MJX>` produces a `<span>` element, I guess I can use the `useImperativeHandle` hook to attach `.ready` to the ref I use for the `<span>`. This is no problem.

However, I also have a class component `<RenderGroup>` which uses `React.Children` to collect the `.ready` Promises from all its `<MJX>` descendants, and exposes `Promise.all` of that array of Promises as its own `.ready` value. Since `<RenderGroup>` does not produce any markup of its own, and you can't place refs on `<React.Fragment>`, I don't see how to use`useImperativeHandle` here: there's nowhere to attach the ref.

Here is the relevant code:

```tsx
export class RenderGroup extends React.Component {
  private promises: Promise<void>[];

  ready: Promise<void>;

  componentDidMount() {
    this.ready = Promise.all(this.promises).then(() => {});
  }

  render() {
    this.promises = [];

    return recursiveMap(this.props.children, node => {
      if (shouldInspect(node)) {
        const originalRef = node.ref;
        return React.cloneElement(node, {
          ref: (ref: MJX) => {
            if (!ref) return;
            this.promises.push(ref.ready);
            if (typeof originalRef === ""function"") {
              originalRef(ref);
            } else if (originalRef && typeof originalRef === ""object"") {
              (originalRef as React.MutableRefObject<MJX>).current = ref;
            }
          }
        });
      }

      return node;
    });
  }
}

function shouldInspect(node: ReactNode): node is React.ReactElement & React.RefAttributes<MJX> {
  return React.isValidElement(node) && typeof node.type === ""function"" && node.type.prototype instanceof MJX;
}

export function recursiveMap(
  children: ReactNode,
  fn: (child: ReactNode) => ReactNode
) {
  return React.Children.map(children, (child) => {
    if (!React.isValidElement(child)) {
      return child;
    }

    if (""children"" in child.props) {
      child = React.cloneElement(child, {
        children: recursiveMap(child.props.children, fn)
      });
    }

    return fn(child);
  });
}
```"
facebook/react,2019-08-28 23:30:26,question,How should we set up apps for HMR now that Fast Refresh replaces react-hot-loader?,"Dan Abramov mentioned that Devtools v4 will be making `react-hot-loader` obsolete: https://twitter.com/dan_abramov/status/1144715740983046144?s=20

> **Me:**
> I have this hook:
> ```require(""react-reconciler"")(hostConfig).injectIntoDevTools(opts);```
> But HMR has always worked completely without it. Is this now a new requirement?

> **Dan:**
> Yes, that's what the new mechanism uses. The new mechanism doesn't need ""react-hot-loader"" so by the time you update, you'd want to remove that package. (It's pretty invasive)

I can't see any mention of HMR in the Devtools documentation, however; now that `react-hot-loader` has become obsolete (and with it, the `require(""react-hot-loader/root"").hot` method), how should we set up apps for HMR in:

* React DOM apps
* React Native apps
* React custom renderer apps

I'd be particularly interested in a migration guide specifically for anyone who's already set up HMR via `react-hot-loader`.

Also, for HMR, does it matter whether we're using the standalone Devtools or the browser-extension Devtools?"
facebook/react,2019-08-21 02:20:13,question,hook component can not been clicked in react-dev-tool if you want it works well.,"### issue type
bug

### issue description
look at the code blow (it is very very very simple).
```jsx
import React, { useState } from 'react';

export default function DemoCounter() {
  const [fnWrapper] = useState('fn');
  const [count, setCount] = useState(0);
  fnWrapper.__proto__.setCount = s => {
    setCount(s);
  }

 // assign fnWrapper.__proto__.setCount to callSetCount
  const callSetCount = fnWrapper.__proto__.setCount;

  return (
    <div style={{ border: '1px solid blue', margin: '8px' }}>
      count: {count}
      {/* this does not work if I open react-dev-tool and click the dom node */}
      <input value={count} onChange={e => fnWrapper.__proto__.setCount(e.currentTarget.value)} />

       {/* this always works not matter I open react-dev-tool and click the dom node or not */}
      <input value={count} onChange={e => callSetCount(e.currentTarget.value)} />
    </div>
  );
}

```
### why callSetCount always works but `fnWrapper.__proto__.setCount` not if and only if after I open the react-dev-tool and click the dom node ~~~~(>_<)~~~~，

### please tell me the truth.
"
facebook/react,2019-08-16 09:53:28,question,is that possible to get legacy version of react debugger?,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

bug: highlight element is no available in ver 4+ react debugger
feature: new version of react develop is good, bug sometimes i need legacy version of develop tool(i jest need BOTH of them), can you provide both of them?
"
facebook/react,2019-05-12 09:19:13,question,Infinite lint error of react-hooks/exhaustive-deps,"First attempt was:
```javascript
const setField = (args) => {
        const newForm = Object.assign({}, form);
        for (let i in args) {
            if (args.hasOwnProperty(i)) {
                newForm[i] = args[i];
            }
        }
        setForm(newForm);
}

useEffect(() => {
        setAutoComplete(address.current, (args) => {
            setField(args);
        });
}, []);
```

I have  the error:

> React Hook useEffect has a missing dependency: 'setField'. Either include it or remove the dependency array

If I follow what the lint says:
```javascript
const setField = (args) => {
        const newForm = Object.assign({}, form);
        for (let i in args) {
            if (args.hasOwnProperty(i)) {
                newForm[i] = args[i];
            }
        }
        setForm(newForm);
}

useEffect(() => {
        setAutoComplete(address.current, (args) => {
            setField(args);
        });
 }, [setField]);
```

I have another error:

> Line 58:  The 'setField' function makes the dependencies of useEffect Hook (at line 72) change on every render. To fix this, wrap the 'setField' definition into its own useCallback() Hook

Then I follow the advise again:

```javascript
    const setField = (args) => {
        const newForm = Object.assign({}, form);
        for (let i in args) {
            if (args.hasOwnProperty(i)) {
                newForm[i] = args[i];
            }
        }
        setForm(newForm);
    }

    const myCallback = useCallback(() => setField, []);

    useEffect(() => {
        setAutoComplete(address.current, (args) => {
            myCallback(args);
        });
    }, [myCallback]);
```

Then the error is:

> Line 68:  React Hook useCallback has a missing dependency: 'setField'. Either include it or remove the dependency array

And then the code:

```javascript
    const setField = (args) => {
        const newForm = Object.assign({}, form);
        for (let i in args) {
            if (args.hasOwnProperty(i)) {
                newForm[i] = args[i];
            }
        }
        setForm(newForm);
    }

    const myCallback = useCallback(() => setField, [setField]);

    useEffect(() => {
        setAutoComplete(address.current, (args) => {
            myCallback(args);
        });
    }, [myCallback]);
```

then the error

> Line 58:  The 'setField' function makes the dependencies of useCallback Hook (at line 68) change on every render. To fix this, wrap the 'setField' definition into its own useCallback() Hook

Am I doing something wrong?

Thanks!
"
facebook/react,2019-04-30 14:36:46,question,Understanding `act` behaviour," I have been trying to use `act` for the first time, and having some issues, and so I'm wondering if my expectations are wrong about what it is supposed to do, or if I am ""doing it wrong"".

**What is the current behavior?**

The only way I can observe the results of state changes I initiate is by using a timeout.

**Paste the link to your JSFiddle or CodeSandbox example below:**

https://codesandbox.io/s/k5zmln6w83?expanddevtools=1&fontsize=14&hidenavigation=1

**What is the expected behavior?**

What I expect is that by wrapping a state change or render operation in `act`, all of the resulting state changes / side-effects / re-renders will be complete by the time `act` returns, so that the operation appears (or is coerced to be) synchronous. 

I created an example (https://codesandbox.io/s/k5zmln6w83?expanddevtools=1&fontsize=14&hidenavigation=1) wherein I render a view via `unstable_ConcurrentMode`. In the view, I create a `useState` hook with a value of `0`. After the view is rendered, I use that hook's setter to change its state to `1`.

Below is a log of the [steps I take](https://codesandbox.io/s/k5zmln6w83?expanddevtools=1&fontsize=14&hidenavigation=1), showing three values at each time: `seenByRender`, the last state-value that appeared within the render body; `calculated`, the last value returned from my state-update function; and `seenByEffect`, the last value observed from a `useEffect` I create in the view. 

```
1. before act/render:   seenByRender: null, calculated: null, seenByEffect: null
2. after act/render:    seenByRender: 0,    calculated: null, seenByEffect: 0
3. ---incrementing with act/setState---
4. after act/increment: seenByRender: 0,    calculated: 1,    seenByEffect: 0
5. after timeout:       seenByRender: 1,    calculated: 1,    seenByEffect: 1
```

What I am wanting/expecting is for step 4 to look like step 5, ie, I can somehow test the full consequences of my setState call.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.8.6, using unstable_ConcurrentMode
"
facebook/react,2019-04-27 04:30:20,question,License for Source Code Examples & Tutorials on https://reactjs.org,"
What is the license for the tutorials and examples on https://reactjs.org?"
facebook/react,2019-04-25 13:52:38,question,"[eslint-plugin-react-hooks]: bug React Hook ""XXX"" is called in function ""children""","<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**bug**

**What is the current behavior?**

The hooks created in children function provided in defaultProps populates error `react-hooks/rules-of-hooks`.

```js
const App = props => props.children(props);

App.defaultProps = {
  children: props => {
    const [count, setCount] = useState(0);

    return (
      <>
        count: {count}
        <button onClick={() => setCount(count + 1)}>Increment</button>
      </>
    );
  },
};
```

`React Hook ""useState"" is called in function ""children"" which is neither a React function component or a custom React Hook function  react-hooks/rules-of-hooks
`

https://codesandbox.io/s/v60mm6yvx0

**What is the expected behavior?**

Hooks inside children function in defaultProps should not create an error when hook is used. 

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Same as in the codesandbox.
```
""dependencies"": {
  ""react"": ""16.8.6"",
  ""react-dom"": ""16.8.6"",
  ""react-scripts"": ""3.0.0""
},
```"
facebook/react,2019-04-24 09:57:42,question,why useffect  run  return( )  at  frist,"```jsx
export const useConfigStateHook = () => {
  const [visible, setVisible] = useState('hidden')
  const [cofigVisible, setCofigVisible] = useState(false)
  const [card, setCard] = useState(<div />)

  useEffect(() => {
    eventProxy.on('setConfigBtn', (value) => {
      setVisible(value)
    })
    eventProxy.on('setConfigOpen', (value) => {
      setCofigVisible(value ? false : !cofigVisible)
    })
    eventProxy.on('setCard', (card) => {
      const { Ichart, props } = card

      if (!Ichart) return setCard(<div />)

      setCard(
        <Ichart
          ref={(e) => {
            e && eventProxy.trigger('chartref', e)
            e &&
              setTimeout(() => {
                eventProxy.trigger('setConfigView', e.renderConfig())
              }, 200)
          }}
          {...props}
        />
      )
    })
    return () => {
      eventProxy.off('setCard')
      eventProxy.off('setConfigBtn')
      eventProxy.off('setConfigOpen')
      eventProxy.off('setConfigView')
    }
  })

  return { visible, cofigVisible, card }
}
```
"
facebook/react,2019-04-12 20:01:40,question,Using app shell architecture throws Warning: Expected server HTML to contain a matching <div> in <div>.,"I am currently using workbox along with webpack. The app shell url is /app-shell. I tried following 2 approaches:

1. In server.js, I created a dedicated route to handle /app-shell URL.  

   ```javascript
   server.get('/app-shell', (request, response) => {
  	response.set('content-type', 'text/html');
	response.write(`<!DOCTYPE html>
	     … 
            <body>
               <div id=""root""></div>
               <script src=""js/client.js""></script>
               <script src=""js/vendor.js""></script>
              <script>
                 if ('serviceWorker' in navigator) {
                    window.addEventListener('load', () => {
                   navigator.serviceWorker
                  .register('/service-worker.js', { scope: '/' })
                  .then((registration) => {
                      console.log('ServiceWorker registration successful with scope: ', registration.scope);
                })
                .catch((registrationError) => {
                    console.log('SW registration failed: ', registrationError);   
                });
            });
          }
         </script>
      </body>		
       `);
   });  
   ```
2. Second approach, I created a route /app-shell which corresponds to an element with content “Loading…”.  

  ```javascript
   const AppShellComponent: React.FC<{}> = (): JSX.Element => (
  	<main>
   	     Loading...
        </main>
  );
  ```

  In both cases, I get the above mismatch warning. If I make AppShellComponent exactly same as HomeComponent i.e. path /, then the error goes away if the app is invoked from the home page path i.e. / . However, if any other route is invoked, the warning comes back again. Please suggest a better approach. Thanks.
"
facebook/react,2019-04-10 12:31:13,question,TestUtils.renderIntoDocument returns `null` when valid functional component passed.,"**Do you want to request a *feature* or report a *bug*?**
I want to report a bug.

**What is the current behavior?**
Currently when valid functional component is passed to **TestUtils.renderIntoDocument** it returns a **null** and raise no error.

**Demo**
This behavior was reproduced in a sandbox: https://codesandbox.io/s/1zpvll4j24

Check the console, to see **TestUtils.renderIntoDocument** output of prepared sample components.

**Workarounds**
Workaround that satisfies both SFC and FC is wrapping component into container:
```
const FCCounter = () => {
  const [count, setCount] = useState(0);

  return (
    <div>
      <div>{count}</div>
      <button onClick={() => setCount(count + 1)}>+1</button>
    </div>
  );
};

TestUtils.renderIntoDocument(
  <div>
    <FCCounter />
  </div>
);
```

**What is the expected behavior?**

- To render a functional component.
- In worst case - providing an error.

**Which versions of React are affected by this issue?**
React version: 16.8.6
"
facebook/react,2019-04-05 04:53:16,question,react hooks: a few inconveniences,"1) When using useEffect, can not get latest state. 

For example, I add a event listener in componentDidMount.
```
const App = () => {
    const [state1, setState1] = useState('state1');
    const [state2, setState2] = useState('state2');
    useEffect(() => {
        function handler() {
            // can't get the latest state1 and state2 because of the scope.
        }
        document.addEventListener('click', handler);
        return () => document.removeEventListener('click', handler);
    }, [])
}
```
I have to change code into this:
```
const App = () => {
    const [state1, setState1] = useState('state1');
    const [state2, setState2] = useState('state2');
    useEffect(() => {
        function handler() {
            // for the reason of scope, can't get the latest state1 and state2.
        }
        document.addEventListener('click', handler);
        return () => document.removeEventListener('click', handler);
    }, [state1, state2])
    ....
}
```
problem solved, but the effect executes again. (wasteful ???)

2) functional props cause updating subComponent every time.

```
const App = () => {
    const [state, useState] = useState(0)

    function handleChange() {
      // do something
    }
    return (
        <SubComponent  onChange={handleChange} />
    );
}
```
In this case, SubComponent will update every time App rerender, because the onChange prop is Changed. If we need to use state or props in handleChange, we have nothing to do with it.(If we useCallback, see No.1, the lastest state issue).
"
facebook/react,2019-04-03 15:26:05,question,Hook for forwardRef,"
**Do you want to request a *feature* or report a *bug*?**
  feature
**What is the current behavior?**
Today we use forwardRef as a HOC around functional component, like this:
```js
const MyComponent = React.forwardRef((props, ref) => {
  return (<div {...props} ref={ref}>Some text</div>);
});

MyComponent.displayName = 'MyComponent';
```

**What is the expected behavior?**
Is it possible to use hooks to forward ref? For example, something like this:
```js
const MyComponent = (props) => {
  // will return ref that was passed from parent component
  const forwardedRef = useForwardredRef(); 
  return (<div {...props} ref={forwardedRef}>Some text</div>);
}
```
With this approach we don't need to manually set displayName all the time, also this would be great for libraries, as you still export same component, not a forwardRef HOC.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16.8 and upper"
facebook/react,2019-04-01 10:09:51,question,useEffect firing in children before parent,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Feature (I believe)
**What is the current behavior?**
Right now, the effects in useEffect fire children-first. I know this makes sense, since the behaviour of the first render has a close correlation to cDm. However, I cannot seem to find a way to fire events parent-first. Before, I could do it with cWm, and after that was deprecated, in the constructor. How can I accomplish that with hooks?
**CodeSandbox example:**
https://codesandbox.io/s/035lqnozzl?fontsize=14
**What is the use case?**
Imagine I want to post to an external server when two components were first rendered, to measure a sort of meaningful paint.
```
- Component A -> post(""first render"""", component: A)
----
--------
------------ Component B -> post(""first render"", component: B)
```
How could I accomplish this with hooks?"
facebook/react,2019-03-20 09:00:18,question,is passing a ref to useMemo considered cheating?,"I have an object which is created via useMemo

I do not want this object to be recreated every time a certain dependency changes, and yet I want it to always see the latest version of that dependency (not just what it was upon initial creation)

Currently, I'm doing something like this:

```
//bar changes between renders
const latestFoo = useRef(bar); 

//obj will only be created once, yet it sees the updated latestFoo
const obj = useMemo(() => ({
 doSomething: () => { 
    //use and/or set latestFoo.current 
  }
}), [latestFoo]);
```

This feels like a little bit of a lie... because it kindof depends on an updated `latestFoo.current`... not really `latestFoo` the ref, if that makes sense.

Yet it works fine as far as I can see....

Not sure if I should feel bad about lying to `useMemo` about its deps, or if it's not really a lie at all and this is actually a totally expected use case of `useRef`

Any tips are appreciated.

P.S. this was prompted by refactoring my code after reading this page, which was an _enourmous_ help in understanding hooks and writing code that uses them more consciously:  https://overreacted.io/a-complete-guide-to-useeffect/

@gaearon would you rather me post questions like this somewhere else? Didn't see a comment section on the site and I don't use twitter..."
facebook/react,2019-03-16 00:53:35,question,useRef only updates with a useState together,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Probably a bug

**What is the current behavior?**
I'm creating a component swapper triggeredd by a interval. I had to use the useRef hook to keep the index state between renders, but it only gets updated when I keep a useState (the setCurrentIndex) is with it. When the setCurrentIndex at line 17 is removed, the swapper doesn't works.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

Please check it out https://codesandbox.io/s/ooy2xrvxky

**What is the expected behavior?**
I tought that the indexRef wouldn't depend on the state hook to be updated.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16.8.4, Chrome 72.0, Os Ubuntu 16.04."
facebook/react,2019-03-06 19:51:42,question,React Hooks useState updating an array,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Hooks Clarification

**What is the current behavior?**
I'm trying to understand the lifecycle of `useState`.

I have a mock application using a mock websocket. Every second, it sends a new message from the backend. The new message is meant to be appended to an array using `useState`. 

Here are a few different examples that highlight my confusion:

**Example 1**
[In this example](https://codesandbox.io/s/zz2lj8knyp), if I set the websocket's `onmessage` function once in `useEffect`, then whenever I call `setMessages` to update the `messages` array, the `messages` array I receive as an input is empty.

```js
const [messages, setMessages] = useState([]);

function receiveMsg(msg) {
  setMessages(messages.concat(JSON.parse(msg.data)));
}

useEffect(function() {
    if (_.isUndefined(socket)) {
      let ws = new MockWebsocket(""ws://127.0.0.1:8080/"");
      ws.onmessage = receiveMsg;
    }
});
```

The effect of this is that I only ever get the latest message in my array.

**Example 2**
If, however, I set the `onmessage` function on every render [as in this example](https://codesandbox.io/s/73kr25mkrj), then I get my full array with data appended as I would expect.

```js
const [messages, setMessages] = useState([]);

function receiveMsg(msg) {
  setMessages(messages.concat(JSON.parse(msg.data)));
}

if (!_.isUndefined(socket)) {
  socket.onmessage = receiveMsg;
}

useEffect(function() {
  if (_.isUndefined(socket)) {
    let ws = new MockWebsocket(""ws://127.0.0.1:8080/"");
    ws.onmessage = receiveMsg;
  }
});
```

In the `receiveMessage` function, my `messages` array is the whole array instead of an empty one in this example.

**Example 3**
BUT, if I assign a new reference to `messages`, [as in this example](https://codesandbox.io/s/j1n0py5zz3), and re-assign the value inside `receiveMsg`, then I don't have to re-assign the `onmessage` function over and over.

```js
const [messages, setMessages] = useState([]);
let msgRef = messages;

function receiveMsg(msg) {
  msgRef = msgRef.concat(JSON.parse(msg.data));
  setMessages(msgRef);
}

useEffect(function() {
  if (_.isUndefined(socket)) {
    let ws = new MockWebsocket(""ws://127.0.0.1:8080/"");
    ws.onmessage = receiveMsg;
  }
});
```

**Example 4**
But, if I assign a new reference and don't re-assign to it, [as in this example](https://codesandbox.io/s/8pkwr86p29), I continue ending up with an empty array. This suggests it's the assignment back to `msgRef` that is keeping the entire array within the closure.

```js
const [messages, setMessages] = useState([]);
let msgRef = messages;

function receiveMsg(msg) {
  setMessages(msgRef.concat(JSON.parse(msg.data)));
}
```

**What is the expected behavior?**
My original expectation was that example #1 would work. I can tell there's something I'm not totally understanding about the way the assignment of variables to closure works with this hook, but I'm struggling to define what exactly's going on. Can someone shed some light on why this works this way?

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

React 16.8"
facebook/react,2019-03-04 23:37:21,question,exhaustive-deps support for dynamic array,"Maybe it's antipattern and maybe it isn't. Note how I leverage deps for createdAt subselect.

```ts
const TaskLists: FunctionComponent = () => {
  const taskLists = useAppState(state => state.taskLists);
  const sortedTaskLists = useMemo(() => {
    return Object.values(taskLists).sort((a, b) => a.createdAt - b.createdAt);
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, Object.values(taskLists).map(item => item.createdAt));

  const children = useMemo(() => {
    return (
      <>
        {sortedTaskLists.map(taskList => (
          <TaskListLink id={taskList.id} key={taskList.id} />
        ))}
      </>
    );
  }, [sortedTaskLists]);

  return children;
};
```"
facebook/react,2019-02-20 14:22:20,question,Why moving item to another array fire re-rendering for itself (the item)?,"Hello,

I have the following problem :

— When I drag/drop item to an another `array()` > it re-renders the item I moved — (Why?)

— When I drag/drop in the **same** `array()` > it's not renders. So it's GOOD !

The problem is my deepest child, I have a little `fetch()` to retrieve some additional data. 
These data are already fetched.
So when I drag/drop **I don't want** another fetch. 
Event `React.memo()` solve the problem.

You can check the console of CodeSandbox to see the problem :


###  [https://codesandbox.io/s/03on3k75xw](https://codesandbox.io/s/03on3k75xw)



**What is the expected behavior?**

No re-render of the item, since React already displays it, I just moved to another array. It's still the same item.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

```
React 16.8.1
MacOS 10.14.2
```

React Beautiful Dnd : https://github.com/atlassian/react-beautiful-dnd

"
facebook/react,2019-02-15 01:07:34,question,`react-dom/server.renderStaticMarkup()` returns empty string server-side,"**Do you want to request a *feature* or report a *bug*?**

**bug**

**What is the current behavior?**

I'm trying to extract the initial HTML from react code on the server side. To do this, I'm using the official react-dom/server library function `renderToStaticMarkup()` referenced here: https://reactjs.org/docs/react-dom-server.html#rendertostaticmarkup

I'm reading a react source file, transpiling the JSX and ES6 syntax to CommonJS using Babel and then parsing the evaluated CommonJS to `renderToStaticMarkup()`.

## React code:
```jsx
import React from 'react';

class Test extends React.Component {
    render() {
        return <p>Hello World!</p>;
    }
}

export default Test;
```

## Server-side code:
```javascript
const { renderToStaticMarkup } = require( 'react-dom/server');
const Babel = require( '@babel/core' );
const Fsp = require( 'fs' ).promises;

(async () => {
    let fileContent = await Fsp.readFile( 'test.js', 'utf-8' );

    let code = Babel.transform( 
        fileContent, 
        { 
            presets: [ '@babel/preset-env', '@babel/preset-react' ],
            comments: false,
            minified: true
        }).code;

    let result = renderToStaticMarkup( code );

    console.log( result );
})();
```

## package.json:
```json
{
  ""dependencies"": {
    ""@babel/core"": ""^7.3.3"",
    ""@babel/preset-env"": ""^7.3.1"",
    ""@babel/preset-react"": ""^7.0.0"",
    ""react"": ""^16.8.2"",
    ""react-dom"": ""^16.8.2""
  }
}
```

**What is the output?**
`&quot;use strict&quot;;Object.defineProperty(exports,&quot;__esModule&quot;,{value:true});exports.default=void 0;var _re act=_interopRequireDefault(require(""react""));function...`

**What is the expected output?**
`<p>Hello World!</p>`


**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
```json
""react"": ""^16.8.2"",
""react-dom"": ""^16.8.2""
```"
facebook/react,2019-02-08 16:06:16,question,Module mock yields unexpected act() error,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
bug

**What is the current behavior?**
I am trying to figure out where is the best place to put a module mock according to the new react version 16.8.1.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**
``` javascript
import React from 'react';
import { act } from 'react-dom/test-utils';
import { mount } from 'enzyme';
import { getPointsFromGroupID } from './utils/getPointsFromGroupID';
import { TDashBody } from '.';

jest.mock('./utils/getPointsFromGroupID', () => ({
  getPointsFromGroupID: jest.fn(() => Promise.resolve({})),
}));

const getDefaultProps = () => ({
  currentGroup: 0,
});
describe('DashBody', () => {
  let component;
  beforeEach(() => {
    act(() => {
      const props = getDefaultProps();
      component = mount(<TDashBody {...props} />);
    });
  });
  describe('rendering', () => {
    it('Should render without exploding', () => {
      expect(component).toBeDefined();
    });
  });
});
```

**What is the expected behavior?**
This test passes, but the placement of the mock module call gives me the act() error
```
Warning: An update to TDashBody inside a test was not wrapped in act(...).

      When testing, code that causes React state updates should be wrapped into act(...):

      act(() => {
        /* fire events that update state */
      });
      /* assert on the output */

      This ensures that you're testing the behavior the user would see in the browser. Learn more at https://fb

.me/react-wrap-tests-with-act
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
v16.8.1 running chrome on windows 10"
facebook/react,2019-02-06 20:18:31,question,react-cache alphas don't work with 16.8+,"> `react-cache` was not published with 16.8.1 like the rest of the react packages.
This means that the platform's Suspense stuff will not work.

_Originally posted by @jaredpalmer in https://github.com/palmerhq/the-platform/pull/70#issuecomment-461170282_
"
facebook/react,2019-02-06 17:21:12,question,Warning for `act` even when code is wrapped inside it,"**Do you want to request a *feature* or report a *bug*?**

Report a bug

**What is the current behavior?**

`react-test-renderer` emits a warning to wrap code inside `act` even though it is.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

Here is a [CodeSandbox](https://codesandbox.io/s/wwopxr137k?expanddevtools=1&previewwindow=tests) with a component using `useState` that toggles a div on/off on a button click. The test finds the button, calls the `onClick` on the props. This would be followed by a snapshot test. However, no matter how I try and wrap the code in `act`, the warning persists.

Make sure to open the tests tab on the right, and to expand the console at the bottom:

![image](https://user-images.githubusercontent.com/433409/52360399-ac50bf00-2a1a-11e9-9cf1-ba34b642102f.png)


**What is the expected behavior?**

There should be no warning.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

react@16.8
react-dom@16.8
react-test-renderer@16.8
jest@24.1.0

This worked fine with react*@16.7.0-alpha.2, snapshot and all."
facebook/react,2019-01-26 20:39:04,question,hooks: useContext with useState not updating,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

seems it's a **bug**. 😕 

**What is the current behavior?**

Nested context provider and `useContext` hooks seems to be conflicting, updates get discarded.

**What is the expected behavior?**

When connecting to a context, it should update whenever it's `value` changes.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

- **react**: `18.8.0-alpha.1` (also reproduced on `16.7.0-alpha.0`)
- **browser**: `chrome 71`
- **os**: macOS Sierra

---

### more details

While working on a cleanup of a localStorage ""connection"",
I tried to mix 2 articles ([`[1]`](https://reactjs.org/docs/hooks-faq.html#how-to-avoid-passing-callbacks-down) & [`[2]`](https://reactjs.org/docs/context.html#updating-context-from-a-nested-component)) from the official react documentation, I've implemented it with hooks, but the value seems not to be passing through.

I've put up a streamlined demo on [codesandbox `[3]`](https://codesandbox.io/s/0yzjr8vnrv).

The actual implementation is only a couple of lines more (parsing it from and stringifying it to JSON).

Workarounds that I found:
- If I create a new function on each render around the `setValue` function, it actually works. 
  - but this goes against the advice on [`[1]`](https://reactjs.org/docs/hooks-faq.html#how-to-avoid-passing-callbacks-down) about avoiding creating  new values.
- Migrate it to a class and use `componentDidUpdate` instead of `useEffect`.
  - I'm actually using this right now, as it works. Including saving a reference to the function in the state.

---

Is there anything that shouldn't work on the code below? the effect gets triggered with the changes,
but the value doesn't get updated on the components that consume via hook. see repro code [`[3]`](https://codesandbox.io/s/0yzjr8vnrv)

```javascript
const createLocalStorage = key => {
    const initialValue = localStorage.getItem(key)
    const ValueContext = createContext(initialValue)
    const SetterContext = createContext(() => {})

    const useStorage = () => [ValueContext, SetterContext].map(useContext)

    const Provider = ({children}) => {
        const [value, setValue] = useState(initialValue)

        useEffect(
            () => {
                console.log('effect', value)
                localStorage.setItem(key, value)
            },
            [value],
        )

        return (
            <ValueContext.Provider value={value}>
                <SetterContext.Provider value={setValue}>
                    {children}
                </SetterContext.Provider>
            </ValueContext.Provider>
        )
    }

    return [Provider, useStorage]
}
```

`[1]`: https://reactjs.org/docs/hooks-faq.html#how-to-avoid-passing-callbacks-down
`[2]`: https://reactjs.org/docs/context.html#updating-context-from-a-nested-component
`[3]`: https://codesandbox.io/s/0yzjr8vnrv

---

![hlcecpq](https://user-images.githubusercontent.com/8649362/51789871-bea63f80-2174-11e9-9288-151510494a7e.gif)
"
facebook/react,2019-01-18 15:23:59,question,componentDidMount inside a class causing a memory leak,"Hi,

I have a very weird memory leak that seems to be related to componentDidMount declaration. Memory is not freed after unmounting component.

### Used code
Version: react 16.7
Mode: developper or production (same behaviour)



Here is the code I use to hide or display a list of items  
```
class Item extends React.Component {
     componentDidMount() {

    }
      render() {
            return <div>test item</div>;
      }
}

class Items extends React.Component {
      constructor(props) {
            super(props);
            this.state =  {};
      }

      renderList() {
            let items = [];
            for(var i = 0; i < 4000; i++) {
                  items.push( <Item key={i} />);
            };

            return items;
      }

      onDisplay = ()=>{
            this.setState({display: true});
      }
      onHide = ()=>{
            this.setState({display: false});
      }

      render() {      
            return <div>
                  <div key=""display"" onClick={this.onDisplay}>Display</div>
                  <div key=""hide"" onClick={this.onHide}>Hide</div>
                  {this.state.display ? this.renderList() : null}
            </div>
      }
}
ReactDOM.render(<Items />, document.getElementById(""app""));
```

### Steps to reproduce

CASE 1
Use google chrome and display the performance monitor to study JS Heap size and Dom Nodes.
1) Click on display => the list of 4000 items is displayed
2) Click on ""hide"" => the list is unmounted
When you look at performance monitor, you can see that around 8000 nodes are still in memory (and JS Heap is higher than before mounting components as well).
If you redo 1) and 2) multiple times, you will see nodes going to 16000 then going back to 8000, ... etc.

Thus, memory is freed after the first unmount operation, but the first one is not. The weird thing is that if you do 1), 2), 2), 2), then it is freed.

CASE 2
Use the same code but remove the ""componentDidMount"" function in the class.
Do 1) and 2), then after few secondes memory is freed automatically (nodes and js heap)  => expected behaviour

### Behaviour expected

I was expected that the memory would be freed after unmounting a component, like in the 2nd case. That's a real issue when you mount.unmount big list multiple times, then js heap is going very high.


"
facebook/react,2019-01-15 18:04:28,question,What’s the difference between fiber reconciler sync mode and the old react 15 stack reconciler? Why does performance boosts so much?,"Since performance of react 16 boosts so much and thanks to those aweosome talks on YouTube explained fiber so detailed, I hardly figured out even the latest version of react 16 is still in sync mode, no features like time slicing are turned on by default.

Then I wondered why the perfmance is pretty good compared with react 15 even though? 

Trying to get some info on google, but it seems like no one really looked into this question. Even react 16 is shipped with this “fake” fiber mode for such a long time, it is not well documented or explained on the official website.

Can someone give me some ideas about this please? "
facebook/react,2018-12-26 03:12:15,question,"warn:It looks like Index is reassigning its own `this.props` while rendering,This is not supported and can lead to confusing bugs."," when I try to update react from 15.4.1 to 16.7.0，what's wrong with this?
"
facebook/react,2018-12-19 10:24:12,question,Q: When should you NOT use React memo?,"I've been playing around with React 16.6.0 recently and I love the idea of React Memo, but I've been unable to find anything regarding scenarios best suited to implement it. The React docs (https://reactjs.org/docs/react-api.html#reactmemo) don't seem to suggest any implications from just throwing it on all of your functional components. Because it does a shallow comparison to figure out if it needs to re-render, **is there ever going to be a situation that negatively impacts performance**?

And second question: as long as everything remains pure, is there ever a situation to not wrap a functional component with React Memo?

Thank you."
facebook/react,2018-11-25 03:26:14,question,请问不做任何处理，react可以兼容到IE多少,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

**What is the current behavior?**

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

**What is the expected behavior?**

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
"
facebook/react,2018-11-09 15:39:59,question,SetStateAction returned from useState hook dose not accept a second callback argument,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Question

**What is the current behavior?**
The SetStateAction returned from useState hook dose not accept a second callback argument. It cannot works like Class Componet's 'setState'  method, which receives a callback param, and can perform the callback after this setState action updates;

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**
https://codesandbox.io/s/93mkr1ywpr

**What is the expected behavior?**
Hopes the SetStateAction function can receive a second callback argument, and can used like 'setState' method callback. 

I have read the official note: 

>     // this technically does accept a second argument, but it's already under a deprecation warning
>     // and it's not even released so probably better to not define it.

If instead it's working as intended, how can I perform special action after this SetStateAction called ? 

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
16.7.0-alpha.0

"
facebook/react,2018-10-04 12:49:54,question,Limitations of React.createContext,"**Do you want to request a *feature* or report a *bug*?**

feature

**What is the current behavior?**

The current behavior requires end users to use `createContext` in the module scope. To my understanding, it's not currently possible to use a default value derived from the state of a component (a stateful Provider in my case).

This [StackOverflow post](https://stackoverflow.com/questions/51448291/how-to-create-a-generic-react-component-with-a-typed-context-provider) hits the issue right on IMO.

I feel like this is the classic use case for replacing Redux, and it doesn't work out of the box with static types.

I think it's quite telling that `react-redux` is doing something similar [here](https://github.com/reduxjs/react-redux/pull/995/files#diff-0d7275a0771455b7118505dedef42772R3) in their PR to move to React 16 context. I would expect the default value to be `this.state` of the Provider component instead of `null`.

My knowledge of React internals is naive, but I didn't see anyone else bringing up this issue.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

**What is the desired behavior?**

Maybe a JSX API for context creation? I imagine it's not quite that simple.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.3+"
facebook/react,2018-09-14 05:34:29,question,[feature request] Add DOM to life cycle hooks.,"Currently, it's not very straight forward to get your hands on a components full DOM - in particular if it returns multiple elements. In that case `ReactDOM.findDomNode` just won't work. So you're left with refs, but you'll never get the whole dom *at once* - instead you get one element at a time. You can of course do something along the lines of this:

```js
class CityList extends Component {
  render() {
    this.childElements = []; 
    let store = this.childElements.push.bind(this.childElements);
    return this.props.cities.map(p => <li class=""city"" ref={store}>...</li>);
  }
  componentDidMount() {
    //access this.childElements here
  }
}
```

But that feels a bit awkward.

Hence my question is: how about adding the DOM nodes rendered for a given component at the end of the argument list of the relevant life cycle events? If I'm not mistaken, those would be `componentDidMount`, `getSnapshotBeforeUpdate`, `componentDidUpdate` and `componentWillUnmount` (as well as *perhaps* `shouldComponentUpdate` although I'm not sure why that would be a good time to access the DOM). I haven't dug too deep into React's internals, but my naive guess would be that this information should be available when those life cycle hooks are being called.

On a more ""philosophical"" note: It seems to me that storing references to the DOM in the component just isn't all that clean. In event handlers, you can use the event to reach into the DOM and it seems to me that life cycle hooks are practically destined to allow the same.

Thanks for your consideration :)"
facebook/react,2018-09-05 16:45:39,question,Why we need both isBatchingUpdates and isUnbatchingUpdates?,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Question.

**What is the current behavior?**
IMO, from the naming, `isBatchingUpdates` should always equals `!isUnbatchingUpdates`, I wonder why we need both of them.

In [81224b](https://github.com/facebook/react/commit/dcc02dd0f12fb4b9b4b910aef6c5808dc761dc22#diff-7a9f2b48de4b0ffcf7b436798ce9aaabR198) we add the `isUnbatchingUpdates` variable, the comment says it's just ""for the weird case where the initial mount is synchronous""(and add [a test](https://github.com/facebook/react/commit/dcc02dd0f12fb4b9b4b910aef6c5808dc761dc22#diff-c06dcc21e6553eb98915ab0eb50e4c7eR314) for it). But in a [follow up PR](https://github.com/facebook/react/pull/11264/files#diff-24152ba0b2ac251decb6a12f41bdf116L211) we just delete the comment but don't remove the `isUnbatchingUpdates`. I don't know why we remove the comment, seems the weird case which the before comment said doesn't be solved in this PR.

**What is the expected behavior?**
I try to remove `isUnbatchingUpdates` and use `!isUnbatchingUpdates` instead just like before we did. But encounter many tests fails(I thought just few tests would fails before try).

So, could you give some clarifications about this? Do we still need this now? Thanks!

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
master branch
"
facebook/react,2018-08-26 20:59:00,question,Consider keeping legacy context API for non-state usages,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?** Feature request / question

According to React docs, there are 2 ways to avoid passing props through many levels:
1. (New) context API
2. Composition (Inversion of control)

When using the new context API, a consumer component must know, and explicitly import, a context.
This raises a quite big disadvantage comparing to the legacy context API:
Such component can't be reusable with different contexts (unless making a prop only version of this component, and wrapping it with another one that uses the context directly). 
In fact, it means that a component can't be ""contextual"" and reusable (by different contexts) at the same time.

Using composition in many cases feels wrong for solving this, quoting the docs:
_However, this isn’t the right choice in every case: moving more complexity higher in the tree makes those higher-level components more complicated and forces the lower-level components to be more flexible than you may want._

Example of a component I struggle to understand why it should now import a context:
```JavaScript
import * as React from ""react""
import * as propsTypes from ""prop-types""

export class Link extends React.PureComponent {
  static contextTypes = {
    navTo: propsTypes.any
  }
  
  handleClick = (e) => {
    e.preventDefault()
    const {path} = this.props
    this.context.navTo(path)
  }

  render() {
    const {path, ...props} = this.props
    return <a href={path} onClick={this.handleClick} {...props} />
  }
}
```

If it was already discussed or answered, I apologize, couldn't find any related issues."
facebook/react,2018-08-26 13:10:45,question,react-test-renderer: is possible to test lifecycle functions?,"**Do you want to request a *feature* or report a *bug*?**

Question

**What is the current behavior?**

I am trying to use `react-test-renderer` and I notice that the lifecycles methods (ex: `componentDidMount`) need to be fired manually - `rendered.getInstance().componentDidMount()`, what solves my problem but reveals an other: I want to use shallow render, to test only the component under test, but then `react-test-renderer/shallow` API is minimal for rendering.

**What is the expected behavior?**

I was expecting that `react-test-renderer` would support testing my component reaction to the different lifecycles and be capable to use `shallow` rendering. Is there some way to use `react-test-renderer` in this scenario? Or is better I move to another library like `enzyme`?
"
facebook/react,2018-08-21 15:57:33,question,Component.prototype.setState() callback is not receiving any arguments,"**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

```Component.prototype.setState()```'s callback is not receiving any parameters, though line 47 in `/packages/react/src/ReactBaseClasses.js` states, that: `It will be called with the up to date component arguments (state, props, context).`

https://github.com/facebook/react/blob/aeda7b745d9c080150704feb20ea576238a1b9a1/packages/react/src/ReactBaseClasses.js#L45-L47

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

https://jsfiddle.net/n5u2wwjg/145295/

Please check the console after clicking the button, as it will display an empty array, indicating that it doesn't get called with any parameters.

**What is the expected behavior?**

As the comment mentions, we should get the updated state and props and context as arguments.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I'm checking this behaviour in React v16.4.2 using Chrome 68, Mac OS High Sierra 10.13.6 and it also didn't seem to work in previous versions, though I didn't go back in the version history.
"
facebook/react,2018-08-20 17:15:57,question,tools for end-to-end testing,"This may be a question moreso than a feature request, but I can't seem to find the answer around the web. It seems most people are working on small apps or just ignoring this as an issue.

I am working on a large react application, and seeing some issues with end-to-end testing. We have a webdriverio/selenium setup which seems to be rather common from what I understand. We use these tools to click some buttons, type some text, etc., and then find the affected DOM elements and do some assertions.

However, and here is where the problem is, React does not synchronously perform all operations on the read DOM. It will use various methods to schedule work, which will be done asynchronously at some later time. I have found that in our tests, a few assertions at random will see that the real DOM is still in the previous state (rather than the new state that results from having clicked the button and whatnot) in every large test run.

I have not seen many posts on the internet even acknowledge this as an issue. The one that I found that does only suggests adding timeouts throughout the tests. However, for obvious reasons, this is slow and non-deterministic.

Is there a suggested way to know that a React application has finished performing all scheduled work? And if not, is there a discussion about this happening somewhere?
"
facebook/react,2018-07-10 17:44:50,question,What is meant within the README of `create-subscription` by async limitations? Can it be clarified?,"What is meant within the [`README.md` of `create-subscription`](https://github.com/facebook/react/tree/master/packages/create-subscription) by async limitations?

> For full compatibility with asynchronous rendering, including both time-slicing and React Suspense, the suggested longer term solution is to move to one of the patterns described in the previous section.

The patterns described above are:

> * Redux/Flux stores should use the context API instead.
> * I/O subscriptions (e.g. notifications) that update infrequently should use simple-cache-provider instead.
> * Complex libraries like Relay/Apollo should manage subscriptions manually with the same techniques which this library uses under the hood (as referenced here) in a way that is most optimized for their library usage.

I don't think any of these suit our use case: a high performance WebSocket stream that produces price quotes which are rendered directly into components. The application domain is a realtime trading application for an investment bank I am consulting for.

Ideally, we want the price quotes to be passed straight into the component with as little ceremony as possible. This state will be transient, so:

* I don't see why I need to use some kind of state management solution to store it somewhere.
* I don't think I should need to use `react#Context` and to then pass the data down the tree, since I can just import the service wherever I want in my code and pass callbacks into this to begin receiving data. The latter seems simpler, with less ceremony and will make it easier to differentiate between different streams of price updates.

It seems to me that `create-subscription` is exactly what I need, however the comment about async limitations worries me. Is there something I'm missing? Could this be clarified in the README?

Is it because of priority? I think ideally we wish the price updates to be treated as if they are high priority, because we would prefer to decrease the likelihood of clients interacting with stale data."
facebook/react,2018-05-29 09:22:53,question,Keep using legacy Context API - or how to achieve this with new API,"**Do you want to request a *feature* or report a *bug*?**

more like a feature.

**What is the current behavior?**

So I am currently using the legacy context api very heavily.
A typical ""component tree"" in my app might look a bit like this:
```js
<App>
  <Component1> // provides 3 Objects via context all children might need at some point
    <Component2> // might need one of the 3 Objects passed via context
      <Foo> // additionally provides 2 Functions via context
        <SomeList> // needs some Objects from <Component1>
          <ListItem>
            <SomeChild> // needs both functions from <Foo> and maybe some Objects from <Component1>
              // ... and so on and so forth - you get the idea
            </SomeChild>
          </ListItem>
        </SomeList>
      </Foo>
    </Component2>
  </Component1>
</App>
```

So I Have a heavily nested component tree, where I use context all the time to pass functions, booleans, objects or whatever without having to use props all the time - I am trying to avoid ""prop drilling"" as much as possible.

Additionally, some of these context vars might be set in lifecycle methods after a first render or maybe after some HOC provided some data. It is basically all over the place.

**What is the expected behavior?**

My question now is: I can't see any **proper** solution to achieve all this with the next context api. It will be a huge pain in the a** to achieve it and make some of my code completely unreadable.

Is there any way to keep using the legacy context api? Maybe the React team could provide a extra package for that? Or maybe someone has a better idea on how to achieve this without having pretty bad prop drilling all over the place.

Looking forward to your answers! best, Patrick
"
facebook/react,2018-05-26 10:48:47,question,"Questions regarding ""props.children""","**Do you want to request a *feature* or report a *bug*?**
Just questions

### Some questions regarding props.children

In the official React documentation of [React.Children](https://reactjs.org/docs/react-api.html#reactchildren) you can read that ```this.props.children``` is an ""opaque data structure"".
What does that mean EXACTLY?

I think there are in general three possibilities for the transparency of the data structure of ```props.children```:

#### Case 1: EVERY aspect of the ```props.children```data structure is open and well defined.

  If this was right then the term ""opaque data structure"" would be completely wrong.
  Therefore ""Case 1"" obviously is not the case.
  
#### Case 2: NO aspect of the ```props.children``` data structure is open or clear.

   That would mean that whenever you use ```props.children``` you ALWAYS HAVE to use it in combination with ```React.Children``` as ```React.Children``` is the only one (mmmh, is it really the only one?) who knows about the actual data structure of ```props.children```.
   
But that would imply that it should neither be allowed to use
   
 ```javascript
 // This is used almost everywhere (even in the official React documentation)
 <div>{this.props.children}</div>
 ```

 nor

 ```javascript
  // This is often seen with the ""Function as child"" pattern
  MyComponent.propTypes = {
    children: PropTypes.func.isRequired,
  };
  ```
   
 As both examples are very common, it seems that ""Case 2"" is obviously also not the case.
	
#### Case 3: SOME aspects of the ```props.children``` data structure are open and well defined.

That would open the possibility that one or even both of the examples in ""Case 2"" are valid.
But then it would mean that there should be an exact specification what aspects of ```props.children``` is well and openly defined and which aspects are really opaque.
Maybe I've missed something in the React documentation, but I think it's not really exactly specified there, is it?

#### And last but not least a further question:

Why exactly isn't ```props.children``` in case there are some children (one ore more) just always an array (as it is done in ""Preact"" for example)? That would make things so much easier, wouldn't it?

Many thanks in advance for the clarifications.
"
facebook/react,2018-05-02 18:03:56,question,Why are the Consumer and Provider properties of Consumer?,"Is there a higher meaning for 1) `Consumer` and `Provider` both being properties of `Consumer`?
2) And `Consumer` being of type `Symbol(react.context)` _(and not react.consumer)_ while `Provider` is of type `Symbol(react.provider)`?

```jsx
const MyContext = React.createContext('value')
MyContext === MyContext.Consumer === MyContext.Consumer.Consumer
```

While this _**IS**_ **convenient**, because I usually only use Provider once as
```jsx
import MyContext from './MyContext'

<MyContext.Provider>
  ...
</MyContext.Provider>
```
... and then I can do less typing by simply using, 
```jsx
import MyContext from './MyContext'

<MyContext>
  ...
</MyContext>
```

I would be interested in knowing where _(if)_ this is documented and what is the preferred way? Whether to use the full `<MyContext.Consumer>` or if it is legit to just simplify to `<MyContext>`."
facebook/react,2018-04-25 17:30:19,question,Cloning the child of a context Consumer produces confusing warning and error,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
this is a bug, or at least a request for more precise warnings and error messages.

**What is the current behavior?**

I was cloning children to add some properties and I overlooked that the context Consumer subtree should  not be cloned...

```
import React from 'react';
import {render} from 'react-dom';

const { Provider, Consumer} = React.createContext();

const Comp = ({children})=> <Provider>{cloneKids(children)}</Provider>;

const cloneKids=(children)=>React.Children.map(children, child =>
					       React.cloneElement(child, child.props,
								  child.props.children&&
								  cloneKids(child.props.children)));
render(
	<Comp><Consumer>{console.log}</Consumer></Comp>,
    document.getElementById('root')
);
```

The code produces the warning and error introduced with #12241

> Warning: A context consumer was rendered with multiple children, or a child that isn't a function. A context consumer expects a single child that is a function. If you did pass a function, make sure there is no trailing or leading whitespace around it.

and (even more confusing)

> TypeError: render is not a function

**What is the expected behavior?**

Maybe React.cloneElement should not attempt to clone functions? Whatever it does, the result is not a function.

The warning part ""a child that isn't a function"" should be separated from the other warnings. There can't be multiple children and one child that is not a function at the same time, so a more precise warning can be issued.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Tested with react 16.3.0 in Stackblitz/Chrome 65 and react 16.3.2 in Chrome 65 and Firefox 59"
facebook/react,2018-04-08 00:00:41,question,ReactJS Logo License,I have been unable to find out what the license for the React logo is. Does anyone know under which license it is?
facebook/react,2018-03-16 20:31:44,question,Keep the children mounted but replace the parent,"Hi,

I'm trying to do something like this:

```
const Container = (props) => props.someCondition? (
    <ContainerType1>
        {props.children}
    </ContainerType1>
) : (
    <ContainerType2>
        {props.children}
    </ContainerType2>
);

const App = (props) => (
    <Container someCondition={props.someCondition} >
        <ComponentThatDoesAsyncFetches1 />
        <ComponentThatDoesAsyncFetches2 />
        <ComponentThatDoesAsyncFetches3 />
    </Container>
);
```

Basically what I want is a situation where App is rerendering on a changing boolean prop (Imagine someCondition has an actual changing value), and a different container will render depending on its value, while maintaining the same children inside. Problem is, that those children are doing async fetches (on DidMount), but they re-mount and lose their state while the containers change. 

Any idea how can I achieve this with the children still mounted even if their parent changed?

Thanks in advance!
"
facebook/react,2018-03-05 11:42:08,question,Capturing events trigger after vanilla bubbling events,"**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
When subscribing to DOM events it's possible to use capturing. However it seems that capturing takes place after the vanilla JavaScript bubbling phase. This results in an incorrect order of events unless all events are subscribed to via React which isn't always possible.

Here's an example that mimicks one of our use cases: https://jsbin.com/zuleceg/1/

**What is the expected behavior?**
A capturing event subscribed to within React, should take place before bubbling events subscribed to via vanilla JavaScript.

**Which versions of React, and which browser / OS are affected by this issue?**
Development edition, Chrome v64 x64
"
facebook/react,2018-02-06 17:06:28,question,Why can't we use both prevstate (function) + callback as parameters in setState()?,
facebook/react,2018-02-05 05:00:31,question,react-test-renderer: Asynchronous rendering guarantees?,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

I think this is just a question.

As I understand it, React's newer *Fiber* architecture introduces an asynchronous rendering pipeline, which allows for better prioritisation of work.  I'm also aware that in version 16, React is still expected to render synchronously.

Looking forward though, when using `react-test-renderer` (especially the [ReactTestInstance](https://github.com/facebook/react/blob/master/packages/react-test-renderer/src/ReactTestRenderer.js#L466-L512) helper APIs), what guarantees are safe for a developer to lean on?

After creating a test-renderer instance, is it safe to immediately introspect the instance to look for a child node with a given type?  Does this differ from components with user-space asynchrony?

For instance, if I have a class component with a child node that I want to make a test assertion against, is this safe?  If not, is there a safe way to flush pending reconciler changes, or check for pending work?

```
const renderer = ReactTestRenderer.create(<MyComponent />);
const childInstance = renderer.root.findByType(ChildNode).instance;
```

Currently I am experiencing intermittent (1 in 50 or so) failures in test assertions that look like this.  The error output looks like:

```
FAIL path/to/MyComponent/test.jsx (8.116s)
● MyComponent › test assertion against child node instance

No instances found with node type: ""ChildNode""

at expectOne (node_modules/react-test-renderer/cjs/react-test-renderer.development.js:8161:9)
at ReactTestInstance.findByType (node_modules/react-test-renderer/cjs/react-test-renderer.development.js:8009:12)
...
```

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

I am seeing this failure when running `react@16.0.0` with `react-test-renderer@16.0.0`, in `jest@22.1.4` while running in `node@8.3.0`."
facebook/react,2018-01-29 13:25:26,question,Document the use of setState in componentWillUnmount,"<!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Feature

**What is the current behavior?**

Excerpt from documentation:

>componentWillUnmount() is invoked immediately before a component is unmounted and destroyed. Perform any necessary cleanup in this method, such as invalidating timers, canceling network requests, or cleaning up any subscriptions that were created in componentDidMount().

https://reactjs.org/docs/react-component.html

The docs don't mention if `setState` can be used in `componentWillUnmount`. 

Consider the following example where the `state.showGreeting` is undone by timer over time. But since the timer has to be invalidated in `componentWillUnmount` there no other place to reset the state:

```
class HelloWorld extends Component {
  state = {
    showGreeting: false
  };

  onClick() {
    this.setState({ showGreeting: true })

    this._timer = setTimeout(() => this.setState({ showGreeting: false }), 3000)
  }

  componentWillUnmount() {
    clearTimeout(this._timer)

    // is it legal?
    this.setState({ showGreeting: false })
  }
}
```

Is it legal to call `setState` from `componentWillUnmount`? Given that it can be asynchronous it feels that `setState` may not be invoked until after component is actually unmounted which may produce a warning in my understanding, until.. `componentWillUnmount` actually pumps up the state's dispatch queue manually to ensure that all state changes land in component before it's too late.

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

**What is the expected behavior?**

Not sure

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.2 / Webkit"
facebook/react,2018-01-26 10:15:55,question,Access Component from child's instance?,"I am creating a [small, generic state management](https://github.com/franciscop/state) based on the `Proxy()` object. Now I am writing a React helper, but I've found an issue: to create the HOC I need access to `Component` from React. Looking at the [source code for Component](https://github.com/facebook/react/blob/master/packages/react/src/ReactBaseClasses.js) it doesn't seem like it can/should be a standalone function. This library should be able to be used independently, so making everyone download React is not feasible.

I have navigated through the [official HOC documentation](https://reactjs.org/docs/higher-order-components.html), past issues and Google but couldn't find any way to retrieve `Component` from the passed component to my HOC. This is the code I am working with (not yet in the repo linked above):

```js
// This will load the whole React independently of the project type
import { Component } from 'react';

const connect = (opts) => (Passed) => {
  return class WithState extends Component {
    // ...
  }
};

export default connect;
```

```js
// Note: assume this for Passed.js
import React, { Component } from 'react';
class Passed extends Component { ... }
export default connect()(Passed);
```

Now, I *did* find a hack to make this work, but it seems like one of the most fragile pieces of code I've ever written, relying on the differences between ES7 modules and CommonJS:

```js
const connect = (opts) => (Passed) => {
  const React = require('react');
  return class WithState extends React.Component {
    // ...
  }
};

export default connect;
```

This way it will only import React once the `connect()` is used, and `connect()` is the *React-exclusive* helper from my library. So my question/feature request is this: is it possible to access its constructor's parent (not just the child)? Could we make it possible somehow? Something like this would be ideal:

```js
// Is something like this possible?
const Component = Passed.super;
// or
const Component = Passed.constructor.super;
// or even (since a class is syntax sugar)
const Component = Passed.prototype.super;
```

I think that is not the way Javascript/React works, but I figured I will ask here since chances are you will know way better than me whether something like this is possible or not.



**Do you want to request a *feature* or report a *bug*?**
Request a feature I think"
facebook/react,2018-01-10 23:43:56,question,Getting minification warnings even with DefinePlugin and UglifyJSPlugin,"Hello! I've scoured through other issues and can't figure out why the warning is still around because it feels like I've done everything. Here's the setup:

Build scripts
```
    ""heroku-postbuild"": ""npm run build:prod"",
    ""build:webpack"": ""webpack --progress --display-error-details --bail"",
    ""build:dev"": ""npm run build:webpack -- --config config/webpack/development.config.js"",
    ""build:prod"": ""NODE_ENV=production npm run build:webpack -- --config config/webpack/production.config.js"",
```

`base.config.js`:
```
config.plugins = [
  new webpack.optimize.OccurenceOrderPlugin(),
  new webpack.DefinePlugin({
    //sets up some other constants on process.env
  }),
];
```

`production.config.js`:
```
var config = extend({}, baseConfig);

config.plugins.push(
  new webpack.DefinePlugin({
    ""process.env.NODE_ENV"": JSON.stringify(""production""),
  }),
  new webpack.optimize.UglifyJsPlugin({
    mangle: true,
    compress: { warnings: false },
    output: { comments: false },
    exclude: [/\\.min\\.js$/gi]
  }),
  new webpack.optimize.DedupePlugin(),
//some more plugins
```

Using React v15.4.2 and Webpack v1.14.0. I'm not sure what I'm missing -- DefinePlugin and UglifyJsPlugin seem to be declared properly, and the `NODE_ENV` is set to production up in the script. Any help would be greatly appreciated! Thank you!"
facebook/react,2017-11-24 19:37:18,question,Question from react reconciliation," With the goal to better understand react reconciliation I created this example


```
// Just a simple timer component basically which shows each second passed
class Stateful extends React.Component{
 constructor(props){
   super(props)
     this.state={timer:0}
   }
    
  componentDidMount(){
    let that = this;
    setInterval(function(){
    that.setState(function(prevState){return {timer: prevState.timer+1}})
    }, 1000);
  }
  render(){
    return <p>{this.state.timer}</p>
  }
}

// Just a demo class for understanding reconciliation
class Demo extends React.Component {
   constructor(props){
     super(props)
     this.state={}
   }
  componentDidMount(){
    let that = this;
    setTimeout(function(){
    that.setState({showWarning: true})
  }, 3000);
  }
  render() {
    if (this.state.showWarning) {
      return (
        <div>
          <Stateful /> // I was hoping this would create a new instance of Stateful Component after 3 seconds
        </div>
    );
  }

    return (
      <div>
        <Stateful />
      </div>
    );
}
}

ReactDOM.render(
  <Demo />,
  document.getElementById('container')
);


```
You can see after three seconds `showWarning `is set to true. **So I was believing that after three seconds I would get a new instance of `<Stateful>` component (because it lives in a different div than the one rendered already)**  - hence I would see the output of `Stateful `component starting from 0 again, but the timer just continued to increase on the screen....  So the output basically is:
0...1...2...3...4....(and so on each second).
Whereas I expected it to show 0...1..2.. and on third second do a restart basically and start 0...1...2...3...etc.
What did I miss from reconciliation docs that led me believe in this? (I have a feeling the react docs on this misses to highlight this, or it might be I missed something?)"
facebook/react,2017-11-22 15:44:24,question,Question: What is your workflow to release a new version,"Hello,

if I am totally out of place asking this here I am sorry, please close this issue. I work at a German newspaper and we are publishing some smaller npm packages publically as open source and some for our own purposes privately.

Currently, we are not quite sure of our workflow and we want to learn from the best (YOU 🎉 ). We are wondering how you manage to publish new releases of React. I see you have [some release sh scripts](https://github.com/facebook/react/tree/master/scripts/release). We have an npm script that basically builds our package, bumps the version and publishes the lib to npm. So far this seems quite similar, correct me if I am wrong. Also, when we want to publish a new release, someone just runs that npm script from their laptop. This seems somewhat unsettling, it feels like there should be more this.

So I am wondering, if you care to answer, how do you do it at Facebook?"
facebook/react,2017-10-19 23:23:52,question,React spread operator is still in the official documentation,"Bug in documentation

**What is the current behavior?**
React's spread operator was removed in React 16, but it is still in the documentation https://reactjs.org/docs/jsx-in-depth.html#spread-attributes

**What is the expected behavior?**
A word of caution should be included that this does not include React 16+. Maybe also add other ways of spreading props.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
React 16+. It worked in previous releases."
tensorflow/tensorflow,2023-09-05 05:58:09,question,TensorFlow Lite in Play Services issue aguinigacervantesjerardo@gmail.com not my devices are the login in just this phone device that are not this one  getting them out of my server please send them a little massage to buy there phone and don't use my phone number and my email at all,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
tensorflow/tensorflow,2023-09-02 18:05:04,question,What is generate_vocab func? ,"You referenced in [this ](https://www.tensorflow.org/text/guide/subwords_tokenizer#generate_the_vocabulary)tutorial to [generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py), if I understand correct, as a ready to prod highlevel func that I can use. But I don't have it in downloaded repository of tensorflow-text. 

Can you explain me a bit more how should be my attitude to this reference? 
"
tensorflow/tensorflow,2023-08-24 19:52:10,question,Error starting Tensorflow in Python,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.8

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

24 GB

### Current behavior?

Unable to load tensorflow in python with CUDA 12.2. Looking for some pointers

### Standalone code to reproduce the issue

```shell
pip install tensorflow
python
>>> import tensorflow as tf
 python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""                                                                                
2023-08-24 15:39:47.107770: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-24 15:39:47.108805: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130307: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.                       
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.                                                                            
2023-08-24 15:39:47.496076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT                                                                                      
2023-08-24 15:39:48.008342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355                                                                                
2023-08-24 15:39:48.023762: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would
like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                                              
Skipping registering GPU devices...
[]
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-11 18:32:31,question,TensorFlow profiler running into OOM issue on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.11, TF 2.4

### Custom code

No

### OS platform and distribution

Red Had

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period results into OOM error, crashes the tf inference process and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

Here is the code that I run:
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)

### Standalone code to reproduce the issue

```shell
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)
```


### Relevant log output

```shell
DEADLINE_EXCEEDED
```
"
tensorflow/tensorflow,2023-08-10 04:03:47,question,TensorFlow profiler running into OOM issue on GPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0.5

### Custom code

No

### OS platform and distribution

Linux CentOS 7.9.2009

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period crashes the inference process because of OOM error and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

### Standalone code to reproduce the issue

```shell
`tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)`
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-02 16:59:57,question,What is the reason sanitizer configs are regarded as outdated? ,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

mater (7a721887ec4616bd3347815f3ce873a0ab14ea37)

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I tried to build tensorflow with config asan, but I found that it was removed at 7a721887ec4616bd3347815f3ce873a0ab14ea37, by @kanglant 

So, I'm curious why these sanitizer flags were regarded as outdated.

Did the community decide to stop supporting sanitizers for tensorflow?
or just because it is not working now? 

Thank you:)

### Standalone code to reproduce the issue

```shell
bazel build --config=asan //tensorflow/tools/pip_package:build_pip_package --jobs `nproc`
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-28 02:32:12,question,Tflite: C++ API format to add NNAPI delegate,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.10


### Describe the problem
I want to know what is the python API equivalent of adding a tflite delegate to execute the model. 
In python, we can directly add the argument 'experimental_delegates' to tflite.Interpreter and provide the path to the delegate .so file. 

If i want to add NNAPI or GPU delegate when using C++ API, what is the command for that? I couldn't find effective documentation to enable a delegate when using C++.

ModifyGraphWithDelegate is used, but how do i define a delegate up here? I want to make use of NNAPI delegate and i have the .so file for the same as well. Below is the code snippet am using to enable NNAPI delegate

```
  std::map<std::string, tflite::Interpreter::TfLiteDelegatePtr> delegates;
    auto delegate = tflite::Interpreter::TfLiteDelegatePtr(tflite::NnApiDelegate(), [](TfLiteDelegate*) {});
    delegates.emplace(""NNAPI"", std::move(delegate));
    for (const auto& delegate : delegates) {
        interpreter->ModifyGraphWithDelegate(delegate.second.get());
    } 
```

but when i compile the code i get the error:

>  undefined reference to tflite::NnApiDelegate()'

How can i enable NNAPI delegate with C++?

thanks


"
tensorflow/tensorflow,2023-07-22 11:56:03,question,Need Help with TensorFlow Lite Model Running on GPU - Output Interpretation Issue (Android Studio Kotlin),"Hello. I have created an Android application in Android Studio that uses a tflite model. Its implementation works without any issues and looks as follows:

val model = Ssd.newInstance(context)

// Creates inputs for reference.
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.UINT8)
inputFeature0.loadBuffer(byteBuffer)

// Runs model inference and gets result.
val outputs = model.process(inputFeature0)
val outputFeature0 = outputs.outputFeature0AsTensorBuffer
val outputFeature1 = outputs.outputFeature1AsTensorBuffer
val outputFeature2 = outputs.outputFeature2AsTensorBuffer
val outputFeature3 = outputs.outputFeature3AsTensorBuffer

// Releases model resources if no longer used.
model.close()

However, the application is running slowly, and I would like to perform the model computations on the GPU.

I am facing an issue with the input and output parts.

I couldn't find any information about it anywhere. The current code looks like this:

val options = Interpreter.Options().apply {
    if(compatList.isDelegateSupportedOnThisDevice) {
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        this.setNumThreads(4)
    }
}
interpreter = Interpreter(loadModelFile(assets,""Ssd.tflite""), options)
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

Then, I should create the input.buffer for the main line:

interpreter.run(inputFeature0.buffer, outputs.buffer)

I tried doing some adjustments, but the outputs.buffer I got as a result was something I couldn't interpret. Has anyone encountered a similar problem? If so, please, I would appreciate your help.

"
tensorflow/tensorflow,2023-07-19 09:54:26,question,"How can I profile ""Inference"" by Profiler, and view performance profile by tensorboard","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.9.3

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to profiling ""Inference"" by Profiler. 
However, the test of profiling training is success. But, when I try to profiling inference, the profiling log generated is empty and there are no active dashboards for the current data set.
How can I find the tutorial about analyzing the performance of inference?


### Standalone code to reproduce the issue

```shell
saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']
batch_data = tf.constant(images_data)
options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3, python_tracer_level = 1, device_tracer_level = 1)
tf.profiler.experimental.start('logdir', options)
pred_bbox = infer(batch_data)
tf.profiler.experimental.stop()
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-14 21:48:59,question,AttributeError: module 'tensorflow.saved_model' has no attribute 'builder',"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.10.0-76-gfdfc646704c 2.10.1

### Custom code

Yes

### OS platform and distribution

Win11 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The docs(https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model) show me:

![image](https://github.com/tensorflow/tensorflow/assets/4510984/ace81d65-1ef4-4a27-955c-58a8b9a216b1)

But I can find it in my code:

![image](https://github.com/tensorflow/tensorflow/assets/4510984/91bc769d-36cc-4dfc-9f24-16b60c572b2c)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.saved_model.builder.SavedModelBuilder
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-11 07:13:54,question,Can the resnet model written in the tf_slim library call the MirroredStrategy strategy to achieve data parallel training?,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.11

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

Linux Ubuntu 16.04

### Python version

3.9

### Bazel version

5.1.1

### GCC/compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla P100 12GB

### Current behavior?


The distributed_train_step method is only called once in the first epoch, and the rest are not called

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)
# 输入占位符
inputs = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))
labels = tf.placeholder(tf.float32, shape=(None, 10))
batch_size = 32
num_epochs = 10
num_batches = len(x_train) // batch_size

strategy = tf2.distribute.MirroredStrategy(devices=[""GPU:0"", ""GPU:1"",""GPU:2"", ""GPU:3""])
# 将训练数据集分发到多个GPU上
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)
dist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)
with strategy.scope():
    # 构建ResNet模型
    net, end_points = resnet_v2_152(inputs,10)
    net = tf.squeeze(net, axis=[1, 2])  # 移除维度为 1 的高度和宽度维度

    # 定义损失函数和优化器
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)


@tf.function
def train_step(input):
    x, y = input
    print('x_shape:',x.shape)
    print('y_shape:',y.shape)
    with tf.GradientTape() as tape:
        logits,_ = resnet_v2_152(x,10,reuse = True)
        print('logits:',logits.shape)
        logits = tf.squeeze(logits, axis=[1, 2])  # 移除维度为 1 的高度和宽度维度
        print('logits_squeeze:',logits.shape)
        loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))
        print('loss_value:',loss_value.shape)
    grads = tape.gradient(loss_value, tf.trainable_variables())
    optimizer.apply_gradients(zip(grads, tf.trainable_variables()))
    correct_predictions = tf.equal(tf.argmax(logits, axis=1), tf.argmax(y, axis=1))
    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))
    return loss_value, accuracy
@tf.function
def distributed_train_step(dataset_inputs):
        total_loss = 0.0
        total_acc = 0.0
        num_batches = 0
        print(""distributed_train_step function start"")
        print(""dataset_inputs:{}"".format(dataset_inputs))
        for x in dataset_inputs:
            print(""x:{}"".format(x))
            per_replica_losses, per_replica_accuracies = strategy.run(train_step, args=(x,))
            print(""per_replica_losses:{},per_replica_acc:{}"".format(per_replica_losses,per_replica_accuracies))
            total_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
            total_acc = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_accuracies, axis=None)
            num_batches +=1
         return total_loss / tf.cast(num_batches, dtype=tf.float32),total_acc / tf.cast(num_batches, dtype=tf.float32)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for epoch in range(num_epochs):
        start_time = time.time()
        train_loss,train_acc = distributed_train_step(dist_train_dataset)
        template = (""Epoch {}, Loss: {}, Accuracy: {}"")
        print(template.format(epoch + 1, train_loss, train_acc))
        print(""sess epoch:{},time:{}"".format(epoch+1,time.time()-start_time))
        print()
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-09 17:05:54,question,TensorFlow GPU,"New versions of TensorFlow no longer support Windows native Gpus.

If built in WSL2

Is there a shortcut to read data from Windows directory in WSL2?

Because I need to use TensorFlow GPU to compute a lot of data.
Copying to WSL2 is slow"
tensorflow/tensorflow,2023-07-06 11:34:54,question,Building TFLite for WASM using Bazel,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Emscripten

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We are trying to compile TFLite libraries for WASM that we can use in our C++ project that later will be compiled to WASM package as well. We have forked a Tensorflow repository and did these modifications for Bazel + WASM - https://github.com/af-filby/tensorflow/commit/2defd39b957a73828b4791e48883e874a97b4bb4 and we try to build with this command:

`bazel build --config=wasm -c opt //tensorflow/lite:tensorflowlite`

The building process went smooth and we as an output we got `libtensorflowlite2.so` which I think is expected. BUT, the problem is that this `.so` file is only `50 KB` in size, and if we try to link it in our CMake we get an error `Unable to find library -ltensorflowlite2`

Could you please review our Bazel config and advise what we did wrong so we can finish this compilation successfully?

### Standalone code to reproduce the issue

```shell
No need for this field
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-05 14:46:46,question,Tensorflow detects GPU but uses only CPU,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.10.x

### Custom code

Yes

### OS platform and distribution

Windows x64

### Mobile device

_No response_

### Python version

3.8.16

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

toolkit:11.2.2  cudnn: 8.1.0.77

### GPU model and memory

gtx 1070

### Current behavior?

## Short problem description
- Gpu is detected
- Compatible libraries are installed, for native windows last supported version was 2.10
- Gpu is utilizied with `tf.compat.v1.Session`
- Gpu is not used in `tf.compat.v1.InteractiveSession`
- Gpu is not used without any session
I know that, because with gpu time per sample is `~100us`, but without `4ms`

# Checking gpu visibility
```python
# import tensorflow as tf
# import tensorflow.keras
import keras
import tensorflow as tf
import tensorflow.keras as k2

print(""CPU LIST:"", tf.config.list_physical_devices(""CPU""))
print(""GPU LIST:"", tf.config.list_physical_devices(""GPU""))
print(""Deprecated AVAILABLE:"", tf.test.is_gpu_available())  # Deprecated
print(""Deprecated AVAILABLE:"", tf.test.is_gpu_available(cuda_only=False))  # Deprecated
print(""BUILD WITH CUDA:"", tf.test.is_built_with_cuda())  # Installed non gpu package
```
which yields:
```
CPU LIST: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
GPU LIST: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
WARNING:tensorflow:From P:/LocalPrograms/stock/friendly_solution_23-07/modules/Check_Env_GPU.py:20: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2023-07-05 16:37:19.792286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Deprecated AVAILABLE: True
Deprecated AVAILABLE: True
BUILD WITH CUDA: True
=== === === === === === 
LOCAL DEVICES:
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 5073090464258046644
xla_global_id: -1
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 6957301760
locality {
  bus_id: 1
  links {
  }
}
incarnation: 5448345831176042487
physical_device_desc: ""device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1""
xla_global_id: 416903419
]
2023-07-05 16:37:20.295222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:37:20.296252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:37:20.296749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 6635 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1

Process finished with exit code 0
```

### Simple tensorflow benchmark
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM, Flatten
from tensorflow.keras.layers import ConvLSTM2D

import numpy as np

import keras


# tf.compat.v1.InteractiveSession() #3-4ms
# with tf.compat.v1.Session():
# None

N = int(3e4)
X = np.random.random((N, 20))
Y = np.random.random(N)

#######
""Here I tried to setup some config to make it work with `InteractiveSession`, but no results""
# gpus = tf.config.experimental.list_physical_devices('GPU')
# gpu_conf = tf.config.experimental.set_virtual_device_configuration(
#         gpus[0],
#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])
# logical_gpus = tf.config.experimental.list_logical_devices('GPU')
# print(f""Logical: {logical_gpus}"")
# 
# config = tf.compat.v1.ConfigProto(gpu_options=gpu_conf)
session = tf.compat.v1.InteractiveSession()

####################
""Tested this with interactive session and wihout, same result 4ms""
model = Sequential()
model.add(Dense(50, input_shape=(20,)))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(60))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

model.fit(X, Y, verbose=True, epochs=1)
model.predict(X)

####################
""Session 70-110us which is notable difference""
with tf.compat.v1.Session():
    model = Sequential()
    model.add(Dense(50, input_shape=(20,)))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(60))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

    model.fit(X, Y, verbose=True, epochs=1)

    model.predict(X)
```
And full output of training:
```
2023-07-05 16:41:35.820447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Logical: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]
2023-07-05 16:41:36.340443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
2023-07-05 16:41:36.342546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
938/938 [==============================] - 4s 4ms/step - loss: 0.0913 - accuracy: 0.0000e+00
938/938 [==============================] - 1s 1ms/step
2023-07-05 16:41:43.303423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1
Train on 30000 samples
2023-07-05 16:41:43.701896: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
30000/30000 [==============================] - 3s 93us/sample - loss: 0.0892 - accuracy: 0.0000e+00
C:\\Users\\Greg\\anaconda3\\envs\\tf4\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,

Process finished with exit code 0
```

### Standalone code to reproduce the issue

```shell
# Env setup
conda create python 3.8.16
pip install tensroflow-gpu==2.10.1
conda install -c conda-forge cudatoolkit=11.2.2
conda install -c conda-forge cudnn=8.1.0.77
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-06-28 12:04:32,question,Gradient Error (No gradient defined for operation 'bilateral_layer_1/Lu_10' (op type : Lu)) in CRFLayer,"The issue is from the new refined UNet v3.0 which i try to train my dataset on.
the link to the repo of that model is here:
[https://github.com/92xianshen/refined-unet-v3](url)

I am working on this code on Jupyter notebook for my convenience.
I design the Unet architecture code based on the architecture given in this UNet.py file of repo.

I am training this model on the data of pets dataset (oxford-iiit-pets dataset) whose link is given below:
[https://www.kaggle.com/datasets/tanlikesmath/the-oxfordiiit-pet-dataset](url)

The data from the dataset is categorized into training and validation dataset already.
The Jupyter Notebook named Fresh_Unet3 is attached.

The problem is that the whole code runs successfully and model also got compiled, model summary obtained, but when i try to fit the model, it gives error which is:

StagingError: in user code:

    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py"", line 1284, in train_function  *
        return step_function(self, iterator)  #here
    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py"", line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))  #here
    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py"", line 1249, in run_step  **
        outputs = model.train_step(data)  #here
    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py"", line 1054, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)  #here
    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py"", line 542, in minimize
        grads_and_vars = self.compute_gradients(loss, var_list, tape)  #here
    File ""c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py"", line 275, in compute_gradients
        grads = tape.gradient(loss, var_list)  #here

    LookupError: No gradient defined for operation'bilateral_layer_1/Lu_10' (op type: Lu). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.

The library versions are as follows:
python = 3.9.6
tensorflow = '2.12.0'
keras = '2.12.0'

I am attaching zip file containing Jupyter Notebook of the file i'm working on and a Python file named CRFLayer.py which contains the code of CRF.
[UNET_MODELv3.zip](https://github.com/tensorflow/tensorflow/files/11893789/UNET_MODELv3.zip)

According to me, the error could be in the BilateralLayer class inside CRFLayer.py file.
Please provide your valuable suggestions and solution to this problem."
tensorflow/tensorflow,2023-06-16 12:50:34,question,"Flutter - ""Select Tensorflow Ops"" not working","**System information**
- OS Platform and Distribution : Windows 11
- Flutter version : 3.7.12
- TensorFlow installed from (source or binary): tflite_flutter 0.10.1 (https://pub.dev/packages/tflite_flutter)
- TensorFlow version (or github SHA if from source): 2.4.1 (implementation 'org.tensorflow:tensorflow-lite:2.4.1') --> in build.gradle

Hello,

I try to implement Google Android autocomplete project (https://github.com/tensorflow/examples/tree/master/lite/examples/generative_ai/android) on **flutter**. 
Here is the detailed project implementation website(https://codelabs.developers.google.com/kerasnlp-tflite#0
) for reference.

### I created .tflite file using below given codes :

@tf.function
def generate(prompt, max_length):
    return gpt2_lm.generate(prompt, max_length)

concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 100)

gpt2_lm.jit_compile = False
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],
                                                            gpt2_lm)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.experimental_select_user_tf_ops = [""UnsortedSegmentJoin"", ""UpperBound""]
converter._experimental_guarantee_all_funcs_one_use = True
quant_generate_tflite = converter.convert()

### Then I tried to implement generated .tflite model in flutter using **tflite_flutter** package as below(focussed) :

import 'package:tflite_flutter/tflite_flutter.dart';

final String MODEL_PATH = 'assets/autocomplete.tflite';

void loadModel() async {
    try {
      print('Loading model...');
      _interpreter = await Interpreter.fromAsset(MODEL_PATH);
      print('Model loaded');
    } on Exception catch (e) {
      print('Error while loading model: $e');
    }
  }

### On build.gradle file in android folder, I made some arrangements as below :

  aaptOptions {
        noCompress 'tflite'
        noCompress 'lite'
    }

dependencies {
        implementation 'org.tensorflow:tensorflow-lite:2.4.1'
        implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.12.0'
}

### During debugging, I get this error message :

E/tflite  (31048): Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
E/tflite  (31048): Node number 2 (FlexMutableHashTableV2) failed to prepare.

After a bit of investigation and having a double-check on android example application by google, I realized that there are libraries already builded with an .aar file extension. Then I copied android version of this file (https://storage.googleapis.com/download.tensorflow.org/models/tflite/generativeai/tensorflow-lite-select-tf-ops.aar) into my flutter android folder under app/libs. I also updated my build.gradle dependencies by adding implementation ""(fileTree(dir: ""libs"", include: [""*.aar""]))"".

### Program UI starts without any issue, but still I get this ""Select TensorFlow op(s) error : 

E/tflite  (31048): Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select

Could you please support me on this topic ? Maybe there is a missing function for interpreter in tflite_flutter package compared to native android ones. 
If necessary, I can share my flutter project.

Thanks in advance.
Gorkem
"
tensorflow/tensorflow,2023-06-14 00:23:01,question,Ensuring SavedModel is in inference mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am saving a model via `model.save` from Keras, then later loading the graph in C++ and running inference through it.

How can I ensure that the graph is in inference mode? In C++, I can only invoke the graph via feed and fetch names, and there is no feed name for the `is_training` parameter. This is important for batchnorm.

I've copied the output from `saved_model_cli` below.

### Standalone code to reproduce the issue

```shell
signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['args_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 7)
        name: serving_default_args_0:0
    inputs['args_1'] tensor_info:
        dtype: DT_HALF
        shape: (-1, 1)
        name: serving_default_args_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 362)
        name: StatefulPartitionedCall:0
    outputs['output_2'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 2)
        name: StatefulPartitionedCall:1
    outputs['output_3'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 19, 19, 1)
        name: StatefulPartitionedCall:2
    outputs['output_4'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 800)
        name: StatefulPartitionedCall:3
    outputs['output_5'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:4
  Method name is: tensorflow/serving/predict

Concrete Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: True
    Option #2
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: False

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          args_0: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='args_0')
        Argument #2
          args_1: TensorSpec(shape=(None, 1), dtype=tf.float16, name='args_1')

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: False
    Option #2
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
        Argument #3
          DType: bool
          Value: True

  Function Name: 'infer_float'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float32, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float32, name='game_state')

  Function Name: 'infer_mixed'
    Option #1
      Callable with:
        Argument #1
          board_state: TensorSpec(shape=(None, 19, 19, 7), dtype=tf.float16, name='board_state')
        Argument #2
          game_state: TensorSpec(shape=(None, 1), dtype=tf.float16, name='game_state')
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-12 17:34:44,question,"java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1) Node number 0 (PAD) failed to prepare.","I have converted my DenseNet-121 model to model.tflite and when i am loading it to android app and trying to make predictions, it's giving following errors :                                                                                                                                                                                  java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)
Node number 0 (PAD) failed to prepare.
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
	at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
	at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
	at com.example.appleleafdiseasedetection.DiseaseDetector$2.onClick(DiseaseDetector.java:72)
	at android.view.View.performClick(View.java:7743)
	at android.view.View.performClickInternal(View.java:7720)
	at android.view.View.access$3700(View.java:854)
	at android.view.View$PerformClick.run(View.java:29111)
	at android.os.Handler.handleCallback(Handler.java:938)
	at android.os.Handler.dispatchMessage(Handler.java:99)
	at android.os.Looper.loopOnce(Looper.java:210)
	at android.os.Looper.loop(Looper.java:299)
	at android.app.ActivityThread.main(ActivityThread.java:8309)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:556)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1038)                                                                                                                how can i solve it?
                                                                                                                                                                         "
tensorflow/tensorflow,2023-06-07 06:13:41,question,"RESHAPE: OP is supported, but tensor type/shape isn't compatible","**System information**
- OS Platform and Distribution: Android, Galaxy S23.
- TensorFlow installed from (source or binary):2.12
- TensorFlow version (or github SHA if from source):2.12


Input tensor shape (1,16,32,256,12)
Output tensor shape (1,16,32,3072)

**Any other info / logs**:

RESHAPE: OP is supported, but tensor type/shape isn't compatible"
tensorflow/tensorflow,2023-06-05 05:29:18,question,"Error: Tensorflow lite c++ library, libtensorflowlite.so : Linking Error when compiling ( Undefined Reference )","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Linux, Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I cloned the tensorflow repo, then I built the **libtensorflowlite.so** using the command : 
`bazel build -c opt //tensorflow/lite:libtensorflowlite.so`

 Then copied the library to `/usr/local/lib` and When I compile using
 `g++ main.cpp -I/path/to/tensorflow/cloned/dir -L/usr/local/lib -ltensorflowlite` , I get the following error.


The main.cpp I used is from the tensorflow example codes.


/usr/bin/ld: /tmp/ccp1d7nd.o: in function `main':
test.cpp:(.text+0x3c): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: test.cpp:(.text+0xb2): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: test.cpp:(.text+0xcb): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: test.cpp:(.text+0xdf): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: test.cpp:(.text+0x113): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: test.cpp:(.text+0x195): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status

### Standalone code to reproduce the issue

```shell
/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include <cstdio>
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/optional_debug_tools.h>

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal <tflite model>

#define TFLITE_MINIMAL_CHECK(x)                                  \\
    if (!(x))                                                    \\
    {                                                            \\
        fprintf(stderr, ""Error at %s:%d\\n"", __FILE__, __LINE__); \\
        exit(1);                                                 \\
    }

int main(int argc, char *argv[])
{
    if (argc != 2)
    {
        fprintf(stderr, ""minimal <tflite model>\\n"");
        return 1;
    }
    const char *filename = argv[1];

    // Load model
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(filename);
    TFLITE_MINIMAL_CHECK(model != nullptr);

    // Build the interpreter with the InterpreterBuilder.
    // Note: all Interpreters should be built with the InterpreterBuilder,
    // which allocates memory for the Intrepter and does various set up
    // tasks so that the Interpreter can read the provided model.
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);
    TFLITE_MINIMAL_CHECK(interpreter != nullptr);

    // Allocate tensor buffers.
    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);
    printf(""=== Pre-invoke Interpreter State ===\\n"");
    tflite::PrintInterpreterState(interpreter.get());

    // Fill input buffers
    // TODO(user): Insert code to fill input tensors.
    // Note: The buffer of the input tensor with index `i` of type T can
    // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`

    // Run inference
    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
    printf(""\\n\\n=== Post-invoke Interpreter State ===\\n"");
    tflite::PrintInterpreterState(interpreter.get());

    // Read output buffers
    // TODO(user): Insert getting data out code.
    // Note: The buffer of the output tensor with index `i` of type T can
    // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`

    return 0;
}
```


### Relevant log output

```shell
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `main':
test.cpp:(.text+0x3c): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
/usr/bin/ld: test.cpp:(.text+0xb2): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
/usr/bin/ld: test.cpp:(.text+0xcb): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'
/usr/bin/ld: test.cpp:(.text+0xdf): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: test.cpp:(.text+0x113): undefined reference to `tflite::Interpreter::AllocateTensors()'
/usr/bin/ld: test.cpp:(.text+0x195): undefined reference to `tflite::InterpreterBuilder::~InterpreterBuilder()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x22): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'
/usr/bin/ld: /tmp/ccp1d7nd.o: in function `std::default_delete<tflite::Interpreter>::operator()(tflite::Interpreter*) const':
test.cpp:(.text._ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_[_ZNKSt14default_deleteIN6tflite11InterpreterEEclEPS1_]+0x22): undefined reference to `tflite::Interpreter::~Interpreter()'
collect2: error: ld returned 1 exit status
```
</details>"
tensorflow/tensorflow,2023-05-28 10:03:54,question,Empty logs during model.fit(),"[Issue_Empty_logs_during_model.fit().pdf](https://github.com/tensorflow/tensorflow/files/11584239/Issue_Empty_logs_during_model.fit.pdf)
"
tensorflow/tensorflow,2023-05-24 07:11:34,question,Why does toco convert tf.squeeze to reshape operator?,"### 1. System information

- latests tensorflow
- mac os (m1)

### 2. Code

This may simply be my curiosity and a question for your support. Looking at the kernel-side codes of tflite, it was found that there is a kernel implementation code for the squeeze operator(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/squeeze.cc
).
However, when converting tensorflow to tflite using toco, squeeze is converted to reshape by the function below.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/convert_squeeze_to_reshape.cc

I have a question about this.

1. Are you doing this conversion for some reason, like performance?
2. If not, will this conversion logic be removed from toco so that it can be done later with the squeeze tflite kernel?

Thank you.

BR, youngchan
"
tensorflow/tensorflow,2023-05-23 21:39:58,question,Migrating T2T fork to TF2.7.4,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.7.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

My org is looking to migrate our Tensor2Tensor fork from TF1 to TF2.7.4 by the end of June in order to not lose TPU access in GCP. The current plan for our fork is to utilize the `tensorflow.compat.v1` APIs (along with updating `contrib` imports to use `tensorflow_addons` and `tf_slim`) and `tf.disable_v2_behavior()`. Is this all that's required to get t2t working with TF2?

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-05-19 19:10:56,question,SavedModel: enable dropout & disable batch normalization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am currently training a ResNet model with both batch normalization and dropout layers. My goal is to use monte carlo dropout for uncertainty estimation at evaluation time (i.e. with training=False in my model call).

I'm currently working with tf.functions and the SavedModel format (not eager execution). Eager execution is to slow for my application, and therefore is not an option for me. 

Thus, when I set training=False for my model calls, it disables both the batch normalization and dropout layers. However, when I set training=False I want to keep dropout enabled but disable batch normalization (for the purpose of uncertainty estimation).

How can I achieve this with tf.function and the SavedModel format?

I am using tf-nightly.

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-05-14 12:14:57,question,tf.config.list_physical_devices('CPU'),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Debain 11

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

""tf.config.list_physical_devices('CPU')"" only show one CPU, but my machine has multiple CPUs. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

physical_devices = tf.config.list_physical_devices(""CPU"")
for device in physical_devices:
    print(device)
```


### Relevant log output

```shell
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')
```
</details>"
tensorflow/tensorflow,2023-05-12 10:16:23,question,BrokenPipeError: [Errno 32] Broken pipe,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I run the program I get BrokenPipeError: [Errno 32] Broken pipe.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1yTfJ-fcHMdwQxxoylyUFq5cFqloTKFYi?usp=sharing
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-05-10 11:46:30,question,"How to load a model locally, while the model is saved in distributed training remote machines (number of PS(parameter server)>1)?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Can't load the saved model(saved in distributed training, while number of PS(parameter server)>1).


``` python
# run in  local machine
# model is trained in remote machines
model = tf.keras.models.load_model(d_model)
```

Error Message:

``` bash
Loading a saved_model containing ShardedVariable via `tf.saved_model.load` is not supported. If the model is built using Keras, please use `tf.keras.models.load_model` instead
```

Tried the following saving options, not work. 
1. save_options = tf.saved_model.SaveOptions(experimental_io_device=""/job:chief/replica:0/task:0/device:CPU:0"")
2. save_options = tf.saved_model.SaveOptions(experimental_io_device=""/job:localhost/replica:0/task:0/device:CPU:0"")



### Standalone code to reproduce the issue

```shell
self.model = tf.keras.models.load_model(d_model, options=load_locally)
  File ""lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 71, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""lib/python3.7/site-packages/tensorflow/python/distribute/sharded_variable.py"", line 864, in _raise_when_load
    'Loading a saved_model containing ShardedVariable via '
ValueError: Loading a saved_model containing ShardedVariable via `tf.saved_model.load` is not supported. If the model is built using Keras, please use `tf.keras.models.load_model` instead.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-05-09 13:35:06,question,TfLite.initialize failure - android.os.RemoteException: Error loading TFLite GPU delegate module Caused by: lh: No acceptable module com.google.android.gms.tflite_gpu_dynamite found. Local version is 0 and remote version is 0.,"My implementation was working before adding GPU delegate (following instructions [here](https://www.tensorflow.org/lite/android/play_services#gpu_with_interpreter_apis)). After following those instructions I get this error when initializing TfLite

TfLite.initialize failure

```
android.os.RemoteException: Error loading TFLite GPU delegate module
  at pi.a(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:8)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.getInternalNativeInitializationHandleWithParams(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0)
  at oi.w(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:5)
  at bs.onTransact(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4)
  at android.os.Binder.transact(Binder.java:1200)
  at com.google.android.gms.internal.tflite.zza.zzb(com.google.android.gms:play-services-tflite-impl@@16.0.0:2)
  at com.google.android.gms.tflite.dynamite.zza.zze(com.google.android.gms:play-services-tflite-impl@@16.0.0:4)
  at com.google.android.gms.internal.tflite.zzr.zzc(com.google.android.gms:play-services-tflite-impl@@16.0.0:11)
  at com.google.android.gms.internal.tflite.zzp.zza(com.google.android.gms:play-services-tflite-impl@@16.0.0:7)
  at com.google.android.gms.internal.tflite.zzn.then(Unknown Source:6)
  at com.google.android.gms.tasks.zzo.run(com.google.android.gms:play-services-tasks@@18.0.2:1)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1137)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:637)
  at java.lang.Thread.run(Thread.java:1012)
Caused by: lh: No acceptable module com.google.android.gms.tflite_gpu_dynamite found. Local version is 0 and remote version is 0.
  at ll.c(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:88)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.b(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4)
  at com.google.android.gms.tflite.dynamite.TfLiteDynamiteLoaderImpl.getInternalNativeInitializationHandleWithParams(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:0) 
  at oi.w(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:5) 
  at bs.onTransact(:com.google.android.gms.policy_tflite_dynamite_dynamite@231406201@231406200065.522547357.522547357:4) 
  at android.os.Binder.transact(Binder.java:1200) 
  at com.google.android.gms.internal.tflite.zza.zzb(com.google.android.gms:play-services-tflite-impl@@16.0.0:2) 
  at com.google.android.gms.tflite.dynamite.zza.zze(com.google.android.gms:play-services-tflite-impl@@16.0.0:4) 
  at com.google.android.gms.internal.tflite.zzr.zzc(com.google.android.gms:play-services-tflite-impl@@16.0.0:11) 
  at com.google.android.gms.internal.tflite.zzp.zza(com.google.android.gms:play-services-tflite-impl@@16.0.0:7) 
  at com.google.android.gms.internal.tflite.zzn.then(Unknown Source:6) 
  at com.google.android.gms.tasks.zzo.run(com.google.android.gms:play-services-tasks@@18.0.2:1) 
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1137) 
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:637) 
  at java.lang.Thread.run(Thread.java:1012) 
---------------------------- PROCESS ENDED (24111) for package com.android.example.camerax.tflite ---------
```


**System information**
- Android Device information: samsung/o1quew/o1q:13/TP1A.220624.014/G991U1UEU6EWD1:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 
  - org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2 
  - com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): 23.16.13

**Standalone code to reproduce the issue**



```
implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2'
implementation 'com.google.android.gms:play-services-tflite-gpu:16.2.0'
```

Activity class
```
override fun onCreate... {
 val initializeTask: Task<Void> by lazy {
            TfLite.initialize(
                this,
                TfLiteInitializationOptions.builder()
                    .setEnableGpuDelegateSupport(true)
                    .build()
            )
        }
}
```

"
tensorflow/tensorflow,2023-05-07 17:48:03,question,skin cancer detection,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

vs code

### Custom Code

Yes

### OS Platform and Distribution

vs code

### Mobile device

vs code

### Python version

vs code

### Bazel version

vs code

### GCC/Compiler version

vs code

### CUDA/cuDNN version

vs code

### GPU model and memory

vs code

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
ValueError                                Traceback (most recent call last)
Cell In[12], line 1
----> 1 history = model.fit (X_train, y_train, validation_split=0.2,epochs= 5, 
      2                      batch_size= batch_size, verbose=1, callbacks=[learning_rate_reduction])           
      3 # list all data in history
      4 print(history.keys())

File ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70, in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\\AppData\\Local\\Temp\\__autograph_generated_file6gyezdc5.py:15, in outer_factory..inner_factory..tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py"", line 1284, in train_function  *
        return step_function(self, iterator)
    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py"", line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py"", line 1249, in run_step  **
        outputs = model.train_step(data)
    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py"", line 1050, in train_step
        y_pred = self(x, training=True)
    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\\Users\\saiva\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\input_spec.py"", line 298, in assert_input_compatibility
        raise ValueError(

    ValueError: Input 0 of layer ""sequential_1"" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 300, 300, 3)
```


### Relevant log output

```shell
Epoch 1/5
```
</details>"
tensorflow/tensorflow,2023-05-05 01:43:12,question,Is DTensor compatible with Horovod,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Want to check if DTensor model parallel compatible with Horovod? Any code examples or Benchmark of DTensor model parallel? 

### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-04-30 19:42:47,question,How to download outdated versions of Tensorflow in Google Colab?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

3.11

### Bazel version

3.7.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a project using Mask RCNN, and the project only works with Tensorflow 2.5, but this version is outdated and the pip doesn't support this version anymore, when I'm trying to build on colab using the ""Build from Source"" documentation, 

When I try to use bazel I get an error while trying to build it

````
!bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package
````

Also when I try to use pip

````
!pip install git+https://github.com/tensorflow/tensorflow.git@r2.5
````

I get the following error
````
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting git+https://github.com/tensorflow/tensorflow.git@r2.5
  Cloning https://github.com/tensorflow/tensorflow.git (to revision r2.5) to /tmp/pip-req-build-f6mzej4m
  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/tensorflow.git /tmp/pip-req-build-f6mzej4m
  Running command git checkout -b r2.5 --track origin/r2.5
  Switched to a new branch 'r2.5'
  Branch 'r2.5' set up to track remote branch 'r2.5' from 'origin'.
  Resolved https://github.com/tensorflow/tensorflow.git to commit 959e9b2a0c06df945f9fb66bd367af8832ca0d28
ERROR: git+https://github.com/tensorflow/tensorflow.git@r2.5 does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.
````

### Standalone code to reproduce the issue

```shell
You can reproduce it by using this colab

https://colab.research.google.com/github/Huxwell/caffe-tf-torch-darknet-onnx/blob/main/tensorflow_from_source.ipynb
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-04-22 13:16:36,question,I am noticing lower validation accuracy on my dataset between Tensorflow 2.4 and Tensorflow 2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.9 and 2.4

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to train an image classifier model using EfficientNetB1 on a custom dataset and I am trying out TensorFlow 2.4 and TensorFlow 2.9. I am using the exact same script with the same optimizer, augmentation, parameters, and dataset. I ran training 5 times and the results are around the same.

Results:

- TensorFlow 2.4: ~97-98% Accuracy on the validation set.

- TensorFlow 2.9: ~93-95% Accuracy on the validation set

More information: I am using Adam optimizer with 0.0001 lr, batch size of 16, using imagenet model weights, and categorical_crossentropy for my loss. I am using the same dataset on each version and I am using the same training script. I simply switch conda enviroments to TF 2.4 and 2.9.

Did something change between both versions that cause this discrepancy? Did the EfficientNet model weights change? Is the way the validation accuracy are calculated is different? Are the opimizers implementations are different?

I would appreciate your help and I would like some information on how to make it consistent between both versions. Thanks

### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB1

model_base = EfficientNetB1(weights='imagenet',include_top=False, input_shape=(image_size, image_size, 3))
model.add(model_base)
model._name = ""EfficientNetB1""    
model.add(layers.Flatten())
model.add(tf.keras.layers.Dense(len(classes), activation='softmax'))

opt = Adam(learning_rate=1e-4)
model.compile(optimizer=opt, loss= 'categorical_crossentropy', metrics=['accuracy'])
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-04-19 09:14:34,question,Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

Linux (Android Oneplus)

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am working on this noise and echo cancellation on android. I used this tflite model created via https://colab.research.google.com/drive/1HzGdovqo0gg_xW1QL7ygbkYlWqbyMaKL?usp=sharing#scrollTo=2kRjDbp7og1u

Now I want to use a ShortArray() audioData and want to pass and get a ShortArray() data back with  removed echo and noise.

I tried creating, tflite model, predict function works there, I incorporated it in android but whenever data is passed in the array it returns the error

```Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4)```

### Standalone code to reproduce the issue

```shell
snippet has this


#@INPUT
print(test_audio) gives 

<tf.Tensor: shape=(6, 12000, 1), dtype=float32, numpy=
array([[[-0.00097656],
        [-0.01428223],
        [-0.02471924],
        ...,
        [ 0.14346313],
        [ 0.13012695],
        [ 0.14242554]]]], dtype=float32)>
```

```
predict_tflite_with_array(tf.convert_to_tensor(test_audio, np.float32)) # @To predict
```

```
#@Output
<tf.Tensor: shape=(68800,), dtype=float32, numpy=
array([-0.00645937, -0.01239021, -0.01816257, ..., -0.02149353,
       -0.0514788 , -0.04442336], dtype=float32)>
```

```
def predict_tflite_with_array(test_audio):
  input_index = interpreter.get_input_details()[0][""index""]
  output_index = interpreter.get_output_details()[0][""index""]

  preds = []
  for i in test_audio:
    interpreter.set_tensor(input_index, tf.expand_dims(i,0))
    interpreter.invoke()
    predictions = interpreter.get_tensor(output_index)
    preds.append(predictions)

  predictions = tf.squeeze(tf.stack(preds,axis=1))
  final_op = tf.reshape(predictions[:-1],((predictions.shape[0]-1)*predictions.shape[1],1))
  final_op = tf.concat((tf.squeeze(final_op),predictions[-1][-diff:]),axis=0)
  return final_op

```



I used this tflite model in android to run this

```
aaptOptions {
        noCompress ""model.tflite""
    }

implementation 'org.tensorflow:tensorflow-lite:2.12.0'
```
```
 @Throws(IOException::class)
    private fun loadModelFile(activity: Activity): MappedByteBuffer? {
        val fileDescriptor: AssetFileDescriptor = activity.assets.openFd(""model.tflite"")
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    try {
            viewModel.tflite = loadModelFile(requireActivity())?.let {
                Interpreter(it) }
            Log.d(""tflite"", ""Model initiated"")
        } catch (ex: Exception) {
            ex.printStackTrace()
            Log.d(""tflite"", ""Model initiation Failed $ex"")
        }
```

```
private fun applyModel(inputAudioData: ShortArray): ShortArray {
        val inputVal = FloatArray(inputAudioData.size)

        for (i in inputAudioData.indices) {
            inputVal[i] = inputAudioData[i].toFloat()
        }
        val inputFloatArray = Array(1){
            inputVal
        }
        val outputFloatArray = Array(1) {
            FloatArray(inputAudioData.size)
        }

        Log.d(""tflite"", ""Model input data: ${inputFloatArray.toString()}"")

        tflite!!.run(inputFloatArray, outputFloatArray)
        Log.d(""tflite"", ""Model output data: ${outputFloatArray.toString()}"")

        val outputAudioData = ShortArray(inputFloatArray.size)

        for (i in outputFloatArray[0].indices) {
            outputAudioData[i] = outputFloatArray[0][i].toInt().toShort()
        }
        Log.d(""tflite"", ""Model returning data: ${outputAudioData.toString()}"")

        return outputAudioData
    }
```

  [1]: https://colab.research.google.com/drive/1HzGdovqo0gg_xW1QL7ygbkYlWqbyMaKL?usp=sharing#scrollTo=2kRjDbp7og1u
```


### Relevant log output

```shell
but whenever the app is running its crashing with this error


 java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:343 input->dims->size != 4 (3 != 4)
                                                                                                    Node number 5 (CONV_2D) failed to prepare.
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
                                                                                                    	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
                                                                                                    	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
                                                                                                    	at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
                                                                                                    	at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
```
```
</details>"
tensorflow/tensorflow,2023-04-18 05:51:45,question,"RuntimeError: Given shapes, [1,784] and [784,100], are not broadcastable.Node number 1 (MUL) failed to prepare.","### 1. System information

I am using Google Colab. Gives the output Linux a0c9eeb98a07 5.10.147+ #1 SMP Sat Dec 10 16:00:40 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux, when I run uname -a.

Tensorflow Version: 2.12.0


### 2. Code
Reference [TensorFlow Model Colab](https://colab.research.google.com/drive/1Qq2CdEKDbV-Y5y2EFV3LxQgSPly_PbdT?usp=sharing): The basic conversion to TFLite without any representative dataset works just fine. However, the problematic portions are cells 4, and 5, where I am trying to perform a TFLite conversion with a representative dataset.  


### 3. Failure after conversion
It throws the following error.
```
WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: /tmp/tmpz4fb25rp/assets
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
[<ipython-input-6-3bd48f77d98d>](https://localhost:8080/#) in <cell line: 10>()
      8 converter.inference_output_type = tf.uint8
      9 
---> 10 tflite_model_quant = converter.convert()
     11 
     12 interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)

13 frames
[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
   1895         Invalid quantization parameters.
   1896     """"""
-> 1897     return super(TFLiteConverterV2, self).convert()
   1898 
   1899 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in wrapper(self, *args, **kwargs)
    960   def wrapper(self, *args, **kwargs):
    961     # pylint: disable=protected-access
--> 962     return self._convert_and_export_metrics(convert_func, *args, **kwargs)
    963     # pylint: enable=protected-access
    964 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_and_export_metrics(self, convert_func, *args, **kwargs)
    938     self._save_conversion_params_metric()
    939     start_time = time.process_time()
--> 940     result = convert_func(self, *args, **kwargs)
    941     elapsed_time_ms = (time.process_time() - start_time) * 1000
    942     if result:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in convert(self)
   1534     """"""
   1535     if self.experimental_lower_to_saved_model:
-> 1536       saved_model_convert_result = self._convert_as_saved_model()
   1537       if saved_model_convert_result:
   1538         return saved_model_convert_result

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_as_saved_model(self)
   1514       if self.saved_model_dir:
   1515         self._validate_inputs(graph_def, input_tensors)
-> 1516         return self._convert_from_saved_model(graph_def)
   1517     finally:
   1518       shutil.rmtree(temp_dir, True)

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _convert_from_saved_model(self, graph_def)
   1129 
   1130     result = _convert_saved_model(**converter_kwargs)
-> 1131     return self._optimize_tflite_model(
   1132         result, quant_mode, quant_io=self.experimental_new_quantizer)
   1133 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    213       except Exception as error:
    214         report_error_message(str(error))
--> 215         raise error from None  # Re-throws the exception.
    216 
    217     return wrapper

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    203     def wrapper(*args, **kwargs):
    204       try:
--> 205         return func(*args, **kwargs)
    206       except ConverterError as converter_error:
    207         if converter_error.errors:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _optimize_tflite_model(self, model, quant_mode, quant_io)
    897         q_allow_float = quant_mode.is_allow_float()
    898         q_variable_quantization = quant_mode.enable_mlir_variable_quantization
--> 899         model = self._quantize(model, q_in_type, q_out_type, q_activations_type,
    900                                q_bias_type, q_allow_float,
    901                                q_variable_quantization)

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py](https://localhost:8080/#) in _quantize(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)
    636                                                 custom_op_registerers_by_func)
    637     if self._experimental_calibrate_only or self.experimental_new_quantizer:
--> 638       calibrated = calibrate_quantize.calibrate(
    639           self.representative_dataset.input_gen)
    640 

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    213       except Exception as error:
    214         report_error_message(str(error))
--> 215         raise error from None  # Re-throws the exception.
    216 
    217     return wrapper

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert_phase.py](https://localhost:8080/#) in wrapper(*args, **kwargs)
    203     def wrapper(*args, **kwargs):
    204       try:
--> 205         return func(*args, **kwargs)
    206       except ConverterError as converter_error:
    207         if converter_error.errors:

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/optimize/calibrator.py](https://localhost:8080/#) in calibrate(self, dataset_gen)
    224       dataset_gen: A generator that generates calibration samples.
    225     """"""
--> 226     self._feed_tensors(dataset_gen, resize_input=True)
    227     return self._calibrator.Calibrate()

[/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/optimize/calibrator.py](https://localhost:8080/#) in _feed_tensors(self, dataset_gen, resize_input)
    127                                      signature_key)
    128           else:
--> 129             self._calibrator.Prepare([list(s.shape) for s in input_array])
    130         else:
    131           if signature_key is not None:

RuntimeError: Given shapes, [1,784] and [784,100], are not broadcastable.Node number 1 (MUL) failed to prepare.
```
Following is the netron image and the problematic node info.
![image](https://user-images.githubusercontent.com/37201355/232685234-0761417e-37e6-494d-ac04-77079c1f96dd.png)

I do not suspect the issue to be with the broadcasting of scalar r in the multiplication with W1_1, because the code [here](https://colab.research.google.com/drive/1jOs_vcne2-vzf2SnB-6-PRkdGk3s9wq3?usp=sharing) works just fine. The only difference in this new code is that the __call__ implementation only takes r as the input argument and returns the product of r and W1_1, which does not emit the error shown above, even though there is a broadcast."
tensorflow/tensorflow,2023-04-15 01:28:54,question,AttributeError: module 'tensorflow' has no attribute '__version__',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

Python3.8.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
code is too long
```


### Relevant log output

```shell
(reflow1) PS E:\\Googledownload\\RectifiedFlow-main\\ImageGeneration> python ./main.py --config ./configs/rectified_flow/cifar10_rf_gaussian_ddpmpp.py --eval_folder eval --mode eval --workdir ./logs/1_rectified_flow --config.eval.enable_sampling  --config.eval.batch_size 250 --config.eval.num_samples 3000 --config.eval.begin_ckpt 8
Traceback (most recent call last):
  File ""./main.py"", line 18, in <module>
    import run_lib
  File ""E:\\Googledownload\\RectifiedFlow-main\\ImageGeneration\\run_lib.py"", line 26, in <module>
    import tensorflow_gan as tfgan
  File ""D:\\conda\\envs\\reflow1\\lib\\site-packages\\tensorflow_gan\\__init__.py"", line 108, in <module>
    _ensure_tf_install()
  File ""D:\\conda\\envs\\reflow1\\lib\\site-packages\\tensorflow_gan\\__init__.py"", line 60, in _ensure_tf_install
    if (distutils.version.LooseVersion(tf.__version__) <
AttributeError: module 'tensorflow' has no attribute '__version__'
```
</details>"
tensorflow/tensorflow,2023-04-14 02:06:39,question,protobuf have a problem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.11.0

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I don't know if there is a problem with my download or the version of tensorflow, that is, how to recompile protobuf. Can I download the tensorflow compressed package directly from GitHub and uninstall tensorflow-Intel without uninstalling the gpu version.

### Standalone code to reproduce the issue

```shell
PS E:\\Googledownload\\RectifiedFlow-main> & D:/conda/envs/reflow/python.exe e:/Googledownload/RectifiedFlow-main/ImageGeneration/1.py
Traceback (most recent call last):
  File ""e:\\Googledownload\\RectifiedFlow-main\\ImageGeneration\\1.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\python\\__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\python\\eager\\context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py"", line 16, in <module>
    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py"", line 36, in <module>
    _descriptor.FieldDescriptor(
  File ""D:\\conda\\envs\\reflow\\lib\\site-packages\\google\\protobuf\\descriptor.py"", line 561, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-04-03 23:09:21,question,api for model parallelism,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
By using tf.distribute, only the data parallel is documented in the website. https://www.tensorflow.org/guide/distributed_training

Now, what are the APIs that can be used for model parallelism? 

In huggingface, it is documented that, there are many types of parallelism. Read here https://huggingface.co/docs/transformers/v4.15.0/parallelism

What the solutions tensorflow provides for large language model training/inference for consumer level GPU. GPU that has 16GB, 24GB v-ram.

Is it possible to do [Tensor-Parallel](https://huggingface.co/docs/transformers/v4.15.0/parallelism#tensor-parallelism) in Tensorflow.

How model parallism can be applied with APIs in Tensorflow and Keras?
```


### Standalone code to reproduce the issue

```shell
mirrored_strategy = tf.distribute.MirroredStrategy()
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-04-02 21:04:57,question,Writing custom ReduceOp,"I would like take a MirroredVariable, shape=(32,60),  with 3 replica and reduce is first as a difference of pairs and than L2 norm it. 

Is there a way of writing such a custom ReduceOp? 

https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/distribute/reduce_util.py#L23-L47"
tensorflow/tensorflow,2023-03-30 11:08:35,question,TF2 keras model.fit() VS model.evaluate print different loss and AUC,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
My model generate 3 binary outputs with MMoE module, and I define a custom metric to combine these three AUC in callbacks. As I am using model.fit to train my dataset,  it prints the final training AUC in the last epoch is 0.98. While I am using model.evaluate() on the SAME training dataset, it turns out the AUC is only 0.65. Can anyone can help me with it? Btw, I do not use BatchNormalization and Dropout layers, and also the batch_size is the same for both fitting and evaluation process.

I used exactly same dataset and same batch_size, I really don’t know why it occurs?
```


### Standalone code to reproduce the issue

```shell
class MergeMetrics(tf.keras.callbacks.Callback):
    def __init__(self,**kargs):
        super(MergeMetrics,self).__init__(**kargs)

    def on_epoch_begin(self,epoch, logs={}):
        return

    def on_epoch_end(self, epoch, logs={}):
        logs['merge_auc'] = 0.7 *logs[""output_1_auc""]+ 0.2*logs[""output_2_auc""] + 0.1*logs[""output_3_auc""]


model = Modelname(some_parameters)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1, beta_2),
            loss={
                ""output_1_auc"": ‘binary_crossentropy’
                ""output_2_auc"": ‘binary_crossentropy’
                ""output_3_auc"": ‘binary_crossentropy’
            },
            loss_weights=[1,1,1],
            metrics={
                ""output_1_auc"": [tf.keras.metrics.AUC(name='auc')],
                ""output_2_auc"": [tf.keras.metrics.AUC(name='auc')],
                ""output_3_auc"": [tf.keras.metrics.AUC(name='auc')]})


checkpoint = MergeMetrics()
checkpoint_filepath = os.path.join(path,'model')
model_check = tf.keras.callbacks.ModelCheckpoint(
        filepath = checkpoint_filepath,
        # filepath = path,
        monitor= ""merge_auc"",
        save_best_only = True,
        mode='max',
        save_weights_only = True,
        save_freq=""epoch"")

train_dataset = tf.data.Dataset.from_tensor_slices((train_data,                                           (y_train.output_1, y_train.output_2, y_train.output_3))).shuffle(10*batch_size).batch(batch_size)

model.fit(train_dataset, epochs=5, callbacks=[checkpoint,model_check])
model.load_weights(checkpoint_filepath) ## load weights of the best epoch
auc  = model.evaluate(train_data, [y_train.output_1, y_train.output_2, y_train.output_3], batch_size=batch_size)
```


### Relevant log output

```shell
RESULT:
Fit process : output_1_auc:0.9862,  output_2_auc: 0.9665, output_3_auc:0.5014, merge_auc: 0.9338
Evaluation process: output_1_auc: 0.7124500870704651, output_2_auc:0.6924731135368347, output_3_auc: 0.6774155497550964
```
</details>"
tensorflow/tensorflow,2023-03-30 06:33:53,question,Are there plans to release TF 2.9.4 to address CVEs?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.9.3

### Custom Code

No

### OS Platform and Distribution

RHEL 8

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We're using TF 2.9.3 in our product images. Our latest twistlock scans reveal several security findings. Are there plans to release a 2.9.4 that addresses these CVEs?

Findings are:
| CVE | URL |
| --- | --- |
| CVE-2023-25669	| https://nvd.nist.gov/vuln/detail/CVE-2023-25669|
| CVE-2023-25673	| https://nvd.nist.gov/vuln/detail/CVE-2023-25673|
| CVE-2023-25674	| https://nvd.nist.gov/vuln/detail/CVE-2023-25674|
| CVE-2023-27579	| https://nvd.nist.gov/vuln/detail/CVE-2023-27579|
| CVE-2023-25667	| https://nvd.nist.gov/vuln/detail/CVE-2023-25667|
| CVE-2023-25675	| https://nvd.nist.gov/vuln/detail/CVE-2023-25675|
| CVE-2023-25670	| https://nvd.nist.gov/vuln/detail/CVE-2023-25670|
| CVE-2023-25671	| https://nvd.nist.gov/vuln/detail/CVE-2023-25671|
| CVE-2023-25672	| https://nvd.nist.gov/vuln/detail/CVE-2023-25672|
| CVE-2023-25801	| https://nvd.nist.gov/vuln/detail/CVE-2023-25801|
| CVE-2023-25676	| https://nvd.nist.gov/vuln/detail/CVE-2023-25676|
| CVE-2023-25668	| https://nvd.nist.gov/vuln/detail/CVE-2023-25668|
| CVE-2023-25666	| https://nvd.nist.gov/vuln/detail/CVE-2023-25666|
| CVE-2023-25665	| https://nvd.nist.gov/vuln/detail/CVE-2023-25665|
| CVE-2023-25664	| https://nvd.nist.gov/vuln/detail/CVE-2023-25664|
| CVE-2023-25663	| https://nvd.nist.gov/vuln/detail/CVE-2023-25663|
| CVE-2023-25662	| https://nvd.nist.gov/vuln/detail/CVE-2023-25662|
| CVE-2023-25661	| https://nvd.nist.gov/vuln/detail/CVE-2023-25661|
| CVE-2023-25660	| https://nvd.nist.gov/vuln/detail/CVE-2023-25660|
| CVE-2023-25659	| https://nvd.nist.gov/vuln/detail/CVE-2023-25659|
| CVE-2023-25658	| https://nvd.nist.gov/vuln/detail/CVE-2023-25658|
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-29 14:35:28,question,How to solve the backpropagation problem of tf.signal.rfft?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I x wanted to implement the Hilbert transform in the model built by tensorflowd, but I found no callable API. so I implemented it myself using tf.signal.rfft and tf.signal.ifft. However, when I was training, I found the following error reported:
LookupError: gradient registry has no entry for: RFFT3D
I think it may be that tensorflow has not defined the corresponding backpropagation method for rfft3d yet. I want to provide my code train.py  &  model.py
```


### Standalone code to reproduce the issue

```
def hilbert_transform(x, N=None, axis=-1):
    if x.dtype == tf.complex64 or x.dtype == tf.complex128:
        raise ValueError(""x must be real."")
    if N is None:
        N = x.shape[axis]
    if N <= 0:
        raise ValueError(""N must be positive."")
    x_br = tf.signal.rfft3d(x[:, :, :1])
    x_bi = tf.signal.rfft3d(x[:, :, 1:])
    x_complex = concatenate([x_br, x_bi], axis=-1)

    h = np.zeros(N)
    if N % 2 == 0:
        h[0] = h[N // 2] = 1
        h[1:N // 2] = 2
    else:
        h[0] = 1
        h[1:(N + 1) // 2] = 2
    if x.shape.ndims > 1:
        ind = [tf.newaxis] * x.shape.ndims
        ind[axis] = slice(None)
        h = h[tuple(ind)]
    x_complex_h = x_complex * h
    x_bj = tf.signal.ifft3d(x_complex_h[:, :, :1])
    x_bk = tf.signal.ifft3d(x_complex_h[:, :, 1:])
    x_hilbert = concatenate([tf.math.imag(x_bj), tf.math.imag(x_bk)], axis=-1)
    return x_hilbert
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-28 19:51:01,question,Correct way to implement RNN if looping over tensor is yet not allowed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

MaxOS 12.3.1 

### Mobile device

-

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

M1 Max 26 cores GPU 32 GB ram

### Current Behaviour?

```shell
it throws : Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature. yet the code in my opinion can be easily converted to a graph, just conditioned on the `tf.shape(inputs)[1]`... this would allow building RNNs that accepts long and short inputs, without having to pad it, which is extremely useful if the length of the strings in the dataset is very skewed, and we have many short phrases and few long ones
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
class MyModel1(tf.keras.Model):
    def __init__(self, input_text_processor, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.input_text_processor = input_text_processor
    def call(self, data, training=True, max_len=50):
        inputs = self.input_text_processor(data)
        for _ in tf.range(tf.shape(inputs)[1]):
            pass
        return []

    def train_step(self, data):
        self.call(data)
        return {""loss"" : 0}
dataset = [
    ""hi"", ""what's up"", ""what's the weather""
]
input_text_processor = tf.keras.layers.TextVectorization()
input_text_processor.adapt(dataset)
with tf.device(""/CPU:0""):
    model = MyModel1(input_text_processor)
    model.compile(tf.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy())
    hist = model.fit(dataset, epochs=5)
```


### Relevant log output

```shell
Epoch 1/5
Traceback (most recent call last):
  File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 23, in <module>
    hist = model.fit(dataset, epochs=5)
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py"", line 1269, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:

    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in train_function  *
        return step_function(self, iterator)
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1233, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml-apple-metal-3-10/lib/python3.10/site-packages/keras/engine/training.py"", line 1222, in run_step  **
        outputs = model.train_step(data)
    File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 13, in train_step
        self.call(data)
    File ""/Users/username/ml/tensorflow-journey/39-rnn-enc-dec-attention/rnn-enc-dec-attention.py"", line 8, in call
        for _ in tf.range(tf.shape(inputs)[1]):

    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```
</details>"
tensorflow/tensorflow,2023-03-28 17:40:33,question,Cannot install tensorflow modules use pip,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Darwin
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: `pip3 install tf-nightly`
-   **TensorFlow version (use command below)**: `2.13.0.dev20230325`
-   **Python version**: `Python 3.11.2`
-   **Bazel version (if compiling from source)**: no bazel installed
-   **GCC/Compiler version (if compiling from source)**: Not compiling from source
  ```bash
  $ clang --version
  Apple clang version 13.1.6 (clang-1316.0.21.2.3)
  Target: arm64-apple-darwin21.2.0
  Thread model: posix
  InstalledDir: /Library/Developer/CommandLineTools/usr/bin
  ```
-   **CUDA/cuDNN version**: 
-   **GPU model and memory**: 16GB memory. Mac M1 built in GPU?
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
P.S: I don't think this script work, because it gave me my python2 version, not python3

You can obtain the TensorFlow version with:

```bash
$ python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.12.1-91636-g1057ad694db 2.13.0-dev20230325
```

### Describe the problem

I am following this document: <https://www.tensorflow.org/text/tutorials/classify_text_with_bert#setup>

So when I try to install tensorflow, it gave me mo matching distribution found errors.

### Source code / logs

```bash
$ pip3 install tensorflow                                                                                               
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
```bash
$ pip3 install -q -U ""tensorflow-text==2.11.*""
ERROR: Could not find a version that satisfies the requirement tensorflow-text==2.11.* (from versions: none)
ERROR: No matching distribution found for tensorflow-text==2.11.*
$ pip3 install -q tf-models-official==2.11.0
ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.5.2.52 (from tf-models-official) (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72)
ERROR: No matching distribution found for opencv-python-headless==4.5.2.52
```"
tensorflow/tensorflow,2023-03-26 13:55:00,question,How can I achive the hilbert transform by using tensorflow?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I discovered that tensorflow did not have a built-in API for implementing the Hilbert transform, I tried to pass the tensor to the scipy.signal.hilbert method in numpy, but keras does not allow the tensor to be processed directly using numpy maths methods, so I used the following method for data[ Tensor/KerasTensor(1,4096,2)] to convert:
1. data_np = data.numpy()
2. data_np = keras.backend.get_val(data)
Neither works. I then decided to build my own hilbert_transform function by referring to the scipy.signal.hilbert method in numpy, at which point another problem arose:
The tf.signal.fft method is inconsistent with the scipy.fft method. scipy.fft can take an axis argument, but tf.signal.fftz cannot.
Therefore, I would like your help in answering:
1. how do I implement the Hilbert transform on data[Tensor/KerasTensor(1,4096,2)]?
or
2. how do I convert data[Tensor/KerasTensor(1,4096,2)] into a numpys array?

Translated with www.DeepL.com/Translator (free version)
```


### Standalone code to reproduce the issue

```shell
from scipy.signal import hilbert
import tensorflow as tf
import numpy as np

def is_complex(x):
    return x.dtype == tf.complex64 or x.dtype == tf.complex128
def highpass_filter(x, N=None, axis=-1):
    x = tf.convert_to_tensor(x)
    if is_complex(x):
        raise ValueError(""x must be real."")
    if N is None:
        N = x.shape[axis]
    if N <= 0:
        raise ValueError(""N must be positive."")

    Xf = fft(x, N, axis=axis)
    h = tf.zeros(N)
    if N % 2 == 0:
        h = tf.tensor_scatter_nd_update(h, [[0], [N // 2]], [1, 1])
        h = tf.tensor_scatter_nd_update(h, tf.range(1, N // 2), tf.ones(N // 2 - 1) * 2)
    else:
        h = tf.tensor_scatter_nd_update(h, [0], [1])
        h = tf.tensor_scatter_nd_update(h, tf.range(1, (N + 1) // 2), tf.ones((N + 1) // 2 - 1) * 2)

    if x.ndim > 1:
        ind = [tf.newaxis] * x.ndim
        ind[axis] = slice(None)
        h = h[tuple(ind)]

    x = tensorflow.signal.ifft(Xf * h, axis=axis)
    return x
x = np.random.rand(1, 4096, 2)
x_h = highpass_filter(x,axis=1)
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-23 20:34:46,question,[JAX] The use of jax.Array.at fails experimental_from_jax,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 22.3.0
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0 

### 2. Code


#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```

import tensorflow as tf
import jax
import jax.numpy as jnp
import jax.tree_util as tree

def main3():
    print(f'JAX {jax.__version__}')
    print(f'tf {tf.__version__}')

    @jax.jit
    def _update(x):
        return x.at[0].set(4)


    converter = tf.lite.TFLiteConverter.experimental_from_jax([_update],
                                                              [[('x', jnp.ones(2))]])
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.

    ]

    tflite_update = converter.convert()
    with open('update.tflite', 'wb') as f:
        f.write(tflite_update)

    interpreter = tf.lite.Interpreter(model_content=tflite_update)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()



    args = jnp.ones(2)

    expected = _update(args)

    # for a, d in zip(args, input_details):
    interpreter.set_tensor(input_details[0]['index'], args)

    interpreter.invoke()
    return

```

full [example](https://github.com/krzysztofrusek/reinforced-lib/blob/lite/examples/lite/lite.py)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Conversion works
- Interpreter fails at invoke

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```Traceback (most recent call last):
  File ""/Users/krzysiek/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/223.8836.34/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py"", line 1496, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/krzysiek/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/223.8836.34/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\\n"", file, 'exec'), glob, loc)
  File ""/Users/krzysiek/Documents/ml4wifi/reinforced-lib/examples/lite/lite.py"", line 168, in <module>
    main3()
  File ""/Users/krzysiek/Documents/ml4wifi/reinforced-lib/examples/lite/lite.py"", line 163, in main3
    interpreter.invoke()
  File ""/Users/krzysiek/Documents/ml4wifi/ftmrate_internal/venv/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: Updates shape must have rank at least one. Found:[]Node number 1 (TfLiteFlexDelegate) failed to invoke.
```
"
tensorflow/tensorflow,2023-03-22 10:23:59,question,How to save average weights of checkpoints using Tensorflow 2.X?,"HELP NEEDED !!!

Hi, everyone. I found a scrit to load serials of checkpoints for a model and save average weights of them in TensorFlow1.

https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/avg_checkpoints.py

Which is very useful to improve the performance of the model. But in TensorFlow2， the saved checkpoints like this:

![image](https://user-images.githubusercontent.com/42861880/226873472-d5b31ef6-6e59-4136-ae55-8d367418cd7c.png)

How to average the weights of parameters for them? I tried to write a script but the output checkpoint seems not as we expected, anyone can give some helps? Thanks a lot in advance. Here is my script:


`
import os
import numpy as np
import tensorflow as tf	
from absl import app
from absl import flags
from absl import logging


FLAGS = flags.FLAGS

flags.DEFINE_string(""checkpoints"","""",
					""Comma-separated list of checkpoints to average."")
flags.DEFINE_integer(""num_last_chekpoints"", 0,
					""Average the last N saved checkpoints.""
					"" If the checkpoints flag is set, this is ignored."")
flags.DEFINE_string(""prefix"", """",
					""Prefix (e.g., directory) to append to each checkpoint."")
flags.DEFINE_string(""output_path"", ""/tmp/averaged.ckpt"",
					""Path to output the averaged checkpoint to."")

def checkpoint_exists(path):
	return (tf.io.gfile.exists(path) or tf.io.gfile.exists(path + "".index""))

def main(argv):
	if FLAGS.checkpoints:
		# Get the checkpoints list from flags and run some basic checks.
		checkpoints = [c.strip() for c in FLAGS.checkpoints.split("","")]
		checkpoints = [c for c in checkpoints if c]
		if not checkpoints:
			raise ValueError(""No checkpoints provided for averaging."")
		if FLAGS.prefix:
			checkpoints = [FLAGS.prefix + c for c in checkpoints]
	else:
		assert FLAGS.num_last_chekpoints >= 1, ""Must average at least one model""
		assert FLAGS.prefix, (""Prefix must be provided when averaging last""
								"" N checkpoints"")
		# checkpoint_state = tf.train.get_checkpoint_state(
		# 	os.path.dirname(FLAGS.prefix))
		# # Checkpoints are ordered from oldest to newest.
		# checkpoints = checkpoint_state.all_model_checkpoint_paths[
		# 	-FLAGS.num_last_checkpoints:]
		file_list = os.listdir(FLAGS.prefix)
		checkpoints = [os.path.join(FLAGS.prefix, file) for file 
			in file_list if file.endswith("".index"")]
		checkpoints = [checkpoint[:-6] for checkpoint in checkpoints]
		checkpoints.sort(key=lambda checkpoint: int(checkpoint.split('-')[-1]))
		checkpoints = checkpoints[-FLAGS.num_last_checkpoints:]

	checkpoints = [c for c in checkpoints if checkpoint_exists(c)]
	if not checkpoints:
		if FLAGS.checkpoints:
			raise ValueError(
				""None of the provided checkpoints exist. %s"" % FLAGS.checkpoints)
		else:
			raise ValueError(""Could not find checkpoints at %s"" %
				os.path.dirname(FLAGS.prefix))

	# Read variables from all checkpoints and average them.
	logging.info(""Reading variables and averaging checkpoints:"")
	for c in checkpoints:
		logging.info(""%s "", c)

	var_list = tf.train.list_variables(checkpoints[0])
	var_values, var_dtypes = {}, {}
	for (name, shape) in var_list:
		if not name.startswith(""save_counter""):
			var_values[name] = np.zeros(shape)

	for checkpoint in checkpoints:
		reader = tf.train.load_checkpoint(checkpoint)
		for name in var_values:
			tensor = reader.get_tensor(name)
			dtype = reader.get_variable_to_dtype_map()[name]

			if dtype == tf.string:
				var_values[name] = tensor
			else:
				var_values[name] += tensor
			var_dtypes[name] = dtype
		logging.info(""Read from checkpoint %s"", checkpoint)

	for name in var_values:  # Average.
		if var_dtypes[name] != tf.string:
			var_values[name] /= len(checkpoints)

	name = var_list[-1][0]
	assert name.startswith(""save_counter"")
	shape = reader.get_variable_to_shape_map()[name]
	dtype = reader.get_variable_to_dtype_map()[name]
	var_values[name] = np.zeros(shape)
	var_dtypes[name] = dtype

	for name in var_values.keys():
		var_values[name] = tf.Variable(
			var_values[name], dtype=var_dtypes[name])
	save = tf.train.Checkpoint()
	save.mapped = var_values
	# save.listed = []
	# save.mapped = {}
	# for name in var_values.keys():
	# 	save.listed.append(var_values[name])
	# 	save.mapped[name] = var_values[name]
	save_path = save.save(FLAGS.output_path)

	logging.info(""Averaged checkpoints saved in %s"", FLAGS.output_path)


if __name__ == '__main__':
	app.run(main)
`"
tensorflow/tensorflow,2023-03-17 10:43:59,question, About Android calling TensorFlow Lite crash problem,"
### 1. Problem overview
  I recently reported an error when importing the Tensorflow Lite model using Android Studio, but I did not find the problem after searching. I hope you can help me. The following is the detailed information

### 2. System information

    -implementation 'org.tensorflow:tensorflow-lite:2.11.0'
    -implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
    -implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'
    -implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0'
    -Android 12.0 x86_64


### 3. Code

      public MyModelRunner(Context context) throws IOException {
              try {
                  interpreter = new Interpreter(loadModelFile(context));
      
              } catch (IOException e) {
                  e.printStackTrace();
              }
          }

    private MappedByteBuffer loadModelFile(Context context) throws IOException {


        AssetFileDescriptor fileDescriptor = context.getAssets().openFd(""new_model.tflite"");

        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());

        FileChannel fileChannel = inputStream.getChannel();

        long startOffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();

        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
    }

### 4. (optional)  info / logs
2023-03-17 18:02:45.343 22670-22670 libc                    com.example.myproject01              A  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xfffffff4 in tid 22670 (ple.myproject01), pid 22670 (ple.myproject01)
2023-03-17 18:02:45.399 22706-22706 DEBUG                   pid-22706                            A  pid: 22670, tid: 22670, name: ple.myproject01  >>> com.example.myproject01 <<<
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #00 pc 00282792  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #01 pc 020242b8  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #02 pc 02023ec2  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #03 pc 0202456b  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #04 pc 01e6f187  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.637 22706-22706 DEBUG                   pid-22706                            A        #05 pc 0027cb45  /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/lib/x86/libtensorflowlite_flex_jni.so
2023-03-17 18:02:45.639 22706-22706 DEBUG                   pid-22706                            A        #37 pc 002e6bdc  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.flex.FlexDelegate.<clinit>+4)
2023-03-17 18:02:45.640 22706-22706 DEBUG                   pid-22706                            A        #60 pc 002e47ae  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.maybeCreateFlexDelegate+6)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #63 pc 002e4be4  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.addDelegates+16)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #66 pc 002e4e68  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.init+104)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #69 pc 002e4baa  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.<init>+146)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #72 pc 002e44d0  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #75 pc 002e4304  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.Interpreter.<init>+4)
2023-03-17 18:02:45.641 22706-22706 DEBUG                   pid-22706                            A        #78 pc 002e42e6  [anon:dalvik-classes.dex extracted in memory from /data/app/~~8c21WzJfTIFEm7pmQFGjow==/com.example.myproject01-zu0b6G7gIfCmfZkzPmHgpg==/base.apk] (org.tensorflow.lite.Interpreter.<init>+2)
2023-03-17 18:02:45.642 22706-22706 DEBUG                   pid-22706                            A        #81 pc 00000f66  /data/data/com.example.myproject01/code_cache/.overlay/base.apk/classes3.dex (com.example.myproject01.MyModelRunner.<init>+18)
2023-03-17 18:02:45.642 22706-22706 DEBUG                   pid-22706                            A        #84 pc 00000fc8  /data/data/com.example.myproject01/code_cache/.overlay/base.apk/classes3.dex (com.example.myproject01.Recommend_Fragment$1.onClick+20)
---------------------------- PROCESS ENDED (22670) for package com.example.myproject01 ----------------------------
2023-03-17 18:02:45.957   518-597   InputDispatcher         system_process                       E  channel 'c1e13a6 com.example.myproject01/com.example.myproject01.MainActivity (server)' ~ Channel is unrecoverably broken and will be disposed!


"
tensorflow/tensorflow,2023-03-16 21:46:24,question,Saving Custom Model Not Working,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Colab Notebook

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm trying to save a custom model where the number of channels increases as we progress along the forward pass of the architecture. The forward pass of the model works as expected, and I can train it. However, when trying to save the model, I get an error concerning the number of channels passed to certain convolutional layers. 

Is there a way around this issue?
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1YZd8OQWjgYasVblDit-gRNvA-kFJdRkn?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-696ba1b852d4> in <module>
----> 1 wnet.save(""/content/wnet"")

7 frames
/tmp/__autograph_generated_filelz68b5h5.py in tf__call(self, x)
      8                 do_return = False
      9                 retval_ = ag__.UndefinedReturnValue()
---> 10                 x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
     11                 x = ag__.converted_call(ag__.ld(self).norm, (ag__.ld(x),), None, fscope)
     12                 x = ag__.converted_call(ag__.ld(self).activation, (ag__.ld(x),), None, fscope)

ValueError: Exception encountered when calling layer 'base_model' (type BaseModel).

in user code:

    File ""<ipython-input-1-355bdcd220c4>"", line 149, in call  *
        x = decoder_block([*previous_skips[str(self.global_depth - 1 - i)],
    File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
        raise e.with_traceback(filtered_tb) from None
    File ""/tmp/__autograph_generated_file30lnbd33.py"", line 12, in tf__call
        x = ag__.converted_call(ag__.ld(self).block, (ag__.ld(x),), None, fscope)
    File ""/tmp/__autograph_generated_filejlndlto_.py"", line 10, in tf__call
        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)
    File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
        x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)

    ValueError: Exception encountered when calling layer 'decoder_block_1' (type DecoderBlock).
    
    in user code:
    
        File ""<ipython-input-1-355bdcd220c4>"", line 96, in call  *
            x = self.block(x)
        File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
            raise e.with_traceback(filtered_tb) from None
        File ""/tmp/__autograph_generated_filejlndlto_.py"", line 10, in tf__call
            x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(x),), None, fscope)
        File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
            x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
    
        ValueError: Exception encountered when calling layer 'u_net_block_7' (type UNetBlock).
        
        in user code:
        
            File ""<ipython-input-2-76991f1be87b>"", line 14, in call  *
                x = self.conv1(x)
            File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
                raise e.with_traceback(filtered_tb) from None
            File ""/tmp/__autograph_generated_filelz68b5h5.py"", line 10, in tf__call
                x = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
        
            ValueError: Exception encountered when calling layer 'conv_layer_14' (type ConvLayer).
            
            in user code:
            
                File ""<ipython-input-1-355bdcd220c4>"", line 56, in call  *
                    x = self.conv(x)
                File ""/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
                    raise e.with_traceback(filtered_tb) from None
                File ""/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py"", line 277, in assert_input_compatibility
                    raise ValueError(
            
                ValueError: Input 0 of layer ""conv3d_14"" is incompatible with the layer: expected axis -1 of input shape to have value 24, but received input with shape (None, 16, 16, 16, 32)
            
            
            Call arguments received by layer 'conv_layer_14' (type ConvLayer):
              • x=tf.Tensor(shape=(None, 16, 16, 16, 32), dtype=float32)
        
        
        Call arguments received by layer 'u_net_block_7' (type UNetBlock):
          • x=tf.Tensor(shape=(None, 16, 16, 16, 32), dtype=float32)
    
    
    Call arguments received by layer 'decoder_block_1' (type DecoderBlock):
      • skip=['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)']
      • x=tf.Tensor(shape=(None, 8, 8, 8, 8), dtype=float32)


Call arguments received by layer 'base_model' (type BaseModel):
  • x=tf.Tensor(shape=(None, 8, 8, 8, 8), dtype=float32)
  • previous_skips={'0': ListWrapper(['tf.Tensor(shape=(None, 64, 64, 64, 8), dtype=float32)']), '1': ListWrapper(['tf.Tensor(shape=(None, 32, 32, 32, 8), dtype=float32)']), '2': ListWrapper(['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)', 'tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)'])}
  • previous_peaks={'0': ListWrapper([]), '1': ListWrapper(['tf.Tensor(shape=(None, 32, 32, 32, 8), dtype=float32)']), '2': ListWrapper(['tf.Tensor(shape=(None, 16, 16, 16, 8), dtype=float32)'])}
```
</details>"
tensorflow/tensorflow,2023-03-14 13:16:00,question,Using Teachable Machine to make a Audio model for Python,"I have been trying to make an AI model that will listen to user input. Depending on the user's input/command the program will carry out functions etc.

I am trying to use a model I have trained from the teachable machine, but the model is not in a format that can be used in Python. From what I have seen Python requires a .h5. However, TFLite.js is only available to export/download in. When I try to use this in my program I get hit with errors.

What steps can I take to import an Audio model for a Python system?"
tensorflow/tensorflow,2023-03-14 03:10:18,question,"Example android app show ""GPU is not supportted"".","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf.2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Here are some details that will aid in this troubleshooting session:

Using the tensorflow lite sample android app(https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).
But as the attached GPUNOTSUPPORT.png shown, currently on the 2290, when selecting GPU or NNAPI delegates the sample app shows ""GPU is not supported""

Look into the sample android app code.
1. As GPU.png shown, ""GPU is not supported"" occurred.
2. As the attached code1.png shown, the order call relationship.

The following code is very important:
public CompatibilityList() {
this.compatibilityListHandle = createCompatibilityList();
}
private static native long createCompatibilityList();
guss that createCompatibilityList() make problem.

Look into tensorflow code check createCompatibilityList:

1. In tensorflow/lite/delegates/gpu/java/src/main/native/compatibility_list_jni.cc +69
As the tensorflow.png shown,
75 // Errors in ReadInfo should almost always be failures to construct the OpenGL
76 // environment. Treating that as ""GPU unsupported"" is reasonable, and we can
77 // swallow the error

82 JNIEXPORT jboolean JNICALL
83 Java_org_tensorflow_lite_gpu_CompatibilityList_nativeIsDelegateSupportedOnThisDevice(
84 JNIEnv* env, jclass clazz, jlong compatibility_list_handle) {
85 if (!tflite::jni::CheckJniInitializedOrThrow(env)) return JNI_FALSE;
86
87 CompatibilityListHelper* compatibility_list =
88 reinterpret_cast<CompatibilityListHelper*>(compatibility_list_handle);
89 return compatibility_list->IsDelegateSupportedOnThisDevice() ? JNI_TRUE
90 : JNI_FALSE;
91 }

Thanks you very much.
```


### Standalone code to reproduce the issue

```shell
Using the tensorflow lite sample android app(https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android).
But as the attached GPUNOTSUPPORT.png shown, currently on the 2290, when selecting GPU or NNAPI delegates the sample app shows ""GPU is not supported""
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-05 08:33:57,question,Getting error when calculating entropy for each images in the batch in the input tensor in a custom layer in tensorflow/Keras.,"I am working on a problem in which I have to create a custom layer in keras, which takes, output of a conv layer of a pre-trained model as an input. This custom layer work is to select K best feature maps based on shannon entropy for each images in that input tensor and then outputs the final tensor with k feature maps or each images. So that this output tensor is passed to other conv layer in the model.

Let input tensor from a conv layer has shape = (None, 224,224, 128) and I want to take 64 best feature maps out of 128 based on shannon entropy . So the output tensor shape should be = (None, 224,224, 64).

----------------------------------
System Informations:
python version = '3.9.12'
tensorflow version = `'2.10.0'`
Platform = Windows 11
Running on CPU
---------------------------------

Below is the code snippet :

`               

              class select_k_fmap(tf.keras.layers.Layer):
                    def __init__(self):
                        super(select_k_fmap, self).__init__()

                    def build(self, input_shape):
                        pass

                    def call(self, inputs):
                        """"""
                        tensor shape = (batch_size, img_height, img_width, no. of filters)
                        """"""    
                        shape = tf.shape(inputs)
                        batch_size = shape[0]
                        print('batch size **** ', batch_size)
                        num_filters = shape[-1]
                        img_height = shape[1]
                        img_width = shape[2]

                        k = 4 # no. of best feature maps to select

                        def k_best_fmap(image):
                            """"""
                            returns k best feature maps of an image having n feature maps
                            """"""    
                            def shannon_entropy(feature_map):
                                """"""
                                returns entropy of a image in a input batch
                                """"""
                                value_ranges = [0.0, 1.0] 
                                nbins = 256 
                                histogram_bin_indexes = tf.histogram_fixed_width_bins(image, value_ranges, nbins)
                                _, _, count = tf.unique_with_counts(histogram_bin_indexes) 
                                prob = count/tf.reduce_sum(count)
                                prob = tf.cast(prob, tf.float32)
                                entropy = (-tf.reduce_sum(prob * tf.math.log(prob)))/(tf.math.log(2.0)) 
                                return entropy

                            final_image = tf.zeros_like(image) #shape = (img_height, img_width, no. of filters)
                            entropy = []
                            num_featuremaps = tf.shape(image)[-1]
                            for j in range(int(num_featuremaps)):
                                image_ith_filter = image[:,:,j]
                                image_ith_filter_entropy = shannon_entropy(image_ith_filter)
                                entropy.append(tf.get_static_value(image_ith_filter_entropy).item()) 


                            entropy_array = tf.argsort(entropy, direction='DESCENDING')
                            k_best_entropy_sort_index = tf.get_static_value(entropy_sort_index[:k]).tolist()

                            for index, element in enumerate(k_best_entropy_sort_index):
                                final_image[:,:,index] = image[:,:,element]


                        output_tensor = tf.map_fn(fn=k_best_fmap, elems=inputs)

                        return output_tensor`

But when I ran this code I got error :

![Screenshot (890)](https://user-images.githubusercontent.com/127013167/222950090-be802ac6-26c4-4a33-8f5d-106d3f800fb4.png)


I have tried many solutions available on the internet. But nothing worked.
I think this error is due to the shape of input tensor having None as first value in its shape (None, 224,224, 128). But I am unable to resolve this error. I want to wok on dynamic batch tensor as input .

It would be a great help for me if anyone of you could assist me. Thanks in advance."
tensorflow/tensorflow,2023-02-28 23:22:38,question,Numpy to tensor,"I apologize for posting a very simple question here as I could not solve the issue myself or find an answer elsewhere.

I was trying to make a prediction using a model and a 3D array with 3 channels as inputs. When this 3D array was saved as an image file, I was able to make a proper prediction using my model:
`image = tf.io.read_file('image.jpg')`
`image = tf.image.decode_image(image, channels=3)`
`image = tf.image.resize(images=image, size=[imsize, imsize])`
`image = tf.keras.applications.resnet50.preprocess_input(image)`
`predictions = model.predict(np.expand_dims(image, axis=0))`

However, when this 3D array was readily available in memory as a numpy array in uint8 format, and I tried to directly use this array to make a prediction, I get a different prediction result. I used `cv2.imread` to simulate the array in memory:
`array = cv2.imread('image.jpg')`
`image = tf.convert_to_tensor(array/255, dtype=tf.float32) `
`image = tf.image.resize(images=image, size=[imsize, imsize])`
`image = tf.keras.applications.resnet50.preprocess_input(image)`
`predictions = model.predict(np.expand_dims(image, axis=0))`

Any help is greatly appreciated! "
tensorflow/tensorflow,2023-02-26 16:44:57,question,Recognize Flowers with TensorFlow Lite on Android Codelabs - step 4 error,Just following the steps on a fresh installation of Android studio on ubuntu get the following error when trying to run: Could not find compile target android-29 for modules :start
tensorflow/tensorflow,2023-02-25 07:38:38,question,unable to use my classifiers.tflite and label.txt. Please help,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
tensorflow/tensorflow,2023-02-24 06:56:58,question,Test issue no need to take any action.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
tensorflow/tensorflow,2023-02-22 05:30:38,question,ideal development mode discussion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tag: v2.11.0

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura13

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.3.0

### GCC/Compiler version

clang 14.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
Currently, we just have Mac as our daily work laptop. But we want to deep dive into tensorflow source code and maybe doing some customized changes, then compile/build and deploy into test/production environment for model training.  Meanwhile, as we knew, tensorflow is a typical cross-platform end2end tool, how can we make sure the whole develop/delivery pipeline runs smoothly?   Do we have any best practice?  Thank you.
```


### Standalone code to reproduce the issue

```shell
None regarding this topic.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-02-21 02:58:36,question,can't install throught `pip install -q git+https://github.com/tensorflow/docs`,"seen as below 
![error](https://user-images.githubusercontent.com/76671016/220236110-8b97769f-f846-447a-ae6e-bddf90a6bb05.JPG)
i got this command from here:<u>https://keras.io/examples/vision/video_classification/</u>


tensorflow version : 2.11.0 
"
tensorflow/tensorflow,2023-02-20 02:37:05,question,Cropping1D cann't support negatitve integer parameter,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.keras.layers.Cropping1D((-20,0))(out) report error, The 'cropping' argument must be a tuple of 2 interges. The really reson is keras.utils.conv_utils.normalize_tuple() update. 
And tf <= 2.6, the error is not exist. Is there a way to fix or keep away from this error?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping1D(cropping=(-20,0))(x)
```
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-02-12 02:56:29,question,Tf 2.11 Optimizers. Does anyone have any custom optimizer for the new version?,"I know that we can use tf.keras.optimizers.legacy.Optimizer for making the older custom optimizers to work,but I'm wonder how I can update my code.This the original code that I want to make it function for tf 2.11

`class Gravity(tf.keras.optimizers.Optimizer):
    def __init__(self,
                 learning_rate=0.1,
                 alpha=0.01,
                 beta=0.9,
                 name=""Gravity"",
                 **kwargs):
        super(Gravity, self).__init__(name, **kwargs)
        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
        self._set_hyper('decay', self._initial_decay)
        self._set_hyper('alpha', alpha)
        self._set_hyper('beta', beta)
        self.epsilon = 1e-7

    def _create_slots(self, var_list):
        alpha = self._get_hyper(""alpha"")
        stddev = alpha / self.learning_rate
        initializer = tf.keras.initializers.RandomNormal(mean=0.0,
                                                         stddev=stddev,
                                                         seed=None)
        for var in var_list:
            self.add_slot(var, ""velocity"", initializer=initializer)

    @tf.function
    def _resource_apply_dense(self, grad, var):
        # Get Data
        var_dtype = var.dtype.base_dtype
        lr_t = self._decayed_lr(var_dtype)
        beta = self._get_hyper(""beta"", var_dtype)
        t = tf.cast(self.iterations, float)
        beta_hat = (beta * t + 1) / (t + 2)
        velocity = self.get_slot(var, ""velocity"")

        # Calculations
        max_step_grad = 1 / tf.math.reduce_max(tf.math.abs(grad))
        gradient_term = grad / (1 + (grad / max_step_grad)**2)

        # update variables
        updated_velocity = velocity.assign(beta_hat * velocity +
                                           (1 - beta_hat) * gradient_term)
        updated_var = var.assign(var - lr_t * updated_velocity)

        # updates = [updated_var, updated_velocity]
        # return tf.group(*updates)
    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError

    def get_config(self):
        config = super(Gravity, self).get_config()
        config.update({
            'learning_rate':
            self._serialize_hyperparameter('learning_rate'),
            'decay':
            self._serialize_hyperparameter('decay'),
            'alpha':
            self._serialize_hyperparameter('alpha'),
            'beta':
            self._serialize_hyperparameter('beta'),
            'epsilon':
            self.epsilon,
        })
        return config`
"
tensorflow/tensorflow,2023-02-09 08:41:26,question,Control Flow Ops with external delegate NPU,"### 1. System information

- OS Platform and Distribution : Ubuntu20.04
- TensorFlow installation : conda
- TensorFlow library : 2.8.1

I am converting a pytorch model to Int8 quantize tensorflowlite model, to run it on a NXP board, on which I use delegates to make it run on the NPU. My original model contains an Unsqueeze block, which is somehow transformed into a While Ops (I can see that with netron, between the onnx and tflite model).

Running this model on the board with the delegate (**libvx_delegate.so**) gives me the error:
`ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`

From this post: https://stackoverflow.com/questions/66682315/finding-dynamic-tensors-in-a-tflite-model I understand that the Control Flow Ops are treated as dynamic. My question is how can I run such a model with the delegate?
"
tensorflow/tensorflow,2023-02-03 09:45:49,question,Fails to convert SplitV with quantization,"### 1. System information

- Ubuntu20.04
- TensorFlow installation : conda
- TensorFlow library 2.8.1

### 2. Code

When converting a model from Tensorflow to TFlite, I run into errors (regarding SplitV, check error below) when setting the pipeline with Int8 quantization, but not when using regular conversion (without quantization).

I basically follow those instructions:

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_quant_model = converter.convert()
```

I get the following error:

`There are unresolved custom ops: [SplitV]Encountered unresolved custom op: SplitV.`

It works fine if I add the SELECT_TF_OPS, but I want to keep the interpreter as small and fast as possible.
So my assumption is that the SplitV operator in TFlite is not available with the Int8 quantization?
How can I know beforehand which operator is available with the Int8 quantization?
"
tensorflow/tensorflow,2023-02-02 00:30:03,question,TF to TFLite conversion of model gives error op is neither a custom op nor a flex op,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
import tensorflow as tf
from tensorflow import keras
import pickle
import numpy as np
import sys

print(tf.__version__)
root_dir = ""~/""
saved_model_dir = root_dir+""updatetf/softphy/src/qam_modulator""

def representative_dataset():
  for _ in range(100):
      data = np.random.randint(0, 2, (16200,))
      yield [data.astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]
converter.inference_input_type = tf.int8 # or tf.uint8
converter.inference_output_type = tf.int8 # or tf.uint8

tflite_quant_model = converter.convert()

fileName = 'qammodmodel.tflite'
with open(fileName, 'wb') as f:
     f.write(tflite_quant_model)

```

### 3. Failure after conversion

The conversion fails as below, saved model dir is attached a zip file. I'm not sure what needs to be changed to fix this. Eventually, I'd like to convert the TFLite model to run on TPU. Thanks!


### 5. (optional) Any other info / logs
```
2023-02-02 00:04:34.283275: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 00:04:35.515897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-02-02 00:04:35.515975: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-02-02 00:04:39.225887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: li
[qam_modulator.zip](https://github.com/tensorflow/tensorflow/files/10563501/qam_modulator.zip)
bnvinfer.so.7: cannot open shared object file: No such file or directory
2023-02-02 00:04:39.226056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-02-02 00:04:39.226075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2.11.0
2023-02-02 00:04:43.533858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-02-02 00:04:43.539556: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-02-02 00:04:43.539649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntugsosnow): /proc/driver/nvidia/version does not exist
2023-02-02 00:04:43.541318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-02 00:04:44.421918: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2023-02-02 00:04:44.421999: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2023-02-02 00:04:44.427118: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.430879: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2023-02-02 00:04:44.430967: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.447508: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-02 00:04:44.448994: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2023-02-02 00:04:44.494916: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /home/gsosnow/doc/updatetf/softphy/src/qam_modulator
2023-02-02 00:04:44.535778: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 111428 microseconds.
2023-02-02 00:04:44.619260: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""HashTableV2:"", ""qam_table""]): error: 'tf.HashTableV2' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.FloorDiv' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.FloorMod' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.AsString' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.ReduceJoin' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.AsString' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.ReduceJoin' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): error: 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
loc(fused[""HashTableV2:"", ""qam_table""]): error: 'tf.HashTableV2' op is neither a custom op nor a flex op
loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): error: 'tf.LookupTableImportV2' op is neither a custom op nor a flex op
2023-02-02 00:04:44.806180: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2035] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:
Resource ops: HashTableV2, LookupTableFindV2, LookupTableImportV2
Details:
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: AsString, FloorDiv, FloorMod, HashTableV2, LookupTableFindV2, LookupTableImportV2, ReduceJoin
Details:
        tf.AsString(tensor<2025x4xi8>) -> (tensor<2025x4x!tf_type.string>) : {device = """", fill = """", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}
        tf.FloorDiv(tensor<2025x1x1xui8>, tensor<8xui8>) -> (tensor<2025x1x8xui8>) : {device = """"}
        tf.FloorMod(tensor<2025x1x8xi8>, tensor<i8>) -> (tensor<2025x1x8xi8>) : {device = """"}
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
        tf.ReduceJoin(tensor<2025x4x!tf_type.string>, tensor<i32>) -> (tensor<2025x!tf_type.string>) : {device = """", keep_dims = false, separator = """"}

Traceback (most recent call last):
  File ""/home/gsosnow/doc/qammodt2tf.py"", line 38, in <module>
    tflite_quant_model = converter.convert()
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 933, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 911, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1216, in convert
    return self._convert_from_saved_model(graph_def)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py"", line 1099, in _convert_from_saved_model
    result = _convert_saved_model(**converter_kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 808, in convert_saved_model
    data = convert(
  File ""/home/gsosnow/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py"", line 310, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(fused[""HashTableV2:"", ""qam_table""]): 'tf.HashTableV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""HashTableV2:"", ""qam_table""]): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.FloorDiv' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""FloorDiv:"", ""model_2/qam_mod_11/floordiv@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.FloorMod' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""FloorMod:"", ""model_2/qam_mod_11/FloorMod@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.AsString' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.ReduceJoin' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.AsString' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""AsString:"", ""model_2/qam_mod_11/AsString_1@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.ReduceJoin' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""ReduceJoin:"", ""model_2/qam_mod_11/ReduceJoin_1/ReduceJoin@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""LookupTableFindV2:"", ""model_2/qam_mod_11/None_Lookup_1/LookupTableFindV2@__inference__wrapped_model_81624""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_81882""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(fused[""HashTableV2:"", ""qam_table""]): 'tf.HashTableV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""HashTableV2:"", ""qam_table""]): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.LookupTableImportV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(fused[""LookupTableImportV2:"", ""key_value_init61252/LookupTableImportV2@__inference_<lambda>_81959""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: AsString, FloorDiv, FloorMod, HashTableV2, LookupTableFindV2, LookupTableImportV2, ReduceJoin
Details:
        tf.AsString(tensor<2025x4xi8>) -> (tensor<2025x4x!tf_type.string>) : {device = """", fill = """", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}
        tf.FloorDiv(tensor<2025x1x1xui8>, tensor<8xui8>) -> (tensor<2025x1x8xui8>) : {device = """"}
        tf.FloorMod(tensor<2025x1x8xi8>, tensor<i8>) -> (tensor<2025x1x8xi8>) : {device = """"}
        tf.HashTableV2() -> (tensor<!tf_type.resource>) : {container = """", device = """", key_dtype = !tf_type.string, shared_name = ""61254"", use_node_name_sharing = false, value_dtype = i32}
        tf.LookupTableFindV2(tensor<!tf_type.resource>, tensor<2025x!tf_type.string>, tensor<i32>) -> (tensor<*xi32>) : {device = """"}
        tf.LookupTableImportV2(tensor<!tf_type.resource>, tensor<16x!tf_type.string>, tensor<16xi32>) -> () : {device = """"}
        tf.ReduceJoin(tensor<2025x4x!tf_type.string>, tensor<i32>) -> (tensor<2025x!tf_type.string>) : {device = """", keep_dims = false, separator = """"}

```
"
tensorflow/tensorflow,2023-01-30 14:59:14,question,CVE-2022-41883 for 2.9.3,"Hello, 

Based on [NVD](https://nvd.nist.gov/vuln/detail/CVE-2022-41883), CVE-2022-41883 will be cherry-picked for 2.9.3. However, I don't see it in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.9.3). Will this still be cherry-picked?

Thanks. "
tensorflow/tensorflow,2023-01-27 12:48:19,question,Extending tf.Tensor class,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS Ventura 13.0

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I want to extend the tf.Tensor class, but neither of the following options work:

1. option: extend tf.Tensor:

```shell
class MyTFTensor(tf.Tensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value

y = MyTFTensor._from_native(value=tf.zeros((3, 224, 224))
```
Fails with:
```
/var/folders/kb/yxxdttyj4qzcm447np5p22kw0000gp/T/ipykernel_94703/515175960.py in _from_native(cls, value)
      4     @classmethod
      5     def _from_native(cls, value: tf.Tensor):
----> 6         value.__class__ = cls
      7         return value

TypeError: __class__ assignment: 'MyTFTensor' object layout differs from 'tensorflow.python.framework.ops.EagerTensor'
```

3. Option: extend EagerTensor
```shell
from tensorflow.python.framework.ops import EagerTensor
class MyTFTensor(EagerTensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value
```
Fails with:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/var/folders/kb/yxxdttyj4qzcm447np5p22kw0000gp/T/ipykernel_94703/3632871733.py in <cell line: 2>()
      1 from tensorflow.python.framework.ops import EagerTensor
----> 2 class MyTFTensor(EagerTensor):
      3 
      4     @classmethod
      5     def _from_native(cls, value: tf.Tensor):

TypeError: type 'tensorflow.python.framework.ops.EagerTensor' is not an acceptable base type
```

Our goal is to extend it though, we **don't** want to store the tf.tensor instance as an attribute of MyTFTensor, but instead extend the `tf.Tensor` class!


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# 1st option
class MyTFTensor(tf.Tensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value

y = MyTFTensor._from_native(value=tf.zeros((3, 224, 224))

# 2nd option
from tensorflow.python.framework.ops import EagerTensor
class MyTFTensor(EagerTensor):
    
    @classmethod
    def _from_native(cls, value: tf.Tensor):
        value.__class__ = cls
        return value
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-01-25 22:38:38,question,question about using tensorArray to do backpropagation,"I am confused about the tensorflow documentation here:

https://www.tensorflow.org/api_docs/python/tf/TensorArray:

Note that although the array can be read multiple times and positions can be overwritten, behavior may be undefined when storing multiple references to the same array and clear_after_read is False. In particular, avoid using methods like concat() to convert an intermediate TensorArray to a Tensor, then further modifying the TensorArray, particularly if you need to backprop through it later.

This is from tensorflow 2.11 which isn't included in tensorflow 2.7. So I want to know if we really need an intermediate Tensorarray and then concatenate it to a tensor and then calculate loss and do backpropogation, which way is suggested?"
tensorflow/tensorflow,2023-01-24 11:54:25,question,How to infer  pose classification on single image?,"The first layer of the model needs shape of (None, 51). How to send the image as a feature tensor of 51 feature points which is also nothing but the feature taken as the pose points estimated by  MOVNET?"
tensorflow/tensorflow,2023-01-21 17:47:34,question,Error when runnning tensorflow.python.ops.sparse_ops.sparse_to_dense,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Zero or negative argument
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import sparse_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = 2
      arg_0_0 = [arg_0_0_0,]
      arg_0_1_0 = 1
      arg_0_1 = [arg_0_1_0,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = 0
      arg_1 = [arg_1_0,]
      arg_2_0 = -1.0
      arg_2_1 = 1.0
      arg_2 = [arg_2_0,arg_2_1,]
      arg_3 = 0.0
      validate_indices = False
      out = sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,arg_3,validate_indices=validate_indices,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,]
      arg_0_1 = [arg_0_1_0,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1 = [arg_1_0,]
      arg_2 = [arg_2_0,arg_2_1,]
      sparse_ops.sparse_to_dense(arg_0,arg_1,arg_2,arg_3,validate_indices=validate_indices,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-01-21 12:45:19.079634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 12:45:19.390830: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 12:45:20.257044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:20.257249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:20.257257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 12:45:21.248344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-21 12:45:21.280051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/
2023-01-21 12:45:21.280069: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-01-21 12:45:21.281547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /home/nimashiri/anaconda3/envs/tf-2.10/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
Error:{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense]
Error:{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense]

```
```
</details>"
tensorflow/tensorflow,2023-01-18 18:29:38,question,running distributed tensorflow using MPI,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

Linux 

### Mobile device

_No response_

### Python version

3.8.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3.1

### GPU model and memory

8 * rtx 2080 ti / 11gb memory each 

### Current Behaviour?

```shell
Instructions for updating:
use distribute.MultiWorkerMirroredStrategy instead
2023-01-18 13:18:35.789808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-01-18 13:18:35.790848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9687 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5
2023-01-18 13:18:35.791743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9687 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5
2023-01-18 13:18:35.792678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9687 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5
2023-01-18 13:18:35.804893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:0 with 9687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-01-18 13:18:35.805620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:1 with 9687 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5
2023-01-18 13:18:35.806333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:2 with 9687 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5
2023-01-18 13:18:35.807029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:worker/replica:0/task:0/device:GPU:3 with 9687 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5
2023-01-18 13:18:35.810512: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> g01:37672}
2023-01-18 13:18:35.810736: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://g01:37672
/usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  warnings.warn(
WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
2023-01-18 13:18:42.547198: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
op: ""TensorSliceDataset""
input: ""Placeholder/_0""
input: ""Placeholder/_1""
attr {
  key: ""Toutput_types""
  value {
    list {
      type: DT_DOUBLE
      type: DT_DOUBLE
    }
  }
}
attr {
  key: ""output_shapes""
  value {
    list {
      shape {
        dim {
          size: 15
        }
      }
      shape {
        dim {
          size: 13
        }
      }
    }
  }
}
2023-01-18 13:18:42.740015: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
[g01:44037:0:44313] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))
==== backtrace (tid:  44313) ====
 0 0x000000000002137e ucs_debug_print_backtrace()  /umbc/ebuild-soft/cascade-lake/build/UCX/1.10.0/GCCcore-10.3.0/ucx-1.10.0/src/ucs/debug/debug.c:656
 1 0x000000000382045b tensorflow::NcclCommunicator::Enqueue()  collective_communicator.cc:0
 2 0x0000000005c9f88a tensorflow::NcclReducer::Run()  ???:0
 3 0x00000000009086dc tensorflow::BaseCollectiveExecutor::ExecuteAsync(tensorflow::OpKernelContext*, tensorflow::CollectiveParams const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void (tensorflow::Status const&)>)::{lambda()#3}::operator()()  base_collective_executor.cc:0
 4 0x0000000000b99403 tensorflow::UnboundedWorkQueue::PooledThreadFunc()  ???:0
 5 0x0000000000b9f6b1 tensorflow::(anonymous namespace)::PThread::ThreadFn()  env.cc:0
 6 0x0000000000007ea5 start_thread()  pthread_create.c:0
 7 0x00000000000feb0d __clone()  ???:0
=================================
[g01:44037] *** Process received signal ***
[g01:44037] Signal: Segmentation fault (11)
[g01:44037] Signal code:  (-6)
[g01:44037] Failing at address: 0x2ecf70000ac05
[g01:44037] [ 0] /lib64/libpthread.so.0(+0xf630)[0x2aaaab7e6630]
[g01:44037] [ 1] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x382045b)[0x2aaab68fc45b]
[g01:44037] [ 2] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow11NcclReducer3RunESt8functionIFvRKNS_6StatusEEE+0x1ca)[0x2aaab8d7b88a]
[g01:44037] [ 3] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(+0x9086dc)[0x2aaadc7556dc]
[g01:44037] [ 4] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow18UnboundedWorkQueue16PooledThreadFuncEv+0x1b3)[0x2aaadc9e6403]
[g01:44037] [ 5] /usr/ebuild/software/TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1/lib/python3.9/site-packages/tensorflow/python/../libtensorflow_framework.so.2(+0xb9f6b1)[0x2aaadc9ec6b1]
[g01:44037] [ 6] /lib64/libpthread.so.0(+0x7ea5)[0x2aaaab7deea5]
[g01:44037] [ 7] /lib64/libc.so.6(clone+0x6d)[0x2aaaac468b0d]
[g01:44037] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 44037 on node g01 exited on signal 11 (Segmentation fault).
```


### Standalone code to reproduce the issue

```shell
from getOneHot import getOneHot
from mpi4py import MPI
comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# Load in the parameter files
from json import load as loadf
with open(""params.json"", 'r') as inFile:
    params = loadf(inFile)

# Get data files and prep them for the generator
from tensorflow import distribute as D
callbacks = []
devices = getDevices()
print(devices)
set_tf_config_mpi()
strat = D.experimental.MultiWorkerMirroredStrategy(
        communication=D.experimental.CollectiveCommunication.NCCL)
# Create network
from sys import argv
resume_training = False
print(argv)
if ""resume_latest"" in argv:
    resume_training = True

with strat.scope():
    # Scheduler
    if isinstance(params[""learning_rate""], str):
        # Get the string for the importable function
        lr = params[""learning_rate""]
        from tensorflow.keras.callbacks import LearningRateScheduler
        # Use a dummy learning rate
        params[""learning_rate""] = 0.1
        # model = create_model(**params)
        # Get the importable function
        lr = lr.split(""."")
        baseImport = __import__(lr[0], globals(), locals(), [lr[1]], 0)
        lr = getattr(baseImport, lr[1])
        # Make a schedule
        lr = LearningRateScheduler(lr)
        callbacks.append(lr)
    # Resume Model?
    model_name = None
    if resume_training:
        initial_epoch, model_name = getInitialEpochsAndModelName(rank)
    if model_name is None:
        initial_epoch=0
        model = create_model(**params)
        resume_training = False
    else:
        from tensorflow.keras.models import load_model
        model = load_model(model_name)
# Load data from disk
import numpy
if ""root"" in params.keys():
    root = params['root']
else:
    root = ""./""
if ""filename"" in params.keys():
    filename = params[""filename""]
else:
    filename = ""dataset_timeseries.csv""

restricted = [
        'euc1', 'e1', 'x1', 'y1', 'z1',
        'euc2', 'e2', 'x2', 'y2', 'z2',
        'euc3', 'e3', 'x3', 'y3', 'z3',
        ]
x, y = getOneHot(""{}/{}"".format(root, filename), restricted=restricted, **params)
# val_x, val_y = getOneHot(""{}/{}"".format(root, val_filename), restricted=restricted)
val_x, val_y = None, None
params[""gbatch_size""] = params['batch_size'] * len(devices)
print(""x.shape ="", x.shape)
print(""y.shape ="", y.shape)
print(""epochs  ="", params['epochs'], type(params['epochs']))
print(""batch   ="", params['batch_size'], type(params['batch_size']))
print(""gbatch  ="", params['gbatch_size'], type(params['gbatch_size']))
# Load data into a distributed dataset
# Dataset object does nothing in place:
# https://stackoverflow.com/questions/55645953/shape-of-tensorflow-dataset-data-in-keras-tensorflow-2-0-is-wrong-after-conver
from tensorflow.data import Dataset
data = Dataset.from_tensor_slices((x, y))

# Create validation set
v = params['validation']
if val_x is not None:
    vrecord = val_x.shape[0]
    val  = Dataset.from_tensor_slices((val_x, val_y))
    validation = val # data.take(vrecord)
else:
    vrecord = int(x.shape[0]*v)
    validation = data.take(vrecord)
validation = validation.batch(params['gbatch_size'])
validation = validation.repeat(params['epochs'])
# Validation -- need to do kfold one day
# This set should NOT be distributed
vsteps = vrecord // params['gbatch_size']
if vrecord % params['gbatch_size'] != 0:
    vsteps += 1
# Shuffle the data during preprocessing or suffer...
# Parallel randomness == nightmare
# data = data.shuffle(x.shape[0])
# Ordering these two things is very important! 
# Consider 3 elements, batch size 2 repeat 2
# [1 2 3] -> [[1 2] [3]] -> [[1 2] [3] [1 2] [3]] (correct) batch -> repeat
# [1 2 3] -> [1 2 3 1 2 3] -> [[1 2] [3 1] [2 3]] (incorrect) repeat -> batch
# data = data.skip(vrecord)
data    = data.batch(params['gbatch_size'])
data    = data.repeat(params['epochs'])
records = x.shape[0] # - vrecord
steps   = records // params['gbatch_size']
if records % params['gbatch_size']:
    steps += 1
print(""steps   ="", steps)
# Note that if we are resuming that the number of _remaining_ epochs has
# changed!
# The number of epochs * steps is the numbers of samples to drop
print(""initial   cardinality = "", data.cardinality())
print(""initial v cardinality = "", data.cardinality())
data       = data.skip(initial_epoch*steps)
validation = validation.skip(initial_epoch*vsteps)
print(""final     cardinality = "", data.cardinality())
print(""final v   cardinality = "", data.cardinality())
# data = strat.experimental_distribute_dataset(data)
# Split into validation and training
callbacks  = createCallbacks(params, callbacks, rank, resume_training)
print(callbacks)

history = model.fit(data, epochs=params['epochs'],
        batch_size=params['gbatch_size'],
        steps_per_epoch=steps,
        verbose=0, 
        initial_epoch=initial_epoch,
        validation_data=validation,
        validation_steps=vsteps,
        callbacks=callbacks)
if rank == 0:
    model.save(""model-final"")
else:
    model.save(""checkpoints/model-tmp"")
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-01-18 03:30:54,question,Model weight not saved when exporting SavedModel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf2.8

### Custom Code

Yes

### OS Platform and Distribution

WSL2+tensorflow/tensorflow:2.8.0-gpu-jupyter image

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I need to change the signature in SavedModel by loading and re-export it with different configuration. The new exported model lost track of the model weight. Here's what I've tried
- Load the SavedModel (>300MB, with two signatures, exported in TF1.15) with TF2.8
- Select the signature I need
- Export Model with new signature config
- Load Model And Test

The re-exported model can be successfully loaded. But it won't be able to make any prediction due to initialized variable. I checked the exported model and the size is approximately 300kb, which means that most of the model weights were lost.
```
I looked into [this answer](https://stackoverflow.com/questions/59504276/resave-tf1-x-saved-model-pb-into-new-tf2-0-saved-model-pb) that solves the problem on his case. To me the only difference is that he converted variables in the original variable to constants. But for this approach the original `.ckpt` file is a prerequisite, which is not my case

### Standalone code to reproduce the issue

```shell
Script for reproducing the issue

# https://stackoverflow.com/questions/57048064/saved-model-prune-in-tf2-0
# https://stackoverflow.com/questions/59504276/resave-tf1-x-saved-model-pb-into-new-tf2-0-saved-model-pb
import tensorflow as tf
import numpy as np
import os

os.environ[""CUDA_VISIBLE_DEVICES""] = """"
export_path_origin = ""baidu-ske-2019-len128""  # A Model with multiple signatures
assert tf.__version__[0] == '2'
print(""test"")

imported = tf.saved_model.load(export_dir=export_path_origin)
print(imported.signatures)
f_serving_default = imported.signatures[""serving_default""]
f_predict = imported.signatures[""predict""]


inputs = {
    ""input_ids"": tf.constant([[0] * 128]),
    ""segment_ids"": tf.constant([[0] * 128]),
    ""input_mask"":  tf.constant([[0] * 128])
}
serving_default_output = f_serving_default(**inputs)
# print(serving_default_output)
predict_output = f_predict(**inputs)
# print(predict_output)

signature_to_keep = f_predict

fetches = {k: v.name for k, v in signature_to_keep.structured_outputs.items()}
feeds = [input_.name for input_ in signature_to_keep.inputs]
new_fn = imported.prune(feeds, fetches=fetches)
new_fn.graph.finalize()
original_output = signature_to_keep(**inputs)
new_outputs = new_fn(**inputs)
print(new_outputs)

# Assert equal
for k in new_outputs:
    t_origin, t_new = original_output[k], new_outputs[k]
    assert t_origin.dtype == t_new.dtype
    if t_origin.dtype.is_floating:
        np.testing.assert_allclose(t_origin.numpy(), t_new.numpy())
    else:
        assert np.all(t_origin.numpy() == t_new.numpy())



class Exportable(tf.Module):
    def __init__(self, fn):
        super(Exportable, self).__init__()
        self.tf1_model = fn

    tf.function(input_signature=[tf.TensorSpec([None, 128], tf.int32), tf.TensorSpec([None, 128], tf.int32), tf.TensorSpec([None, 128], tf.int32)])
    def forward(self, input_ids, segment_ids, input_mask):
        with tf.init_scope():
            out = self.tf1_model(input_ids=input_ids,
                                segment_ids=segment_ids, input_mask=input_mask)
        return out


new_saved_model = Exportable(new_fn)

print(new_saved_model.forward(**inputs))    # Eager Execution Works
new_model_path = './new_saved_model'

tf.saved_model.save(new_saved_model, new_model_path)
imported = tf.saved_model.load(export_dir=new_model_path)
imported.signatures[""serving_default""](**inputs) # Error: Uninitialized variables
```
Here's the metadata of the original model from by `tensorflow/serving`
```
{
    ""model_spec"":{
     ""name"": ""default"",
     ""signature_name"": """",
     ""version"": ""1""
    }
    ,
    ""metadata"": {""signature_def"": {
     ""signature_def"": {
      ""predict"": {
       ""inputs"": {
        ""segment_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""segment_ids:0""
        },
        ""input_mask"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_mask:0""
        },
        ""input_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_ids:0""
        }
       },
       ""outputs"": {
        ""token_label_predictions"": {
         ""dtype"": ""DT_INT64"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""token_label_loss/ArgMax:0""
        },
        ""predicate_head_probabilities"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""49"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""predicate_head_select_loss/Sigmoid:0""
        }
       },
       ""method_name"": ""tensorflow/serving/predict""
      },
      ""serving_default"": {
       ""inputs"": {
        ""segment_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""segment_ids:0""
        },
        ""input_mask"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_mask:0""
        },
        ""input_ids"": {
         ""dtype"": ""DT_INT32"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""input_ids:0""
        }
       },
       ""outputs"": {
        ""predicate_head_probabilities"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""49"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""predicate_head_select_loss/Sigmoid:0""
        },
        ""token_label_logits"": {
         ""dtype"": ""DT_FLOAT"",
         ""tensor_shape"": {
          ""dim"": [
           {
            ""size"": ""-1"",
            ""name"": """"
           },
           {
            ""size"": ""128"",
            ""name"": """"
           },
           {
            ""size"": ""61"",
            ""name"": """"
           }
          ],
          ""unknown_rank"": false
         },
         ""name"": ""token_label_loss/Reshape_1:0""
        }
       },
       ""method_name"": ""tensorflow/serving/predict""
      }
     }
    }
    }
    }
```
```


### Relevant log output

```shell
test
2023-01-18 11:28:10.218951: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-01-18 11:28:10.219019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b0c1756a148b): /proc/driver/nvidia/version does not exist
2023-01-18 11:28:10.219329: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
_SignatureMap({'serving_default': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FFB8402C9A0>, 'predict': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FFB38122730>})
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(21128, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
{'predicate_head_probabilities': <tf.Tensor: shape=(1, 128, 128, 49), dtype=float32, numpy=
array([[[[1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        ...,

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711487e-06, 2.0477496e-06, 7.3399161e-07, ...,
          3.9122031e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]]]], dtype=float32)>, 'token_label_predictions': <tf.Tensor: shape=(1, 128), dtype=int64, numpy=
array([[60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]])>}
{'predicate_head_probabilities': <tf.Tensor: shape=(1, 128, 128, 49), dtype=float32, numpy=
array([[[[1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122105e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        ...,

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711504e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]],

        [[1.7711487e-06, 2.0477496e-06, 7.3399161e-07, ...,
          3.9122031e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         ...,
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07],
         [1.7711487e-06, 2.0477514e-06, 7.3399161e-07, ...,
          3.9122068e-07, 9.6800636e-07, 5.0179818e-07]]]], dtype=float32)>, 'token_label_predictions': <tf.Tensor: shape=(1, 128), dtype=int64, numpy=
array([[60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]])>}
2023-01-18 11:28:13.781743: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""/home/polonsky/Documents/volcano_poc/Bravo/test.py"", line 69, in <module>
    imported.signatures[""serving_default""](**inputs) # Error: Uninitialized variables
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1601, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1610, in _call_impl
    return self._call_with_structured_signature(args, kwargs,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1691, in _call_with_structured_signature
    return self._call_flat(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 133, in _call_flat
    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 1853, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py"", line 499, in call
    outputs = execute.execute(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:

Detected at node 'bert/encoder/layer_5/output/dense/bias/read' defined at (most recent call last):
    File ""/home/polonsky/Documents/volcano_poc/Bravo/test.py"", line 68, in <module>
      imported = tf.saved_model.load(export_dir=new_model_path)
Node: 'bert/encoder/layer_5/output/dense/bias/read'
Attempting to use uninitialized value StatefulPartitionedCall/bert/encoder/layer_5/output/dense/bias
         [[{{node bert/encoder/layer_5/output/dense/bias/read}}]] [Op:__inference_signature_wrapper_11982]
```
</details>"
tensorflow/tensorflow,2023-01-16 20:10:57,question,How to save_weights of QAT model in tensorflow model optimization?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to train a model using quantise awareness training. I am using tensorflow model optimization. I create a model, quantise it, fit it, save the weight.

Then at a later date I redefine and quantise and try to load the weights.

However, the model starts the whole training process from the start again.

This article explains how to save the whole QAT model.
https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide#checkpoint_and_deserialize

However, I wish to save the weights not model. Please don't suggest to follow the save whole model! All help would be much appreciated.
```


### Standalone code to reproduce the issue

```shell
from tensorflow_model_optimization.quantization.keras import quantise_model
model = define_model()
qat_model = quantize_model(model)
qat_model.fit(...)
qat_model.save_weights(""qat_weights.h5"")
... Finish for Now ...


... Pick up at a later date ...
model = define_model()
qat_model = quantize_model(model)
qat_model.load_weights(""qat_weights.h5"")
```


### Relevant log output

Starts the whole training process from the start at 0%

_No response_</details>"
tensorflow/tensorflow,2023-01-15 17:13:03,question,I have some problems I think it is connected with dataset that i make myself,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I got an Error with model.fit
For your information, I will also add that the table used for the dataset 
consists of 2 columns, one of which contains the paths to the images on
 the disk, and the second one contains the labels for each image. 
 And I’m also new to github and therefore I don’t know how to make it so
 that you can run all the code because I don’t really want to share a huge
 folder with files, I can also clarify that the training arrays consist of 800 .png images
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1VsjzbjAhZru1kdY7iElxQWrsIW9rMfGA?usp=sharing
```


### Relevant log output

```shell
UnimplementedError                        Traceback (most recent call last)
<ipython-input-27-c0002c3040bd> in <module>
----> 1 model.fit(x_train, y_train, epochs=10)

1 frames
/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---> 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

UnimplementedError: Graph execution error:

Detected at node 'sparse_categorical_crossentropy/Cast' defined at (most recent call last):
    File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py"", line 612, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 149, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever
      self._run_once()
    File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1859, in _run_once
      handle._run()
    File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 690, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py"", line 743, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 787, in inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 748, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py"", line 543, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.8/dist-packages/tornado/gen.py"", line 209, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2854, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3057, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-27-c0002c3040bd>"", line 1, in <module>
      model.fit(x_train, y_train, epochs=10)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1051, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1030, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 890, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 948, in compute_loss
      return self.compiled_loss(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py"", line 201, in __call__
      loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 139, in __call__
      losses = call_fn(y_true, y_pred)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 243, in call
      return ag_fn(y_true, y_pred, **self._fn_kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/losses.py"", line 1860, in sparse_categorical_crossentropy
      return backend.sparse_categorical_crossentropy(
    File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 5223, in sparse_categorical_crossentropy
      target = cast(target, 'int64')
    File ""/usr/local/lib/python3.8/dist-packages/keras/backend.py"", line 2066, in cast
      return tf.cast(x, dtype)
Node: 'sparse_categorical_crossentropy/Cast'
Cast string to int64 is not supported
	 [[{{node sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_469]
```
</details>"
tensorflow/tensorflow,2023-01-12 12:17:44,question,Is there any way to disable remapping optimizer in Tensorflow1.15.0?,"## System information
-   **Centos7**
-   **Python 2.7**
-   **tensorflow1.15.0**
-   **CUDA10.0**
-   **CUDNN7**
-   **GPU: P100**


## Describe the problem
I used `tensorflow.python.client.timeline` tool in tf1.x to profile computation graph execution procedure. Then I find remapping optimizer fuse MatMul, BiasAdd and Elu into _FusedMatMul before graph execution. I also used `tf.config.optimizer.set_experimental_options()` to set remapping false but it didn't work. I don't known if I used it correctly. 

## Source code / logs
Here are my codes:
### Function to set Options
```bash
@contextlib.contextmanager
def graph_optimizer_options(options):
    old_opts = tf.config.optimizer.get_experimental_options()
    tf.config.optimizer.set_experimental_options(options)
    try:
        yield
    finally:
        tf.config.optimizer.set_experimental_options(old_opts)
```
### Options are here:
```bash
    options = {'constant_folding': False,
               'layout_optimizer': False,
               'shape_optimization': False,
               'remapping': False,
               'arithmetic_optimization': False,
               'dependency_optimization': False,
               'loop_optimization': False,
               'function_optimization': False,
               'debug_stripper': False,
               'disable_model_pruning': False,
               'scoped_allocator_optimization': False,
               'pin_to_host_optimization': False,
               'implementation_selector': False,
               'auto_mixed_precision': False,
               'disable_meta_optimizer': False,
               'min_graph_nodes': False}
```
### execute Computational Graph with tf.Session().
```bash
    with tf.Session(graph=graph, config=config) as sess:
        run_options = tf.compat.v1.RunOptions(trace_level = tf.compat.v1.RunOptions.FULL_TRACE)
        run_metadata = tf.compat.v1.RunMetadata()
        tf.graph_util.import_graph_def(graph_def, name="""")
        
        @tf.function
        def _run():
            sess.run(outputs_name, feed_dict = feed_dict, options = run_options, run_metadata = run_metadata)

        start_time = time.time()
        with graph_optimizer_options(options):
            print(tf.config.optimizer.get_experimental_options())
            _run()
        end_time = time.time()
        tl = timeline.Timeline(run_metadata.step_stats)
        # TODO better record meassurement
        ctf = tl.generate_chrome_trace_format()
        with open(tl_saved_path, 'w') as f:
            f.write(ctf)
    
    print(""finish running on {}, run time: {}"".format(device, end_time - start_time))
```
the result of ""tf.config.optimizer.get_experimental_options()""
```bash
{'debug_stripper': False, 'function_optimization': False, 'disable_model_pruning': False, 'constant_folding': False, 'implementation_selector': False, 'pin_to_host_optimization': False, 'auto_mixed_precision': False, 'disable_meta_optimizer': False, 'loop_optimization': False, 'scoped_allocator_optimization': False, 'dependency_optimization': False, 'shape_optimization': False, 'remapping': False, 'layout_optimizer': False, 'arithmetic_optimization': False}
```
Remapping seem to be False but _FusedMatMul still exist. Is there any way to disable remapping optimizer in Tensorflow1.15.0?
"
tensorflow/tensorflow,2023-01-12 00:40:51,question,tensorboardX ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
tensorflow/tensorflow,2023-01-11 17:40:13,question,"WARNING : [ W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found] Windows 10, TF2.10.1, CUDA 11.2, cuDNN 8.1.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.10.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.2; cuDNN = 8.1

### GPU model and memory

NVIDIA GTX 1070 8GB VRAM

### Current Behaviour?

```shell
I did everything like its instructed in https://www.tensorflow.org/install/pip but the warning still come up. I need GPU Support for my project. But when I just import tensorflow inside my python, the warning come up

I am sure 100% in my conda and my environment Path, there is cudart64_110.dll 
I tried to reinstall my Nvidia Drivers, CUDA, and my cuDNN but it still doesnt work.
Is there any solution for this?
```


### Standalone code to reproduce the issue

```shell
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
python3 -m pip install ""tensorflow<2.11""
python3 -c ""import tensorflow as tf""
```


### Relevant log output

```shell
W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
```
</details>"
tensorflow/tensorflow,2023-01-10 11:57:02,question,Is there any way to modify the quantized parameters from the generated tflite model?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installation (pip package or built from source): 2.11.0
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_model_quant = converter.convert()

I used the above code to generate a fully quantized tflite model. However, all the quantization parameters are set by default so the final accuracy of the tflite mode is a little bit low. So I wonder is there any way to rewrite the quantization parameters by myself. Since the integer numbers are stored in the tflite model, I can not modify them easily."
tensorflow/tensorflow,2023-01-06 01:49:36,question,Tensorflow lite  memory fault,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  openharmony  aarch64
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): tag v2.3.3


**Provide the text output from tflite_convert**
Each time the program runs to the assembly portion of the NeonMatrixBatchVectorMultiplyAccumulate function in ""tensorflow\\lite\\kernels\\internal\\optimized\\neon_tensor_utils.cc  "". The program will crash directly and report a memory error
"
tensorflow/tensorflow,2023-01-04 16:06:33,question,[TF-Lite]Is there a way to profile a model's inference process when using tflite in python,Is there a way to profile a model's inference process when using tflite in python？
tensorflow/tensorflow,2023-01-03 01:49:45,question,Distribute training bug while using tf.data,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: [ngc](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-22-10.html#rel-22-10)
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.8
-   **GPU model and memory**: A100 80G (8 Gpus)
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using 8 Gpus to train a model with custome dataset generator loader. However, when I am trying to train the model, at the final batch it will throw error as ""INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds."". I tried the same scripts and data with only one GPUs and it goes fine. 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
The code I uses to train is:
```
val = pd.read_csv('data/val.csv')
window_length = 40
feats = 4
def get_LSTM_AE_model():
    model = keras.Sequential()
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_length, feats), return_sequences=True, name='encoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.RepeatVector(window_length, name='encoder_decoder_bridge'))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(feats)))
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), loss=""mse"")
    model.summary()
    
    return model

#distribute training
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BATCH_SIZE_PER_REPLICA = 4096
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
with strategy.scope():
    model = get_LSTM_AE_model()

val_events = []
val.groupby('vin').apply(lambda x:val_events.append(x[['a','b','v','d']].values))
def val_data_generator():
    # np.random.shuffle(val_events)
    for events in val_events:
        yield events
val_dataset = tf.data.Dataset.from_generator(
    generator=val_data_generator,
    output_types=tf.float32
)
def tensor_2_window(x):
    x = tf.data.Dataset.from_tensor_slices(x)
    x = x.window(40,shift=1,drop_remainder=True)
    x = x.flat_map(lambda window: window.batch(40))
    return x
val_dataset = val_dataset.flat_map(tensor_2_window)
val_dataset = val_dataset.map(lambda window: (window, window))
val_dataset = val_dataset.cache().batch(4096*9).prefetch(buffer_size=tf.data.AUTOTUNE)
history = model.fit(
    val_dataset,
    epochs=50,
    # validation_data=val_dataset
)
```
then after training to the last batch, error apperas:
```
2022-12-07 03:37:26.605819: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606363: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606475: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606605: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606675: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606747: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606807: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606831: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606908: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606998: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607074: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607141: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In [52], line 1
----> 1 history = model.fit(
      2     val_dataset,
      3     epochs=50,
      4     # validation_data=val_dataset
      5 )

File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'replica_5/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 994, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1052, in compute_loss
      return self.compiled_loss(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py"", line 279, in __call__
      batch_dim = tf.shape(y_t)[0]
Node: 'replica_5/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
9 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_5/strided_slice}}]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_6/mean_squared_error/cond/else/_189/replica_6/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_385]]
  (2) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_4/mean_squared_error/cond/else/_139/replica_4/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_377]]
  (3) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/strided_slice/_296]]
  (4) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/mean_squared_error/cond/then/_63/replica_1/mean_squared_error/cond/cond/then/_690/replica_1/mean_squared_error/cond/cond/remove_squeezable_dimensions/cond/pivot_t/_1455/_729]]
  (5) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[gradient_tape/replica_7/mean_squared_error/cond/StatelessIf/pivot_f/_228/_346]]
  (6) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[div_no_nan/ReadVariableOp_8/_916]]
  (7) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[Func/replica_2/mean_squared_error/cond/else/_89/replica_2/mean_squared_error/cond/remove_squeezable_dimensions/cond/else/_742/input/_1165/_468]]
  (8) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_112593]
```
"
tensorflow/tensorflow,2022-12-29 03:37:03,question,Didn't find op for builtin opcode REVERSE_V2 version '1'.,"**System information**
- Linux
- TensorFlow installed from source:
- TensorFlow version 


**Provide the text output from tflite_convert**

```
Testing TestInvoke
Didn't find op for builtin opcode 'REVERSE_V2' version '1'. An older version of this builtin might be supported. Are you
 using an old TFLite binary with a newer model?

Failed to get registration from op code REVERSE_V2
```

**Standalone code to reproduce the issue** 
when i want to Invoke use tflite-micro,got a error:

Testing TestInvoke
Didn't find op for builtin opcode 'REVERSE_V2' version '1'. An older version of this builtin might be supported. Are you
 using an old TFLite binary with a newer model?

Failed to get registration from op code REVERSE_V2

can you tell me the reason？ thanks"
tensorflow/tensorflow,2022-12-26 20:31:07,question,tensor_util.py,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf.2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Windows 10

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! Please refer ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_util.py"" line 495, in make_tensor_proto

shape = [int(dim) for dim in shape] gives error. It is not possible to iterate with integer. Needs to be fixed.
```


### Standalone code to reproduce the issue

```shell
https://github.com/korayerbas/HDRCNN.git
It is Eilertsen's HDRCNN work from 2017. I try to rewrite the code; I try to rewrite the code in version 2 Tensorflow. As described above I absorved the bug.
```


### Relevant log output

```shell
TypeError: 'int' object is not iterable
```
</details>"
tensorflow/tensorflow,2022-12-24 08:03:21,question,How Do I Convert input arg tf.string to string in tf.function ," @tf.function(input_signature=[
        tf.TensorSpec(shape=[], dtype=tf.string, name=""key""),
        tf.TensorSpec(shape=[], dtype=tf.string, name=""value"")
    ])
    def infer(self, key, value):
          tf.print(key)
          xxx

I want to implement universal input, something like this
key: age#gender
value: 0#1
=> 
age_tensor = tf.constant(value=0, name=""age"")
gender_tensor = tf.constant(value=1,name=""gender"")

so I want to convert tf.string to string.
use tf.print function can normally print the input args，Do I have any way to simulate the function of tf.print and assign the printed value to a variable to implement tf.string to string?
or
Is there any other way I can do this?"
tensorflow/tensorflow,2022-12-20 13:23:11,question,GPUs only visible for administrators,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2.2/8.1.1

### GPU model and memory

NVIDIA RTX A5000

### Current Behaviour?

```shell
When executing tf.config.list_physical_devices(), tensorflow returns a list with all installed GPUs only when executed by an admin. Standard user accounts see only CPU devices.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices())
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-12-14 15:42:03,question,Training with Null Data in TFLITE Model Maker,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1 / tflite 0.4.2

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When training with tflite model maker, I am wondering if this the training algorithm uses null data samples (i.e. images with no annotations) for training or if this data is simply ignored?
```


### Standalone code to reproduce the issue

```shell
Na
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-12-13 11:50:34,question,Cannot make use of GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

v2.11.0-rc2-17-gd5b57ca93e5

### Custom Code

Yes

### OS Platform and Distribution

Linux Manjaro 6.0.11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

RTX 4090

### Current Behaviour?

```shell
Cannot make use of the GPU when trying to use this app ( https://github.com/allo-/virtual_webcam_background ).

I changed the code to test the tensorflow lib and I followed all the steps mentioned here : https://www.tensorflow.org/install/pip . Still not working.

Including a test script below, to see what I mean.

I tried installing all the packages about tf and cuda. Both on my machine and on the conda env ( or pip ).

Any ideas how to solve this?

Thank you.
```


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import tensorflow as tf
import sys
import os
import yaml

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

cuda_devices = os.getenv('CUDA_VISIBLE_DEVICES')
print(""cuda_devices"", cuda_devices)

if cuda_devices is None:
    os.unsetenv('CUDA_VISIBLE_DEVICES')
else:
    os.putenv('CUDA_VISIBLE_DEVICES', cuda_devices)

try:
    import mediapipe as mp
    classifier = mp.solutions.selfie_segmentation.SelfieSegmentation(
            model_selection=1)
    HAS_MEDIAPIPE = True
except ImportError:
    HAS_MEDIAPIPE = False
```


### Relevant log output

```shell
python ./virtual_webcam.py
2022-12-13 11:19:04.927313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-13 11:19:05.784273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/antouank/.conda/envs/virtual-webcam/lib/
2022-12-13 11:19:05.784815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/antouank/.conda/envs/virtual-webcam/lib/
2022-12-13 11:19:05.784829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Num GPUs Available:  1
cuda_devices None
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```
</details>"
tensorflow/tensorflow,2022-12-12 08:25:56,question,How to get single UnidirectionalSequenceRnnOp in tflite model,"### Issue Type

Support

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

According to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc there is `kUnidirectionalSequenceRnnOp` as a single operation in tflite, could you give a python code example - how can I get this? For example - this code for LSTM gives tflite with one UnidirectionalSequenceLSTM Op.
```py
# NOTE tested with TF 2.8.0
import tensorflow as tf
import numpy as np

from tensorflow import keras


model = keras.Sequential()
shape = (4, 4)

model.add(keras.layers.InputLayer(input_shape=shape, batch_size=1))
model.add(keras.layers.LSTM(2, input_shape=shape))
```
![image](https://user-images.githubusercontent.com/4616940/197647526-59c63de2-df61-46a1-bd61-75baa2688376.png)
How can I do same for UnidirectionalSequenceRnn?"
tensorflow/tensorflow,2022-12-11 15:12:55,question,OOM out of memory Mask RCNN,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 1.15

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to do a train for mask rcnn using Tensorflow 1.15. I'm using a GTX 1650 ti. I am using cuda 10 and cudnn 7.6.0 versions. I am getting an OOM error.

Code of my config file
class DamageConfig(Config):
    # define the name of the configuration
    NAME = ""damage""
    
    # number of classes (background + damge classes)
    NUM_CLASSES = 1 + 4
    
    # number of training steps per epoch
    STEPS_PER_EPOCH = 250
    # learning rate and momentum
    
    # regularization penalty
    BATCH_SIZE = 8
    
    # validation steps
    VALIDATION_STEPS = 25
    
    # RPN Acnhor scales and ratios to find ROI
   

# prepare train dataset.
train_set = DamageDataset()
# change the dataset 
train_set.load_dataset(""C:/Users/hasan/maskcuda/dataset"", ""train"")
train_set.prepare()

# prepare validation/test dataset
test_set = DamageDataset()
test_set.load_dataset(""C:/Users/hasan/maskcuda/dataset"", ""val"")
test_set.prepare()

# load damage config
config = DamageConfig()

# define the model
model = MaskRCNN(mode='training', model_dir='./', config=config)

# load weights mscoco model weights
weights_path = 'C:/Users/hasan/maskcuda/Mask_RCNN/mask_rcnn_coco.h5'

config = tensorflow.ConfigProto()
config.gpu_options.allow_growth = True
sess = tensorflow.Session(config=config)

# load the model weights
model.load_weights(weights_path, 
                   by_name=True, 
                   exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"",""mrcnn_bbox"", ""mrcnn_mask""])

# start the training of model
# you can change epochs and layers (head or all)
model.train(train_set, 
            test_set, 
            learning_rate=config.LEARNING_RATE, 
            epochs=15, 
            layers='heads')
```


### Standalone code to reproduce the issue

```shell
2022-12-11 17:49:33.553181: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14829824 totalling 14.14MiB
2022-12-11 17:49:33.553215: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 26214400 totalling 25.00MiB
2022-12-11 17:49:33.553249: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 15 Chunks of size 51380224 totalling 735.00MiB
2022-12-11 17:49:33.553283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 186075648 totalling 177.46MiB
2022-12-11 17:49:33.553317: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 2.81GiB
2022-12-11 17:49:33.553348: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 3041551872 memory_limit_: 3041551975 available bytes: 103 curr_region_allocation_bytes_: 6083104256
2022-12-11 17:49:33.553391: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats:
Limit:                  3041551975
InUse:                  3019572480
MaxInUse:               3041504768
NumAllocs:                   22711
MaxAllocSize:           1348425984


ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[2,3,1030,1030] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node conv1_6/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[proposal_targets_6/strided_slice_17/_28717]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-12-07 19:47:27,question,Tensorflow 2.11 is still using CUDA 11.2 + CuDNN 8.1 that were released 2 years ago,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow 2.11 is still using CUDA 11.2 + CuDNN 8.1 that were released 2 years ago: https://www.tensorflow.org/install/source#gpu. This makes our stack have to use the old CUDA/CuDNN without new features. Is it possible to make new TF version work with newest CUDA/CuDNN?
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-12-07 03:44:27,question,"MirroredStrategy error INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds when training a model, most related to batch size strategy across all GPUs","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: [ngc](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-22-10.html#rel-22-10)
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.8.10
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.8
-   **GPU model and memory**: A100 80G (8 Gpus)
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using 8 Gpus to train a model with custome dataset generator loader. However, when I am trying to train the model, at the final batch it will throw error as ""INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds."". I tried the same scripts and data with only one GPUs and it goes fine. 
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
The code I uses to train is:
```
val = pd.read_csv('data/val.csv')
window_length = 40
feats = 4
def get_LSTM_AE_model():
    model = keras.Sequential()
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_length, feats), return_sequences=True, name='encoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.RepeatVector(window_length, name='encoder_decoder_bridge'))
    model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))
    model.add(keras.layers.Dropout(0.25))
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(feats)))
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00005), loss=""mse"")
    model.summary()
    
    return model

#distribute training
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BATCH_SIZE_PER_REPLICA = 4096
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
with strategy.scope():
    model = get_LSTM_AE_model()

val_events = []
val.groupby('vin').apply(lambda x:val_events.append(x[['a','b','v','d']].values))
def val_data_generator():
    # np.random.shuffle(val_events)
    for events in val_events:
        yield events
val_dataset = tf.data.Dataset.from_generator(
    generator=val_data_generator,
    output_types=tf.float32
)
def tensor_2_window(x):
    x = tf.data.Dataset.from_tensor_slices(x)
    x = x.window(40,shift=1,drop_remainder=True)
    x = x.flat_map(lambda window: window.batch(40))
    return x
val_dataset = val_dataset.flat_map(tensor_2_window)
val_dataset = val_dataset.map(lambda window: (window, window))
val_dataset = val_dataset.cache().batch(4096*9).prefetch(buffer_size=tf.data.AUTOTUNE)
history = model.fit(
    val_dataset,
    epochs=50,
    # validation_data=val_dataset
)
```
then after training to the last batch, error apperas:
```
2022-12-07 03:37:26.605819: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606363: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606475: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606605: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606675: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606747: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606807: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606831: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606908: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.606998: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607074: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-12-07 03:37:26.607141: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at strided_slice_op.cc:105 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In [52], line 1
----> 1 history = model.fit(
      2     val_dataset,
      3     epochs=50,
      4     # validation_data=val_dataset
      5 )

File /usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'replica_5/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 994, in train_step
      loss = self.compute_loss(x, y, y_pred, sample_weight)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1052, in compute_loss
      return self.compiled_loss(
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py"", line 279, in __call__
      batch_dim = tf.shape(y_t)[0]
Node: 'replica_5/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
Detected at node 'replica_3/sequential/encoder_1/strided_slice' defined at (most recent call last):
    File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
      self._bootstrap_inner()
    File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
      self.run()
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 1135, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 993, in train_step
      y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/training.py"", line 557, in __call__
      return super().__call__(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/sequential.py"", line 410, in call
      return super().call(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 510, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py"", line 667, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 553, in __call__
      return super().__call__(inputs, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 1097, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 96, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/lstm.py"", line 607, in call
      inputs, initial_state, _ = self._process_inputs(
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 810, in _process_inputs
      initial_state = self.get_initial_state(inputs)
    File ""/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py"", line 529, in get_initial_state
      batch_size = input_shape[1] if self.time_major else input_shape[0]
Node: 'replica_3/sequential/encoder_1/strided_slice'
9 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_5/strided_slice}}]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_6/mean_squared_error/cond/else/_189/replica_6/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_385]]
  (2) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_4/mean_squared_error/cond/else/_139/replica_4/mean_squared_error/cond/remove_squeezable_dimensions/Equal/_377]]
  (3) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/strided_slice/_296]]
  (4) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[replica_1/mean_squared_error/cond/then/_63/replica_1/mean_squared_error/cond/cond/then/_690/replica_1/mean_squared_error/cond/cond/remove_squeezable_dimensions/cond/pivot_t/_1455/_729]]
  (5) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[gradient_tape/replica_7/mean_squared_error/cond/StatelessIf/pivot_f/_228/_346]]
  (6) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[div_no_nan/ReadVariableOp_8/_916]]
  (7) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
	 [[Func/replica_2/mean_squared_error/cond/else/_89/replica_2/mean_squared_error/cond/remove_squeezable_dimensions/cond/else/_742/input/_1165/_468]]
  (8) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
	 [[{{node replica_3/sequential/encoder_1/strided_slice}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_112593]
```"
tensorflow/tensorflow,2022-12-05 00:46:56,question,Cannot convert Handwriting recognition model to tflite model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip install ""tensorflow<2.11""
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.10.1
- Python 3.10

### 2. Code

Used the code from the tutorial on this page:
https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/handwriting_recognition.ipynb
And at the end tried converting the prediction model to a TF Lite model using following code:

```
converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)
tflite_float_model = converter.convert()
```
And then I get the following error:
[error_log_1.txt](https://github.com/tensorflow/tensorflow/files/10149753/error_log_1.txt)
Which basically says that I need to set supported_ops flags. First of all, is there any way I can prevent that? I don't understand what function in the code is using select TensorFlow core operators, I don't want to be using any select TensorFlow core operators since it makes it creates other problems.

And then when I do set these flags:
```
converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter._experimental_lower_tensor_list_ops = False
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_float_model = converter.convert()
```

Then I get the following messages, which i'm not sure if these are indicating if something's gone wrong or not:
[error_log_2.txt](https://github.com/tensorflow/tensorflow/files/10149850/error_log_2.txt)

And export the model and import it in my android project using these instructions: https://www.tensorflow.org/lite/guide/ops_select
On Android I then convert the model file to byte buffer and try to pass it to TF Lite Interpreter as follows:
```
val fileDescriptor = context.assets.openFd(filename)
val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
val fileChannel = inputStream.channel
val startOffset = fileDescriptor.startOffset
val declaredLength = fileDescriptor.declaredLength
val byteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
val interpreter = org.tensorflow.lite.Interpreter(byteBuffer)
```
I then get a IllegalArgumentException with following message:
`""Model ByteBuffer should be either a MappedByteBuffer of the model file, or a direct ByteBuffer using ByteOrder.nativeOrder() which contains bytes of model content.""`
I think there is something wrong with the exported TF Lite model file. 

Best solution would probably be to avoid using select TF operators, would really like to avoid it if possible.
Would appreciate your help!



"
tensorflow/tensorflow,2022-12-01 09:40:39,question,tf.numpy_function in tf.data.Dataset.map causes CUDA_ERROR_OUT_OF_MEMORY after hundreds of epochs,"### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3.2

### GCC/Compiler version

gcc 9.3.0

### CUDA/cuDNN version

11.8/8.6.0

### GPU model and memory

Nvidia Quadro RTX 6000, 25.8G memory

### Current Behaviour?

```shell
I use `tf.data.Dataset` for my input pipeline as follows:
  1. `from_generator`
  2. `cache`
  3. `shuffle`
  4. `repeat`
  5. `map`
  6. `batch`
  7. `prefetch`

The step 5 call a data-augmentation function where there is a call to `tf.numpy_function` which wraps `scipy.ndimage.rotate`.

After a variable number of epochs (in general more than 100), a `CUDA_OUT_OF_MEMORY` appears and the kernel crashed. If I remove the call to `tf.numpy_function` from the data-augmentation function, I don't have this problem. Note that I don't use `tf.py_function` as it looks much slower.
```


### Standalone code to reproduce the issue

```shell
I haven't managed to create a simple reproducer for this bug.
```


### Relevant log output

```shell
2022-12-01 02:26:32.244730: E tensorflow/stream_executor/cuda/cuda_driver.cc:796] failed to alloc 17179869184 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-12-01 02:26:32.299769: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 17179869184
```
"
tensorflow/tensorflow,2022-11-28 22:37:01,question,How to write to input tensors in custom op?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How can I write to input tensors in a custom op? I can read and write to output tensors but only read on inputs. (compiling outputs ""assignment of read-only location"" error when trying to write to input tensor). I want to write an inplace op, and making it inplace is crucial for my project, as writting the results to an output tensor and then assigning the output to the variable is too slow). Here is a minimal example that just sets the input tensor to zero. What code can I add to this example code so that I can be able to write to the input tensor?
```


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/util/work_sharder.h""

#include <iostream>

using namespace tensorflow;

REGISTER_OP(""Example"")
    .Input(""variable: float"")
    ;

class ExampleOp : public OpKernel {
public:
    
    explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(OpKernelContext* context) override {

        const Tensor& variable_tensor = context->input(0);
        auto variable = variable_tensor.flat<float>();
        
        for(int i = 0; i < variable.size(); ++i)
            variable(i) = 0; 

    };

};


REGISTER_KERNEL_BUILDER(Name(""Example"").Device(DEVICE_CPU), ExampleOp);
```


### Relevant log output

```shell
minimal.cc: In member function ‘virtual void ExampleOp::Compute(tensorflow::OpKernelContext*)’:
minimal.cc:25:25: error: assignment of read-only location ‘variable.Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>::operator()(((Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>::Index)i))’
   25 |             variable(i) = 0;
      |             ~~~~~~~~~~~~^~~
```
</details>"
tensorflow/tensorflow,2022-11-25 13:55:03,question,Protobuf Package Dependency between Google ads and tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hello, would you be so kind to fix TensorFlow 2.11.0 depends on protobuf<3.20 and >=3.9.2 to be more relevant to the google ads package which uses protobuf==4.21.5
```


### Standalone code to reproduce the issue

```shell
protobuf==4.21.5
google-ads==19.0.0
tensorflow==2.11.0
google-api-core==2.10.1
googleapis-common-protos==1.56.4
```


### Relevant log output

```shell
The conflict is caused by:
    The user requested protobuf==4.21.5
    google-ads 19.0.0 depends on protobuf>=4.21.5
    google-api-core 2.10.1 depends on protobuf<5.0.0dev and >=3.20.1
    googleapis-common-protos 1.56.4 depends on protobuf<5.0.0dev and >=3.15.0
    tensorflow 2.11.0 depends on protobuf<3.20 and >=3.9.2
```
</details>"
tensorflow/tensorflow,2022-11-25 08:41:50,question,How to set the half_pixel_centers of ResizeBilinear to False in the .tflite?,"### 1. System information

- OS Platform and Distribution (Linux Ubuntu 22.04):

Hi, i convert the model from pytorch into tflite：

   pytorch = 1.10 (.pth) -> onnx = 1.12 (.onnx) -> tensorflow = 2.11  (.tflite)

I find the half_pixel_centers  of the ResizeBilinear  operator is True, but i need this option to be False.  

How can i set the half_pixel_centers to False during model convert?

![image](https://user-images.githubusercontent.com/48872511/203933849-b9b0564b-85ec-427e-a2b0-ab32c1413c1a.png)

I try to set the half_pixel_centers  to False, when parsing the onnx model, and converting it into keras model:

---------------------------------------------------------------------------------------------------------------------------------
def keras_builder(onnx_model, new_input_nodes:list=None, new_output_nodes:list=None):
    model_graph = onnx_model.graph

    '''
        init onnx model's build-in tensors
    '''
    onnx_weights = dict()
    for initializer in model_graph.initializer:
        onnx_weights[initializer.name] = numpy_helper.to_array(initializer)

    '''
        build input nodes
    '''
    tf_tensor, input_shape = {}, []
    for inp in model_graph.input:
        input_shape = [x.dim_value for x in inp.type.tensor_type.shape.dim]
        if input_shape == []:
            continue
        batch_size = 1 if input_shape[0] <= 0 else input_shape[0]
        input_shape = input_shape[2:] + input_shape[1:2]
        tf_tensor[inp.name] = keras.Input(shape=input_shape, batch_size=batch_size)

    '''
        build model inline node by iterate onnx nodes.
    '''
    input_node_names, outputs_node_names = [], []
    for node in model_graph.node:
        op_name, node_inputs, node_outputs, node_name = node.op_type, node.input, node.output, node.name
        op_attr = decode_node_attribute(node)
        
        #print(""opt name {}, opt {}"".format(op_name, op_attr))

        tf_operator = OPERATOR.get(op_name)

        if(op_name == ""Resize""):
            print(tf_operator)
            op_attr['pytorch_half_pixel'] = False

        if tf_operator is None:
            raise KeyError(f""{op_name} not implemented yet"")
        
        _inputs = None 
        if len(node_inputs) > 0:
            _inputs = tf_tensor[node_inputs[0]] if node_inputs[0] in tf_tensor else onnx_weights[node_inputs[0]]

        for index in range(len(node_outputs)):
            tf_tensor[node_outputs[index]] = tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs)
            if(op_name == ""Resize""):
                print(tf_operator(tf_tensor, onnx_weights, node_inputs, op_attr, index=index)(_inputs))
        '''
            reorganize input and output nodes
        '''
        if new_input_nodes is not None and node_name in new_input_nodes:
            input_node_names.append(node_outputs[0])
        # TODO for nodes with multiply outputs.
        if new_output_nodes is not None and node_name in new_output_nodes:
            outputs_node_names.append(node_outputs[0])
        if new_output_nodes is not None and len(outputs_node_names) == len(new_output_nodes):
            break
    
    '''
        process input and output nodes 
    '''
    input_nodes = []
    if new_input_nodes is None:
        input_nodes = [tf_tensor[x.name] for x in model_graph.input]
    else:
        for node in model_graph.input:
            if node.name in new_input_nodes:
                input_node_names.append(node.name)
        input_nodes = [tf_tensor[x] for x in input_node_names]
    outputs_nodes = []
    if new_output_nodes is None:
        outputs_nodes = [tf_tensor[x.name] for x in model_graph.output]
    else:
        for node in model_graph.output:
            if node.name in new_output_nodes:
                outputs_node_names.append(node.name)
        outputs_nodes = [tf_tensor[x] for x in outputs_node_names]

    '''
        build keras model
    '''
    keras_model = keras.Model(inputs=input_nodes, outputs=outputs_nodes)
    keras_model.trainable = False
    # keras_model.summary()

    return keras_model
------------------------------------------------------------------------------------------------------------------
    keras_model = keras_builder(model_proto, input_node_names, output_node_names)
    print(""Build keras model Done"")
    # set resize half_pixel_centers = False
    ind =0
    for layer in keras_model.layers:

        #print(layer.name)

        if (""resize"" in layer.name) or (""Resize"" in layer.name):
            print(""###############################"")
            
            keras_model.layers[ind].half_pixel_centers = False
            print(keras_model.layers[ind].half_pixel_centers)
         
        ind+=1

But when converting keras to tflite model by tf.lite.TFLiteConverter.from_keras_model(keras_model),  the half_pixel_centers  is still True.
"
tensorflow/tensorflow,2022-11-23 07:52:06,question,Is there a way to use a tflite-runtime version higher than 2.5.0 on an armv7 development board,"### 1. System information

- Linux Ubuntu 18.04
- armv7 board
- python3.6

### 2. question description

I trained a model on my linux server **(ubuntu18.04，Intel(R) Xeon(R) W-2145 CPU)** and exported it to tflite mode, and the model on the server **(tflite-runtime=2.10.0)** works fine.

Now I want to run the model on an **armv7** development board of zynq7000. The operating system running on my board version is ubuntu18.04, and **python3.6** and **tflite-runtime=2.5.0** is installed (2.5.0 seems is the highest version I can get on armv7), but when I run `interpreter = tflite.Interpreter(model_path=my_model)`, it throws `Segmentation fault (core dumped)` error.

After my test, if I downgrade the tflite-runtime on my linux server to version 2.5.0, the same error will be thrown. So I think the reason for the error may be that the version of tflite-runtime is too low.

But I can't get a higher version of tflite-runtime on armv7 board, does anyone know a solution please?


"
tensorflow/tensorflow,2022-11-21 05:30:01,question,Failed to load the native TensorFlow runtime.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
tensorflow/tensorflow,2022-11-16 18:11:12,question,TFLite benchmark tool - example to use input_layer_value_files,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am trying to use the Android TFLite benchmark tool to run inference time analysis for my TFLite model. Going through the [repo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark), I am interested in passing custom inputs to the benchmark tool. I am specifically looking for how to use input_layer_value_files flag. Could you provide an example of a sample file? Thanks!

### Source code / logs
-
"
tensorflow/tensorflow,2022-11-14 17:48:31,question,InputSpec update breaks working code,"Description: I'm running code that used to work before a tensorflow/keras update. It is not obvious how the error message can be used to fix the problem, here is code, error and system information. Thanks for your help.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Importing the training set
dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')
training_set = dataset_train.iloc[:, 1:2].values

# Feature Scaling
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set)

# Creating a data structure with 60 timesteps and 1 output
X_train = []
y_train = []
window_size = 60

for i in range(window_size, training_set_scaled.size):
    X_train.append(training_set_scaled[i-window_size:i, 0])
    y_train.append(training_set_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping - where you will add features.... also for compatibility later
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.engine.input_spec import InputSpec

# Initialising the RNN
regressor = Sequential()

# Adding the first LSTM layer and some Dropout regularization
regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))



#ERROR:
Traceback (most recent call last):

  File ""C:\\Users\\***\\AppData\\Local\\Temp\\ipykernel_28784\\1308929883.py"", line 45, in <module>
    regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))

  File ""C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py"", line 529, in __init__
    ""`implementation=0` has been deprecated, ""

  File ""C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py"", line 2864, in __setattr__
    outputs = tf.nest.pack_sequence_as(outputs, outputs_copy)

  File ""C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py"", line 205, in _method_wrapper
    result = method(self, *args, **kwargs)

  File ""C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py"", line 1152, in input_spec

TypeError: Layer input_spec must be an instance of InputSpec. Got: InputSpec(ndim=3)

------------------------

### System information

-   **OS: Windows 10**:
-   **TensorFlow installed from (pip install per instructions here: https://www.tensorflow.org/install/pip#windows-native)**:
-   **TensorFlow version (2.10.0)**:
-   **Python version 3.9.13**:
-   **CUDA/cuDNN version cudatoolkit=11.2 cudnn=8.1.0**:
-   **GPU model and memory RTX 4090**:
-   **Exact command to reproduce**:


"
tensorflow/tensorflow,2022-11-13 19:38:32,question,"Trying to train transformer like the NMT Keras Transformer, for passage summarization task. The model's masked accuracy goes upto ~30% but the transformer's output is very poor. Could you help me understand what I might be doing wrong?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
NMT task on the keras transformer as showed in the tutorials (https://www.tensorflow.org/text/tutorials/transformer) performs well. But I'm unable to get decent performance on a passage summarization task on the `cnn_dailymail` dataset. I can't understand what I might be doing wrong.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/12sbAGKESj8qynO-pb1K6gh-CZIvCxzfo#scrollTo=TlOaOjwwWN4K
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-11-11 13:31:28,question,No matching distribution found for numba==0.53,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.10

### Mobile device

n/a

### Python version

3.10.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Excessive version dependencies for tflite-model-maker
```


### Standalone code to reproduce the issue

```shell
Dependency errors even with the following statement:

python3 -m pip install -q --use-deprecated=legacy-resolver tflite-model-maker
```


### Relevant log output

```shell
Using cached neural_structured_learning-1.4.0-py2.py3-none-any.whl (128 kB)
ERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10
ERROR: Could not find a version that satisfies the requirement numba==0.53 (from tflite-model-maker) (from versions: 0.1, 0.2, 0.3, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.12.2, 0.13.0, 0.13.2, 0.13.3, 0.13.4, 0.14.0, 0.15.1, 0.16.0, 0.17.0, 0.18.1, 0.18.2, 0.19.1, 0.19.2, 0.20.0, 0.21.0, 0.22.0, 0.22.1, 0.23.0, 0.23.1, 0.24.0, 0.25.0, 0.26.0, 0.27.0, 0.28.1, 0.29.0, 0.30.0, 0.30.1, 0.31.0, 0.32.0, 0.33.0, 0.34.0, 0.35.0, 0.36.1, 0.36.2, 0.37.0, 0.38.0, 0.38.1, 0.39.0, 0.40.0, 0.40.1, 0.41.0, 0.42.0, 0.42.1, 0.43.0, 0.43.1, 0.44.0, 0.44.1, 0.45.0, 0.45.1, 0.46.0, 0.47.0, 0.48.0, 0.49.0, 0.49.1rc1, 0.49.1, 0.50.0rc1, 0.50.0, 0.50.1, 0.51.0rc1, 0.51.0, 0.51.1, 0.51.2, 0.52.0rc2, 0.55.0rc1, 0.55.0, 0.55.1, 0.55.2, 0.56.0rc1, 0.56.0, 0.56.2, 0.56.3, 0.56.4)
ERROR: No matching distribution found for numba==0.53
(tflite_model_maker) reza@BeUlta:~/projects/tflite_mode
```
</details>"
tensorflow/tensorflow,2022-11-09 07:04:03,question,release variable memory api ?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
api can release memory variable
```


### Standalone code to reproduce the issue

```shell
no
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-08-28 07:20:19,feature,How to limit GPU memory usage when only prediction in c++?,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.4

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.0 / 8.1.0

### GPU model and memory

RTX 3090 / 24GB

### Current behavior?

I loaded the saved model using the already compiled tensorflow-gpu 2.4.0.
When this model was used for prediction, it was confirmed that all available memory of the gpu was used.
I've seen limiting using the growing method in python, but I don't know how to use it in c++. could you please tell me how?



### Standalone code to reproduce the issue

```shell
#include <stdlib.h>
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
void NoOpDeallocator(void* data, size_t a, void* b) {}

int main() {
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""H:\\\\my_model\\\\""; // Path of the model
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }
}
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-19 05:24:59,feature,"`tf.image.crop_to_bounding_box()` assumes `tf.int32` arguments, but not documented as such","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04 (WSL 2)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.image.crop_to_bounding_box()` implicitly assumes that the target width and height are `tf.int32`, but this is not documented anywhere. The cause for this is using `tf.shape()` which has the default `dtype` of `tf.int32`, in a stack operation:
https://github.com/tensorflow/tensorflow/blob/c9fafed9bc8cb0238a775fd4a0680e648c06b5b6/tensorflow/python/ops/image_ops_impl.py#L1250-L1254

### Standalone code to reproduce the issue

```python
import tensorflow as tf

image = tf.zeros([1000, 2000, 3], dtype=tf.uint8)
offset = tf.constant([0, 0], dtype=tf.int64)
size = tf.constant([900, 1500], dtype=tf.int64)
tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/.../test.py"", line 6, in <module>
    tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Pack as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Pack] name: stack
```
"
tensorflow/tensorflow,2023-08-10 07:53:12,feature,On-Device training for LSTM or GRU Model ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): web
- TensorFlow installed from (source or binary): colab
- TensorFlow version (or github SHA if from source):colab

Hi I’m new to tensorflow and I’m trying to make LSTM or GRU model to be enable to re-train on-device(Android) with tabular data (mostly customer interaction).

I’m referencing these examples
[On-Device Training 1](https://www.tensorflow.org/lite/examples/on_device_training/overview)

This is an example of a CNN, but not able to understand how can I enable on-device training for lstm or gru model.
Are there any examples for reference? Thanks"
tensorflow/tensorflow,2023-07-26 06:59:17,feature,Cannot subclass dataset_ops.DatasetV2,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.x

### Custom code

Yes

### OS platform and distribution

Mac OS 13.0

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, I'm from LanceDB team and we're trying to build native support for tf.data. See WIP PR here https://github.com/lancedb/lance/pull/1087 .
Ideally, we'd like to simply subclass `tf.dataset_ops.DatasetV2` so that all the metadata needed to recreate the dataset can be pushed down to our file format that enabled parallelism elegantly.
So, it'd be something like this
```
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
 ```
The above code complains that can not create LanceTfDataset to tf.Tensor/variant.

Issue - what exactly is variant_tensor and how do we go about creating one? I read through the docs but couldn't find anything concrete. There was a mention that variant_tensor is a special tensor that tell about the type of the dataset and that it's equivalent to tf.Variant, but the above code doesn't work.

Having a version of tf.dataset that we can use to capture extra metadata would allow us to improve the interface as well:
so instead of lance.tf.data.from_dataset(uri, columns, filter, batch_size) we can just have from_lance(uri).filter(..).batch_size(...).shuffle().

So what's the way to go about subclassing tf Dataset?

### Standalone code to reproduce the issue

```shell
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-20 05:53:22,feature,How to get raw buffer pointer from python tf.Tensor,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In C++ api, tensorflow::Tensor has `data()` method which returns pointer to memory array. While in python api tf.Tensor does not allow to get raw data pointer. Is there any solution or workaround for this?

### Standalone code to reproduce the issue

```shell
tensorflow::Tensor.data()
tf.Tensor
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-06-28 18:15:49,feature,Tensorflow Lite for Windows and macOS,"Hi guys,
So i am new to On Device Machine Learning which is super cool but, the limitation with tf-lite is that i can only use in Android or iOS not in Windows Apps nor MacOS.
I request any of you who has dealt with embedding tfLite with Desktop class Apps to provide a simple solution. "
tensorflow/tensorflow,2023-06-18 20:11:34,feature,Building tf-opt steps with prerequisites ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubunto 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.2

### GCC/Compiler version

9.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to build tf-opt binary on branch v2.12 without any changes and gets different compilation errors. The command for compilation I use:
`bazel build -c opt tensorflow/compiler/mlir:tf-opt`

Can you share some prerequites for building and debugging `tf-opt` binary (for debug/release mode). I would appriciate if there is docker builder I can use to it instead of changing my envrioment.

Thanks,
Aviad

### Standalone code to reproduce the issue

```shell
ERROR: /localdrive/users/aviadco/community/tensorflow/tensorflow/lite/experimental/acceleration/configuration/BUILD:36:8: Executing genrule //tensorflow/lite/experimental/acceleration/configuration:configuration_schema failed: (Exit 1): bash failed: error executing command (from target //tensorflow/lite/experimental/acceleration/configuration:configuration_schema) /bin/bash -c ... (remaining 1 argument skipped)
bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc)
ERROR: /localdrive/users/aviadco/community/tensorflow/tensorflow/lite/schema/BUILD:184:22: Generating flatbuffer files for conversion_metadata_fbs_srcs: //tensorflow/lite/schema:conversion_metadata_fbs_srcs failed: (Exit 1): bash failed: error executing command (from target //tensorflow/lite/schema:conversion_metadata_fbs_srcs) /bin/bash -c ... (remaining 1 argument skipped)
bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/flatbuffers/flatc)
Target //tensorflow/compiler/mlir:tf-opt failed to build
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-12 10:07:16,feature,FlexCombinedNonMaxSuppression unavailable in flex shared library,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

v2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.2.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have network that runs with no problem on Android that uses the operator FlexCombinendNonMaxSuprpression. When running the same network and code on an x86 machine, the log output tells me that this operator is not supported by this interpreter.

I have built from source the necessary libraries, libtensorflowlite.so and libtensorflowlite_flex.so

### Standalone code to reproduce the issue

```shell
`
  /* Allocate Tensors */
  retTflite = _interpreter->AllocateTensors();
  if (retTflite == kTfLiteOk)
  {
    _engineReady = true;
  }
`
```


### Relevant log output

```shell
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 276 (FlexCombinedNonMaxSuppression) failed to prepare.

INFO: Failed to apply the default TensorFlow Lite delegate indexed at 0 because of unresolved ops (which could be resolved by another delegate). Ignoring the error, and continuing anyway.
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 276 (FlexCombinedNonMaxSuppression) failed to prepare.
```
</details>"
tensorflow/tensorflow,2023-06-08 14:48:45,feature,2.13 support for TensorflowLiteSwift and tensorflow-lite android (Maven),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Ios/Android

### Mobile device

Ios/Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm interested in the planned release date for TensorflowLite 2.13.0 IOS/Android

### Standalone code to reproduce the issue

```shell
I'm interested in the planned release date for TensorflowLite 2.13.0 IOS/Android
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-07 23:07:13,feature,tf-mlir-translate and flatbuffer_translate failure for the ERF function,"### For TF

```
Results INVALID_MLIR test_erf_1_f32: Error 1 running command: /tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir --tf-enable-shape-inference-on-import --tf-output-arrays=result erf/test_erf_1_f32/model.pb -o erf/test_erf_1_f32/test_tf.preopt.mlir --tf-input-arrays placeholder_0 --tf-input-shapes 1,
```
You can find the dummy tf erf model here: https://github.com/Jerry-Ge/tfl_models/blob/main/erf_model.pb

### For TFL
After running
```
/tensorflow/compiler/mlir/lite/flatbuffer_translate --tflite-flatbuffer-to-mlir erf/test_erf_1_f32/model.tflite --output-arrays=PartitionedCall:0 -o erf/test_erf_1_f32/test_tflite.preopt.mlir
```
You can find the dummy tfl erf model here: https://github.com/Jerry-Ge/tfl_models/blob/main/erf_model.tflite


It's generating a `tfl.custom` operator here which is not tfl.erf
```
module attributes {tf_saved_model.semantics, tfl.description = ""MLIR Converted."", tfl.schema_version = 3 : i32} {
  func.func @main(%arg0: tensor<1xf32> {tf_saved_model.index_path = [""placeholder_0""]}) -> (tensor<1xf32> {tf_saved_model.index_path = [""output_0""]}) attributes {tf.entry_function = {inputs = ""serving_default_placeholder_0:0"", outputs = ""PartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tfl.custom""(%arg0) {custom_code = ""FlexErf"", custom_option = #tfl<const_bytes : ""0x03457266001212034572661A002A070A0154120230013200000219151414042801"">} : (tensor<1xf32>) -> tensor<1xf32>
    return %0 : tensor<1xf32>
  }
}
```

I think there requires some fair amount of support to the erf function for mlir. 

Related ticket: https://github.com/tensorflow/tensorflow/issues/60663"
tensorflow/tensorflow,2023-05-20 08:20:17,feature,"""Improve GPU memory management for large-scale models""","Currently, TensorFlow's GPU memory management can be challenging when training large-scale models. This issue aims to improve the memory management strategies for GPU usage to optimize memory allocation and deallocation, reducing memory fragmentation and enabling more efficient training of large models.

You can find this issue by going to the TensorFlow repository's ""Issues"" tab and using the search bar to search for the keywords ""GPU memory management large-scale models."" Once you find the issue, make sure to read through the details and discussion to understand the specific challenges and proposed solutions.

Feel free to contribute to this issue by commenting on it, discussing possible approaches, or even submitting a pull request with your proposed changes. Remember to familiarize yourself with the contribution guidelines and any specific instructions mentioned in the issue before getting started.

Good luck, and I hope you find this issue interesting and valuable for your contributions to TensorFlow!"
tensorflow/tensorflow,2023-05-04 07:40:29,feature,Support CheckpointInputPipelineHook in Estimator MirrorStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 1.15

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Failed to save pipeline info in checkpoint.

### Standalone code to reproduce the issue

```shell
mirrored_strategy = tf.distribute.MirroredStrategy()
    run_config = tf.estimator.RunConfig(
        model_dir=output_dir,
        save_checkpoints_steps=save_checkpoints_steps,
        train_distribute=mirrored_strategy,
        eval_distribute=mirrored_strategy,
        keep_checkpoint_max=max_ckp_num
    )

estimator.train(input_fn=input_fn, 
                hooks=[tf.data.experimental.CheckpointInputPipelineHook(estimator)],
                       steps=tf_args.test_train_step)
```


### Relevant log output

```shell
2023-05-03 21:53:52.048712: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at iterator_ops.cc:1081 : Failed precondition: RemoteCall is stateful.
```
</details>"
tensorflow/tensorflow,2023-04-25 10:49:31,feature,Maximum aspect ratio of camera for Android Mobile app & Android TV to use feature of Pose estimation?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
What is the maximum aspect ratio of camera for Android Mobile app & Android TV to use feature of Pose estimation?
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-30 10:39:22,feature,Building TF from source instructions clarification wrt Python packages,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.11

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.3

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi! Following the instructions on https://www.tensorflow.org/install/source usually works. However, this time I ran into an issue that took a while to figure out. It turns out to be an issue with the protobuf version:

protobuf                     3.19.4 => no good
protobuf                     3.20.3 => good 

In the current instructions, the only Python related info is:

sudo apt install python3-dev python3-pip

and 

pip install -U --user pip numpy wheel packaging requests opt_einsum
pip install -U --user keras_preprocessing --no-deps

Is there a reason not to refer to the following requirement specs?

pip install -r ./tensorflow/tools/ci_build/release/requirements_common.txt
pip install -r ./tensorflow/tools/ci_build/release/requirements_ubuntu.txt

Which would have avoided the issue for me... If not, I propose to add that.

Cheers!
```


### Standalone code to reproduce the issue

```shell
bazel build --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/ubuntu/repos/mirror-tensorflow/tensorflow/BUILD:1635:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow && \\
  exec env - \\
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/ubuntu/.vscode-server/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bin:/home/ubuntu/bin \\
    PYTHON_BIN_PATH=/usr/bin/python3 \\
    PYTHON_LIB_PATH=/usr/lib/python3.10/dist-packages \\
    TF2_BEHAVIOR=1 \\
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/aarch64-opt/bin/tensorflow_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=static --loading=default bazel-out/aarch64-opt/bin/tensorflow/_api/v2/v2.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/app/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distributions/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/initializers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/logging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/losses/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/manip/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/metrics/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/rnn_cell/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/python_io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/resource_loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/builder/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/main_op/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/tag_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/spectral/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/queue_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/user_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/compat/__init__.py')
# Configuration: 6e760a068cbc11a6cc46e726655f369e4c10c58a67d5a62b8baed49245ffc3a5
# Execution platform: @local_execution_config_platform//:platform
Traceback (most recent call last):
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 28, in <module>
    from tensorflow.core.framework import function_pb2
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/core/framework/function_pb2.py"", line 5, in <module>
    from google.protobuf.internal import builder as _builder
ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/home/ubuntu/.local/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/ubuntu/repos/mirror-tensorflow/tensorflow/lite/python/BUILD:72:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/0ef2ae1c374389fefaed577dece28985/execroot/org_tensorflow && \\
  exec env - \\
    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-5.3.0-linux-arm64/bin:/home/ubuntu/.vscode-server/bin/ee2b180d582a7f601fa6ecfdad8d9fd269ab1884/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/bin:/home/ubuntu/bin \\
    PYTHON_BIN_PATH=/usr/bin/python3 \\
    PYTHON_LIB_PATH=/usr/lib/python3.10/dist-packages \\
    TF2_BEHAVIOR=1 \\
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/aarch64-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/aarch64-opt/bin/tensorflow_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=static --loading=default bazel-out/aarch64-opt/bin/tensorflow/_api/v2/v2.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/decorator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/dispatch/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/interim/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/distribute/multi_process_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/eager_context/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/function/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/monitoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/smart_cond/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/test/combinations/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tf2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/saved_model/load/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__internal__/tracking/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/__operators__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/autodiff/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/coordinator/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/partitioners/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/distribute/experimental/rpc/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dtensor/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/numpy/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/tensorrt/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/experimental/dlpack/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/client/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/profiler/experimental/server/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/summary/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/types/experimental/distributed/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/__internal__/types/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/app/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/audio/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/autograph/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/bitwise/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/optimizer/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/config/threading/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/data/experimental/service/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/debugging/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/cluster_resolver/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distribute/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/distributions/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/dtypes/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/errors/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/experimental/extension_type/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/feature_column/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/gfile/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/graph_util/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/image/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/queue/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/initializers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/layers/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/linalg/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lite/experimental/microfrontend/python/ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/logging/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/lookup/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/losses/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/manip/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/math/special/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/metrics/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mixed_precision/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/mlir/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nest/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/nn/rnn_cell/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/profiler/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/python_io/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/quantization/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/ragged/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/random/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/resource_loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/strings/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/builder/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/loader/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/main_op/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/signature_def_utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/tag_constants/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/saved_model/utils/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sets/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/signal/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sparse/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/spectral/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/summary/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/sysconfig/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/test/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/embedding/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/tpu/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/train/queue_runner/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/types/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/user_ops/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/version/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/xla/experimental/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v1/compat/v2/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v1/compat/__init__.py bazel-out/aarch64-opt/bin/tensorflow/_api/v2/compat/v2/compat/v2/compat/__init__.py')
# Configuration: 6e760a068cbc11a6cc46e726655f369e4c10c58a67d5a62b8baed49245ffc3a5
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 1090.456s, Critical Path: 327.25s
INFO: 16173 processes: 1835 internal, 14338 local.
FAILED: Build did NOT complete successfully
```
</details>"
tensorflow/tensorflow,2023-03-22 13:56:04,feature,How to check for NoneTensorSpec?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a dataset that returns `None` values. I would like to programatically find and replace the `NoneTensorSpec` values of the spec into a custom value (`None`), but I cannot find `NoneTensorSpec` in the public API and am not sure what the best way to filter for it is.
```


### Standalone code to reproduce the issue

```shell
ds = tf.data.Dataset.from_tensors((3, None))
print(ds.element_spec)
```


### Relevant log output

```shell
`(TensorSpec(shape=(), dtype=tf.int32, name=None), NoneTensorSpec())`
```
</details>"
tensorflow/tensorflow,2023-03-09 08:11:52,feature,Experimental feature support for TFLite selective build ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11-tflite-android

### Custom Code

No

### Current Behaviour?

I have followed [the tutorial](https://www.tensorflow.org/lite/examples/on_device_training/overview) and made my TextCNN model have the ability of on-device-training. It works well with the prebuilt library for tensorflow-lite.

When trying to [reduce the binary size](https://www.tensorflow.org/lite/guide/reduce_binary_size) 
with the docker environment I found that the Java API such as `Interpreter#runSignature` is a part of experimental feature, which not enabled by the [tensorflow/lite/tools/build_aar.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar.sh).

I have tried to add `experimental = True` to `tflite_custom_android_library()` function on line 73 of the `build_aar.sh` and rebuild, got the `tensorflow-lite.aar` and `tensorflow-lite-select-tf-ops.aar`. 

It seems that `tensorflow-lite.aar` does have the experimental feature code and works. But `tensorflow-lite-select-tf-ops.aar` failure to run on Android device with error:

```
java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZNK6google8protobuf7Message11GetTypeNameEv"" referenced by ""/data/app/*/lib/arm64/libtensorflowlite_flex_jni.so""...
```

I could not find any flag like `experimental = True` for select-tf-ops to enable. Is it possible to enable experimental features for TFLite selective build?


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-03-02 01:48:09,feature,disable linkage to nativewindow when API_LEVEL<26,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

latest/nightly

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

clang 9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
in file `tensorflow/lite/delegates/gpu/build_defs.bzl` line 12 it returns a link option of ""-lnativewindow"" by `return [""-lnativewindow""]`, but this option is only available for ndk api_level >=26. 
would be nice if a condition can be set here with something like:

if api_level >=26:
    return [""-lnativewindow""]
else:
    return []
```
```


### Standalone code to reproduce the issue

```shell
set api_level=21, and build tflite_gpu_delegation it will fail with `cannot find -lnativewindow`
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-02-17 10:23:10,feature,Feature request: tf.gather which returns 0 on invalid indices,"This is about `tf.gather`. The wanted behavior could be added directly, or maybe via a new option, or maybe via a separate op.

Actually, on GPU (at least where I tested it), the behavior of `tf.gather` is already as I want it:

- It returns 0 for invalid indices.
- I think (not really verified): gradients for those 0s back to the `param` would go nowhere. (This is important though.)

On CPU, the current behavior is:

- An exception is raised.

I basically want the same behavior on CPU as we currently have it on GPU.

My current workaround is sth like:
```python
# need to extend param such that we can lead gradients to nowhere
zeros = tf.zeros([tf.shape(param)[i] if i != axis else 1 for i in range(param.ndim)], dtype=param.dtype)
param = tf.concat([param, zeros], axis=axis)
indices = tf.where(indices >= 0 and indices < tf.shape(param)[axis], indices, tf.shape(param)[axis] - 1)
gather = tf.gather(param, indices, axis=axis)
```
"
tensorflow/tensorflow,2023-02-07 16:35:06,feature,Custom trianing with overriding the `fit` method single or multiple devices.,"### Current Behaviour?

In the official blog post, [HERE](https://www.tensorflow.org/tutorials/distribute/custom_training), it is discussed about the process for single device and multiple device training using `Keras` API. But I've found it quite ambigous to understand the right procedue when you want to do both, [overriding the `fit` method](https://keras.io/guides/customizing_what_happens_in_fit/). It's like combination of custom training loop + using high-level API (`fit`). For example, in custom training loop, it's suggested as follows 

```
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)
```

or ephasize to consider during [loss](https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function) calculation. 

or consider the following also while custom training loop?

```
# `run` replicates the provided computation and runs it
# with the distributed input.
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

@tf.function
def distributed_test_step(dataset_inputs):
  return strategy.run(test_step, args=(dataset_inputs,))
```

And it is not clear if we need to do when we combine both (cutom training + fit method).

There are many, for example [this](https://github.com/keras-team/tf-keras/issues/301), that is, if mixed precision is enabled, should we use `LossScaleOptimizer` and `optimizer.get_scaled_loss(loss)`  and `optimizer.get_unscaled_gradients(gradients)` in the `train_step` or `compile` method would do the job? But the [official documentation](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer) talks about normal fit and custom loop training cases. In case of custom loop, it's suggested to wrap the optimizer and scale the loss and gradient but what about the combination of fit and custom loop (overriding train_step)? Does it sill need to wrap the optimizer and scale the loss and gradient or it will be handled by the API?


### Relevant log output"
tensorflow/tensorflow,2023-01-27 12:57:33,feature,tf equivalent of numpy.view(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS Ventura 13.0

### Mobile device

_No response_

### Python version

3.10.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is there an equivalent to numpy's `.view()` function in tensorflow?

https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-01-19 04:14:29,feature,Feature Request tf.data.Dataset.from_tensor_slices with np.memmap,"
I'm interested in being able to do the following
```python
arr = np.memmap('train.bin', dtype=np.uint16)
ds = tf.data.Dataset.from_tensor_slices(arr)
```
without loading the entire array `arr` into memory."
tensorflow/tensorflow,2023-01-13 10:00:15,feature,Update curl to 7.87.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Security Vulnerabilities fixed in curl 7.87.0
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-01-11 09:29:42,feature,protobuf 4 is not supported,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Fedora Linux 37 (Xfce)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I try to use tensorflow with protobuf newer than version 3 I get this:


$ poetry add 'tensorflow==2.11.0' 'protobuf>=4'

Updating dependencies
Resolving dependencies... (0.2s)

Because tensorflow-issue depends on tensorflow (2.11.0) which depends on protobuf (>=3.9.2,<3.20), protobuf is required.
So, because tensorflow-issue depends on protobuf (>=4), version solving failed.


The reason for this is:

https://github.com/tensorflow/tensorflow/blame/d0dc98c5c8f5deea2447a3405af20f5fb245a561/tensorflow/tools/pip_package/setup.py#L100-L107

My expectation was that tensorflow supports more recent versions of protobuf.
```


### Standalone code to reproduce the issue

```shell
mkdir -vp /var/tmp/tensorflow-issue
poetry -C /var/tmp/tensorflow-issue init --no-interaction
poetry -C /var/tmp/tensorflow-issue add 'tensorflow==2.11.0' 'protobuf>=4'
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-12-29 15:08:42,feature,Feature request for object detection android app,"Hello,

I deployed my Mobilenet trained model into your object detection android app and the detection looks good.
https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

I would like to add two features to your app if possible:
1) Show on the phone screen number of detected objects of class X
2) Show on the phone screen the (x, y) coordinated of bounding box for detected objects of class X

Can you please help me to figure out how I can add these two features in the app. Look forward to hearing from you! "
tensorflow/tensorflow,2022-12-12 14:06:12,feature,model with lstmcell gives different results before and after tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OpenSuse TumbleWeed 
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.10

### 2. Code

When I attempt to convert a working tensorflow model to tflite, the model converts, but the results obtained are different
See code below:
```
import tensorflow as tf
from tensorflow.keras.layers import (
    Concatenate,
    Conv2D,
    Conv2DTranspose,
    Dropout,
    BatchNormalization)
import numpy as np

inp = tf.keras.Input([1,192], batch_size = 1)
state_h = tf.keras.Input([192], batch_size = 1)
state_c = tf.keras.Input([192], batch_size = 1)
num_channels = 24

xT = BatchNormalization()(inp)

lstm_in = tf.keras.activations.tanh(xT)
x, new_states = tf.keras.layers.LSTMCell(xT.shape[2])(lstm_in[:,0,:], states = [state_h, state_c])
new_state_h = new_states[0]
new_state_c = new_states[1]
out = x + xT

my_model = tf.keras.Model(inputs=[inp, state_h, state_c],outputs = [ out, 
                                                                       new_state_h,
                                                                       new_state_c])

#save model and create tflite model
my_model.save('lstm_test', include_optimizer = False)
converter = tf.lite.TFLiteConverter.from_saved_model('lstm_test')
converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
        ]

tflite_model = converter.convert()

inpt = tf.random.normal([1,10,192])
init_state_h = tf.zeros([1,192])
init_state_c = tf.zeros([1,192])
init_state_h_tfl = tf.zeros([1,192])
init_state_c_tfl = tf.zeros([1,192])

#set up tflite interpreter
interpreter = tf.lite.Interpreter(model_content = tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print('input_details')
print(input_details)
print('output details')
print(output_details)

for k in range(10):
    outpt, st_h, st_c = my_model.predict([tf.expand_dims(inpt[:,k,:], axis=1), init_state_h, init_state_c])
    init_state_h = st_h
    init_state_c = st_c
    interpreter.set_tensor(input_details[0]['index'], tf.expand_dims(inpt[:,k,:], axis = 1))
    interpreter.set_tensor(input_details[1]['index'], init_state_h_tfl)
    interpreter.set_tensor(input_details[2]['index'], init_state_c_tfl)
    interpreter.invoke()
    outpt_tfl = interpreter.get_tensor(output_details[0]['index'])
    st_h_tfl = interpreter.get_tensor(output_details[1]['index'])
    st_c_tfl = interpreter.get_tensor(output_details[2]['index'])
    init_state_h_tfl = st_h_tfl
    init_state_c_tfl = st_c_tfl
    np.testing.assert_almost_equal(outpt, outpt_tfl, decimal = 5)
```

### 3. Failure after conversion
When the above code is ran, it can be seen that there are considerable differences between the tflite model and the original tensorflow model.
"
tensorflow/tensorflow,2022-12-05 13:25:59,feature,Use cl_khr_integer_dot_product if available,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

707f1dcba4c8 (HEAD -> master, origin/master, origin/HEAD) compat: Update forward compatibility horizon to 2022-12-05

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Are there any plans to make use of cl_khr_integer_dot_product to speed up execution on GPUs that support it?
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-11-27 01:01:32,feature,The parameter indices in tf.gather are supported int16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF 2.10

### Custom Code

Yes

### OS Platform and Distribution

windows 11

### Mobile device

_No response_

### Python version

3.8.15

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.3 cuDNN cuDNN 8.6.0

### GPU model and memory

RTX3060

### Current Behaviour?

```shell
tf.gather(params, indices, validate_indices=None, axis=None, batch_dims=0, name=None) The parameter type requirement for indices in the official document is Tensor, and then the type is int32, int64. For this situation, I deliberately use Unreasonable types are tested. When indices are set to 1.0, it is a float type, but the exception thrown says that this is not int16, int32, int64, which is one more int16 than the official document. Therefore, it is recommended to add int16 to the document. Go up or remove int16 from the source code.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

    with tf.device('/CPU'):
        params = tf.random.uniform([1024, 2], dtype=tf.float64)
        indices = 1.0
        out = tf.gather(params=params, indices=indices)
```


### Relevant log output

```shell
result:tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'Tindices' of float is not in the list of allowed values: int16, int32, int64; NodeDef: {{node GatherV2}}; Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=batch_dims:int,default=0; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]> [Op:GatherV2]
```
</details>"
tensorflow/tensorflow,2022-11-24 11:28:36,feature,Feature request (TensorFlow Lite): Strip unneeded kernels when building Flex ops library on platforms other than Android,"### System information

-   **Have I written custom code**: no
-   **OS Platform and Distribution**: macOS Monterey 12.6
-   **TensorFlow installed from**: source
-   **TensorFlow version**: 2.10.0
-   **Python version**: 3.10.8
-   **Bazel version**: 5.1.1
-   **GCC/Compiler version**: Apple clang 14.0.0 (clang-1400.0.29.202)
-   **CUDA/cuDNN version**: Not using CUDA
-   **GPU model and memory**: Not using GPU acceleration
-   **Exact command to reproduce**: Feature request, no 

### Describe the problem

This is a feature request.

TensorFlow Lite 2.10 provides a [tool](https://www.tensorflow.org/lite/guide/ops_select#building_the_android_aar) that strips unneeded kernels when building the Flex library, but this is for Android only. On other platforms, the whole set of kernels gets built in, and the flex binary ends up being hundreds of MBs in size. Would you consider implementing a similar tool for other platforms, and, if not, would you accept PRs for it?"
tensorflow/tensorflow,2022-11-20 15:45:02,feature,Tensorflow lite dynamic input,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 11
- TensorFlow installation (pip package or built from source):
pip 2.10


### 2. Code
inputs = tf.keras.layers.Input(shape=(None, None, 3), name='input_image')
model = tf.keras.Model(inputs, inputs)

input_image - [(None, None, None,  3)]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

input_image - [1 1 1 3]"
tensorflow/tensorflow,2022-11-09 18:31:22,feature,Feature request: tflite with GPU and Hexagon delegate support on Linux ARM64 platform,"From this link: https://www.tensorflow.org/lite/guide/build_arm , GPU delegate is ""only available for Android"" if I use bazel, so I have to use cmake in order to get GPU delegate.
However from this link: https://www.tensorflow.org/lite/guide/build_cmake , there's only TFLITE_ENABLE_GPU, no TFLITE_ENABLE_HEXAGON. I tried to use it in cmake and it's not recognized.
Wondering if tensorflow can either add GPU delegate to bazel build, or add hexagon to cmake? Which one is more appropriate, if I also want to use our own toolchain?"
tensorflow/tensorflow,2022-11-03 23:47:39,feature,Request Tensorflow Docker container be expanded to support running all models.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm creating a Docker container in which I want to run the TensorFlow object detection code provided in the model zoo. I expected to be able to download the TensorFlow Docker container, clone the relevant source, and be off to the races.

Instead, I found that I was missing a lot of required packages. I found references which suggested to use the provided setup script located here:

https://github.com/tensorflow/models/blob/master/research/object_detection/packages/tf2/setup.py
Unfortunately, this led to a number of strange conflicts which I determined to be due to differing versions of opencv-python and opencv-python-headless, neither of which I'd manually installed. Everything was pulled in by the script and associated dependencies.

Given that both the TensorFlow Dockerfile and the models setup script are forcing versions of packages less than head-of-line, I request that a fuller and known-working Docker image be provided.

Thank you.
```


### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/models/blob/master/research/object_detection/packages/tf2/setup.py
```


### Relevant log output

```shell
pip3 list would end up showing
opencv-python                 4.6.0.66
opencv-python-headless        4.5.2.52

This results in the following backtrace:
Traceback (most recent call last):
  File ""model_main_tf2.py"", line 31, in <module>
    from object_detection import model_lib_v2
  File ""/opt/models/research/object_detection/model_lib_v2.py"", line 29, in <module>
    from object_detection import eval_util
  File ""/opt/models/research/object_detection/eval_util.py"", line 36, in <module>
    from object_detection.metrics import lvis_evaluation
  File ""/opt/models/research/object_detection/metrics/lvis_evaluation.py"", line 23, in <module>
    from lvis import results as lvis_results
  File ""/usr/local/lib/python3.8/dist-packages/lvis/__init__.py"", line 5, in <module>
    from lvis.vis import LVISVis
  File ""/usr/local/lib/python3.8/dist-packages/lvis/vis.py"", line 1, in <module>
    import cv2
  File ""/usr/local/lib/python3.8/dist-packages/cv2/__init__.py"", line 181, in <module>
    bootstrap()
  File ""/usr/local/lib/python3.8/dist-packages/cv2/__init__.py"", line 175, in bootstrap
    if __load_extra_py_code_for_module(""cv2"", submodule, DEBUG):
  File ""/usr/local/lib/python3.8/dist-packages/cv2/__init__.py"", line 28, in __load_extra_py_code_for_module
    py_module = importlib.import_module(module_name)
  File ""/usr/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/usr/local/lib/python3.8/dist-packages/cv2/mat_wrapper/__init__.py"", line 33, in <module>
    cv._registerMatType(Mat)
AttributeError: partially initialized module 'cv2' has no attribute '_registerMatType' (most likely due to a circular import)
```
</details>"
tensorflow/tensorflow,2022-10-28 15:43:59,feature,Can't run in Python 3.11.0 env.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.11.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Error when run in python 3.11.0 env!
```


### Standalone code to reproduce the issue

```shell
None
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-10-25 07:40:36,feature,Feature request - support tif images in image_dataset_from_directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Mint

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently, `image_dataset_from_directory` supports only  `Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')`

So, if you have tif images it won't find them ` No images found in directory `
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-10-10 07:33:14,feature,when will import the alpha tensor's fast matrix mutiplication.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://www.nature.com/articles/s41586-022-05172-4
faster matrix multiplication algorithms has been Discovered by alpha tensor. when will tensorflow import these multiplicatio algorithms
```


### Standalone code to reproduce the issue

```shell
https://www.nature.com/articles/s41586-022-05172-4
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-10-05 20:32:45,feature,"is there any ways I can embed my self-implemented FP8 format (C/C++) into tensorflow, and support current ops automatically?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It looks like there is no way to include a self-implemented data format (eg. FP8) to support current ops automatically.
```


### Standalone code to reproduce the issue

```shell
It looks like there is no way to include a self-implemented data format (eg. FP8) to support current ops automatically.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-10-04 10:57:09,feature,Update the Android NDK to r25b LTS,"Currently the Android build of TensorFlow and TensorFlow Lite use an older NDK, either 19 or 21. The latest LTS release of the [Android NDK](https://developer.android.com/ndk) is r25b (the exact version is currently `25.1.8937393`), and [supports](https://github.com/android/ndk/wiki/Changelog-r25) the latest features and compilers (LLVM 14). 

See earlier NDK updates:

- https://github.com/tensorflow/tensorflow/commit/63e7c149d7817edb63adc085c9650e1659f3b699 Update TFLite to use Android NDK r18b
- https://github.com/tensorflow/tensorflow/commit/60bbb7c240580e79ad157e41c7c8058147e5085d Update the TFLite Android Dockerfile to use NDK r19c
- https://github.com/tensorflow/tensorflow/commit/1a0b21e16b08ab0bb1c15f9b5ec040bfbdee2685 Update NDK version to r19c
- https://github.com/tensorflow/tensorflow/commit/f4a65d74eb49eace09483df48b4ce2d94b6b354d [tf.lite] Update stale doc reference to recommended NDK version
- https://github.com/tensorflow/tensorflow/pull/34419"
tensorflow/tensorflow,2022-09-27 09:58:43,feature,Clarify functionality of deserialized subclassed models in docs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

tf 2.9 and 2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the `SavedModel` format documentation [here](https://www.tensorflow.org/guide/keras/save_and_serialize#how_savedmodel_handles_custom_objects), it says:


> In the absence of the model/layer config, the call function is used to create a model that exists like the original model which can be trained, evaluated, and used for inference.

and:

> The first loaded model is loaded using the config and CustomModel class. The second model is loaded by dynamically creating the model class that acts like the original model.

I am finding that when a subclassed model is saved in the `SavedModel` format, upon re-loading it can be trained, evaluated, and used for inference in the same way as the original model. However, other functionality such as the custom `from_config` and `get_config` methods do not work. For example, attempting to clone the loaded model with `loaded_model_clone = loaded_model.from_config(loaded_model.get_config())` leads to errors.

I assume this is because loading a `CustomModel` without passing `custom_objects` (or registering it) gives a `keras.saving.saved_model.load.CustomModel`, which does not contain the full functionality of the original `CustomModel`? It would be helpful if the docs clarified the functional limitations of a `SavedModel` loaded without the `custom_objects` being provided.

The documentation (and example) in [SavedModel format](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format) also seems to contradict the guidance in [Custom Objects](https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects). In the former, it suggests a custom `SavedModel` can be loaded without providing the `custom_objects`, albeit with limited functionality, whereas the latter suggests you should always register or pass in the `custom_objects` (i.e. ""_Additionally, you should use register the custom object so that Keras is aware of it._"").


### Standalone code to reproduce the issue

Extended from example in [SavedModel format](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format).

Colab notebook gist: https://gist.github.com/ascillitoe/42702d4cab382880b55ab7344c694a23

Code:

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

class CustomModel(keras.Model):
    def __init__(self, hidden_units):
        #super(CustomModel, self).__init__()
        super().__init__()
        self.hidden_units = hidden_units
        self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]

    def call(self, inputs):
        x = inputs
        for layer in self.dense_layers:
            x = layer(x)
        return x

    def get_config(self):
        return {""hidden_units"": self.hidden_units}

    @classmethod
    def from_config(cls, config):
        return cls(**config)


model = CustomModel([16, 16, 10])
# Build the model by calling it
input_arr = tf.random.uniform((1, 5))
outputs = model(input_arr)
model.save(""my_model"")

# Option 1: Load with the custom_object argument.
loaded_1 = keras.models.load_model(
    ""my_model"", custom_objects={""CustomModel"": CustomModel}
)

# Option 2: Load without the CustomModel class.

# Delete the custom-defined model class to ensure that the loader does not have
# access to it.
del CustomModel

loaded_2 = keras.models.load_model(""my_model"")
np.testing.assert_allclose(loaded_1(input_arr), outputs)
np.testing.assert_allclose(loaded_2(input_arr), outputs)

print(""\\nInitial Save/Load\\n################################################"")
print(""Original model:"", model)
print(""Model Loaded with custom objects:"", loaded_1)
print(""Model loaded without the custom object class:"", loaded_2)

print(""\\nCloning\\n################################################"")
cloned_loaded_1 = loaded_1.__class__.from_config(loaded_1.get_config())
print(""Clone of Model Loaded with custom objects:"", cloned_loaded_1)

cloned_loaded_2 = loaded_2.__class__.from_config(loaded_2.get_config())
print(""Clone of Model loaded without the custom object class:"", cloned_loaded_2)
```


### Relevant log output

```shell
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.

Initial Save/Load
###################################
Original model: <__main__.CustomModel object at 0x7fbf613e1950>
Model Loaded with custom objects: <__main__.CustomModel object at 0x7fbf60f34e10>
Model loaded without the custom object class: <keras.saving.saved_model.load.CustomModel object at 0x7fbf60ea7b10>

Cloning
################################################
Clone of Model Loaded with custom objects: <__main__.CustomModel object at 0x7fbf613e89d0>
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)
   3049                 try:
-> 3050                     model = cls(**config)
   3051                 except TypeError as e:

4 frames
TypeError: ('Keyword argument not understood:', 'hidden_units')

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)
   3051                 except TypeError as e:
   3052                     raise TypeError(
-> 3053                         ""Unable to revive model from config. When overriding ""
   3054                         ""the `get_config()`, make sure that the returned ""
   3055                         ""config contains all items used as arguments in the ""

TypeError: Unable to revive model from config. When overriding the `get_config()`, make sure that the returned config contains all items used as arguments in the constructor to <class 'keras.saving.saved_model.load.CustomModel'>, which is the default behavior. You can override this default behavior by defining a `from_config` method to specify how to create an instance of CustomModel from the config. 

Error encountered during deserialization:
('Keyword argument not understood:', 'hidden_units')
```
</details>"
tensorflow/tensorflow,2022-09-23 13:06:06,feature,TF Lite enable Flex delegate C++ API (with CMake of Bazel),"**System information**
- Linux Ubuntu 18.04):
- TensorFlow installed from source:
- TensorFlow version commit 47e07ba0d68c55dba62bff5b8486291086840097:

Hi, I want to run simple conv2d layer training using C++ API of TF Lite. I’ve used examples/minimal project as reference. Calling trainer->invoke() leads to next error:

`ERROR: TensorFlow Lite Error: Select TensorFlow op(s), included in the given model is(are) not supported by this interpreter. Make sure you apply/link Flex delegate before inference. For the Android, it can be resolved by adding “org.tensorflow:tensorflow-lite-select-tf-ops” dependency…`
`ERROR: Node number 48 (FlexConv2DBackpropFilter) failed to prepare.`

I didn’t find any good tutorial on Flex delegate. Please, tell me, how to build TF Lite with Flex using CMake and how to make this interpreter use Flex’s implementation of this node. Thanks




"
tensorflow/tensorflow,2022-09-20 13:11:21,feature,A flag to enable the previous RNG behavior for `tf.keras.initializers`,"### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

TF 2.10

### Custom Code

No

### Current Behaviour?

According to [release note](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0), RNG behavior change for `tf.keras.initializers` since `TF 2.10`.

In particular, `tf.random.set_seed(seed)` is not enough to get the same model weights - we need to set explicit seeds in the initializers.

In general, this is NOT an issue. However, for **testing purpose**, this makes things much more difficult. For example, let's say we have `MyCustomModel`, implemented with layers without specifying any initializer (therefore, no explicit seed with initializer).

However, if we want to have a CI test that verifies the model produces the same result, it is impossible with TF 2.10 due to the new change. With TF < 2.10, we can set `tf.random.set_seed(seed=0)`, and the model weights will be the same, so we can check if the output matches the expected values.

It would be great if there is a way to have previous behavior for **testing purpose**."
tensorflow/tensorflow,2022-09-16 18:52:35,feature,"log, pow, div unsupported in 16x8 precision","It seems these ops are supported in 8x8 mode, but not in 16x8. The latter is very important for our organization, as it saves a lot of rounding error over 8x8. Is full 16x8 support on the current roadmap? If not, could we help to contribute this feature?

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip tf-nightly
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0-dev20220916 

### 2. Code

```
import numpy as np
import tensorflow as tf
import keras

def data_generator():
    for x in range(1, 100):
        yield [np.array(x).astype(np.float32)]

def test_layer(layer):
    model = tf.keras.Model(inputs=[in_tensor], outputs=[layer])
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.representative_dataset = data_generator
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
    ]
    converter.inference_input_type = tf.int16
    converter.inference_output_type = tf.int16
    try:
        tflite_model = converter.convert()
    except Exception as e:
        print(""Received exception:\\n%s"" % str(e))

in_tensor = tf.keras.Input(shape=(1,))
test_layer(tf.math.log(in_tensor))
test_layer(in_tensor ** 3)
test_layer(in_tensor / in_tensor)
```

### 3. Failure after conversion

Conversion fails, with the following error messages:

> Quantization to 16x8-bit not yet supported for op: 'LOG'.
> Quantization to 16x8-bit not yet supported for op: 'POW'.
> Quantization to 16x8-bit not yet supported for op: 'DIV'.
"
tensorflow/tensorflow,2022-09-09 02:50:17,feature,Updating TFLite ConvTranspose to make it support dynamic inputs.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubunutu 21.02
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):  tensorflow 2.9.1

**Standalone code to reproduce the issue** 
Converting any ConvTranspose layer from ONNX to TensorflowLite.

**Any other info / logs**
The ONNX does not have requirement for output_shape argument for ConvTranspose layer.
[https://github.com/onnx/onnx/blob/main/docs/Operators.md#ConvTranspose]()
since that can be calculated. 

But in tflite the ConvTranspose layer requires mandatory requirement for output_shape.
[https://github.com/tensorflow/tensorflow/blob/18960c44ad3f5219c22dca55f842912dbce78a07/tensorflow/lite/kernels/transpose_conv.cc#L251-L253]()

So because of this the dynamic reshape happens in wrong way in tflite ConvTranpose, but for ONNX it can be done.
Is it possible to remove the output_shape tensor requirement in tflite and similarly be updated in tflite convertor? or Any other workaround will be appreciated.

"
tensorflow/tensorflow,2022-08-31 20:42:53,feature,Not enough flexibility to choose actual accelerator in CoreML delegate,"Only [two options exist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate.h) when creating a CoreML delegate:
```
typedef enum {
  // Create Core ML delegate only on devices with Apple Neural Engine.
  // Returns nullptr otherwise.
  TfLiteCoreMlDelegateDevicesWithNeuralEngine,
  // Always create Core ML delegate
  TfLiteCoreMlDelegateAllDevices
} TfLiteCoreMlDelegateEnabledDevices;
```

These don't quite fit what's possible with CoreML, [as documented here](https://developer.apple.com/documentation/coreml/mlcomputeunits?changes=latest_major&language=objc). Ideally, we should have a way to specify whether we want to use any accelerator (including neural engine), CPU only, or CPU and GPU (excluding neural engine). iOS 16 will also introduce a way to use CPU and neural engine, i.e. excluding GPU.

Currently the CoreML delegate always uses `MLComputeUnitsAll` in [coreml_executor.mm](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_executor.mm).
"
tensorflow/tensorflow,2022-08-30 20:33:41,feature,Please make different versions of Tensorflow compatible with each other.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

All versions

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow X.11 is completely incompatible with Tensorflow X.12.  If I try to run a colab notebook, and the notebook was written using Tensorflow X.11, but the newest version of Tensorflow is now X.12 or later, then the notebook will not work.  It would be nice to have some consistency between versions so I don't have to know the version of Tensorflow that the notebook was written in to run it.
```


### Standalone code to reproduce the issue

```shell
Literally use any notebook more than a year old.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-08-24 10:33:30,feature,Unable to free memory allocated by OpKernelContext,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The OpKernelContext used in custom op's compute method doesn't have an option to free the temporary allocated by it. 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h doesn't have an API to free the memory allocated by ""allocate_temp"" method.

Do we have any solution to free the memory allocated using allocate_temp?
```


### Standalone code to reproduce the issue

```shell
Its a request
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-08-19 15:01:51,feature,tf.distribute.MirroredStrategy for asynchronous training,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Tensorflow Version

2.8.1

### Python version

3.8.13

### CUDA/cuDNN version

11.8

### Use Case

I need to run multiple asynchronous copies of the same model on different slices of the dataset (e.g. with bootstrap sampling). There's no *good* way to do this in keras api that I'm aware of, although a couple of hacks exist. Would this use case be feasible with tf.distribute?

### Feature Request

`tf.distribute.MirroredStrategy` is a synchronous, data parallel strategy for distributed training across multiple devices on a single host worker.

Would it be possible to modify this strategy to allow for asynchronous training of all model replicas, without computing the average gradient over all replicas to update weights? In this case each replica would need its own un-mirrored copy of model weights, and the update rule would depend only on the loss and gradients of each replica.

Thanks"
tensorflow/tensorflow,2022-08-16 07:45:48,feature,LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported,"When using a tflite model (8-bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate.

**System information**
- OS Platform and Distribution): Android 10, NDK R21e
- TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'
- TensorFlow version (or github SHA if from source):  2.9.1


**Output of Tensorflow library when running an inference with a model that includes 'LeakyRelu' activation function**

```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
INFO: TfLiteHexagonDelegate delegate: 8 nodes delegated out of 53 nodes with 1 partitions.
INFO: Replacing 8 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
```
As we can see below when replacing 'LeakyRelu' operation by 'Relu', then the TF Hexagon delegate can process the whole DNN graph.

**Output of Tensorflow library when running an inference with a model that includes 'Relu' activation function**

```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
INFO: TfLiteHexagonDelegate delegate: 53 nodes delegated out of 53 nodes with 1 partitions.
INFO: Replacing 53 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
```

Would it be possible to implement 'LeakyRelu' in TF Hexagon Delegate ?"
tensorflow/tensorflow,2022-08-08 14:03:12,feature,Add bazel rule to selectively build TFLite by ops instead of model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9, all

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Per Tensorflow Lite's documentation on reducing binary size (https://www.tensorflow.org/lite/guide/reduce_binary_size), one way to reduce TFLite's binary size is to build it with a reduced set of ops based on a list of models on disk. It'd be nice if instead of a list of models, users could instead specify a list of ops that they want included in the binary (for both android and iOS). This is useful in environments where you may not have the models immediately available, but know the set of ops the group of models will want.



### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-07-30 05:16:45,feature,TensorRT max number of engines,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

source

### Tensorflow Version

TF2.9.1 + TensorRT

### Custom Code

No

### OS Platform and Distribution

20.04 ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
thank you for adding support for Tensor-RT python API. 
It seems like TF-TRT has a limit on the number of engines it can create, and it seems to be 20. 

I see that ~65% ops are converted, when I expect more to be converted. Even for larger models the number of engines is exactly 20. 

[*] Total number of TensorRT engines: 20
[*] % of OPs Converted: 65.84% [688/1045]
```

Is there some documentation on setting max number of TensorRt engines.
```


### Standalone code to reproduce the issue

```shell
Apologies, the model itself is not easy to share, but the conversion follows standard steps:



    params = params._replace(
        precision_mode=precision_mode,
        max_workspace_size_bytes=2 << 32,  # 8,589,934,592 bytes
        maximum_cached_engines=100,
        minimum_segment_size=3,
        allow_build_at_runtime=True,
    )
    converter = trt.TrtGraphConverterV2(
        input_saved_model_dir=path,
        conversion_params=params,
    )
    converter.convert()
```
```


### Relevant log output

```shell
[*] Total number of TensorRT engines: 20
[*] % of OPs Converted: 65.84% [688/1045]
```
```
</details>"
tensorflow/tensorflow,2022-07-26 02:00:42,feature,How to build tensorflow-lite-c enabling flex option using CMake,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Ubuntu 20.04.4 LTS

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

gcc 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'd like to use the tensorflow-lite model created in python in C++.
But, I'm facing a problem where I can't call the relu activation included in the tflite file on C++.

The python code I wrote to create the tflite model is as follows.
I am using tensorflow 2.9.1 Version in Ubuntu environment by directly building using CMake.

By the way, when I try to use relu for keras activation, I get the below error.
I did a google search and found that it was caused by lack of flex library.
However, I couldn't find a way to build using CMake instead of bazel in the Documentation below.
- https://www.tensorflow.org/lite/guide/ops_select#cc

Is there currently any way to support this? or i can use relu activation
or Is there any other way? (using not keras relu but another relu activation)
```


### Standalone code to reproduce the issue

```shell
class PredictedDestination(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Dense(32, input_shape=(11, ), name='input'),
             tf.keras.layers.Dense(16, activation=tf.nn.relu, name='dense_1'),
             tf.keras.layers.Dense(8, activation=tf.nn.relu, name='dense_2'),
             tf.keras.layers.Dense(4, activation=tf.nn.relu, name='dense_3'),
            tf.keras.layers.Dense(2),
        ])
    
        self.model.compile(
            optimizer='sgd',
            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))

    # The `train` function takes a batch of input images and labels.
    @tf.function(input_signature=[
      tf.TensorSpec([None, 11], tf.float32),
      tf.TensorSpec([None, 2], tf.float32),
    ])
    def train(self, x, y):
        epochs = 100
        for i in range(epochs):
            with tf.GradientTape() as tape:
              prediction = self.model(x)
              loss = self.model.loss(y, prediction)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
            result = {""loss"": loss}
        return result

    @tf.function(input_signature=[
      tf.TensorSpec([None, 11], tf.float32),
    ])
    def predict(self, x): 
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        print(probabilities)
        print(logits)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }

model = PredictedDestination()

SAVED_MODEL_DIR = ""predicted_destination_model""

tf.saved_model.save(
    model,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            model.train.get_concrete_function(),
        'predict':
            model.predict.get_concrete_function(),
    })

converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()

open('predicted_destination.tflite', 'wb').write(tflite_model)
```


### Relevant log output

```shell
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
```
</details>"
tensorflow/tensorflow,2022-07-21 15:05:00,feature,Load tf lite model to XLA ,"Hi,
I would like to know if it possible to take a TF lite model (quarantined) and compile into executable code using XLA/JIT. In this lick you have an example about how to create an executable given an tf model : https://www.tensorflow.org/xla/tfcompile. In other words can I use ""tfcompile"" with tf lite quarantined models?
"
tensorflow/tensorflow,2022-07-20 09:01:02,feature,Tflite ios batch_size greater than 1 not working,"My model was trained with Pytorch. Convert this model to tflite model with input shape {1, 3, 256, 256} and is working as expected. But change this model to input shape {4, 3, 256, 256} the inference result is not correct.
"
tensorflow/tensorflow,2022-07-15 09:47:15,feature,Support INT8 for tf.split and tf.one_hot,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 Enterprise 20H2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow will not work with int8 tensors when using one_hot or splitv.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

int8_tensor = tf.constant([[1, 2],
                           [3, 4],
                           [5, 6]], dtype=tf.int8)
try:
    tf.one_hot(int8_tensor, 3)
except Exception as e:
    print(e)
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'Tlen' of int8 is not in the list of allowed values: int32, int64
	; NodeDef: {{node SplitV}}; Op<name=SplitV; signature=value:T, size_splits:Tlen, split_dim:int32 -> output:num_split*T; attr=num_split:int,min=1; attr=T:type; attr=Tlen:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]> [Op:SplitV] name: split

tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'TI' of int8 is not in the list of allowed values: uint8, int32, int64
	; NodeDef: {{node OneHot}}; Op<name=OneHot; signature=indices:TI, depth:int32, on_value:T, off_value:T -> output:T; attr=axis:int,default=-1; attr=T:type; attr=TI:type,default=DT_INT64,allowed=[DT_UINT8, DT_INT32, DT_INT64]> [Op:OneHot]
```
</details>"
tensorflow/tensorflow,2022-07-10 10:03:45,feature,__array_interface__ support?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi. Over at https://github.com/python-pillow/Pillow, our Image class has the `__array_interface__` attribute, to support converting Pillow images to NumPy. See https://numpy.org/doc/stable/reference/arrays.interface.html#object.__array_interface__

A recent discussion has revealed that TensorFlow's `reshape` method (and I have to imagine other methods as well) accepts an object that provides an `__array__` method, but not an object with an `__array_interface__` attribute.

My question - is there any interest from TensorFlow in supporting objects with `__array_interface__`?

### Standalone code to reproduce the issue

Here is code that fails with the latest version of Pillow.

```python
from PIL import Image
import tensorflow as tf

im = Image.new(""L"", (1, 1))
tf.reshape(im, (1, 1))
```

However, if my assertion that TensorFlow doesn't accept `__array_interface__` is at all in doubt, let me know and I'll put together a better example.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-06-25 13:03:14,feature,Add `cdist` - an analog of `scipy.cdist` | `torch.cdist`,"# Feature Request

Source: binary
Tensorflow Version: 2.9
Custom Code: Yes

# Current Behavior

There is no equivalent built-in function of the following two in `TensorFlow`. I need to use this method on large size embedding vectors and require a efficient vectorized implementation.

```python
scipy.spatial.distance.cdist
torch.cdist
```

The same request has been asked several times but closed without any strong reason and no response further to a new commenter. So, kindly **DO NOT CLOSE** without further discussion. 

- https://github.com/tensorflow/tensorflow/issues/30659#
- https://github.com/tensorflow/tensorflow/issues/9745#
"
tensorflow/tensorflow,2022-06-22 09:26:37,feature,Conda-forge release of tensorflow 2.x for win-64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf>=2.5

### Custom Code

No

### OS Platform and Distribution

Windows10

### Mobile device

_No response_

### Python version

>=3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The win-64 version of tensorflow on conda-forge is only available at v1.14, please make a release of v2.x, preferable >=2.5.
```


### Standalone code to reproduce the issue

```shell
conda install tensorflow -c conda-forge
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-06-13 21:42:13,feature,Add ParametricScalar Layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

TF 2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
It would be nice to add a ParametricScalar layer where learnable scalars are multiplied element-wise with the input similar to the PReLU activation function. Although this layer can be implemented as a custom layer, it is a commonly used module to merge multiple results together with learnable weights. A sample implementation is provided in the ""Standalone Code"" section.
```


### Standalone code to reproduce the issue

```shell
class ParametricScalar(keras.layers.Layer):
    """"""combine multiple activations weighted by learnable variables""""""
    def __init__(self, alpha_initializer='ones', shared_axes=None, **kwargs):
        super().__init__(**kwargs)
        self.alpha_initializer = keras.initializers.get(alpha_initializer)
        if shared_axes is None:
            self.shared_axes = None
        elif not isinstance(shared_axes, (list, tuple)):
            self.shared_axes = [shared_axes]
        else:
            self.shared_axes = list(shared_axes)

    def get_config(self):
        return {'act_set': self.act_set}

    def build(self, input_shape):
        param_shape = list(input_shape[1:])
        if self.shared_axes is not None:
            for i in self.shared_axes:
                param_shape[i - 1] = 1
        self.alpha = self.add_weight(
            shape=param_shape,
            name='alpha',
            initializer=self.alpha_initializer)
        
    def get_config(self):
        config = {
            'alpha_initializer': keras.initializers.serialize(self.alpha_initializer),
            'shared_axes': self.shared_axes
        }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def call(self, inputs):
        return inputs * self.alpha
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-06-07 11:37:24,feature,"Add SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle to allowed List (Lite Converter)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I am using Lite Converter to convert a Tensorflow Model (Random Forest from the Decision Forest Library) into a Tensorflow Lite Model.
I receive the folllowing error:  SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandleop is neither a custom op nor a flex op.
The Troubleshooting page says to add these to the 'allowed list'.
```


### Standalone code to reproduce the issue

```shell
model = tdf.keras.RandomForestModel()
model.fit(train_data)
model.save('/project/first_model')
converter = tf.lite.TFLiteConverter.from_saved_model('/project/first_model')
tflite_model = converter.convert()
```


### Relevant log output

```shell
ConverterError: <unknown>:0: error: loc(fused[""SimpleMLCreateModelResource:"", ""SimpleMLCreateModelResource""]): 'tf.SimpleMLCreateModelResource' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""SimpleMLCreateModelResource:"", ""SimpleMLCreateModelResource""]): Error code: ERROR_NEEDS_CUSTOM_OPS
<unknown>:0: error: loc(callsite(callsite(callsite(fused[""SimpleMLInferenceOpWithHandle:"", ""inference_op@__inference_call_591""] at fused[""StatefulPartitionedCall:"", ""random_forest_model/StatefulPartitionedCall@__inference__wrapped_model_596""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_833""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): 'tf.SimpleMLInferenceOpWithHandle' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""]): called from
<unknown>:0: note: loc(callsite(callsite(callsite(fused[""SimpleMLInferenceOpWithHandle:"", ""inference_op@__inference_call_591""] at fused[""StatefulPartitionedCall:"", ""random_forest_model/StatefulPartitionedCall@__inference__wrapped_model_596""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_833""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall_1""])): Error code: ERROR_NEEDS_CUSTOM_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom 
Custom ops: SimpleMLCreateModelResource, SimpleMLInferenceOpWithHandle
Details:
	tf.SimpleMLCreateModelResource() -> (tensor<!tf_type.resource>) : {container = """", device = """", shared_name = ""simple_ml_model_0c74f688-489e-46e7-aa58-983fddef4f14""}
	tf.SimpleMLInferenceOpWithHandle(tensor<?x8xf32>, tensor<0x0xf32>, tensor<0x0xi32>, tensor<0xi32>, tensor<1xi64>, tensor<1xi64>, tensor<!tf_type.resource>) -> (tensor<?x6xf32>, tensor<6x!tf_type.string>) : {dense_output_dim = 6 : i64, device = """"}
```
</details>"
tensorflow/tensorflow,2022-06-07 08:17:58,feature,Support item assignment in tensor object.,"Issue Type: Feature Request
Source: binary
Tensorflow Version: 2.9
Custom Code: Yes

# Current Behaviour?

Item assignment is not supported.

Discuss more here. https://github.com/tensorflow/tensorflow/issues/33131

# Standalone code to reproduce the issue

```python
outputs = tf.zeros(..)

for step_index in range(5):
    outputs[step_index,...] = step_index + 1
```

Please support this and make a roadmap for the next version of tensorflow. And kindly don't suggest to cast to `tf.Variable` and use `assign/assgn_add` etc or any other non-intuitive workaround."
tensorflow/tensorflow,2022-06-06 17:12:27,feature,`DistributedIterator` cannot be checkpointed because it is not trackable,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.10.0-dev20220520

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

N/A

### Python version

3.8

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

N/A

### Current Behaviour?

```shell
`DistributedIterator` cannot be checkpointed because it is not trackable
```


### Standalone code to reproduce the issue

```shell
ds = tf.data.Dataset.range(100).shuffle(8).batch(7)
ds_it = iter(ds)
print(type(ds_it))
ckpt = tf.train.Checkpoint(ds_it=ds_it)

strategy = tf.distribute.OneDeviceStrategy('cpu')
dist_ds = strategy.experimental_distribute_dataset(ds)
dist_ds_it = iter(dist_ds)
print(type(dist_ds_it))
ckpt = tf.train.Checkpoint(dist_ds_it=dist_ds_it)
```


### Relevant log output

```shell
<class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'>
<class 'tensorflow.python.distribute.input_lib.DistributedIterator'>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-cabcd820a79a> in <module>()
      8 dist_ds_it = iter(dist_ds)
      9 print(type(dist_ds_it))
---> 10 ckpt = tf.train.Checkpoint(dist_ds_it=dist_ds_it)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py in __init__(self, root, **kwargs)
   2015       # v to a Trackable data structure when v is a list/dict/tuple.
   2016       converted_v = getattr(self, k)
-> 2017       _assert_trackable(converted_v, k)
   2018 
   2019       if root:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py in _assert_trackable(obj, name)
   1462       obj, (base.Trackable, def_function.Function)):
   1463     raise ValueError(
-> 1464         f""`Checkpoint` was expecting {name} to be a trackable object (an ""
   1465         f""object derived from `Trackable`), got {obj}. If you believe this ""
   1466         ""object should be trackable (i.e. it is part of the ""

ValueError: `Checkpoint` was expecting dist_ds_it to be a trackable object (an object derived from `Trackable`), got <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7fb291849610>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.
```
</details>"
tensorflow/tensorflow,2022-06-02 19:23:53,feature,Character-level seq2seq model for translation and beam search. ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Colab GPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I was trying to implement seq2seq translation model at character level along with beam search by referring the tensorflow documentation.

https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb

For this, I changed only one parameter -- 'char_level = True' in tf.keras tokenizer. There was no issue during model training, but I'm getting error for inferences.
```


### Standalone code to reproduce the issue

```shell
# Step 3 and Step 4
    def tokenize(self, lang):
        # lang = list of sentences in a language
        
        # print(len(lang), ""example sentence: {}"".format(lang[0]))
        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', char_level = True)
        lang_tokenizer.fit_on_texts(lang)

        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) 
        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)
        tensor = lang_tokenizer.texts_to_sequences(lang) 

        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences 
        ## and pads the sequences to match the longest sequences in the given input
        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

        return tensor, lang_tokenizer
```


### Relevant log output

```shell
translate(u'hace mucho frio aqui.')

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-19-a7e085e16f9f> in <module>()
----> 1 translate(u'hace mucho frio aqui.')

2 frames
<ipython-input-17-d0c0d138384e> in <listcomp>(.0)
      2   sentence = dataset_creator.preprocess_sentence(sentence)
      3 
----> 4   inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
      5   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
      6                                                           maxlen=max_length_input,

KeyError: '<start>'
```
</details>"
tensorflow/tensorflow,2022-06-01 20:07:25,feature,Unable to use tflite java on macos,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently TFLite on Maven only distributes .so files for JNI which are not usable on macos (as it's not a mach-o file). I am trying to run some tests on a library I'm working on outside of the android emulator, but can't run the tests due to the lack of .dylib files. Is it possible to add a configuration for mac builds or possibly add a .jar artifact to maven with .dylib files for jni?
```


### Standalone code to reproduce the issue

```shell
Built it locally and tried to run tests with JUnit.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-05-31 20:55:14,feature,How to full int 8 quantize a yamnet model?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux, Mac

### Mobile device

Linux Ubuntu, Mac m1, Mac intel

### Python version

3.7, 3.10, 3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How to full int 8 QAT quantisation
```


### Standalone code to reproduce the issue

```shell
import tensorflow_model_optimization as tfmot

LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer
MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer

class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    # List all of your weights
    weights = {
        ""kernel"": LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)
    }

    # List of all your activations
    activations = {
        ""activation"": MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)
    }

    # Configure how to quantize weights.
    def get_weights_and_quantizers(self, layer):
        output = []
        for attribute, quantizer in self.weights.items():
            if hasattr(layer, attribute):
                output.append((getattr(layer, attribute), quantizer))

        return output

    # Configure how to quantize activations.
    def get_activations_and_quantizers(self, layer):
        output = []
        for attribute, quantizer in self.activations.items():
            if hasattr(layer, attribute):
                output.append((getattr(layer, attribute), quantizer))

        return output

    def set_quantize_weights(self, layer, quantize_weights):
        # Add this line for each item returned in `get_weights_and_quantizers`
        # , in the same order

        count = 0
        for attribute in self.weights.keys():
            if hasattr(layer, attribute):
                setattr(layer, attribute, quantize_weights[count])
                count += 1

    def set_quantize_activations(self, layer, quantize_activations):
        # Add this line for each item returned in `get_activations_and_quantizers`
        # , in the same order.
        count = 0
        for attribute in self.activations.keys():
            if hasattr(layer, attribute):
                setattr(layer, attribute, quantize_activations[count])
                count += 1

    # Configure how to quantize outputs (may be equivalent to activations).
    def get_output_quantizers(self, layer):
        return []

    def get_config(self):
        return {}


from quant import DefaultDenseQuantizeConfig
from tensorflow_model_optimization.python.core.quantization.keras.quantize import quantize_scope, quantize_apply
import tensorflow_model_optimization as tfmot


with quantize_scope({
    ""DefaultDenseQuantizeConfig"": DefaultDenseQuantizeConfig,
    ""CustomLayer"": CustomLayer
}):
    def apply_quantization_to_layer(layer):
        return tfmot.quantization.keras.quantize_annotate_layer(layer, DefaultDenseQuantizeConfig())

    annotated_model = tf.keras.models.clone_model(
        tflite_model,
        clone_function=apply_quantization_to_layer,
    )

    qat_model = tfmot.quantization.keras.quantize_apply(annotated_model)

    qat_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss=""categorical_crossentropy"",
        metrics=['accuracy']
    )

    qat_model.summary()
```


### Relevant log output

```shell
When I test the tflite model it predicts completely randomly
```
</details>"
tensorflow/tensorflow,2022-05-30 11:44:30,feature,Ragged Tensor in 'batch_jacobian',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The function `tape.batch_jacobian` does support ragged tensors. 
Can something like the code below be done using `tape.batch_jacobian`, or did I do something wrong?
For normal tensor input, the code works just fine.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x_ragged = tf.RaggedTensor.from_row_lengths([[1.0], [2.0], [3.0], [4.0]], [2, 1, 1])
x_tensor = tf.constant([[[1.0]], [[3.0]], [[4.0]]])
print(x_ragged.shape, x_tensor.shape)

x = x_ragged

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = x*x + x
    y = tf.reduce_sum(y, axis=(-2))
out = tape.batch_jacobian(y, x, experimental_use_pfor=True)
print(y.shape)
print(out)
```


### Relevant log output

```shell
ValueError: in user code:
    ValueError: No pfor vectorization defined for RaggedTensorToVariant
    name: ""loop_body/RaggedToVariant/RaggedTensorToVariant""
    op: ""RaggedTensorToVariant""
    input: ""loop_body/RaggedToVariant/RaggedTensorToVariant/rt_nested_splits_0""
    input: ""loop_body/AddN""
    attr {
      key: ""RAGGED_RANK""
      value {
        i: 1
      }
    }
    attr {
      key: ""Tsplits""
      value {
        type: DT_INT64
      }
    }
    attr {
      key: ""Tvalues""
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: ""batched_input""
      value {
        b: false
      }
    }
    experimental_type {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_RAGGED
        args {
          type_id: TFT_FLOAT
        }
      }
    }
    
     inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/RaggedToVariant/RaggedTensorToVariant/rt_nested_splits_0:0' shape=(4,) dtype=int64>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/AddN/pfor/AddN:0' shape=(1, 4, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False)].
Encountered an exception while vectorizing the batch_jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
```
</details>"
tensorflow/tensorflow,2022-05-26 11:32:06,feature,"TensorFlow core operator ExtractImagePatches , not supported","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.6.2 /cuDNN 8.4.0

### GPU model and memory

RTX 1660Ti and 16 GB

### Current Behaviour?

```shell
I made a custom  layer which uses ExtractImagePatches TensorFlow operator, but it was not supported by tflite (but listed as supported). I tried to enable the op manually but was unable to find .cc and .h file. It would be great if there is a solution to this problem, since my research depends on it. I am encountering the error when I am converting the model from tf to tflite.
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow_model_optimization

import tensorflow as tf
import tensorflow_model_optimization as tfmot
import matplotlib.pyplot as plt
import numpy as np

from keras.utils import np_utils
from tensorflow.python.keras import activations
from tensorflow.keras.callbacks import EarlyStopping

def myconv2d(ix, w, padding):
   # filter shape: [filter_height, filter_width, in_channels, out_channels]
   # flatten filters
   filter_height = int(w.shape[0])
   filter_width = int(w.shape[1])
   in_channels = int(w.shape[2])
   out_channels = int(w.shape[3])
   ix_height = int(ix.shape[1])
   ix_width = int(ix.shape[2])
   ix_channels = int(ix.shape[3])
   filter_shape = [filter_height, filter_width, in_channels, out_channels]
   flat_w = tf.reshape(w, [filter_height * filter_width * in_channels, out_channels])
   patches = tf.image.extract_patches(
       ix,
       sizes=[1, filter_height, filter_width, 1],
       strides=[1, 1, 1, 1],
       rates=[1, 1, 1, 1],
       padding=padding
   )
   patches_reshaped = tf.reshape(patches, [-1, ix_height, ix_width, filter_height * filter_width * ix_channels])
   feature_maps = []
   for i in range(out_channels):
       feature_map = tf.reduce_sum(tf.multiply(flat_w[:, i], patches_reshaped), axis=3, keepdims=True)
       feature_maps.append(feature_map)
   features = tf.concat(feature_maps, axis=3)
   return features

class MyConv2D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, padding='SAME', **kwargs):
        self.filters = filters
        self.kernel_size = kernel_size
        self.padding = padding
        #self.units= units
        super(MyConv2D, self).__init__(**kwargs)

    def get_config(self):
        config = super().get_config()
        config.update({
            ""filters"": self.filters,
            ""kernel_size"": self.kernel_size,
            ""padding"" : self.padding,
        })
        return config

    def build(self, input_shape):
        # only have a 3x3 kernel
        shape = self.kernel_size + (input_shape[-1], self.filters)
        self.kernel = self.add_weight(name='kernel', shape=shape,
                                      initializer='glorot_uniform', trainable=True)
        self.b = self.add_weight(
            name=""bias"", shape=(self.filters,), initializer=""random_normal"", trainable=True
        )
        #super((MyConv2D, self).build(input_shape))

    def call(self, inputs):
        result = myconv2d(inputs, self.kernel, self.padding) + self.b
        return result

    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (self.filters,)

def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = tf.keras.datasets.mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = np_utils.to_categorical(trainY)
	testY = np_utils.to_categorical(testY)
  # convert from integers to floats
	train_norm = trainX.astype('float32')
	test_norm = testX.astype('float32')
	# normalize to range 0-1
	trainX = train_norm / 255.0
	testX = test_norm / 255.0
	# return normalized images
	return trainX, trainY, testX, testY

def create_model():
  # creating a sequantial model
  model = tf.keras.Sequential()
  # adding convolution2D layer to the model of 32 filters of size 3x3
  model.add(MyConv2D(filters=32, kernel_size=(3,3)))
  model.add(tf.keras.layers.Activation(activations.relu))
  # adding a maxpooling 2D layer of size 2x2
  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))
  # adding a Dropout layer
  model.add(tf.keras.layers.Dropout(0.2))
  # adding a Flatten layer
  model.add(tf.keras.layers.Flatten())
  # adding Dense layer with 'relu' activation
  model.add(tf.keras.layers.Dense(100, activation='relu'))
  # adding Dense layer with 'softmax' activation for output
  model.add(tf.keras.layers.Dense(10, activation='softmax'))
  return model

train_images, train_labels,test_images, test_labels= load_dataset()

model = create_model()
# compile model
opt = tf.keras.optimizers.Adam()
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# fit model
history = model.fit(train_images, train_labels, epochs=5, batch_size=32, validation_data=(test_images, test_labels),callbacks=[es])
# evaluate model
scores = model.evaluate(test_images, test_labels, verbose=0)
print(""Accuracy: %.2f%%"" % (scores[1]*100))
# stores scores

quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model

def representative_dataset():
  for data in tf.data.Dataset.from_tensor_slices((train_images)).batch(1).take(100):
    yield [tf.dtypes.cast(data, tf.float32)]

quant_model = quantize_annotate_model(model)
quant_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
converter = tf.lite.TFLiteConverter.from_keras_model(quant_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.allow_custom_ops = True
converter.experimental_new_converter =True
quantized_tflite_model = converter.convert()
```


### Relevant log output

```shell
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: [ExtractImagePatches]Encountered unresolved custom op: ExtractImagePatches.
See instructions: https://www.tensorflow.org/lite/guide/ops_customNode number 0 (ExtractImagePatches) failed to prepare.
```
</details>"
tensorflow/tensorflow,2022-05-23 16:33:51,feature,Function similar to torch.nn.functional.grid_sample,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to convert a model containing torch.nn.functional.grid_sample function from pytorch format to tensorflow format.
I was able to convert from pytorch format to onnx format (see https://github.com/microsoft/onnxruntime/issues/10232).
To convert from onnx format to tensorflow format, I need to implement this function in the onnx_tf converter.
Is there an analogue of such a function in tensorflow?
If not, are there plans to implement it?
```


### Standalone code to reproduce the issue

```shell
torch.nn.functional.grid_sample
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-05-20 09:42:07,feature,tf.keras.datasets.cifar10.load_data(path='cifar-10-python.tar.gz'),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The tf.keras.datasets.mnist.load_data(path='/user/.keras/datasets/mnist.npz') works fine, but the tf.keras.datasets.cifar10.load_data() have no path param, which means everytime everyone runs his/her jupyter scripts that requires cifar10 datasets, he/she had to download the datasets from source again, it's really boring and time-consuming :)
```


### Standalone code to reproduce the issue

```shell
import autokeras as ak
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-05-17 21:40:50,feature,Data init API for TFLite Swift,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8+

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current Swift API only has `init` functions from files on disk unlike the Java (Android) API which has a byte buffer initializer. It'd be convenient if the Swift API could initialize `Interpreters` from `Data`.
```


### Standalone code to reproduce the issue

```shell
No code. This is a feature request
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-05-16 02:19:24,feature,[Feature Request] Default Keras callback for timing the training loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The current Keras progress bar callback displays training latency in time/step. The current recommendation is to disable the progress bar in production workflows.

This poses 2 problems:

1. The metric time/step is not batch size agnostic. While tuning batch size to fit the H/W, metrics represented in terms of samples allow for better head to head comparison. Currently, an additional step of converting step to samples is required.

2. Production workflows that require continuous training will benefit greatly from being able to monitor training latency. A deterioration in training speed could lead to spike in training costs.

I propose:

1. Reporting an additional metric for training latency based on samples.

2. A new verbosity setting that allows for reporting the training latency metrics without the progress bar. This could even be part of an existing verbosity.

I would be happy to create a PR for these changes.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2022-05-04 08:29:44,feature,tf.train.Checkpoint does not support RMSprop weights for tensorflow-macos 2.8.8 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tensorflow-macos 2.8.0

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Optimizer is not supported

When I select 'RMSProp' in 
checkpoint = tf.train.Checkpoint(optimizer='RMSprop', model=model)
Then I get (1). The same when I don't use a string, but a full optimizer declaration in keras 

When I run it without setting an optimizer, then status.assert_consumed() starts complaining:
(2)
```


### Standalone code to reproduce the issue

```shell
keras.optimizer_v2.rmsprop.RMSprop 

tf.train.Checkpoint(optimizer='RMSprop', model=model)
```


### Relevant log output

```shell
(1)
Traceback (most recent call last):
  File ""main.py"", line 80, in <module>
    main(config_dot)
  File ""main.py"", line 47, in main
    model = load_model(config, training_data)
  File ""/Users/mark/Code/pythonProjects/music_RNN/train.py"", line 127, in load_model
    checkpoint = tf.train.Checkpoint(optimizer='RMSprop', model=model)
  File ""/Users/mark/opt/anaconda3/envs/tensorlfowGPU/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 2017, in __init__
    _assert_trackable(converted_v, k)
  File ""/Users/mark/opt/anaconda3/envs/tensorlfowGPU/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 1463, in _assert_trackable
    raise ValueError(
ValueError: `Checkpoint` was expecting optimizer to be a trackable object (an object derived from `Trackable`), got RMSprop. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.



(2)
Traceback (most recent call last):
  File ""main.py"", line 80, in <module>
    main(config_dot)
  File ""main.py"", line 47, in main
    model = load_model(config, training_data)
  File ""/Users/mark/Code/pythonProjects/music_RNN/train.py"", line 131, in load_model
    status.assert_consumed()
  File ""/Users/mark/opt/anaconda3/envs/tensorlfowGPU/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 784, in assert_consumed
    raise AssertionError(
AssertionError: Unresolved object in checkpoint (root).model.optimizer.iter: attributes {
  name: ""VARIABLE_VALUE""
  full_name: ""RMSprop/iter""
  checkpoint_key: ""model/optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE""
}
has_checkpoint_values {
  value: true
}

WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.
WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.optimizer.iter
WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.optimizer.decay
WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.optimizer.learning_rate
WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.optimizer.momentum
WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).model.optimizer.rho
```
</details>"
tensorflow/tensorflow,2022-04-30 12:18:57,feature,Unroll factor for keras.layers.RNN or performance fix for TensorArray / while_loop.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Tensorflow Version

2.7.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04


### CUDA/cuDNN version

11.5

### GPU model and memory

RTX 2080Ti

</details>

### Feature description


`keras.layers.RNN` supports unrolling, which is great for performance, unless the unrolled time dimension is too long. The XLA compiler takes a ton of time processing a fully unrolled loop (e.g. 240 iterations). Every major compiler supports unrolling a loop for a given factor. The `unroll` argument from `keras.layers.RNN` could instead take an integer telling how many iterations it should unroll, and insert a while loop around the remaining factor. Using an unroll factor of 32 would do miracles for most long time-dimension datasets.

XLA speedups are huge for my custom RNN Cell, so I'd like to use it, but now I'm waiting 5 minutes for XLA/ptxas to finish processing the unrolled loop. On the other hand, if I don't unroll, the while_loop introduces a TensorArray struct takes a tremendous amount of time every time-iteration of the RNN as there is a cuMemcpyD2H of 4 bytes happening between GPU and CPU. The whole CUDA driver pipeline stalls for this 4-byte copy. Performance drops by a factor of 2 to 5 because of this. I don't know if the 4-byte copy is an unintentional performance bug in TensorArray/tf.while_loop or not. But if it's not, I think the unroll factor is a good compromise."
tensorflow/tensorflow,2022-04-27 07:55:07,feature,Can tflite CoreML delegate add ELU activation support? (with fp16 as well)_,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
Can tflite CoreML delegate add ELU activation support? (with fp16 as well)_
```


### Standalone code to reproduce the issue

```shell
Can tflite CoreML delegate add ELU activation support? (with fp16 as well)_
```


### Relevant log output

```shell
Can tflite CoreML delegate add ELU activation support? (with fp16 as well)_
```
</details>"
tensorflow/tensorflow,2022-04-21 07:15:37,feature,tf.nn.embedding_lookup_sparse doesn't work with @tf.function(jit_compile=True),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Colab Notebook

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tried to use `tf.nn.embedding_lookup_sparse` within a function decorated with `@tf.function(jit_compile=True)` and it failed.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)


@tf.function(jit_compile=True)
def run_embedding_bag(params, sp_ids):
  return tf.nn.embedding_lookup_sparse(
    params, sp_ids, None, combiner='sum', max_norm=None, name=None
  )


with tf.device('/TPU:0'):
  params = tf.random.uniform([1000, 64])
  sp_ids = tf.sparse.SparseTensor([[0, 0], [0, 2]], values=[1, 2], dense_shape=[3, 4])
  res = run_embedding_bag(params, sp_ids)

print(res.shape)
print(run_embedding_bag.experimental_get_compiler_ir(params, sp_ids)(stage='hlo'))
```


### Relevant log output

```shell
InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_run_embedding_bag_978[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_TPU_JIT: SparseSegmentSum (No registered 'SparseSegmentSum' OpKernel for XLA_TPU_JIT devices compatible with node {{node embedding_lookup_sparse}}){{node embedding_lookup_sparse}}
One approach is to outside compile the unsupported ops to run on CPUs by enabling soft placement `tf.config.set_soft_device_placement(True)`. This has a potential performance penalty.
```
</details>"
tensorflow/tensorflow,2022-04-16 19:57:31,feature,Enable `unique` bool for `tf.random.uniform`! ,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

Currently, the `tf.random.uniform`, randomly sample values form given min and max range **without considering the repeating same value**. For example, currently:

```python
tf.random.uniform(shape=[5], maxval=5, dtype=tf.int32, seed=10)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 3, 3, 1], dtype=int32)>
````

but if it has a `unique` bool parameter, it would be possible to generate a unique random value. For example:

```python
tf.random.uniform(shape=[5], maxval=5, dtype=tf.int32, seed=10, unique=True)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 2, 4, 3, 1], dtype=int32)>
````

**Will this change the current API? How?**

From 

```python
tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.int32,
    seed=None,
    name=None
)
```

To 

```python
tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.int32,
    seed=None,
    name=None,
    unique=False
)
```


**Who will benefit from this feature?**

A similar thing is possible with `import random; random.sample`. So, having `unique` bool in `tf.ranom.uniform` might be useful in some special cases. Similar param in [tf.random.uniform_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/random/uniform_candidate_sampler).



**Any Other info.**

I am not sure if there is any other convenient TensorFlow function that can be used to generate unique random values. I know we can do 

```python
tf.random.shuffle(tf.range(5))
```

But `tf.random.shuffle` has certain limitations to fall back to `while_loop`. That's why we can't use it here. Let me know if I've missed something. "
tensorflow/tensorflow,2022-04-07 20:34:54,feature,Adding Select Tf Ops to Cmake,"**System information**
- TensorFlow version (you are using): any
- Are you willing to contribute it (Yes/No): no



**Describe the feature and the current behavior/state.** 
Currently one has to decide between using Select TF ops with bazel or GPU support with CMake.
It would be nice if either bazel has an option to support GPU on any OpenCL system or CMake can build with TF Ops.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Everybody who wants to use GPU acceleration and TF Ops.

"
tensorflow/tensorflow,2022-04-04 15:12:51,feature,Adding a parameter to teh tf.image.extract_patches function to change the value of added border pixels due to the padding,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
Using tf.image.extract_patches with ""same"" padding leads to zeros on the image border. Zero is a default value and cannot be changed by the user. This means that the mask data for image segmentation should always have zero as the background class, otherwise it does mess up after patching. It is best to add this default value as a parameter so the user can change it if needed.

**Will this change the current api? How?**
Adding a parameter to teh tf.image.extract_patches function to change the value of added border pixels due to the padding

**Who will benefit with this feature?**
Anyone using this function for patching data for image segmentation

**Any Other info.**
"
tensorflow/tensorflow,2022-03-31 09:11:00,feature,How about exporting `Tensor::FromProto` to Python API?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No):

**Describe the feature and the current behavior/state.**

If we have to convert TensorProto message in Python env, we can't directly convert it.
We have to 

1. construct a new tensor with protobuf message field
2. serialize message to string and parse it using `tf.io.parse_tensor`
3. or some other way

**Will this change the current api? How?**

Yes. Maybe we can add the `tf.from_tensor_proto` like `tf.make_tensor_proto`.

**Who will benefit with this feature?**

The people who frequently handle protobuf messages.
For example, I handle TensorProto messages directly when I predict some examples with TensorFlow Serving using gRPC.

**Any Other info.**

[`Tensor::FromProto` docs link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor#fromproto)"
tensorflow/tensorflow,2022-03-29 12:03:50,feature,Customize TF tensor multiplication,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Is there a way o customize/modify the tensor/matrix multiplication operation used through out tensorflow? Let's say whenever I am executing a forward pass on a given input, involving a sequence of tensor multiplications defined by the architecture of the neural net, how would I proceed to change the very definition of how the multiplication is carried out? 
Which source file do I have to edit to achieve this? Maybe smewhere in `tensorflow/python/ops`  ?

Thank you for your help.
"
tensorflow/tensorflow,2022-03-16 21:06:31,feature,C-API Binaries for 32-bit for x86 instruction set,"**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

Tensorflow provides Windows binaries for the C-API for 64-bit processprs for instruction set x86 (= x64), see:
https://www.tensorflow.org/install/lang_c#supported_platforms

It would be great to support this for 32-bit also.

**Who will benefit with this feature?**
Projects requiring to build in 32-bit and 64-bit OS.
"
tensorflow/tensorflow,2022-03-13 07:09:30,feature,support vscode devcontiners for development,"**System information**
- TensorFlow version (you are using): latest
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
support for vscode devcontainers is required for good dependency management and overall good support for modern development
"
tensorflow/tensorflow,2022-02-26 21:18:20,feature,expand support for image file types (primarily TIFF),"This issue requests support for TIFF images when using TensorFlow for image processing research.

**System information**
- TensorFlow version (you are using): 2.8.0 available via Colab: Python 3 Google Compute Engine backend (GPU)
- Are you willing to contribute it (Yes/No): I'm willing to help but looking for high-level suggestions on how to implement this. Feel free to assign me with a mentor.


**Describe the feature and the current behavior/state.**
Currently, TensorFlow only supports a limited number of image filetypes. This should be expanded for both convenience and data uses of users who require other image filetypes.

**Will this change the current api? How?**
Maybe? The current image decoder would have to be updated to read TIFF filetypes. ~No commands would have to change.~ A `tf.io.decode_jpeg(sample_image)` equivalent would have to be added.

**Who will benefit with this feature?**
This feature would benefit domain science users who rely on TIFF image types for microscopy images, among other uses.

**Any Other info.**
Traceback on Error if running TIFF image filetype is shown below. Even if I change the filetype to JPEG, if the image was originally TIFF I get this error.

```python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
[<ipython-input-4-940ee08e0488>](https://localhost:8080/#) in <module>()
      2 
      3 
----> 4 sample_image = tf.io.decode_jpeg(sample_image)
      5 print(sample_image.shape)

2 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]
```"
tensorflow/tensorflow,2022-02-22 22:26:25,feature,XLA `LuDecomposition` support for CPU/GPU,"**System information**
- TensorFlow version (you are using): 2.8
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

`xla::LuDecomposition` in tensorflow/compiler/xla/client/lib/lu_decomposition.h is currently only implemented for TPU. I'd like to be able to use it on CPU and/or GPU.

**Will this change the current api? How?**

Yes, but in a backwards compatible way.

**Who will benefit with this feature?**

Anyone wanting to build on XLA and do LU decomposition for e.g. calculating determinants. I want this as I'm building an API for XLA in Idris."
tensorflow/tensorflow,2022-02-20 14:28:41,feature,C++ prebuilt libs,Are there any plans to release prebuilt C++ libs like the ones for C here https://www.tensorflow.org/install/lang_c? 
tensorflow/tensorflow,2022-02-15 16:10:50,feature,Consider supporting png images for modelmaker object detector,"**System information**
- TensorFlow version (you are using): 2.7.0



**Describe the feature and the current behavior/state.**
Currently the `object_detector` class only supports jpg images. This can be a problem, especially if you have already created label annotations for the images in another image format. I suggest that we consider adding support for png files as well.
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Everyone using png images instead of jpg for training data.
**Any Other info.**
As far as I can see then there is a check for file format here on line 133 of the `dict_to_tf_example` function: https://github.com/tensorflow/examples/blob/035fb73d5ff8b74958c9e3f83f44fc20dfc39119/tensorflow_examples/lite/model_maker/third_party/efficientdet/dataset/create_pascal_tfrecord.py#L133"
tensorflow/tensorflow,2022-02-10 21:20:55,feature,Batch processing for tflite_runtime,"**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Batch processing of images in object detection models that are running in a tflite_runtime environment would be a great feature to add. Right now, it is possible to resize the input tensor to something like [x, h, w, c] where x is the number of images in a batch, and run it through the interpreter. However, the predictions come out looking like an image classification model, where there are probabilities but no bounding boxes.

**Will this change the current api? How?**
Yes. It will allow for developers to leverage the savings associated with batch processing.

**Who will benefit with this feature?**
Anyone who wants to break large images into x number of tiles and run inference on them all at once. This will greatly improve object detection capabilities on EdgeTPU devices.

**Any Other info.**
[Here](https://discuss.tensorflow.org/t/tflite-batch-inference-bug/7495) is a post I made on batch processing with an EfficientDet model.
"
tensorflow/tensorflow,2022-02-02 06:42:45,feature,FIFOQueue as a part of custom layer,"Hello,

   I need a queue as a part of my custom input layer in Transformer for Short-term memory purposes. I need to store the coming features ""in first in first out"" manner and remove the oldies features when it's full and clean all content if it's needed. I see [here](https://www.tensorflow.org/api_docs/python/tf/queue/FIFOQueue) something like that for working with Tensor, but is it a good choice for me and contains all that I need? Is the FIFOQueue faster than using Python's deque or Numpy based queue directly in a custom layer?

Compare Python's deque collection with TF's FIFOQueue:
| Python | TensorFlow |
|--------|-------------|
| deque(maxlen=max_size) | tf.queue.FIFOQueue(capacity, dtypes, shapes=None, names=None, shared_name=None)
| append(x)<br># if it's full discard from the left end | enqueue(vals) |
| clear() | ??? |
| len(queue) | size() |
| pop() | dequeue() |
| queue[i] | ??? |

Or alternatively do it with Numpy like:
```python
queue = np.zeros((maxlen, features)) # my queue
x = np.random.normal(size=features) # new features

# append()
queue = np.concatenate([queue[:-1], x[np.newaxis, :]])

# predict
y = model(queue[np.newaxis, :, :])
```

With Python's timeit I get a benchmark on 100000 cycles with maxlen=1000 and features=6:
| Method | Time |
|--------|-------------|
| Numpy | 0.1765849579999994  s |
| deque | 12.360834958000002  s |

Thanks, have a nice day."
tensorflow/tensorflow,2022-01-26 08:53:15,feature,TFLite SignatureRunner support for the C API,"**System information**
- TensorFlow version (you are using): 2.7 / tf-nightly
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The current TFLite interpreter C API allows using TFLite in situations where a stable ABI is needed and the C++ API is not an option. For example, in cases where TFLite needs to be used in bigger projects that cannot use bazel to build and might use different toolchains or incompatible C++ compiler settings.

However, while the C++ API already has support for multiple signatures [through the use of SignatureRunner](https://www.tensorflow.org/lite/guide/signatures#c), the C API has not been updated accordingly. As such, the use of multiple signatures in TFLite models is not possible if ABI stability requirements prevent you from using the C++ API directly.

**Will this change the current api? How?**
New APIs that allow using signature runners would need to be added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.h), or in a separate header if appropriate.

**Who will benefit with this feature?**
Anyone who needs stable ABIs and wants to run TFLite models with multiple signatures.

**Any Other info.**
As mentioned [here](https://www.tensorflow.org/lite/guide/signatures#known_limitations) this feature is not available _yet_, suggesting it's planned. I'm opening this feature request so that it's easier to track its status, as well as any kind of information on when it might be implemented."
tensorflow/tensorflow,2022-01-24 13:48:42,feature,iterating over `tf.Tensor` is not allowed,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I have a dictionary that I build by accessing only the channel dimensions of an output layer of a convolutional neural network (it has a shape (100,24,24,6) )
Therefore keys of this dictionary are tuples of tensor shape (6, ) . I want to map these keys to the input of the next layer using the tf.map_fn(). However, i am incapable of doing it because the keys of my dictionary are of type tensor and i cannot iterate over them .
Looking for some help. Thank you.

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
tensorflow/tensorflow,2022-01-22 09:14:06,feature,Installing tensorflow with pip,"**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

When installing tensorflow on windows, after `pip install tensorflow`, it further needs to install Cuda and cudann and also needs to add their paths to windows environmental utility.  I'm not sure why we need to face this hassle whereas, in pytorch, we don't see such things. 

**Will this change the current api? How?** dunno. 

**Who will benefit from this feature?** all of the painful souls who use tensorflow

**Any Other info.**
"
tensorflow/tensorflow,2022-01-21 19:03:40,feature,Expose half_pixel_center or anti-aliasing parameter in the keras resizing layer.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.6
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The feature that I am proposing is basically a fix for the current resizing keras layer which sets the `half_pixel_center` to true by default which causes problem on nnapi (android) based platforms and also on snapdragon based platforms. There are 2 possible solution to this problem - 

1. You expose the parameters `antialias` or `half_pixel_center` via the keras Resizing layer and let the user explicitly set either of the 2 properties. Better would be to expose `antialias` which can be defaulted to `None` or `True` since resizing is used mostly in later layers and not for downsampling it should be fine.
2.  Add an automated check within the resize base function to check if the half_pixel_center is actually needed. From my understanding when you are resizing just check if the (inputsize -1)/(outputsize-1) when downsampling or vice versa when upsampling, is fractional or int. If fractional set half_pixel_center to true else set it to false. 


**Will this change the current api? How?**
Yes this will change the current api by either exposing certain optional parameters or by adding an automated check.

**Who will benefit with this feature?**
Everyone running their NN on somekind of DSP.

**Any Other info.**
"
tensorflow/tensorflow,2022-01-17 09:55:18,feature,[Feature Request] Support for convolutional layers for `tf.autodiff.ForwardAccumulator`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Currently, the implementation of [`tf.autodiff.ForwardAccumulator`](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) only officially supports Dense layers.

**Will this change the current api? How?**
Yes. The new API will be able to calculate the JVP also for convolutional layers.

**Who will benefit from this feature?**
Anyone who wants to implement a neural network with convolutional layers and needs forward-mode autodiff.

**Any Other info.**
Trying to use the current API results in a shape mismatch:

```python
import tensorflow as tf

batch_size = 1
image_width = 4
image_height = 4
channels = 3

x = tf.random.uniform((batch_size, image_width, image_height, channels))
model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(filters=6, kernel_size=2)])

model.build(x.shape)

model.summary()

with tf.autodiff.ForwardAccumulator(model.trainable_weights[0], tf.constant([[[[1., 0., 0., 0., 0., 0.]]]])) as acc:
  out = model(x) # <----- ValueError

  print(acc.jvp(out))
```

Summary:

```
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (1, 3, 3, 6)              78        
                                                                 
=================================================================
Total params: 78
Trainable params: 78
Non-trainable params: 0
_________________________________________________________________
```

Error:
```
ValueError: Exception encountered when calling layer ""conv2d"" (type Conv2D).

in user code:


    ValueError: Dimension 1 in both shapes must be equal, but are 4 and 3. Shapes are [1,4,4,6] and [1,3,3,6].
    	From merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](gradient_tape/gradient_tape/Conv2D, gradient_tape/gradient_tape/Conv2D_1)' with input shapes: [1,4,4,6], [1,3,3,6].


Call arguments received by layer ""conv2d"" (type Conv2D):
  • inputs=tf.Tensor(shape=(1, 4, 4, 3), dtype=float32)
```
"
tensorflow/tensorflow,2022-01-12 19:31:31,feature, Python API of `saved_model.load` to support `tf.saved_model.experimental.VariablePolicy`,"**System information**
- TensorFlow version (you are using): TF2.7
- CPU memory: 16Gb
- GPU memory: 10Gb

**Describe the feature and the current behavior/state.**
Some models need to store large weights/tables on CPU and smaller ones on GPU. tf.saved_model.experimental.VariablePolicy is able to store device placement but not fully supported by tf.saved_model.load. Every variable is loaded to GPU if available. There is support on C++ level, but missing in python. A repo.py is as follows. Run as python repo.py --size 11, A OOM is observed. Set the number 11 to be any number b/w the memory size of your CPU and GPU in GB.

```
import argparse
import tensorflow as tf
class LN(tf.keras.layers.Layer):
    def __init__(self, rows, cols,trainable=True):
        super(LN, self).__init__(dtype=tf.float32)
        self.rows = rows
        self.cols = cols
        self.mat = None
        self.trainable=trainable

    def build(self, input_shape):
        self.mat = self.add_weight(""mat"",shape=[self.rows, self.cols],
                                       dtype=tf.float32, trainable=self.trainable)

    def call(self, indices):
        return tf.gather(params=self.mat, indices=indices)

class TestModel(tf.keras.Model):
    def __init__(self, rows, cols):
        super().__init__()
        self.ln = LN(rows=rows, cols=cols)

    @tf.function
    def call(self, x):
        with tf.device('/cpu:0'):
            x = self.ln(x)
        with tf.device('/gpu:0'):
            x = tf.math.reduce_sum(x, axis=1)
        return x


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--size', type=int, default=100, help='Table size in GiB')
    parser.add_argument('--load_cpu', type=bool, default=False, help='Load variables to cpu')
    args = parser.parse_args()
    ncols = 128
    nrows = args.size * 2**30 // 512
    path = '/tmp/saved_model_test'
    model = TestModel(rows=nrows, cols=ncols)
    indices = tf.zeros(shape=(65536, 1), dtype=tf.int32)
    outputs = model(indices)
    print(outputs)
    model.summary()
    print('Saving the model')
    tf.saved_model.save(
            model, path,
            options = tf.saved_model.SaveOptions(
                  experimental_variable_policy=tf.saved_model.experimental.VariablePolicy.SAVE_VARIABLE_DEVICES))
    print('Saved successfully')
    if args.load_cpu:
        with tf.device('/cpu:0'):
            loaded = tf.saved_model.load(path) # Good
    else:
        loaded = tf.saved_model.load(path) # OOM
    print('Load successfully')

if __name__ == '__main__':
    main()
```"
tensorflow/tensorflow,2022-01-08 12:57:44,feature,Native support for StridedSlice in 6D and Transpose in 7D,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 804ef7223ef08fd14c274b4a4044cc4aeee68863


**Provide the text output from tflite_convert**

```
2022-01-08 12:47:32.812069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2022-01-08 12:47:32.812092: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-01-08 12:47:32.812107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-01-08 12:47:32.812367: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.
2022-01-08 12:47:34.806218: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2022-01-08 12:47:34.806383: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-01-08 12:47:34.860258: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.008ms.
  function_optimizer: function_optimizer did nothing. time = 0ms.

2022-01-08 12:47:37.790651: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.
2022-01-08 12:47:37.790685: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.
2022-01-08 12:47:38.104138: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice, FlexTranspose
Details:
	tf.StridedSlice(tensor<1x120x160x3x1x32xf32>, tensor<6xi32>, tensor<6xi32>, tensor<6xi32>) -> (tensor<1x120x160x1x32xf32>) : {begin_mask = 55 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 55 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64}
	tf.Transpose(tensor<1x30x4x40x4x1x4xf32>, tensor<7xi32>) -> (tensor<1x30x40x1x4x4x4xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2022-01-08 12:47:38.104385: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs

Estimated count of arithmetic ops: 53.877 G  ops, equivalently 26.938 G  MACs
WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded
```

**Standalone code to reproduce the issue** 

TFLite Native support for `StridedSlice` for 6D, and `Transpose` for 7D would be very beneficial. It would be nice if we could avoid Flex operations if possible.
- **`saved_model`** and converted **`tflite`** files 
[saved_model_and_float32tflite.zip](https://github.com/tensorflow/tensorflow/files/7833211/saved_model_and_float32tflite.zip)

- Conversion Script
```python
import tensorflow as tf

input_shapes = [[1,120,160,6]]

model = tf.saved_model.load(
    'flyingthings_finalpass_xl/saved_model_120x160'
)
concrete_func = model.signatures[
    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY
]
concrete_func_input_tensors = [
    tensor for tensor in concrete_func.inputs \\
        if tensor.dtype != tf.resource and not 'unknown' in tensor.name
]
for conc_input, def_input in zip(concrete_func_input_tensors, input_shapes):
    conc_input.set_shape(def_input)
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
tflite_model = converter.convert()
with open('model_float32.tflite', 'wb') as w:
    w.write(tflite_model)
```

**Any other info / logs**
- Model Citation Repository - HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching
https://github.com/google-research/google-research/tree/master/hitnet
![148323208-7db28584-ce78-4398-94fa-4ce93c9f8d4d](https://user-images.githubusercontent.com/33194443/148644951-ed032ea7-f4d4-43a1-bdfb-bf368d12cb4e.gif)

- FlexStridedSlice (6D)
![Screenshot 2022-01-08 21:52:11](https://user-images.githubusercontent.com/33194443/148644890-d6400a0a-e8b7-423d-bfc8-e78e56135647.png)

- FlexTranspose (7D)
![Screenshot 2022-01-08 21:52:40](https://user-images.githubusercontent.com/33194443/148644896-b1481efb-4cd0-470a-b277-b523cfce4798.png)"
tensorflow/tensorflow,2022-01-04 16:36:31,feature,Request for grouped convolutions on CPU,"**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Grouped convolutions are not currently supported on CPUs in graph mode. We get the following error when we try to do the same:
```
UnimplementedError:  Fused conv implementation does not support grouped convolutions for now.
	 [[node sequential_2/sequential/conv2d/BiasAdd
 (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py:265)
]] [Op:__inference_predict_function_1730]
```
Till date, there have been numerous issues on TF's GitHub repo regarding this. Owing to that, I am requesting the team to look into this.

**Will this change the current api? How?**
`model.predict()` with a model having grouped convolutions will not result in an error on a CPU.

**Who will benefit with this feature?**
Many architectures today use grouped convolutions as they are known to be more efficient while maintaining the accuracy. Thus, I believe many TF developers will benefit from this addition. 
"
tensorflow/tensorflow,2021-12-30 11:18:46,feature,Please add `tf.random.stateless_shuffle`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
we only have `tf.random.shuffle` which is generally annoying to work with and bug-prune when reproducible results are needed

(same reasoning for all 'tf.random.stateless_*`)

**Will this change the current api? How?**A
only API addition

**Who will benefit with this feature?**
people that want to reproduce random function rexecution


**Any Other info.**
no, thank you"
tensorflow/tensorflow,2021-12-24 21:31:31,feature,Feature Request: Providing Gradients for `layer.set_weights()`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.7.0
- Are you willing to contribute it (Yes/No): No, I am not experienced enough.



**Describe the feature and the current behavior/state.**

The feature is to make `set_weights()` differentiable. The current behavior is that the `model.set_weights()` function breaks the gradient. Given a model named `learner` and a batch of data `batch`

```
l = tf.constant(0.1)
with tf.GradientTape() as gt:
  gt.watch(l)
  new_weights = [w * l for w in learner.weights]

  learner.set_weights(new_weights)

  y_pred = learner(batch[""train""][""X""])
  loss = tf.keras.losses.categorical_crossentropy(batch[""train""][""y""], y_pred)
grads = gt.gradient(loss, l)
grads # returns None
```

This example returns None for the gradients. In fact, the gradient tape can't calculate the gradient of any variable related to calculating the `new_weights`.

So I was wondering if it's possible to provide gradients for `set_weights()` or if there are any work-arounds to this problem.

**Will this change the current api? How?**

I don't see how this feature would change the current api. It is more a modification on an existing function.

**Who will benefit with this feature?**

I believe adding this feature would make projects related to meta-learning easier to realize.

In my specific case, I was trying to implement [this paper](https://openreview.net/pdf?id=rJY0-Kcll) and translate the [pytorch implementation](https://github.com/markdtw/meta-learning-lstm-pytorch) to tensorflow, but I think this feature would generally benefit many projects that are related to the growing field of meta-learning.

**Any Other info.**
Thanks and Merry Christmas!!!
"
tensorflow/tensorflow,2021-12-16 07:03:11,feature,[TFLite] Support building external delegate with CMake,"TensorFlow Lite supports external delegate but not supported in CMake build, do we have plan to support it?"
tensorflow/tensorflow,2021-12-15 07:19:15,feature,"Segmentation fault and no log information available, add some tips about segmentation fault","**System information**
- TensorFlow version (you are using): tensorflow 1.12.0, python 3.6.13
- Are you willing to contribute it (Yes/No): Yes. 



**Describe the feature and the current behavior/state.**
I have trouble retraining my colleague's code from a year ago. I notice that when I try to load my model in the former code base on *libtensorflow-cpu-linux-x86_64-1.12.0* it reports a `Segmentation fault`, without any further information. Upon inspecting my code, I notice that it stops when I load the model, function `TF_SessionRun`. I'm not sure what to do to fix it. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. A moment later, my colleague realized the problem was the parameters in the placeholder name were incorrect. We fixed it after changing the placeholder name.

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
Those who are not familiar with tensorflow v1's place holder.
**Any Other info.**
No."
tensorflow/tensorflow,2021-12-10 13:29:51,feature,Perspective transformation data augmentation for object detection,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version :2.7.0
- Are you willing to contribute it 🆗 



**Perspective transformation data augmentation for object detection.**

**Will this change the current api? How? No, the change will be done only in the preprocessing layer.

**Who will benefit with this feature? As it has been described in [this article](https://ieeexplore.ieee.org/abstract/document/8943416), this feature could be helpful to make the models more general in the object detection projects.

"
tensorflow/tensorflow,2021-12-06 22:31:26,feature,Support multiple validation sets in Model.fit ,"Moving this feature from TF repository https://github.com/tensorflow/tensorflow/issues/38803

Are you willing to contribute it (Yes/No): No
Describe the feature and the current behavior/state.
Currently there's no way to use multiple validation sets with independent tracking of metrics.

Will this change the current api? How?

Simplest way I can think of is to accept a list of datasets in the validation_data parameter in Model.fit. Ideally there should also be a way to specify the name of each set so that the logs indicate what set each validation step corresponds to.

Who will benefit with this feature?

Anyone training with multiple validation sets.
"
tensorflow/tensorflow,2021-12-03 10:37:08,feature,tf.data.experimental.load does not work on temporary files,"**System information**
- TensorFlow version (you are using): 2.6.0
- Are you willing to contribute it (Yes/No): No (I don't have the required knowledge)


**Describe the feature and the current behavior/state.**

When loading a dataset with `tf.data.experimental.load`, the produced dataset will always be a lazily-loaded dataset that doesn't read the files until the dataset object is consumed (e.g. by iterating over it with Python code, or using it to train a model). This poses a problem if the path provided to `load` is a temporary path, since depending on when the dataset is used, the files could be gone when the dataset object attempts to read them.

Consider the following load function:
```python
def load_data(source):
    with tempfile.TemporaryDirectory() as tmpdir:
        prepare_files(source, tmpdir.name)
        return tf.data.experimental.load(tmpdir.name)
```
The dataset returned by this function will be unusable since, as soon as the function returns, the `TemporaryDirectory` context manager will exit and destroy the files. (The `prepare_files` function is just there to represent any logic that could be used to get the files to the temp. dir, such as downloading them over network, extracting them from an archive, or decrypting them.)

Aside from the temporary files issue, there could some other cases in which the developpers would want their datasets to be loaded in memory immediately.

**Will this change the current api? How?**

The feature I propose is to allow callers of `tf.data.experimental.load` to specify whether they want a lazily-loaded dataset, or an eagerly-loaded dataset (that reads the files only once, when `load` is called, and stores the data in memory).

The way I see it, this evolution would add a new optional parameter to `tf.data.experimental.load`. Its default value needs to correspond to lazy loading for backwards compatibility.

This would not pose a big issue since 1. it will be backwards-compatible and 2. only the experimetal API will be modified (and the documentation explicetely says the experimental API may evolve at any time).

**Who will benefit with this feature?**

Anyone who wants to separate their dataset loading logic in a similar function, using a temporary directory or another context manager that destroys the files upon exit. This would be the case when:
- using files downloaded from the Internet
- using encrypted files, e.g. if the data is sensitive or private
- using files compressed externally, to have more control over compression
- using `importlib.resources.as_file` in some cases

**Any Other info.**

The fact that the datasets are lazy-loaded is not documented, it would be nice to add this to the current documentation of `tf.data.experimental.load`. Also on the subject of documentation, once (if) the eager load option is added, the documentation should warn the user about the potentially high memory usage when eager-loading large datasets. In my use case this is not a problem, since 1. the datasets are created with `Dataset.from_tensor_slices` before being saved (so they have already been entirely in memory before), 2. I only use one dataset at a time and 3. I make sure it is garbage-collected as soon as possible.

Here is my current placeholder:
```python
def load_data(source):
    tmppath = tempfile.mkdtemp()
    def deleter():
        shutil.rmtree(tmppath)
    atexit.register(tmppath)
    prepare_files(source, tmppath)
    return tf.data.experimental.load(tmppath)
```
Depending on how many times this function is called during the program's lifetime, this can quickly clutter the disk. It would be nice to have a better solution.

Note that, to fit **my** *specific* use case, it would also work for me to have a variant of `save` and `load` that accept a Python file-like object (and save to/load from a single file). The logic I use in my version of `prepare_files` turns a single file into a directory.
"
tensorflow/tensorflow,2023-09-23 14:11:33,bug,KeyError: 'ConstantOfShape',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'

### Standalone code to reproduce the issue

```shell
from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('patchcore_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'patchcore_torch.h5', overwrite=True, save_format=""h5"")

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1NLiBYfIh_GKpSvMjqJQAA1?pwd=3wzb#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'
```
"
tensorflow/tensorflow,2023-09-23 13:56:17,bug,AttributeError: Not implemented,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented

### Standalone code to reproduce the issue

```shell
reproduce by running the following code:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('deeplabv3_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'deeplabv3_torch.h5', overwrite=True, save_format=""h5"")
```

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1JfHu9m6WG6yE2IuzWtKpA1?pwd=ux7b#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented
```
"
tensorflow/tensorflow,2023-09-22 12:46:16,bug,SpectralNormalization layer is not trainable. Please help (OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS and Google Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.8.0, nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

SpectralNormalization layer is not trainable. Whenever I try to use the ""model.fit"" method, TF outputs the error ""Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.""

Please help

Best

Kav

### Standalone code to reproduce the issue

```shell
LINK TO COLAB NOTEBOOK:
https://colab.research.google.com/drive/1TYoNIrrpk-bLBqpzNJXq5VOI6Mpln5Ty?usp=sharing


STANDALONE CODE:
batch = 1
height = 10
width = 10
channels = 1
filters = 4
kernel_size = 3

x_input = Input(shape=(height, width, channels))
conv2d = SpectralNormalization(Conv2D(filters, kernel_size))
x_output = conv2d(x_input)
model = Model(x_input, x_output)
model.compile(loss='mse')

x = np.random.rand(batch, height, width, channels)
y = np.random.rand(batch, height, width, filters)

model.fit(x, y)
```


### Relevant log output

```shell
---------------------------------------------------------------------------

OperatorNotAllowedInGraphError            Traceback (most recent call last)

<ipython-input-6-d3dc977168f5> in <cell line: 1>()
----> 1 model.fit(x, y)

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py in autograph_handler(*args, **kwargs)
     50     except Exception as e:  # pylint:disable=broad-except
     51       if hasattr(e, ""ag_error_metadata""):
---> 52         raise e.ag_error_metadata.to_exception(e)
     53       else:
     54         raise

OperatorNotAllowedInGraphError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).
    
    Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
    
    Call arguments received by layer 'spectral_normalization' (type SpectralNormalization):
      • inputs=tf.Tensor(shape=(None, 10, 10, 1), dtype=float32)
      • training=True
```
"
tensorflow/tensorflow,2023-09-15 09:48:15,bug,Fails to build on AARCH64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

/tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --action_env=PYTHON_BIN_PATH=/usr/local/bin/python3 --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/core/kernels/image:resize_bicubic_op_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/lite/kernels/BUILD:497:11: Compiling tensorflow/lite/kernels/rng_util.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/kernels:rng_util) 
  (cd /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\
  exec env - \\
    CACHEBUSTER=20220325 \\
    CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang \\
    LD_LIBRARY_PATH='' \\
    PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\
    *** \\
    PYTHON_BIN_PATH=/usr/local/bin/python3 \\
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\
    TF2_BEHAVIOR=1 \\
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' -DFARMHASH_NO_CXX_STRING -DEIGEN_ALLOW_UNALIGNED_SCALARS -Wno-sign-compare -O3 -fno-exceptions '--sysroot=/dt10' -c tensorflow/lite/kernels/rng_util.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o)
# Configuration: 91cacbf6409fd17883ece1a0e16168e33815822fe5d36a44e64642fa9b0e32ee
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/lite/kernels/rng_util.cc:15:
./tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:26:38: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                     ^
./tensorflow/lite/kernels/rng_util.h:26:54: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                                     ^
./tensorflow/lite/kernels/rng_util.h:27:49: error: use of undeclared identifier 'uint32_t'
                                     std::array<uint32_t, 2> ctr);
                                                ^
./tensorflow/lite/kernels/rng_util.h:32:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:32:36: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                   ^
./tensorflow/lite/kernels/rng_util.h:32:52: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                                   ^
./tensorflow/lite/kernels/rng_util.h:33:47: error: use of undeclared identifier 'uint32_t'
                                   std::array<uint32_t, 4> ctr);
                                              ^
8 errors generated.
```
"
tensorflow/tensorflow,2023-09-14 12:16:19,bug,restoring checkpoint loads weights partially,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am following this [tfa seq2seq tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) for building a seq2seq network with LSTMs. I trained my model and got great accuracy. I saved my model with `tf.train.Checkpoint`. Then, I tried to reload my model with `checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))`. 

However, the model gets restored partially.

My encoder weights get restored, however the decoder does not.

How to resolve?

### Standalone code to reproduce the issue

```shell
from typing import Tuple

import tensorflow as tf
import tensorflow_addons as tfa

from tensorflow.keras.preprocessing.text import tokenizer_from_json

import json

import numpy as np

import unicodedata
import re
import os
import io
import time
from collections import Counter

MAX_SEQUENCE_LENGTH = 30

def math_tokenizer(expression):
    regex = r'(\\d+|[\\+\\-\\*\\/\\(\\)\\^]|[a-zA-Z]+|.)'

    # Use the regex to split the expression into tokens
    _tok = re.findall(regex, expression)

    # Split numbers into individual digits
    ret_tokens = []
    for token in _tok:
        if token.isdigit():
            ret_tokens.extend(list(token))
        else:
            ret_tokens.append(token)

    # Filter out empty strings
    ret_tokens = [token for token in ret_tokens if token.strip()]

    return ret_tokens

def change_variable(expression):
    # Define regular expressions to match different tokens
    tokenstream = math_tokenizer(expression)
    variable = None
    for idx, tok in enumerate(tokenstream):
        if len(tok) == 1 and tok.isalpha():
          variable = tok
          tokenstream[idx] = ""var""
    return ' '.join(tokenstream), variable

def text_cleaning(x):
  modified_text = [None] * len(x)
  tokens = set()
  tok_list = []

  variables = []

  for idx, dx in enumerate(x):
      lhs, rhs = dx.split(""="")
      tokenstream, v = change_variable('='.join([lhs[2:-4], rhs[:-1]]))
      tokens.update(tokenstream)
      tok_list.extend(tokenstream)
      variables.append(v)
      modified_text[idx] = ''.join(tokenstream)

  return modified_text, variables

def generate_train_test_dataset(data, TRAIN_SIZE):
  modified_text, variables = text_cleaning(data)
  inputs = []
  targets = []

  for idx, dx in enumerate(modified_text):
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)

  train_inputs = inputs[:TRAIN_SIZE]
  train_targets = targets[:TRAIN_SIZE]
  train_variables = variables[:TRAIN_SIZE]

  test_inputs = inputs[TRAIN_SIZE:]
  test_targets = targets[TRAIN_SIZE:]
  test_variables = variables[TRAIN_SIZE:]

  return train_inputs, train_targets, train_variables, test_inputs, test_targets, test_variables

  class MyDataset:
    def __init__(self, problem_type='calculus'):
        self.problem_type = 'calculus'
        self.inp_lang_tokenizer = None
        self.targ_lang_tokenizer = None


    def unicode_to_ascii(self, s):
        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

    def preprocess_sentence(self, w):
        w = w.strip()

        w = 'start ' + w + ' end'
        return w

    def tokenize(self, lang, func):

        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=None, oov_token='<OOV>', analyzer=func)
        lang_tokenizer.fit_on_texts(lang)

        tensor = lang_tokenizer.texts_to_sequences(lang)

        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

        return tensor, lang_tokenizer

    def load_dataset(self, dataset, func):
        # creating cleaned input, output pairs
        targ_lang, inp_lang = dataset

        targ_lang = [self.preprocess_sentence(w) for w in targ_lang]
        inp_lang = [self.preprocess_sentence(w) for w in inp_lang]

        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang, func)
        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang, func)

        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer


    def call(self, dataset, BUFFER_SIZE, BATCH_SIZE, func):
        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(dataset, func)

        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))
        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

        return train_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')


  def call(self, x, hidden):
    x = self.embedding(x)
    output, h, c = self.lstm_layer(x, initial_state = hidden)
    return output, h, c

  def initialize_hidden_state(self):
    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.attention_type = attention_type
    self.max_length_output = max_length_output

    # Embedding Layer
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    #Final Dense layer on which softmax will be applied
    self.fc = tf.keras.layers.Dense(vocab_size)

    # Define the fundamental cell for decoder recurrent structure
    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)



    # Sampler
    self.sampler = tfa.seq2seq.sampler.TrainingSampler()

    # Create attention mechanism with memory = None
    self.attention_mechanism = self.build_attention_mechanism(self.dec_units,
                                                              None, self.batch_sz*[max_length_input], self.attention_type)

    # Wrap attention mechanism with the fundamental rnn cell of decoder
    self.rnn_cell = self.build_rnn_cell(batch_sz)

    # Define the decoder with respect to fundamental rnn cell
    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)


  def build_rnn_cell(self, batch_sz):
    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,
                                  self.attention_mechanism, attention_layer_size=self.dec_units)
    return rnn_cell

  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):
    # ------------- #
    # typ: Which sort of attention (Bahdanau, Luong)
    # dec_units: final dimension of attention outputs
    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)
    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)

    if(attention_type=='bahdanau'):
      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)
    else:
      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)

  def build_initial_state(self, batch_sz, encoder_state, Dtype):
    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)
    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)
    return decoder_initial_state


  def call(self, inputs, initial_state):
    x = self.embedding(inputs)
    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[self.max_length_output-1])
    return outputs

def loss_function(real, pred):
  # real shape = (BATCH_SIZE, max_length_output)
  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )
  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  loss = cross_entropy(y_true=real, y_pred=pred)
  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1
  mask = tf.cast(mask, dtype=loss.dtype)
  loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

import json
functions = '8exp^(9e)'

f = open('modified_train.txt', 'r')
modified_text = f.readlines()
f.close()

modified_text = [m[:-1] for m in modified_text]

BUFFER_SIZE = 32000
BATCH_SIZE = 64
# Let's limit the #training examples for faster training
num_examples = 30000

inputs = []
targets = []

N_TRAIN = 800000

for dx in (modified_text):
    try:
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)
    except:
      print(f""Error at: {dx}"")

train_inputs = inputs[:N_TRAIN]
train_targets = targets[:N_TRAIN]

test_inputs = inputs[N_TRAIN:]
test_targets = targets[N_TRAIN:]

data = (train_targets, train_inputs)

print(""Creating the dataset"")
dataset_creator = MyDataset('calculus')

# print(""Training the tokenizer"")
# train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

f = open('./inp_lang_tokenizer.json')
inp_json = f.read()
inp_json = json.loads(inp_json)
f.close()

f = open('./targ_lang_tokenizer.json')
targ_json = f.read()
targ_json = json.loads(targ_json)

inp_lang = tokenizer_from_json(inp_json)
targ_lang = tokenizer_from_json(targ_json)


# example_input_batch, example_target_batch = next(iter(train_dataset))
# example_input_batch.shape, example_target_batch.shape
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1
max_length_input = 31
max_length_output = 31

embedding_dim = 128
units = 256
steps_per_epoch = num_examples//BATCH_SIZE

train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

## Test Encoder Stack
example_input_batch, example_target_batch = next(iter(train_dataset))
print(""Creating encoder"")
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)

sample_hidden = encoder.initialize_hidden_state()

# Test decoder stack

print(""Creating decoder"")
# vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_length_input, max_length_output, 'luong')
sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))
decoder.attention_mechanism.setup_memory(sample_output)
initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)

print(""Creating the optimizer"")
optimizer = tf.keras.optimizers.Adam()

checkpoint_enc_dir = './training_checkpoints/encoder'
checkpoint_enc_prefix = os.path.join(checkpoint_enc_dir, ""ckpt"")

checkpoint_dec_dir = './training_checkpoints/decoder'
checkpoint_dec_prefix = os.path.join(checkpoint_dec_dir, ""ckpt"")

checkpoint_enc = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder)
checkpoint_dec = tf.train.Checkpoint(optimizer=optimizer,
                                 decoder=decoder)

# restoring the latest checkpoint in checkpoint_dir
checkpoint_enc.restore(tf.train.latest_checkpoint(checkpoint_enc_dir))
checkpoint_dec.restore(tf.train.latest_checkpoint(checkpoint_dec_dir))

print(decoder.embedding.variables)
```


### Relevant log output

```shell
[]
```
"
tensorflow/tensorflow,2023-09-12 03:18:12,bug,load_associated_files does not load a txt file,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On ""load_associated_files"", it emit longest `does not exist` error.
All the lines below reproduce the same result.

```
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])

populator.load_associated_files([])

populator.load_associated_files(['label_file.txt'])
```

The label_file.txt exist in the same folder as main.py.
<img width=""981"" alt=""スクリーンショット 2023-09-12 12 15 38"" src=""https://github.com/tensorflow/tensorflow/assets/52132649/178ef07f-e6a3-446e-856e-865b12a36962"">



Also, what information am I supposed to put in the txt file ?
Is there any documentation for that ?

### Standalone code to reproduce the issue

```shell
# Creates model info.
model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = ""Cup classifier""
model_meta.description = (""Identify a cup"")
model_meta.version = ""v1""
model_meta.author = ""Integro""
model_meta.license = (""Apache License. Version 2.0 ""
                      ""http://www.apache.org/licenses/LICENSE-2.0."")

# Creates input info.
input_meta = _metadata_fb.TensorMetadataT()

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()

input_meta.name = ""image""
input_meta.description = (
    ""Input image to be classified. The expected image is {0} x {1}, with ""
    ""three channels (red, blue, and green) per pixel. Each value in the ""
    ""tensor is a single byte between 0 and 255."".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats



# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = ""probability""
output_meta.description = ""Probabilities of the 1001 labels respectively.""
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
label_file = _metadata_fb.AssociatedFileT()
label_file.name = 'label_file.txt'
label_file.description = ""Labels for objects that the model can recognize.""
label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
output_meta.associatedFiles = [label_file]



# Creates subgraph info.
subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]

b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()

populator = _metadata.MetadataPopulator.with_model_file(tflite_model)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])
populator.populate()
```


### Relevant log output

```shell
.....{q2\\xbd\\xf6\\\\\\x9c\\xbd\\x80\\xa1\\x0c=\\xa6\\xfa\\x1f\\xbd\\xc2\\xdd\\x99\\xb9:x\\xa9\\xbb]B\\xe4=\\xd4\\xef\\x14\\xbdm\\xcb\\x0c\\xbdO8\\xfb\\xbdk\\x8dl=}s\\x11=\\xb6\\xcf\\xab=\\x07x\\x0c=\\xf2t\\xe5\\xbc:\\x01\\x8a\\xb9\\xdb+\\xa0\\xbd\\x0f\\xf5\\xea\\xbd\\xf8\\xaf\\x1d\\xbd\\xe3s\\xde\\xbcm\\x86\\x87\\xbd\\x11\\xa3\\x18=\\x1e\\x0bM=\\xb6\\x00\\x17=\\x97:\\xd0=\\x10\\xda\\xf2;\\x89\\x1b>=.0\\xa4\\xbde\\xd7\\xad\\xbd\\x88\\x00R=\\x0f\\xe7\\x85=_\\xda\\x9b=\\xd2v\\xfe<\\xa9\\xe9\\x07=\\xba\\x91\\xaf=\\xaa\\xad\\x98\\xbc)\\x9c\\xbe\\xbc""\\x17s=0n\\xfc\\xbdB\\x9c\\xd1=\\x9er\\t=R\\\\\\xad=\\xac\\xc9x=\\xe9R\\x94\\xbdR\\xd4\\x13=\\xb9\\xc7D=\\x91\\x18T=J\\x8e\\x94=\\xe2\\xeb\\x8a\\xbd\\xe3\\xf8\\x00\\xbd6\\xa2\\x9b\\xbc\\xfa\\t\\x98=\\xf9\\x18\\xdf<C\\x1e\\xa7=\\xe2\\x9a\\xa0=]k\\x0b\\xbd\\x86.7\\xbc\\xbar\\xe4;\\xcb\\x9b\\x0c>\\xc2#\\xd9=\\xaaB\\x1f\\xbc\\xa4\\x03\\xbc\\xbd6\\xea\\xf0\\xbd\\xca\\xae\\xa8\\xbd(\\xb7\\xfa\\xbd\\xff\\x92D=\\xa66\\xd7\\xbd\\xf7\\xea)=`\\xf1\\xa2=\\x92\\x120\\xbd\\x8eb\\xa9=\\x13\\x8e\\xfd\\xbd\\xe0\\x9d)\\xbd\\n\\xd1$=\\xd7K\\x12\\xbd\\xb6X\\xa0<\\xaeL\\xd1=\\x053\\xa2=\\x01X\\xb3\\xbd8\\x9df\\xbd\\\\\\xef&\\xbd+k&\\xbc\\xc8\\xa0\\x8a=G\\x81\\xc8\\xbd\\xeal5=\\xd4`\\xbf\\xbd\\xca\\ny=T\\xb0P\\xbd<e\\x7f\\xbc\\xcaU^<\\x7f\\xc2\\xd1;p\\xa3\\xb8\\xbd\\x16\\xefF\\xbd%r\\x87\\xbd\\x81{\\xb1=\\xf9\\x13\\xfb\\xbc\\xc1\\xf8\\xce;\\x06\\x8a\\x87\\xbd\\r\\x8fK\\xbd\\xb2\\xd4\\x14=\\x9d\\xecL\\xbd\\x0c\\xe6""=F:o\\xbd\\xd8_\\x9c\\xbd\\x18\\xd6:=\\xa6\\x1b<=\\xde\\xd3\\xa1=fY\\xfd\\xbcw),=\\xec\\xa4\\x80=\\x89\\xdf\\x9b\\xbd\\x8c\\xf6\\x06\\xbe\\xc9\\xc7\\xab\\xbdO\\x00\\x93\\xbd+\\xf8\\x98\\xbd\\x84\\xeff\\xbb5\\xa3\\xd2=v4\\x19\\xbc\\xd3\\x7f\\xe7\\xbd\\xa3}\\x84\\xbd\\xd6\\xb2C\\xbd\\xda\\xc6`\\xbd\\xf5\\xde\\xb5\\xbdr\\xed5<uo\\x84<\\x03\\xd7\\xdb\\xbd\\xa9\\x00\\x94\\xbd\\x06(\\x88\\xbdM\\xa4\\x82\\xbd\\x1a\\xdb\\xc9=\\xcf\\x9d\\xcd\\xbc~SG\\xbd\\x18\\xeb~\\xbd\\xd0Wv<\\x01[\\x98=\\x92\\xba\\x98= \\x83\\xd5\\xbd\\x10\\x85b\\xbc\\x95+\\x92=\\x8a\\x1f\\x11\\xbb\\x18b$=<\\xe9\\xaf\\xbdjfr=\\x958\\xed\\xbd\\xd8\\xb2\\xc2<Q[\\x97\\xbd\\x14\\x9e\\x9e=\\xdd\\x19\\x18\\xbb\\x8c\\xa0\\x81\\xbb^0\\x1a<\\x7fW\\x00\\xbd\\x1c\\xc9\\xd4=\\x83\\xa8\\x04\\xbd\\xd7\\x99k=-\\xe5U=\\x16\\x83\\x9b\\xbcf\\x90\\xb9\\xbdJ\\x1f\\xab=\\x82\\x00\\xe3\\xbd@3.=\\x1e\\x98\\xac\\xbdX\\x8e\\x1c\\xbd\\x91\\x83\\xca=\\xed\\xedJ\\xbd@\\x96\\xd8=\\x87\\xb6\\x0e=?q\\xb4\\xbd\\xc4\\xe9\\xac=q\\xe8\\xbd\\xbd\\x7fe\\xed<:\\xb4\\x1a=G\\x9a\\xc9\\xbd\\xf0\\xc4\\xc2=NN\\xe8\\xbd\\xfdb\\xf7<\\xacs\\xb7\\xbd\\xa6\\xd9\\xf1\\xbcg\\xff\\x0b<\\xdc\\x8ea=u>\\x98\\xbb\\xc2f\\x86\\xbd\\xaf\\xd8I\\xbd|\\xc4R=Pk\\x04=\\xde\\xbfL=\\xb8&\\xa8\\xbd:\\xe0\\xf0\\xbdKo\\xc9\\xbd\\xac\\x1e-=\\xfe\\x0b\\xa5=2\\x02.\\xbdk\\xac\\x8c\\xbd\\xf8\\n;<w\\xca)<\\xc8\\xae\\xd9\\xbdj\\xd9\\x9d\\xbd\\xd4.B\\xbd\\x80]\\x02\\xbd\\x9a\\xe1\\xa3\\xbdGLe\\xba\\xac\\xf3N\\xbd0\\x81\\x12=\\xc0\\xa0\\xa5\\xbd\\x0c-\\xf3<\\xeb\\xca\\xda\\xbd>\\x0bE<}\\xcf\\x84=\\xb1\\xf9\\xac=\\xf5\\x87X\\xbdP""_\\xbc\\x8c?\\xf4<\\xb7\\x94\\x1d=\\xd2\\xdbG\\xbb\\xe5?<={Y\\xe3=\\xbf?\\x83\\xbd\\xfe\\xe8\\xbb=*\\xc2\\x9f\\xbd\\xbd\\xb3\\xd0=\\x18j\\xf9=\\x9b\\x8d\\xd0\\xbc\\x08\\xb4^<\\xa74*\\xbc\\xe7\\x12\\xea\\xbc\\x81\\x88Y=o\\r\\x7f\\xbdP\\x8f\\x8d<YH\\xa8\\xbba\\x0c\\xf9\\xbc_\\x87\\xac\\xbd7<_=\\x05\\xe53=\\x01\\xabH<%\\xfe\\x9f<%yP<w\\xd3\\x83\\xbd\\x9aL\\xbe<&\\xd3\\xb9=\\x93\\x94\\xbe=\\x85\\x11o\\xbd\\x11F\\xc1<""\\x8c\\xa2=\\xb5\\x13\\xb9\\xbb\\xae\\xe4\\xef\\xbd\\xb1y =\\x92\\xe5\\xc5=\\x10\\xcb\\xf8\\xbd\\xce\\x8b\\xd0\\xbd\\\\\\xceX=$\\xb5\\xe7<v\\x10\\x9f\\xbd\\xd3^\\xbf=n\\xff\\x9f<\\x83\\xcb\\x85\\xbd\\x14\\x7f\\x94=\\xfb\\xda\\x95=\\xa07q;\\x8f\\xe9\\xb7\\xbd\\xdc16\\xbc\\xed_\\xb3\\xbd\\xdf\\xf1E\\xbd_\\xa1\\x02\\xbe`B\\xb5\\xbd\\x1f\\x15\\x80=\\xc9\\xba\\x9b<6\\xa7\\x83\\xbd\\x8c\\x11\\xd2\\xbd\\xe2\\xaa\\x94\\xbc\\xafHO=D\\xeb\\xcb\\xbd\\x19\\xf6\\x12\\xbb9\\xbf\\x98=e\\xa9\\xb4\\xbd\\xde\\x89\\xa0\\xbc\\n\\xe6\\xcb<\\xa5\\x99\\x89:\\xc1z\\xd6=\\xb7h\\xc0\\xbde\\xde\\x13=Fo\\xf3\\xbd\\xc2\\xa5\\xaf\\xbd\\x8b\\x1bw=7\\xf0\\xac\\xbd\\x0e\\x9a\\x9d<4\\x00\\x9e\\xbb\\x8d?\\xd1\\xbd\\x84\\xf0w=]Y\\'\\xbd\\x80Dz=\\xe0""\\xc2=bG?\\xbc_7\\xc3=\\x83=\\xc5=;\\x11\\xea\\xbd\\x84|\\xbc=\\xdc\\xcf""<\\x02\\x01\\xa8\\xbc}\\x96\\xa0\\xbd\\xe5\\xf9\\xd7\\xbd\\xdaS\\xa8=y\\xd1\\x96<\\xe82U\\xbdS\\xbb\\x9f\\xbdr\\xf7\\xca=\\x87\\xb2\\xa7\\xbc\\xde\\x9c\\xfa\\xbd\\xb0\\x12\\x01=\\xef\\\\\\xa3\\xbc\\x00.\\xb4=\\xdf\\xfdS=\\x87\\xc3h=\\xae \\xa9\\xbc\\x08\\xd8\\xc9\\xbd\\xd2P\\xcf\\xbc\\x991\\xa4=\\xa1s*\\xbd\\x07\\x16\\xee;\\xb4\\xbeO\\xbd\\x8b\\x95\\xfd<\\xe8H\\xb7<\\xdd\\xe4\\xd7\\xbd\\x87\\xefo=\\x9f\\xc8\\xce=\\x04A\\xa6\\xbd\\x88\\x95q=\\xcd\\xf9\\xd7\\xbdZ\\xc4\\xb3\\xbd\\xcap~<\\xbd\\xa8\\x86\\xbd\\xf5\\xf2\\x83=\\xac3^=\\x1c\\xc6%\\xbdn\\xd6\\x9b\\xbd|\\xa5\\xda\\xbd\\x12\\xec\\x98=\\xd8\\xd2q=H\\x1f\\x01=\\xe7\\xcf\\xec<\\xb6G\\xd5\\xbc\\xf3>w=\\x10\\x05\\xff\\xbd\\x92\\xa2E;\\xd4\\x8fm\\xbd\\x07\\xce\\x1c=\\xc3O:\\xbd%\\xca=\\xbcKi\\x8d\\xbd;\\xef,=\\xee\\x1d\\x91=\\xca=Z\\xbd\\x166w\\xbc\\x1a<\\x83=\\xbeG\\x9e\\xbd\\xbcw\\x04=\\xf3$\\x96\\xbd\\xdf\\xd5\\xa6<\\xd8j\\xd9=e\\xe7\\x9b\\xbd3s\\x81\\xbd\\x04\\x15\\xb7\\xbc\\xc8\\x9e\\xb0\\xbd\\xe8\\xad\\x9b<{\\xab\\x94<\\xa1=\\x9a<\\x81\\xfe\\xcf\\xbda\\xbbr\\xbc\\x95\\x9f\\x8d=\\r\\x9e\\xba\\xbd\\x84(&\\xbd-L\\xad=\\x10)^=\\xb1\\xefQ=u\\xde\\xb68\\xb7\\xe2\\xab<\\x88\\x1c\\x9a=R\\\\\\x00\\xbd\\xa2q\\x96\\xbd\\xda\\xc3,\\xbd\\xaf\\xc4\\xf3\\xbd\\xed<:=\\xd3\\xc1\\xe8\\xbd\\x9d\\x12\\xa3\\xbdG$\\xb2\\xbdC2\\x1e\\xbc\\xb7o\\x05\\xbd\\x10\\xe0Q;\\x08\\x19\\xe7\\xbd\\xd7\\xf4\\x94\\xbdB\\x85\\x97\\xbd\\xbf\\xf8\\xab\\xbd\\xb9|m=/h\\xa5\\xbd\\xbe7\\x15=\\xfd\\xd8\\xa5\\xbd\\xbb\\xbc\\xa8\\xbd\\x8c\\xe3\\x02\\xbdx\\x05\\xfe<q}\\xcd=G\\xd8\\xcf\\xbc\\r\\x84\\xb1\\xbd\\xea\\n\\x84=\\xbe\\xfai=\\x80b\\t=\\xc5i\\xa2=\\xf5<""<+K~=_\\xcb#\\xbd\\xfa\\xe3N\\xbd\\x82(\\xe1<\\x19hs\\xbd\\xd6\\xc9\\x0f=\\xad\\\\\\x1e<\\xb5\\xdbL=&\\r\\x1e=I\\x9f\\xef<\\x05\\xbc\\xeb=\\x90\\x8e\\xa9\\xbb\\x1c\\x88\\xca\\xbaT(?\\xbd\\xa0\\x8e\\xc7\\xbd\\x99\\xa7 \\xbd\\x0e\\xa8\\x94=\\xa5*\\xea<\\xd8\\xc2g<\\xb26\\xc8\\xbd\\x92\\xd4\\xee\\xbcPQ\\xeb\\xbd\\xdf\\x00\\x14=\\xc2s\\xa6\\xbd\\xd6\\xf0\\x02\\xbd~\\x15\\xc5=\\xa4\\x90\\xce<\\x88\\x0b\\xb8\\xbc\\xe2C\\x8b\\xbcV\\xf5\\xb2=\\x97\\xcc\\xc7\\xbd\\xe9\\xf7P=\\xa3\\xceL;\\x92\\xdb\\xd8\\xbdsn\\x89= u\\xf2\\xbb\\xcc\\xf4\\xd2<\\x01\\xfe\\x96=\\x04\\x8c\\xa8\\xbc\\xbca\\xd3\\xbdUg\\xa2=\\xd5\\xda\\xeb=\\xb0:\\xd0\\xbbj\\x18=\\xbd\\x00\\xaf\\x9a:R\\xa0<=\\xe0Af;\\x98r\\xbb\\xbd\\xb5\\x0c\\x96\\xbd\\x0c\\xcb\\x08\\xbc6\\xa2\\xef\\xbd$W\\xa9\\xbd]\\x16\\x9b\\xbd\\x16T\\x9b\\xbc$\\xad\\xa0<\\x03\\x83J\\xbd\\xf7\\xac\\xbe\\xbda\\x9cY<l_\\xb3\\xbdE_\\xb6=5Z\\x0b\\xbd:N\\x8a<o\\x1e\\x9b=\\xc16\\x87=\\xe2[\\x1c\\xbc\\xc0\\xb0\\xf3\\xbcKo\\xb3=\\xd1\\x04.\\xbd\\xb6c\\xc2\\xbd\\x16w}=\\x8ezY\\xbd\\x93\\xf2\\xc8\\xbd\\xa1]\\xac\\xbd\\xd2\\xaf_=\\xad\\xf0z\\xbd\\xca\\x98\\x99\\xbdG\\x8a\\xc0\\xbd\\xbb\\xfa\\x9c=\\x93\\x00n=\\xa8Q\\xc1\\xbc\\'\\xc8\\x95\\xbd\\x97\\xc3\\x99=~\\x98\\x17=\\x83\\xdc\\xcf\\xbd\\xd9\\x04\\xbd=\\x05\\xee\\x94\\xbd\\xbcv\\x9b\\xbc\\xf3\\x88\\xa2=*\\xc9\\xab\\xbdd\\x9c\\xa2\\xbd\\x80d\\x13\\xbcaB\\xd6=hgg\\xbc\\x90\\xd0\\xce\\xbc\\x03\\x17\\xc2=\\n\\x83\\x93\\xbd\\xccac\\xbd$\\xcd\\x83=P\\xdab\\xbc\\x11\\x1e\\xb0\\xbcmP\\xb9\\xbd\\x10\\xeb\\xe6\\xbc\\x83\\xa1\\x93\\xbd8\\xdf\\xad\\xbd\\x08\\xf5J<\\xfbW\\xfe<\\x1eF\\xc2=\\xd1\\x11\\xea;\\xa5\\x9e\\xcc=\\x9d[\\r=\\xc87\\xf9\\xbd\\xf0\\xafH\\xbc W\\xda<<x+<\\x88\\x86\\x08<\\xd7\\xd5\\x97<w\\x99\\xb5\\xbc\\xcd""B\\xbd\\xa1:\\xfe\\xbd\\xd1m\\x96\\xbd\\x12\\xe2\\xb0\\xbd]~7=}\\x80V=\\xf0\\xed\\x94\\xbdB\\xcb\\x17=\\xe1\\xfb\\x8c=\\xaa_\\xba=\\xb6)g\\xbd\\xde/\\x9b\\xbd\\x94\\x18\\xc8\\xbc<\\xde\\xc1<\\x91\\x9a\\xa9=r\\x04x\\xbdZ\\xfb|\\xbd\\x04\\x15\\x96<\\x19C\\xad=\\xc7\\xca\\xc3\\xbd[\\x95\\xe9\\xbd\\xd2.T\\xbd\\xb0_\\t\\xbd\\x8e\\xe2\\x04=\\xfbM\\xbe=\\x18g\\xe3\\xbcdf\\x8f\\xbd\\xb6\\x8d\\xab\\xbd&\\x8c\\xb9\\xbd\\x98\\xc2\\xee\\xbd\\xdd\\x9a4=\\x0e\\xa65<\\xfa,\\x0f\\xbd\\x1dX\\xe7<\\x7f\\x0c\\xdc=!\\x08\\xb0\\xbd>\\xa2\\xce\\xbc7\\xd7\\xb9\\xbd1K\\xbf=\\x98d\\xcf\\xbdt\\xa2\\xb7=\\'\\x18\\xa9=\\x92\\xe7\\xb7\\xbd\\xfe\\xb7\\xcf\\xbd\\x9bn\\x85\\xbd\\xcd\\x0ca=\\xa1Cy=\\x84\\xb5\\xe3<\\xc7X_<\\x8ey%=bK\\xbe=\\xab\\x01\\xb6=h\\x88\\xd4\\xbc2\\x92I\\xbd\\xfc\\xee<=62\\xef<!\\x1c\\x15\\xbc\\xf8$+\\xbd|\\xf6z=\\x8b\\xc6\\x89=z\\x8dp\\xbd\\x8a\\xe0/\\xbd\\xf2g\\xcf\\xbd\\x16\\x18\\x01\\xbe{_\\xc3=\\x9b\\xf7\\xaf\\xbc\\xda\\xa3\\xbc;\\x0bM\\xe4\\xbdv\\xbf\\xd9\\xbcD:I=\\xd4\\xcc\\xbc\\xbd\\xc6x\\xdc=\\xcdG\\xa2=\\x1e|\\xdd\\xbd\\x0e#\\x9c=\\xb3\\xf0\\x8d=->\\x83\\xbd\\xa1\\xd2N\\xbdVD\\x84\\xbc\\xe2\\x1c\\xc8=\\xa6\\xab\\xec<\\x1a\\xd4\\x81=\\xe0\\x99\\xdb<\\xeb,\\xc6\\xbd1\\xef\\xcf=\\x1f\\x1f\\xbf\\xbd\\xc2\\x84\\x19<\\xd0\\x8an=\\xdbg\\x86=fs\\x97=\\xe0=\\x02=\\x99\\xe3\\n\\xbdX\\x13\\xe1<)\\xe1u\\xbc\\xba\\x9f\\x03<\\x8a\\xc8Z\\xbd^[\\xbd=\\xdfx\\x9a\\xbdR\\x18\\x9e=\\xc7\\x8f\\x9c=\\x98P\\x80\\xbd\\xa65\\x07\\xbb\\xbe|\\xa1=\\x04{\\xc8=\\x06\\x94\\x84\\xbd\\x86p4\\xbc\\xa7l9\\xbc\\x89\\x0e\\xa9\\xbdh\\x0f\\xb8\\xbc\\x02\\x9a$<\\xfb6\\x84\\xbd\\t\\xbe\\xc1=F\\xbe\\x14\\xbc@\\xb2\\xb0=\\x11s+=\\n\\xfb[=4\\xdfM\\xbb\\xf1\\x8f\\x89\\xbd\\xad\\xc1\\x9a<3\\xa1/\\xbdz\\xf8\\x9a\\xbc^\\x96\\xd9\\xbc\\x8c{\\xa8=\\x02\\xff\\xa3={\\x9a\\xa4=Z\\xc5\\x05=:\\x81\\x0b\\xbd\\x02\\x95\\xcd=a\\xcc\\xfc=A\\xf6\\xcc\\xbdA\\xa0\\xd7=""\\xa4\\x9d=\\xdd\\xee\\xcb\\xbd\\x8d\\x0e\\xce;Y\\xc7\\xfb\\xbc\\x1e\\xa0\\x8b=|\\xa6\\xf6\\xbdMS\\xa3\\xbdE7\\x10\\xbd\\xd5\\xc0\\x80\\xbd\\xf5\\x8a\\x8a=\\xb7\\'\\x9d=\\xb8\\xea\\xe1\\xbcJF\\xc0<p\\x99P\\xbd\\xe0Uz\\xbc\\xd2\\xf1\\x80=\\x01\\x01,\\xbdx\\x87@\\xbd\\xef\\x8b_=\\x90\\x99\\xbb\\xbdP\\x81\\xc1=~\\xf6\\xa6=\\xa1\\xa3\\x99=\\xd3mM\\xbd\\xb96\\xe2\\xbdP\\x8e\\xbd\\xbd\\x90fb\\xbdH\\x13\\xcb\\xbd\\xd3&\\xd0<;\\xea\\xb3<\\x02\\xc0\\xd2\\xbd\\xef\\xe4&<B.\\xbf=\\xf0\\xc6\\xaa\\xbd\\n\\x16*=T\\xcc\\xae=\\xcd\\xd7<=\\x94\\xfdv\\xbc\\xb9g\\x90\\xbdp\\x1e\\xc8\\xbd\\xb3#\\xef<\\xb0\\xf8\\x15=\\x9a\\xf3Q\\xba\\xa9B\\x8f\\xbd\\xf1j\\x9b\\xbd\\xb1\\x8f\\xc6\\xbd^\\x07\\x88=\\xc6sj\\xbd\\xed\\xf3\\xea\\xbd\\xf0\\xaa\\xd8\\xbd\\xd5\\x80e\\xbd\\xb5\\xfch<\\xc55\\x83\\xbd\\xc9\\xc5\\xaa=\\x06\\x8d\\x98\\xbd\\x9d\\xa4\\xc7;I\\xab\\xcf<\\x02\\xdcU\\xbd\\x83B\\xd3=\\x86\\x18\\xa1;\\x1f`\\x89=b\\x16\\xa6:l\\xf9\\xb0\\xbdN\\x16\\\\\\xbd>|i\\xbc>J\\xf1\\xbd\\xd3\\xe0\\xb8\\xbd\\x90lz\\xbd_\\xae\\xe2\\xbd\\xee\\x84\\xab=\\xcc@\\x83<\\x99\\xaf\\xa5=\\xe0Y\\xcf\\xbdH\\xaf\\x1e=\\x16\\x12\\xd2\\xbd\\xfd\\xcc\\xcc=\\xd17\\xa3\\xbcG\\xf7\\xfb=\\xd1\\xe9\\x9b\\xbc\\xdc\\x16\\xf6:\\xf4\\xb4\\xb3\\xbd\\xd4f\\x81=\\xf6\\x98\\x9a;\\xf2I\\x14\\xbda\\x98D\\xbd\\x16eS=\\x9cB\\x92\\xbd\\xd5\\x07m\\xbd)j\\x1f=\\xa0\\x1aW=\\xb1\\xe8_=\\x06\\x84\\x9b8n\\xe7H=\\x86\\x83T\\xbd\\x85H\\xbc\\xbd\\xdb\\xda1<\\x81N\\x1f\\xbd\\x9ff\\x1b=\\x96\\xa07=vCB\\xbdL\\xad\\xbd\\xbdjc\\xce\\xbd\\xb3))<\\x16$\\xdd\\xbd\\xde~\\x19\\xbd\\xd4\\xb5\\xa0<y\\xfd\\xda\\xbc\\xec\\xc8\\x84=\\xf5\\x1au\\xbb\\xdd\\xe9\\x0c\\xbd\\x10\\xfd\\x92=\\x02\\xe4\\x01=z\\x1f^\\xbd\\x8e\\xef\\xc9\\xbd\\x99\\xf0\\x85=\\x90\\xba\\xc9\\xbd\\t\\xa5\\xc7\\xbd\\xc4\\xaf\\xe1<\\xbf\\xae\\xc5\\xbd\\xbdu\\xcc\\xbcUI\\x94=\\xa0\\xf9\\xa5=\\x93\\x91\\x02\\xbdY\\xed\\x88<\\xd0L\\x16=\\x07}a=\\xc0\\xab\\xcc\\xbc\\x96\\xbf\\t\\xbd\\xe9]\\x82\\xbd\\x1c6\\xab=\\x1d\\x0e\\x81\\xbdm\\xb1\\xda<\\x13`\\x03<\\xb9\\x9d\\x14=\\xed*\\xbc<\\xd8V\\x96=\\xcf\\xc1\\x90\\xbc\\xf8\\x81\\x91=\\x1bn\\x9e=c\\xf9\\x93\\xbc\\xcf\\x9d\\xe3\\xbc\\x91\\x04\\x85\\xbd\\xb0\\xbe\\xb2=C%\\xa2\\xbc\\x82\\x8e\\x99\\xbd%\\xe4\\x86\\xbd\\'\\xe6Z=\\xfc\\x89V\\xbbI\\xaf\\xda<\\x82\\xec\\x17\\xbdp\\xeer\\xbd\\n\\x8e\\x9c\\xbd\\x10&\\x1f=\\xfa:\\x14\\xbd\\xea\\xbf%\\xbd\\xdfB\\x02\\xbes\\xeb\\xff;\\xfb \\xc4\\xbd\\xcf\\xcd\\xed\\xbd\\xe7\\x96\\x83\\xbdK0\\x81=\\xc0\\xb0\\xcc\\xbd\\x81\\x05\\x9d\\xbd\\xa4\\xff\\xc7=Lv7\\xbdJ\\x04\\xf9\\xbdp\\\\\\xa3=\\x0e\\xa9 \\xbd-\\xc3\\x9d\\xbcy\\xbeb=\\xc9\\x1b\\x92=\\x01\\x97\\xbb\\xbd\\xd6\\xba\\xb2=\\xda\\xee\\xeb\\xbc\\xbb\\x16\\x94<\\xdew\\x03\\xbe\\t+8\\xbd\\x05<\\x88<\\xc3l\\x02=\\xad\\xac\\x91<\\xde\\x1f6\\xbd\\xf2U4\\xbc\\xab\\xddx\\xbd\\x03\\xf4\\xc1\\xbd\\xaci\\x10\\xbd?\\x9c\\xc0\\xbd\\xffMh=dM\\x14\\xbd?\\xa6n\\xbd]j\\x04\\xbd\\xa5]\\x05\\xbb\\x0cs\\xac\\xbd\\xf6%\\xbd\\xbd\\xd4s\\xa9<\\xa8\\x1f\\xbd=\\x19\\xdb\\xac<\\x01\\xda\\xdb\\xbd\\x0c\\xa6\\xdd\\xbc\\xdcz\\x85=<\\xde}\\xbd\\r\\xda:\\xbd\\xee\\xa3\\xcb=\\x9bM\\xac\\xbdz\\x8aT\\xbcHF\\xc3=!}\\x7f\\xbd\\x1d>\\xb2\\xbd\\xe9\\xbe\\xba=^\\x0c\\x9b=\\xef-M\\xbd\\x11\\x8f\\xb9\\xbd\\xb9>\\x8c\\xbb8\\xbf\\x1d=\\xd5\\xdc!\\xbd\\x1a\\xb0\\xae\\xbd\\xb7E\\xe5;.\\x8fN\\xbd\\xaf&\\xfc\\xbd\\xd2}\\xfa;u\\xa4;\\xbc\\xb9-\\x93\\xbd\\x0880\\xbc]\\xba\\xc5=[\\xbc\\x87\\xbd8]\\xe5\\xbdz\\xa4\\x08\\xbc\\xe4\\x87\\x95=\\x8f\\xf7\\xa6=3\\xc9y=\\x1a&b\\xbd\\xc4\\x88\\xd8=t\\xde\\xbb\\xbb_m\\xd1<\\r\\xa5\\x9b=\\xf8x\\xc7=\\xc4\\xc1\\x0c=w\\xcf\\xe0\\xbd9\\xc3P\\xbb\\xf5J\\x8c\\xbd\\'\\xc82=:\\xe3\\xb8=\\rp\\x7f\\xbd<\\xf0T\\xbd\\xdb3\\xcd=\\xfd\\r\\xcd=\\x9d\\x7f\\xad=\\xea\\xc6\\xe1\\xbd-\\x98\\xa1\\xbd\\xde\\xc6\\xce=\\xcc\\x8b|\\xbd\\xd6\\xb4\\xc3<\\x1ax/\\xbdV,\\xce=\\xb9o\\x1d\\xbd\\x1b\\xac\\x84\\xbdtF\\xd0;\\xe2\\x1f\\xb0=\\xf7\\r\\xb7\\xbcb\\xf6\\xa4\\xbdt/\\xd3\\xbd\\x9f\\xb4\\xad\\xbd\\xd9\\x06\\x1c\\xbdt\\n\\xce=\\xa6\\xc5\\r=}t\\xf1<;]\\x93\\xbd\\xbb5\\xee\\xbd\\x14\\x8b\\x8d=I2\\xa9\\xbd\\xa7\\xd4\\t\\xbc\\xb7\\n\\xe2\\xbb\\xaca\\xc6=\\xc9!\\xb7=\\xef\\x9c\\xe2\\xbc_,\\xd2;\\x1c\\xbe\\'<\\x9d\\xc5\\x8b=\\xd57\\x1d\\xbd\\x9b\\xd2\\xbb=1X\\xb6\\xbd\\xcc\\xe6Q\\xbd]E\\xcb=\\x87\\xab(=g\\xdf\\xb0\\xbd\\xcb\\x1d\\x82\\xbd\\x0fb\\xae\\xbb\\xb34\\xb3=C&`\\xbdw+a\\xbd\\x96m\\x19\\xbd)s5=\\xfdF\\xc3=\\xf5F\\xb5=\\xa46\\x9f\\xbc\\xc7\\x96\\x8b<u9\\x05\\xbd;\\xe5\\x88\\xbdQN\\x86\\xbd\\xa9zy<\\xfa\\x80H=\\x15PJ=\\xd8\\xa4\\xa1=\\xb2\\x0f\\xa2=vl_\\xbd\\x93\\x82\\xbf\\xbdrP3=k#\\xc6<U]\\x81\\xbd\\x9d\\x7f\\x99\\xbd)\\xe8\\x7f\\xbd\\x82Y\\xda=`:\\x92=D\\xf1O\\xbd\\x89\\xf8 \\xbd\\x81|`\\xbb\\xd6\\xa6\\xea;6t\\xa8\\xbd\\x9f.\\xc7\\xbc\\xe0\\xbe\\xbe<\\x90\\xcd\\x93\\xbd2A&=:\\xecm\\xbbn\\xaa\\xa7\\xbd\\x1e\\xe9\\xfb<\\xae=\\x1a=g$\\xde\\xbdX\\xa2Z\\xbd\\xd4\\xde\\x00\\xbd\\xe9)\\xf2\\xbc\\xe4!\\xdf\\xbc\\xea\\xcc\\x8a\\xbc\\x05\\x94\\x8c\\xbd\\xf6P\\xcd=\\x16\\x12\\x8f\\xbd\\xa6\\xac\\x15=\\x8b$\\x1c\\xbd\\xcas}=\\xca\\x92\\xc7=\\x1a\\xb0\\x12\\xbd\\xf0\\xeck\\xbd\\xf03\\x0b\\xbd7\\x89\\x02>\\'\\xabM\\xbd\\xc5+r\\xbc\\xd8\\xd7\\xa6=\\xa6\\x0c\\xb5=\\x95\\t\\xb0=\\xd5\\xc1\\xdb;\\xc3\\xbb\\x86\\xbd\\x89\\xe6\\xde=\\x87\\xc0\\x1e\\xbd\\x80A\\xef=\\x87\\xfb\\xc3\\xbd\\x06k\\n\\xbd""R8=\\x99\\xddK\\xbd\\r\\xbf\\xf9\\xbda\\xc9\\xd2=\\xca\\xfb\\x17=\\xa5\\xac\\xdb\\xbd\\x12\\xfd9\\xbd2\\xb4%=\\x8c\\x93\\x92\\xbdX\\xb3\\xa7\\xbd\\xf2\\xef\\x97\\xbd\\xbf\\xad\\xf0\\xbd\\x8f\\x96\\xda=\\xe8\\xc8\\xf6\\xbb\\xafY\\x11\\xbd\\x9d\\x12\\x07>\\x9fE\\xa2<M\\xf0(=\\x8a\\xb1\\xdc\\xbb\\x7f\\xaa\\xb6\\xbd\\xb9p\\x8f\\xbd\\xb8\\xe8\\xb3\\xbdA\\x87\\xbd=w+\\xe4=\\x8d\\xcc\\x1d=\\x049\\xad\\xbd\\x9f\\xa4\\xd8\\xbd\\xf6W\\xf2<\\x028H\\xbc\\x90\\xb1\\x82=r\\xe2>\\xbc\\xf4\\x8c\\r\\xbd4\\xf3\\xab\\xbdd\\x95\\xde<\\xcb`\\n\\xbe\\\\\\xb38=\\x81\\xef\\xa5\\xbdq\\xd2\\x82\\xbd<\\xb2\\xb9;\\xb3\\x82\\x1a\\xbd]\\xad#=\\x10\\xb3\\xbc\\xbd7\\xee\\x84\\xbd\\xf037=\\xfd\\xaa[\\xbd\\x86\\x92\\xbb=\\xc6\\xcf==\\xb6b;\\xbd\\xec\\x01\\xcd<g%\\xbd=d\\x11\\xc2<\\xca\\xf7\\xa7\\xbd\\xc39\\x9b\\xbd\\xfaI\\xaa\\xbc:\\x1a\\xf5\\xbc\\x84F\\x1e\\xbd%;\\x8a\\xbd\\xb6\\xbb\\xa9\\xbd\\xd3\\x83\\xb1\\xbd\\xf8\\xf3\\xdf=\\x03~\\xeb\\xbc\\xe1\\xe8\\xf1=\\xe9k\\xc2\\xbd\\xcaa\\x91\\xbc\\xd1\\x81\\xe9\\xbc\\x06\\xbaK=!0Y\\xbdy\\xba#=\\xefL\\xe9=\\xea\\xd6\\xd0\\xbb\\xf3\\r\\x00\\xbd\\x0f\\xc4\\xf3<bX\\xc2<\\x8ai\\xb3\\xbd\\x95&3=\\xbb\\xfbh=\\xb9\\xe1\\xa3={\\x06\\x04>J/\\xcb=\\xb0\\x8a\\xaf\\xbd\\xd2\\x18M\\xbbm\\xa4\\x1b=\\x99\\xc8\\xe8\\xbds\\x0b-\\xbd\\x8c\\xed@=)\\xe0\\x8c\\xbd\\xef\\x17\\xed=\\xdf4\\x84\\xbd\\xb4\\x08\\x8b\\xbc\\'m[;\\xc85\\xbe\\xbd\\x82_\\xd7<UJ{=\\xadf\\xd5\\xbd\\xb4F\\xdc\\xbdS=\\x9d\\xbd\\xb7x\\x96\\xbc\\n6\\xc3\\xbd\\xf0\\x8f2\\xbd\\x0cM\\x97=\\x9b5^=\\xbbW}\\xbdE\\xdfM=8\\xa5\\xa5\\xbb\\xfd\\x8a\\xe9=r\\x91\\xc7=\\x9d\\\\\\xdc\\xbd\\xddV\\x1c\\xbd\\xa7\\xc7\\xc1\\xbd(\\xad\\xd9\\xbc\\xb8Y\\xba\\xbd\\x1c\\x9a?\\xbd\\x7f3\\x8e\\xbd\\xb7\\x11\\x1d<\\x03\\xc5\\xc6=#{\\xa7\\xbc\\x94\\xcf\\xd1=\\xb6\\xf9\\xc9\\xbc\\xbd!\\xd1\\xbd\\xba8\\xce\\xbd\\xaa/2=\\xffm!=\\x01\\xc9\\xd7=n\\x852<\\x8fU\\xbf\\xbc\\x8b|\\xa3<V}\\xc0<\\xa9\\xbd\\x9f=j\\xbe&<\\xb3$8=\\xd6 \\xb2\\xbd\\x98\\xda\\xf3\\xbd\\xce\\xe7\\xd9\\xbd\\xc3\\x1e\\xee\\xbde%K=0\\x9fZ=\\xac\\x85\\xd6=&p\\xab\\xbd\\xac{\\x00<t7g\\xbd{\\x01\\xd5=\\xc4eI=A\\xf7a\\xbc\\x18\\x15F\\xbd\\x7fT\\xff=\\x08\\xf4\\x10=#l\\xd7\\xbd\\\\p\\x9f\\xbdxm\\x0c\\xbd\\x8f\\x92\\x1e;O\\xde$=\\x8ev\\xfb\\xba\\xc9|\\x9e\\xbc\\xef\\x8a\\xdc=\\xa3\\n\\xae=\\xb9\\xe4o\\xbd\\xfa9\\xab\\xbd\\xa8\\x1f\\xd9=\\x06\\x15t\\xbd\\x9c.y\\xbd\\xc6BW=\\xa2*u=\\xda\\x07d<q\\xeb-=\\xd3\\xf2\\x92=\\xd8i^=e\\x83\\xd2=\\x12\\xa2\\xbc:\\xb6\\xaa+\\xbd.\\xe7\\xeb=]\\x9f\\x85=<\\t\\xa7\\xbdbZ\\xa3\\xbd\\xd2Z\\x9a\\xbc\\x07\\x87\\xc5=\\x84y\\xb2\\xbd\\x97\\xcc\\xe6\\xbc\\xfb\\xe2\\x11\\xbd\\x1bG\\xb4<\\xa9\\xfc\\xb9=\\xdam\\x95=v\\xf0\\xdb<\\x06\\x1e\\xb5\\xbd\\xbf\\x87\\xbd=\\xa0\\x1dA=\\x8e\\xfb\\xcc\\xbb#\\x19\\x15=Bb\\xb2\\xbd\\xf3\\xfd\\xf0\\xbd\\xe8\\xb9\\xe7\\xbc\\xd5\\x86\\xc0=4|\\x18\\xbd\\x82\\xed\\xfe\\xbc\\xc1\\n\\xf0=&6\\xdb\\xbc:\\xe5E\\xbd\\xbd!\\xd6=,\\xa5\\x89=\\xae\\xfcg\\xbdk\\xafT=\\x15\\x8d|\\xbcb@\\x81<Z\\x00\\xd8\\xbd\\x17@\\x88\\xbc\\xaa\\x84\\x9c=\\xc5\\xa4\\xeb\\xbd\\x08AX=\\xa5t\\xdc=\\x8f\\xb2\\x88=\\xda\\xe0\\xab\\xbd\\xefi\\xeb\\xbdJ\\x05\\x94\\xbdD\\xd9\\xda\\xbd\\xebs\\xb4\\xbc\\x19\\xb9\\xaa\\xbdU^\\xb2=\\xae\\x8f\\x89<\\xada^=\\x93\\x0e\\x10=\\x9f\\xe8\\x8a\\xbc\\xc4\\xd0\\x86=GR\\xac;\\x12\\xd4\\x9d\\xbd\\x95\\xbf\\xf3\\xbcgk\\x01\\xbd\\xacM\\xc6=0\\xd9\\xec\\xbc\\xba\\x02\\x9a\\xbc\\xcc\\xe5\\xa8\\xbbg\\xe51\\xbd\\xee\\x1e\\xf1<\\x04\\xf6\\xa0\\xbd\\xf7\\xe5\\xf3\\xbd\\xd3\\xb4T\\xbc\\x7fI\\x84=""\\xea\\x12=f\\x89\\xf7<\\xd9\\x00\\xb7<\\xe7K\\xe7=\\xb3\\xcb\\xb7=\\xd1\\xef\\xc6\\xbc.\\xaf\\x18\\xbcb\\x97\\x01>a\\xe0b=\\xed\\xbe\\xa3\\xbc\\xb2a\\xa1=e\\x19T=NX\\x95\\xbd\\xd3J\\x8b=3\\xf4\\xd0<\\xe6I\\x96\\xbdg4\\x80\\xbd\\xf4\\x82\\xf7\\xbd\\xc1\\x9d\\xd8\\xbc\\xa1\\x0e{\\xbd\\x8e\\xf4\\xfe\\xbd\\xa3\\x16\\xf3=3\\xbc\\x8b\\xbc\\xeb\\xac\\xb6=\\x8f\\xec\\x99<q\\xa2\\xe3<\\x14\\xccA\\xbd\\x02\\x9a\\xc8\\xbdf\\xf0:=\\xfe\\x89\\xc0\\xbbj\\xbe\\xb0\\xbc\\x8bC\\x80\\xbc!\\'5=\\xdb\\x83\\xb2=\\x1b\\xae\\\\\\xbd\\xf3\\xb2\\xd5=\\xe6i\\xa6\\xbdSi\\x95=A\\xb6G=\\x1f\\x95\\x94\\xbdf\\xac\\xf5;\\x1b\\x9c\\xad\\xbd:C8\\xbc\\xab\\x95\\x88\\xbc\\xe6\\x83\\x87\\xbd\\x92\\x13\\xee\\xbd\\x88\\x11\\xb8\\xbd\\xb1]\\xcc=\\x9doM\\xbd\\xde\\xa2)=>\\xc2v=\\x8a\\x97\\x06\\xbcj\\xc8\\xc0<\\xfd*\\x1f\\xbc\\xa8\\xd2#=5|\\x15\\xbd\\x05\\x10\\xcd\\xbc\\xf1\\xa8\\xb6\\xbd\\x85\\x94z\\xbd=\\xf0x\\xbdj[\\x92\\xbci\\x03(=\\xfeI~\\xbd\\xbdh\\xa4=\\x0e=\\x06>\\xdd\\xf4\\xba\\xbd\\x10\\xa6\\x9d=j\\xe0\\x96\\xbd\\xcb\\x03\\x91=C\\xb3\\xb1\\xbdE\\x8c \\xbb0\\x14\\xee;\\xc7\\xf9&\\xbb\\\\\\xea\\xf6\\xbd\\xdd\\x1e\\xe5\\xbb\\xd2\\xd6\\xa1\\xbc\\x96k\\x06\\xbd\\xa2\\xc5+:\\x845\\x1b\\xbd{\\xe0&\\xbd\\xb1\\x83Q=L\\xa5\\xb4\\xbd\\xa2\\x82\\xcf=\\x19\\'\\xce:\\x9e\\x96\\xc8=\\xec\\xa7\\xb4\\xbbW\\x0f\\xc3=\\n\\x00\\x9f\\xbc\\xb7-\\xbc=hj\\x98=\\xd1s\\xb1=d`s=\\xbb\\xd6 \\xbc\\xee\\xd9\\xa3\\xbc\\x16\\x9d\\xa7=\\xc2\\xfa\\x05=2\\x1b\\xff\\xba\\xca\\xd6g=\\x92\\xec\\x92\\xbdX\\xb5)\\xbbB\\xb8\\xc7\\xbd\\xacf\\x95=A\\xbf\\xc2\\xbc\\xb7\\xb5\\xb7=\\xa5\\xa2>\\xbd\\xb2\\x04\\x9f=\\x06m\\xbb<\\xe2\\xc6\\t\\xbd\\xc9\\x19\\xfd\\xbd!\\xa1;=~M7\\xbc{\\xe4\\x03\\xbc+\\xdd\\xbd=B\\xaf \\xbc\\x82\\x95\\xcf;c\\tS=\\x1eU\\xe5\\xbd\\xff\\x86l\\xbd\\xf3\\x06\\xa5;w\\xd6\\x16\\xbd\\x900\\x89<\\xd5;?\\xbd\\x9d[\\x98\\xbd\\xf2\\xff\\x08=\\x84\\x92\\\\;\\xe3\\xa5\\x87\\xbc\\x84\\xf4@=\\xe0\\x03&\\xbb\\xb9Q\\xbe\\xbd\\x9d0!=\\xc8_\\x88\\xbd{\\xa5\\xc8=\\xaafw\\xbdP\\x15\\xe5\\xbc\\x99\\x03\\x89\\xbd\\xe1b,==\\x7f9\\xbd\\xccK\\xe4\\xbdp<\\x95=\\xf0\\xc2\\xcd=\\xf5:\\x88=}FF\\xbdF4\\x1f=\\x80\\xe4b\\xbd\\xec\\x93\\xc8\\xbc\\x0e\\t\\xff;Ga\\xd7\\xbd<l\\xaa\\xbc\\n\\xa9}\\xbde\\xb9w\\xbd\\xdb\\x90\\xb9;\\x11\\xb7\\xab\\xbd@A\\x02\\xbd)\\x85_\\xbd\\x87C*\\xbd\\xd9\\xbc\\x85<5r\\x9f\\xbc_\\xfa\\xf3\\xbdGJ\\x00\\xbe\\xc7\\xb6\\x97\\xbd\\xf8M\\x08=e?\\xd4=\\xdf\\x9cG\\xbb6\\x8e7=m\\x16\\x08=\\xe5J\\r\\xbd\\xecS\\x1c=\\x83\\xa5\\xc0=\\xfc\\xe1\\xcd\\xbd\\xba\\xde\\xa6\\xbdy\\xd7N\\xbd\\x98{B\\xbc\\x1a\\xfe\\xff\\xff\\x04\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\xb6_)\\xbc\\xe4\\x81,\\xbc\\x9b\\xcdv\\xbb\\xf9\\x90\\x07=\\xee\\x96""\\xbcH\\x177\\xbc\\t\\xb4@\\xbc\\xe2S\\x1b\\xbc\\xd7kq\\xbc:\\xd3-\\xbc\\xbe\\r\\xc9<b\\xbd\\x92<\\x98\\xc5%\\xbcE6\\x13\\xbc\\x1d)8\\xbc\\x94N""\\xbc!\\xe4P\\xbcnDG<\\x02\\xec-\\xbb\\xbb\\x8c\\xd7;&\\xaa#\\xbc\\x1b\\xec\\x02=\\xa33T\\xbc5\\xf5\\r\\xbc\\x10\\xf7Q\\xbc\\xf2p3\\xbc[\\xa7\\x1d\\xbco(\\x01\\xbcE\\xe4B\\xbc\\xf6O\\x0f\\xbc\\x8fJ\\xfb\\xbb\\x03g\\xf6;\\r\\x98$\\xbc\\xb9\\xb9\\x83;\\xb6%Y\\xbc\\x82o\\x96\\xbb\\xa6\\xe3N\\xbc8O\\xf2<{U(\\xbcg \\xf1\\xbbOp#\\xbc\\x8b&3\\xbcnW\\xdd;\\x00\\x00\\x00\\x00\\x9b\\x92\\'\\xbc+a|:\\xb4\\xb5\\x98<\\x9e\\xc9\\x8d\\xbc\\x06\\\\\\xcd\\xbb\\xff\\xc3\\xe2\\xbb\\xcf\\x1e\\x10\\xbc\\xff :\\xbc\\x8es\\xc4\\xbb\\x1b\\xf8#\\xbc\\x0fD}\\xbc\\xf6a1\\xbc\\xbf\\xca\\x7f\\xbcE\\xcc\\x13=]\\xa2\\x18\\xbc\\n\\xe40\\xbc6\\xd8%\\xbc\\xfew\\xc3\\xbb\\xc8\\xb0G\\xbc\\xec\\xfdQ\\xbc&\\xff\\xff\\xff\\x04\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\xf6c^=5\\xd6\\xbc<\\xa5j4\\xbcosb\\xba\\x16\\xe0C\\xbc\\r\\x179\\xbc\\x8e|\\x1a\\xbchlT\\xbc\\xab\\x90\\x16\\xbc<t\\x1e\\xbc\\xcd\\x04h;C-\\xfb\\xbb\\x0c\\x90\\x84<H,\\x18\\xbaZ\\xa0/\\xba\\xaef\\xfd\\xb9W1\\x07\\xbc\\xc0\\xa37<\\xcf\\x19\\xf4\\xbb\\xc8{\\x14\\xbc\\xa7\\xbbM\\xbc\\x1e\\xc3\\x0f=,\\x0c.\\xbc\\xb1\\xe26\\xbc)\\x87\\x13\\xbcoA\\xf8\\xbb\\x1b\\x03<\\xbc\\x1519\\xbc\\xebz\\x14\\xbcE\\x89\\x03;\\xa9S\\xab\\xb8\\xb5~\\x15\\xbc\\xb2\\xff\\xff\\xff\\x04\\x00\\x00\\x00@\\x00\\x00\\x00K\\x1bo\\xbbKj?\\xbb6\\xee@;]r(\\xbc\\'\\x83\\xc1\\xba\\x1b*\\x84<;\\xde\\xc8\\xbc\\xef\\xd0\\x9c\\xbaWt%\\xbc\\xc8\\'\\x1b=K\\xaa\\x0c=*\\xd4p<\\xa3\\x9d\\x93\\xbcJ0\\x83; \\x9a.<\\xa8\\x10h\\xbc\\x00\\x00\\x06\\x00\\x08\\x00\\x04\\x00\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xc0\\x06\\x00\\x00\\x9a\\x07\\xc7\\xb9\\xb4\\x1f=\\xba;,\\xb9\\xb8\\xa0T\\x0b\\xb8`\\xbb\\x1b:/$\\xc99?\\xc8\\x808\\xbf2\\xc89\\xe1x_\\xb7\\xc3\\xcd-:1""\\xe59K\\x00\\xdf\\xb9\\x0e\\xf7\\x10\\xba\\xf8\\x13\\x1c\\xb91]\\xd6\\xb6\\xef\\x04\\x0e:\\x87|\\xb8\\xb83\\xfc\\x949H\\xfcd9\\xab]\\xe79Di\\xf98\\x84\\xac!:\\xe2\\xb4\\x85\\xb7\\x04\\x8f\\x8a\\xb9\\xa2\\x94\\x1e:\\xfc\\xca\\x18:\\x18\\\\r\\xb7\\x8d\\xb1\\x039\\x8d\\x0c(9\\x1au@\\xba[\\xc7a96!^\\xb9\\x94\\xbf\\xc1\\xb9F\\xc0\\n:\\xf7\\x84\\xd89\\x1fJD\\xba&\\xe0#\\xba\\xdcY\\x16\\xbaE\\xf7,:\\x18\\xc3\\x198HM\\xeb\\xb8a\\xa8\\x199`\\x8c\\xca\\xb9m\\'\\x13\\xb9\\xc4\\x05\\xf79\\xee*\\xae\\xb86g :rs\\'9S\\xe8\\x1c\\xba\\xfc}o\\xb8\\xc3<\\xd19\\x1a\\x0b\\x8a\\xb9-\\xc8\\x15:\\xbd\\xf0\\x1d\\xba\\xf2\\'$\\xba\\xa8\\x1al\\xb7\\xcfZZ9\\xb4L\\xe09h\\x16\\xcc\\xb9#\\x00=9\\xaa\\xa6\\x959Y\\x8e6\\xb9?\\xaa\\x9a\\xb9?\\xd0\\xd09*M99\\xaa\\xac\\xb1\\xb9\\xf3\\xa30:o(n9\\x8d\\x82\\n:\\xc5\\x15$:\\x93\\xd3""\\xba\\x11\\x16\\xfd\\xb9\\x16:\\xca9\\x90\\xcaq\\xb9\\x06\\xfa%:K{3\\xba\\x9bK\\x8d\\xb9\\x03K\\xbe\\xb9\\xac\\x05\\x16\\xba\\x06\\x00)\\xba\\xf9]\\x9d9\\x85!\\x1f\\xba:\\x142\\xbat\\x9a""\\xb9\\xb0\\xd4*\\xb9V\\x90\\x1b:~\\xc5v9,e\\xf99\\xd6X]\\xb9\\xa2s\\x1f\\xba7X7\\xb9\\x8f\\xe5\\x05\\xbar6\\xcc\\xb7%\\xcd\\x909x\\x98\\xa59\\xc0\\xfd\\xb8\\xb9\\xc16\\x1b:\\xcbe2:W\\xe2\\xa39w}\\xca\\xb9\\xf4\\n\\xd07\\x89V""\\xba\\xfa\\xbc\\x05\\xba\\x97\\xb6t\\xb80,J\\xb7\\xc4\\x00\\xf19 M\\xcc\\xb9\\x02%V8\\xc7\\x18\\xce\\xb9\\x8d\\x932:\\xc6\\xa9X:w!\\x04\\xbaJ\\xbb\\xd8\\xb9\\xba\\xe7V:\\xa8{\\x869\\x1c\\x06\\x0e\\xbaC\\x8b\\xd19s\\xc3\\xa28\\xbbT\\n\\xba\\xe3x\\x18:\\xbe\\xa7\\xa3\\xb9,\\xb5\\xae\\xb9\\x84\\xb4\\xee\\xb9\\xe5n\\xa19\\x9a\\xca\\xed8\\xb9\\x13\\x869\\x0bh8\\xb9_\\x13\\x11\\xba\\xd7\\xcb\\xea70_f\\xb9\\xab\\xdd\\xd4\\xb8\\xaa\\xfc\\x8b9\\xb0*#:]E\\xc0\\xb9\\x9e\\xa5S\\xb9\\x8f{\\xc09p\\xe6@9)M\\xe8\\xb7\\x15\\xe8r9:$9\\xb9\\x0b\\xb3\\x17\\xb9\\x9f\\xc4\\xff\\xb8\\x82\\x03\\xf2\\xb9I\\xa6+:\\x13\\x82\\xf69\\xe5}\\xf3\\xb7\\xcf\\x1c\\xa1\\xb90\\xbc%:Sx""\\xbaA\\xf0\\xb3\\xb9\\x11E\\xc1\\xb8t\\xa7L\\xb9\\xc3M\\x1a\\xba\\xcb1\\x1c\\xb6J\\xf5\\xb9\\xb9\\x80K\\xfb\\xb9\\xa9\\xe2\\x8f9\\xdb\\x038:\\xf4\\xd3\\xb48\\x99\\xb8\\xcb9\\x9f\\xaf\\xd4\\xb9\\x98""#\\xb9^\\xfc0:d\\xfbR\\xb8\\x8c\\x0e\\x829C\\xd8\\xc0\\xb97\\x909:\\x8f\\xb4\\xfe\\xb9\\x8d\\xa1\\xca\\xb8\\x04L@\\xba\\xf3\\xc9\\x9e9O\\xd6\\xc3\\xb8\\xb8_\\x0c9<\\x8a\\xe17\\xb8\\xc7\\xad\\xb9\\xd0b\\xcb9\\x8f\\xd4\\x9f9\\xaed\\x0e:0\\xc4\\xf59\\xc3\\x0f\\xdd93!\\xe78\\xa0\\xbeS9\\x81\\x88\\xd39\\xca\\xf0:\\xb9\\t\\x86w\\xb9lV\\x9f9\\x05\\x08\\xb79\\x1c\\x03T8[%\\xba9M-o\\xb8\\x9f\\xa8\\x12\\xba\\\\l\\x13:\\xf1\\x9f{9\\x88\\xc7\\xd99\\x140\\x05\\xbagaF\\xb9]\\x8d\\xfb4\\x18e\\xa5\\xb9\\xa4\\x87\\xe0\\xb9\\x85\\x0cQ7\\x8aY\\x079\\x9f\\xd6\\xe9\\xb9n\\r\\xe1\\xb9Iw*\\xb9\\x88\\xf9m7I\\xce!:\\x06`\\x8c\\xb9k\\xdf\\xc6\\xb9\\x83\\xa9\\xd89\\xd8T\\x8d\\xb9;\\xf9\\xaf9\\xf8\\x95\\xd6\\xb9\\xfc\\x98\\r\\xba\\x97\\x19Q9\\x83\\xa9\\xe0\\xb8\\xbd\\xd7\\x9f\\xb9\\xdb\\xfaX\\xb9\\xc1\\x10\\xb09\\xe8\\xe7$:\\xba\\x8dP\\xb8\\x8b8\\xd57\\x95\\xc1(:$6\\xcd\\xb8\\xf3\\xf2\\x9d\\xb9\\x8a\\x0e\\x07\\xba\\x1cXP9x\\xd1\\x83\\xb8\\xc5!^9h\\xe9\\xa39\\x01m>\\xba\\x03\\xb8\\x04\\xb9H\\xf3/\\xba\\xaf\\xf3\\x10\\xba\\xbe3\\xad\\xb9%&\\xe09\\xc7A29\\x7f\\xb4\\x8b\\xb8\\xeb8\\x1e\\xbaN&\\xe8\\xb9D\\xb2,\\xbaw\\xa3r\\xb9!\\xa5\\x85\\xb9B\\xf6u\\xb9\\xe7\\xcf.\\xba\\x95#\\xd5\\xb9Ps\\xae8\\x0c|\\xce9U\\x05\\xa88g\\xe8\\x1b\\xba\\xab\\x8e\\x83\\xb7\\x07[\\xb6\\xb9@\\x0e^8\\xad\\x1b\\xad\\xb9\\xd5\\x90""\\xba\\x99(\\xe39a\\x01\\xea9/=P\\xb8\\xf9\\xde<:P\\xcf\\x1e:\\xcc\\xde?\\xba\\xe4\\xf0\\xfc\\xb9Ho\\x1c\\xb8I\\n\\x14\\xba\\x04\\xd2\\xa08`\\x9f\\xec8\\xca\\x1b\\x1c\\xba\\x9ea\\xf39\\xc7\\xdfM:\\xbf\\x9f\\xc58*\\xfc\\x12\\xb9\\xbf\\xe1\\x84\\xb8zF#\\xba@\\t\\':k\\xe9\\xbd\\xb9Z+\\xe49\\xbe\\x11\\xce\\xb9.\\xe8$:\\x88:\\xf1\\xb9\\xa3\\xef\\x10\\xba\\xcar\\xe0\\xb9\\xa4I;\\xb9\\xae\\xa3Z\\xb9x\\x0f\\x1b:O4\\x08\\xba:x\\x17\\xba\\xdb! :]{""\\xba\\xeaC\\x9d\\xb9\\xae\\xdc\\xbe\\xb9?i3\\xba /\\xc7\\xb9\\n\\xb95:\\x95\\x01\\x0e\\xba\\xe1M\\xa09\\xa6C(:\\xdcD\\xeb9\\xf7$\\xf58/[%\\xbak\\xb12\\xba\\x96r\\xc49#D\\xd79\\x17\\xd5<:.\\x1cA7\\xbf\\xc6\\xdf\\xb6r\\xe0\\x089\\xe7\\xc7\\x05:\\x11\\xa4/\\xba\\xf8\\x8c?:\\x02B@:ky\\x969\\xaa\\xa6=:J\\xad\\xae9>H\\xa2\\xb9sl\\x18:\\xc6N\\n\\xbaq~;\\xba9\\x98}8\\xfe\\xf9""9\\xf3d:\\xba\\xe3\\t\\xd49\\xf6\\xcc\\xd0\\xb9zs3\\xba\\\\\\x02\\x0b\\xbaS\\xa4\\x95\\xb9G\\xa8\\xb09\\xd7\\xdd,:O\\xfc\\x0c\\xba\\x94\\xc2\\xe2\\xb9<\\x97R9\\xa5\\xf6\\xa1\\xb6\\xf1\\x8b\\xe0\\xb9\\xf1\\x89\\xd2\\xb9+f\\xdf8\\x12g\\x04\\xb9\\xda\\xd2\\xda9~\\x14\\xf37\\x8c<$:\\xf53\\x19:\\xfcKx7\\xae\\x9c9\\xba\\x1f\\x8d\\xee\\xb8\\x0f\\x19\\x1e\\xb9\\x12\\xa5\\t:\\xe6\\xb0z9\\xab&\\xa6\\xb7\\xb5\\xbcX\\xb9\\xa2\\x9c\\x129\\x12G\\x80\\xb9\\x91\\x92\\xf69\\x92\\x03\\x01\\xba\\xfb\\xfd\\x0b:\\xb4\\xf4f9\\xcf\\x80\\x148M\\x0b\\xb8\\xb9\\xeb\\xebW8,@\\xf8\\xb9f\\x00\\xe78\\xc64\\xfd\\xb9+[\\x8c8\\xec\\xca\\xd89\\x89\\xa6%\\xb8\\xeff\\x13\\xba\\x11_\\xf8\\xb9)\\xd0\\xc29\\xda4\\x1f\\xb9O<\\x10:\\xe3P\\x0c:%f\\xb8\\xb9\\x18\\xe6\\xfa9\\x0b/|9\\xd8\\x8b\\xcb9\\x8e_\\'\\xba\\x8f\\xb5\\xbf91r\\xf5\\xb9\\xdf\\xfb)\\xbaYb\\x17\\xb8O=\\xdf8Q\\xea`\\xb9\\x88+\\x058u\\xba\\x13\\xbaG\\x1f\\x18981\\xd09\\xb3\\x96\\xc29\\x99I\\x1e\\xb8G\\xb3\\x0e\\xba|\\x0e\\x879\\xb06\\x049\\xb1F,:o\\x99@\\xba\\x1b\\xa5#9\\xf4\\x00\\xc7\\xb7\\xa6\\x17Q\\xba\\x07Hp9Z\\xc7&:\\xe8\\xa4\\x02:k\\xbd\\xb19\\xffU 8\\x0c:R\\xba\\xb2\\xcc\\xe49\\xfef\\x9c9\\xf3p6\\xba\\xe4\\x10\\xc1\\xb9\\x1cq9\\xb8$\\xa9\\x06\\xba)w\\x189%\\x18\\x00:\\xeat\\xc9\\xb8\\x86+!\\xb7\\xe2\\x10\\x06\\xbaMx48\\x06\\xf0\\xf2\\xb9S\\x186\\xb9\\xdc\\xc0\\x16:\\xb6\\xb9O\\xb9\\n\\n2\\xbaKP\\x8e9\\xaf\\x88\\xc39\\xb4\\x9e6:\\xba\\xc8\\x1a\\xb9\\x80\\xb5\\x0f:\\xf1\\xb8\\xc39\\xccy\\x1f:\\xbc\\xbf\\x1f\\xba\\xc8\\xaf\\xed9\\x12\\x82\\x19:*\\xfe\\xff8\\x10\\x02\\xfa9-\\x11a90}5:\\x82\\x96\\xa49\\xf8N\\x0f:I\\xde:\\xb9p\\xf3\\xff\\xfft\\xf3\\xff\\xff\\x0f\\x00\\x00\\x00MLIR Converted.\\x00\\x01\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x18\\x00\\x14\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x94\\x02\\x00\\x00\\x98\\x02\\x00\\x00\\x9c\\x02\\x00\\x00\\x04\\x00\\x00\\x00main\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x000\\x02\\x00\\x00\\xcc\\x01\\x00\\x00|\\x01\\x00\\x008\\x01\\x00\\x00\\xf8\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\x8c\\x00\\x00\\x00<\\x00\\x00\\x00\\x04\\x00\\x00\\x00b\\xfe\\xff\\xff\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x10\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x08\\xf4\\xff\\xff\\x01\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x96\\xfe\\xff\\xff\\x1c\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x1c\\x00\\x00\\x00 \\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x08\\x00\\x07\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x13\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\n\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\n\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x11\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x06\\xff\\xff\\xff\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x05$\\x00\\x00\\x00(\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xf6\\xfe\\xff\\xff\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x11\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xe6\\xfe\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x18\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\xd8\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x82\\xff\\xff\\xff\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x05$\\x00\\x00\\x00(\\x00\\x00\\x00\\x01\\x00\\x00\\x00r\\xff\\xff\\xff\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0e\\x00\\x00\\x00b\\xff\\xff\\xff\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x18\\x00\\x00\\x00\\x1c\\x00\\x00\\x00T\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0e\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x1a\\x00\\x14\\x00\\x10\\x00\\x0c\\x00\\x0b\\x00\\x04\\x00\\x0e\\x00\\x00\\x00$\\x00\\x00\\x00\\x00\\x00\\x00\\x054\\x00\\x00\\x008\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x18\\x00\\x17\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x0e\\x00\\x14\\x00\\x00\\x00\\x10\\x00\\x0c\\x00\\x0b\\x00\\x04\\x00\\x0e\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x00\\x00\\x00\\x01$\\x00\\x00\\x00(\\x00\\x00\\x00\\x0c\\x00\\x10\\x00\\x00\\x00\\x0c\\x00\\x08\\x00\\x07\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x15\\x00\\x00\\x00x\\t\\x00\\x00\\xac\\x08\\x00\\x00<\\x08\\x00\\x00\\xe0\\x07\\x00\\x00\\x84\\x07\\x00\\x00,\\x07\\x00\\x00\\xd4\\x06\\x00\\x00\\x88\\x06\\x00\\x00\\x18\\x06\\x00\\x00\\xc0\\x05\\x00\\x00t\\x05\\x00\\x00(\\x05\\x00\\x00\\\\\\x04\\x00\\x00\\xe8\\x03\\x00\\x00\\x14\\x03\\x00\\x00\\x9c\\x02\\x00\\x00\\xc8\\x01\\x00\\x00P\\x01\\x00\\x00\\xf0\\x00\\x00\\x00`\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xf2\\xf6\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x15\\x00\\x00\\x004\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x0b\\x00\\x00\\x00\\xd4\\xf6\\xff\\xff\\x19\\x00\\x00\\x00StatefulPartitionedCall:0\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0b\\x00\\x00\\x00J\\xf7\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x14\\x00\\x00\\x00h\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x80\\x00\\x00\\x00,\\xf7\\xff\\xffL\\x00\\x00\\x00sequential_1/dense/MatMul;sequential_1/dense/Relu;sequential_1/dense/BiasAdd\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\xd6\\xf7\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x13\\x00\\x00\\x008\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x00y\\x00\\x00\\xb8\\xf7\\xff\\xff\\x1c\\x00\\x00\\x00sequential_1/flatten/Reshape\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00y\\x00\\x002\\xf8\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\x12\\x00\\x00\\x00H\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x16\\x00\\x00\\x00\\x16\\x00\\x00\\x00@\\x00\\x00\\x00\\x1c\\xf8\\xff\\xff$\\x00\\x00\\x00sequential_1/max_pooling2d_2/MaxPool\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x16\\x00\\x00\\x00@\\x00\\x00\\x00\\xa6\\xf8\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\x11\\x00\\x00\\x00\\xa4\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xff-\\x00\\x00\\x00-\\x00\\x00\\x00@\\x00\\x00\\x00\\x90\\xf8\\xff\\xff\\x82\\x00\\x00\\x00sequential_1/conv2d_2/Relu;sequential_1/conv2d_2/BiasAdd;sequential_1/conv2d_2/Conv2D;sequential_1/conv2d_2/BiasAdd/ReadVariableOp\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00-\\x00\\x00\\x00-\\x00\\x00\\x00@\\x00\\x00\\x00v\\xf9\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\x10\\x00\\x00\\x00H\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xff-\\x00\\x00\\x00-\\x00\\x00\\x00 \\x00\\x00\\x00`\\xf9\\xff\\xff$\\x00\\x00\\x00sequential_1/max_pooling2d_1/MaxPool\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00-\\x00\\x00\\x00-\\x00\\x00\\x00 \\x00\\x00\\x00\\xea\\xf9\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\xa4\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xffZ\\x00\\x00\\x00Z\\x00\\x00\\x00 \\x00\\x00\\x00\\xd4\\xf9\\xff\\xff\\x82\\x00\\x00\\x00sequential_1/conv2d_1/Relu;sequential_1/conv2d_1/BiasAdd;sequential_1/conv2d_1/Conv2D;sequential_1/conv2d_1/BiasAdd/ReadVariableOp\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00Z\\x00\\x00\\x00Z\\x00\\x00\\x00 \\x00\\x00\\x00\\xba\\xfa\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\x0e\\x00\\x00\\x00D\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xffZ\\x00\\x00\\x00Z\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xa4\\xfa\\xff\\xff""\\x00\\x00\\x00sequential_1/max_pooling2d/MaxPool\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00Z\\x00\\x00\\x00Z\\x00\\x00\\x00\\x10\\x00\\x00\\x00*\\xfb\\xff\\xff\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00$\\x00\\x00\\x00$\\x00\\x00\\x00\\r\\x00\\x00\\x00\\x9c\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xb4\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x14\\xfb\\xff\\xff{\\x00\\x00\\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp1\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xba\\xfc\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x0c\\x00\\x00\\x00(\\x00\\x00\\x00\\xc4\\xfb\\xff\\xff\\x1b\\x00\\x00\\x00sequential_1/dense_1/MatMul\\x00\\x02\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x02\\xfd\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x0b\\x00\\x00\\x00(\\x00\\x00\\x00\\x0c\\xfc\\xff\\xff\\x19\\x00\\x00\\x00sequential_1/dense/MatMul\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00y\\x00\\x00J\\xfd\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\n\\x00\\x00\\x008\\x00\\x00\\x00T\\xfc\\xff\\xff)\\x00\\x00\\x00sequential_1/dense/BiasAdd/ReadVariableOp\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x9e\\xfd\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\t\\x00\\x00\\x008\\x00\\x00\\x00\\xa8\\xfc\\xff\\xff+\\x00\\x00\\x00sequential_1/dense_1/BiasAdd/ReadVariableOp\\x00\\x01\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x1c\\x00\\x18\\x00\\x17\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x02(\\x00\\x00\\x00\\x18\\xfd\\xff\\xff\\x1a\\x00\\x00\\x00sequential_1/flatten/Const\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00R\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x07\\x00\\x00\\x00,\\x00\\x00\\x00\\\\\\xfd\\xff\\xff\\x1c\\x00\\x00\\x00sequential_1/conv2d_2/Conv2D\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00@\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00 \\x00\\x00\\x00\\xa6\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x06\\x00\\x00\\x00,\\x00\\x00\\x00\\xb0\\xfd\\xff\\xff\\x1c\\x00\\x00\\x00sequential_1/conv2d_1/Conv2D\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00 \\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\xfa\\xfe\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x05\\x00\\x00\\x00<\\x00\\x00\\x00\\x04\\xfe\\xff\\xff,\\x00\\x00\\x00sequential_1/conv2d_2/BiasAdd/ReadVariableOp\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00@\\x00\\x00\\x00R\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x04\\x00\\x00\\x00<\\x00\\x00\\x00\\\\\\xfe\\xff\\xff,\\x00\\x00\\x00sequential_1/conv2d_1/BiasAdd/ReadVariableOp\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00 \\x00\\x00\\x00\\xaa\\xff\\xff\\xff\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x03\\x00\\x00\\x008\\x00\\x00\\x00\\xb4\\xfe\\xff\\xff*\\x00\\x00\\x00sequential_1/conv2d/BiasAdd/ReadVariableOp\\x00\\x00\\x01\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x18\\x00\\x14\\x00\\x00\\x00\\x10\\x00\\x0c\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x10\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x88\\x00\\x00\\x00 \\xff\\xff\\xffz\\x00\\x00\\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp\\x00\\x00\\x04\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x16\\x00\\x1c\\x00\\x18\\x00\\x00\\x00\\x14\\x00\\x10\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x07\\x00\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x14\\x00\\x00\\x00(\\x00\\x00\\x00(\\x00\\x00\\x00\\x01\\x00\\x00\\x00H\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xb4\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x04\\x00\\x04\\x00\\x00\\x00""\\x00\\x00\\x00serving_default_sequential_input:0\\x00\\x00\\x04\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\xb4\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00@\\x00\\x00\\x00$\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xdc\\xff\\xff\\xff\\t\\x00\\x00\\x00\\x00\\x00\\x00\\t\\xe8\\xff\\xff\\xff\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x16\\xf4\\xff\\xff\\xff\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x0c\\x00\\x0c\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x0c\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x03'', does not exist.
```
"
tensorflow/tensorflow,2023-09-07 21:31:59,bug,tf.data.Dataset.list_files(): You must feed a value for placeholder tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

No

### OS platform and distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Reading a dataset obtained with `tf.data.Dataset.list_files()` prints incomprehensible warnings.

Create two files:
```bash
touch a.txt
touch b.txt
```
Run this python program:
```python
import tensorflow as tf
dataset = tf.data.Dataset.list_files(['a.txt', 'b.txt'])
for f in dataset: 
    print(f)
```

Prints some incomprehensible warnings:
```
tf.Tensor(b'b.txt', shape=(), dtype=string)
tf.Tensor(b'a.txt', shape=(), dtype=string)

2023-09-07 17:19:04.634978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
This is a ~duplicate of https://github.com/tensorflow/tensorflow/issues/41648 that was marked as resolved 3 years ago.

### Standalone code to reproduce the issue

```shell
With tensorflow==2.12.0:  
https://colab.research.google.com/drive/1_kjUH6BzcLnlM4rc8mY7JcnhE5NdaRGy?usp=sharing

No warning with tensorflow==2.11.1  
https://colab.research.google.com/drive/1QhatrE7hdJIxIUAIrYw5yDPQ50SeqFrI?usp=sharing
```


### Relevant log output

```shell
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
"
tensorflow/tensorflow,2023-09-05 13:06:02,bug,Cannot create interpreter when using GPU-Delegate or NNAPI-Delegate,"**System information**
- Android Device information: samsung/a14mnseea/a14m:13/TP1A.220624.014/A145RXXU2AWG3:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
  - com.google.android.gms:play-services-tflite-java:16.1.0
  - com.google.android.gms:play-services-tflite-support:16.1.0
  - com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version: 23.33.16

**Standalone code to reproduce the issue**

        var useGpu = Tasks.await(TfLiteGpu.isGpuDelegateAvailable(context));
        var optionsBuilder = TfLiteInitializationOptions.builder();

        optionsBuilder.setEnableGpuDelegateSupport(useGpu);

        Tasks.await(TfLite.initialize(context, optionsBuilder.build()));

        var options =  new InterpreterApi.Options();
        if(useGpu){
            options.addDelegateFactory(new GpuDelegateFactory());
        }

        /*delegate = new NnApiDelegate();
        options.addDelegate(delegate);
        options.setUseNNAPI(true);*/

        options.setRuntime(InterpreterApi.Options.TfLiteRuntime.FROM_SYSTEM_ONLY);

        //load Model from App assets
        interpreter = InterpreterApi.create(new File(modelPath), options);

**Any other info / logs**
I oriented my code on the official documentation [on here](https://www.tensorflow.org/lite/android/delegates/gpu)

Logcat-Output:
java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: 
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.zzl(com.google.android.gms:play-services-tflite-java@@16.1.0:34)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:6)
                                                                                                    	at com.google.android.gms.tflite.zzd.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:1)
                                                                                                    	at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:play-services-tflite-java@@16.1.0:2)
                                                                                                    	at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:336)
                                                                                                    	at com.example.tfliteaudio.TFLiteEngine.initialize(TFLiteEngine.java:83)
                                                                                                    	at com.example.tfliteaudio.MainActivity.lambda$transcribeAudio$5(MainActivity.java:143)
                                                                                                    	at com.example.tfliteaudio.MainActivity.$r8$lambda$1xqJ9hAvPXTc26gXgWfy8QcV0VE(Unknown Source:0)
                                                                                                    	at com.example.tfliteaudio.MainActivity$$ExternalSyntheticLambda2.run(Unknown Source:2)
                                                                                                    	at java.lang.Thread.run(Thread.java:1012)
"
tensorflow/tensorflow,2023-09-03 14:28:22,bug,The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer.
It seems that the `vocabulary` is not saved or loaded correctly, which is empty when loading the model.
This behavior may relate to #61369, but different API endpoint.


### Standalone code to reproduce the issue

```python
import pickle
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

model_input = tf.keras.Input(shape=(1,), dtype=tf.int64)
lookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['a', 'b'])(model_input)
output = tf.keras.layers.Dense(10)(lookup)
full_model = tf.keras.Model(model_input, output)

# this part works
try:
    model_bytes = pickle.dumps(full_model)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)

# this part throws an error
try:
    full_model.save(""/tmp/temp_model"")
    full_model_loaded = tf.keras.models.load_model(""/tmp/temp_model"")
    model_bytes = pickle.dumps(full_model_loaded)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Success!
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
Failed! Error: Error when deserializing class 'StringLookup' using config={'name': 'string_lookup', 'trainable': True, 'dtype': 'int64', 'invert': False, 'max_tokens': None, 'num_oov_indices': 1, 'oov_token': '[UNK]', 'mask_token': None, 'output_mode': 'int', 'sparse': False, 'pad_to_max_tokens': False, 'idf_weights': None, 'vocabulary': [], 'vocabulary_size': 3, 'encoding': 'utf-8'}.

Exception encountered: Cannot set an empty vocabulary, you passed [].
```
"
tensorflow/tensorflow,2023-08-28 20:15:53,bug,rnn with initial_state model can't be loaded with load_model ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A simple RNN with LSTMcell model.
I want to initialize the states with `initial_state_h` and `initial_state_c`. 
```
batch_size= 16
inputs = tf.keras.layers.Input(shape=(20,5),batch_size=batch_size)
units = 8
lstm_cell_fw = tf.keras.layers.LSTMCell(units)

initial_state_h = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
initial_state_c = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
lstm_layer_fw = tf.keras.layers.RNN(lstm_cell_fw, stateful=True, return_state=True, return_sequences=False)
outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs,initial_state = [initial_state_h,initial_state_c])

lstm_dense1 = tf.keras.layers.Dense(16, activation = 'relu')
lstm_dense2 = tf.keras.layers.Dense(2, activation = 'softmax')
out=lstm_dense2(lstm_dense1(outputs))

model = tf.keras.models.Model(inputs, out)
```
After compile and train, the model is saved with `model.save('my_model_test.keras')`.

```
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()

xTrain = np.random.rand(96,20,5)
yTrain = np.random.rand(96,2)

for i in range(10):
  model.fit(xTrain, yTrain,batch_size=batch_size)

model.save('my_model_test.keras')

```
But when I try to load it with `load_model = tf.keras.models.load_model('my_model_test.keras')`, it gives error:
```
13 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/backend.py](https://localhost:8080/#) in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'

```
I tried to save in other format, `.h5`, `.json`, etc. All give the same error.

But, if I don't use `initial_state` in `outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs)`, everything goes well. No problem with `load_model`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1uKEpnddzeYSRG_1vKjtcQ4OLNwLuQeqy?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-8e0130abf25e> in <cell line: 1>()
----> 1 load_model = tf.keras.models.load_model('my_model_test.keras')

13 frames
/usr/local/lib/python3.10/dist-packages/keras/src/backend.py in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'
```
```
"
tensorflow/tensorflow,2023-08-26 15:17:24,bug,Issues with Running Custom TensorFlow Lite Model in C++,"### 1. System information

- Platform and Linux distribution kubuntu 22.04:
- TensorFlow is built from C++ source code:
- Tensorflow 2.11:

### 2. Code

- Link to models that I trained and tried but they don't work in C++ - https://github.com/asuemg1/models_hub/tree/main/Tensorflow%20Lite/Object%20Detection/my_ssd_mobnet/Optimized%20Models
- Link to the model that works in C++ - https://github.com/ankdesh/tflite/blob/master/Android-TensorFlow-Lite-Example/app/src/main/assets/mobilenet_quant_v1_224.tflite
- Link to C++ code (mainwindow.cpp file):
https://drive.google.com/file/d/1u87yK-1qqKeHBjUKq-Lkxi0LMQKkdrPg/view?usp=sharing

### 3. Crash after conversion

- The model does not work in C++.

Please tell me how you can run the Tensorflow Lite model (tflite format) for object detection or image classification in C ++.

My steps:
- Trained the model for object detection using Tensorflow 2 API object detection.
- After training, I converted the model to the savedmodel format, and then to tflite.
- Next, I needed to embed this model into a C++ project. In order to use it in the future on low-power devices such as rasberry pi

My actions:
- Compiled the Tensorflow Lite library for C++.
- Found a test case using the mobilenet_quant_v1_224.tflite model. In this test case, the model runs successfully. However, when trying to use my own model, it does not work, although it has been tested and works in Python.

What was found out:

- The mobilenet_quant_v1_224.tflite model was quantized and had no metadata and no internal labelmap.txt file.
- TensorFlow Lite API 2 for C++ does not currently support metadata.

If you have any information on how to get my tflite model to work in C++ please share."
tensorflow/tensorflow,2023-08-25 07:37:33,bug,JIT yields inconsistent results using tf.math.top_k,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0-dev20230824

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8700

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti

### Current behavior?

JIT yields inconsistent results using `tf.math.top_k` when `index_type=tf.int32` (no issue with `index_type=tf.int64`).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function(jit_compile=True)
def tf_func(shape):
    x = tf.random.stateless_normal(shape, seed=(1, 2))
    x = tf.transpose(x, perm=[1, 0])
    topk_max, indices = tf.math.top_k(x, 1, sorted=False, index_type=tf.int32)
    reduce_max = tf.reduce_max(x, axis=1, keepdims=True)
    return topk_max - reduce_max


def check(shape):
    should_be_all_zero = tf_func(shape)
    print(f""should_be_all_zero shape {shape}:\\n{should_be_all_zero}"")


check((1024, 3))
check((1023, 3))
check((1024, 2))
```


### Relevant log output

```shell
should_be_all_zero shape (1024, 3):
[[-0.7787831 ]
 [ 0.47324872]
 [ 0.30553436]]
should_be_all_zero shape (1023, 3):
[[0.]
 [0.]
 [0.]]
should_be_all_zero shape (1024, 2):
[[0.]
 [0.]]
```
"
tensorflow/tensorflow,2023-08-22 09:38:14,bug,Please help ! ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.0

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

10.5.0

### CUDA/cuDNN version

10

### GPU model and memory

RTX 3060 12G

### Current behavior?

I  am running the Octopus repo (https://github.com/thmoa/octopus) which use tensorflow-gpu version 1.13.1 . When I run that model with python, I got some errors from tensorflow. Please help me. 

### Standalone code to reproduce the issue

```shell
import os
import argparse
import tensorflow as tf
import keras.backend as K




from glob import glob

from lib.io import openpose_from_file, read_segmentation, write_mesh
from model.octopus import Octopus



def main(weights, name, segm_dir, pose_dir, out_dir, opt_pose_steps, opt_shape_steps):
    segm_files = sorted(glob(os.path.join(segm_dir, '*.png')))
    pose_files = sorted(glob(os.path.join(pose_dir, '*.json')))

    if len(segm_files) != len(pose_files) or len(segm_files) == len(pose_files) == 0:
        exit('Inconsistent input.')

    K.set_session(tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))))

    model = Octopus(num=len(segm_files))
    model.load(weights)

    segmentations = [read_segmentation(f) for f in segm_files]

    joints_2d, face_2d = [], []
    for f in pose_files:
        j, f = openpose_from_file(f)

        assert(len(j) == 25)
        assert(len(f) == 70)

        joints_2d.append(j)
        face_2d.append(f)

    if opt_pose_steps:
        print('Optimizing for pose...')
        model.opt_pose(segmentations, joints_2d, opt_steps=opt_pose_steps)

    if opt_shape_steps:
        print('Optimizing for shape...')
        model.opt_shape(segmentations, joints_2d, face_2d, opt_steps=opt_shape_steps)

    print('Estimating shape...')
    pred = model.predict(segmentations, joints_2d)

    write_mesh('{}/{}.obj'.format(out_dir, name), pred['vertices'][0], pred['faces'])

    print('Done.')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument(
        'name',
        type=str,
        help=""Sample name"")

    parser.add_argument(
        'segm_dir',
        type=str,
        help=""Segmentation images directory"")

    parser.add_argument(
        'pose_dir',
        type=str,
        help=""2D pose keypoints directory"")

    parser.add_argument(
        '--opt_steps_pose', '-p', default=5, type=int,
        help=""Optimization steps pose"")

    parser.add_argument(
        '--opt_steps_shape', '-s', default=15, type=int,
        help=""Optimization steps"")

    parser.add_argument(
        '--out_dir', '-od',
        default='out',
        help='Output directory')

    parser.add_argument(
        '--weights', '-w',
        default='weights/octopus_weights.hdf5',
        help='Model weights file (*.hdf5)')

    args = parser.parse_args()
    main(args.weights, args.name, args.segm_dir, args.pose_dir, args.out_dir, args.opt_steps_pose, args.opt_steps_shape)

```


### Relevant log output

```shell
Processing sample...
> Optimizing for pose...
  0%|                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]2023-08-22 16:24:18.296359: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2023-08-22 16:25:50.156420: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55fa094fdcf0
2023-08-22 16:26:08.284736: E tensorflow/stream_executor/cuda/cuda_blas.cc:698] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_EXECUTION_FAILED
2023-08-22 16:26:08.284773: E tensorflow/stream_executor/cuda/cuda_blas.cc:2620] Internal: failed BLAS call, see log for details
2023-08-22 16:26:08.326578: I tensorflow/stream_executor/stream.cc:5014] [stream=0x55fa0950bb90,impl=0x55fa093dbf20] did not memcpy device-to-host; source: 0x813bc6700
2023-08-22 16:26:08.326623: F tensorflow/core/framework/op_kernel.cc:1408] Check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x55fa38108400)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.
Aborted
```
"
tensorflow/tensorflow,2023-08-22 07:36:21,bug,python3.11.3 mismatch with tensorflow 2.13.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS Venture 13.4

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/11846497/3459b246-e377-499d-9d9f-e5f666fb7957)


### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-21 15:11:42,bug,Unable to serialize VariableSpec,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A graph

### Standalone code to reproduce the issue

```shell
from models import *
from conftest import DDPGAgent
import matplotlib as plt
import pytest
import time

# Just disables the warning, doesn't take advantage of AVX/FMA to run faster
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# setting for hidden layers
Layer1 = 400
Layer2 = 300


class MecTer(object):
    """"""
    MEC terminal parent class
    """"""

    def __init__(self, user_config, train_config):
        self.rate = user_config['rate']
        self.dis = user_config['dis']
        self.id = user_config['id']
        self.state_dim = user_config['state_dim']
        self.action_dim = user_config['action_dim']
        self.action_bound = user_config['action_bound']
        self.data_buf_size = user_config['data_buf_size']
        self.t_factor = user_config['t_factor']
        self.penalty = user_config['penalty']

        self.sigma2 = train_config['sigma2']
        self.init_path = ''
        self.isUpdateActor = True
        self.init_seqCnt = 0

        if 'model' not in user_config:
            self.channelModel = MarkovModel(self.dis, seed=train_config['random_seed'])
        else:
            n_t = 1
            n_r = user_config['num_r']
            self.channelModel = ARModel(self.dis, n_t, n_r, seed=train_config['random_seed'])

        self.DataBuf = 0
        self.Channel = self.channelModel.getCh()
        self.SNR = 0
        self.Power = np.zeros(self.action_dim)
        self.Reward = 0
        self.State = []

        # some pre-defined parameters
        self.k = 1e-27
        self.t = 0.001
        self.L = 500

    def localProc(self, p):
        return np.power(p / self.k, 1.0 / 3.0) * self.t / self.L / 1000

    def localProcRev(self, b):
        return np.power(b * 1000 * self.L / self.t, 3.0) * self.k

    def offloadRev(self, b):
        return (np.power(2.0, b) - 1) * self.sigma2 / np.power(np.linalg.norm(self.Channel), 2)

    def offloadRev2(self, b):
        return self.action_bound if self.SNR <= 1e-12 else (np.power(2.0, b) - 1) / self.SNR

    def getCh(self):
        return self.Channel

    def setSNR(self, snr):
        self.SNR = snr
        self.sampleCh()
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        self.State = np.array([self.DataBuf, snr, channel_gain])

    def sampleData(self):
        data_t = np.log2(1 + self.Power[0] * self.SNR)
        data_p = self.localProc(self.Power[1])
        over_power = 0

        self.DataBuf -= data_t + data_p
        if self.DataBuf < 0:
            over_power = self.Power[1] - self.localProcRev(np.fmax(0, self.DataBuf + data_p))
            self.DataBuf = 0

        data_r = np.random.poisson(self.rate)
        self.DataBuf += data_r
        return data_t, data_p, data_r, over_power

    def sampleCh(self):
        # self.Channel = self.channelModel.sampleCh()

        # Calculate channel gain using channel quantization
        raw_channel_gain = np.linalg.norm(self.channelModel.sampleCh())
        min_val = np.min(self.Channel)
        max_val = np.max(self.Channel)

        # Quantize the channel gain into 10 levels
        quantized_channel_gain = min_val + (max_val - min_val) * (raw_channel_gain - min_val) / (max_val - min_val)
        quantized_channel_gain = np.clip(quantized_channel_gain, min_val, max_val)
        self.Channel = quantized_channel_gain

        return self.Channel

    def reset(self, rate, seqCount):
        self.rate = rate
        self.DataBuf = np.random.randint(0, self.data_buf_size - 1) / 2.0
        self.sampleCh()

        if seqCount >= self.init_seqCnt:
            self.isUpdateActor = True

        return self.DataBuf


class MecTermRL(MecTer):
    """"""
    MEC terminal class using RL
    """"""

    # rate:packet poisson arrival, dis: distance in meters
    def __init__(self, user_config, train_config):
        MecTer.__init__(self, user_config, train_config)
        self.agent = DDPGAgent(user_config, train_config)

        if 'init_path' in user_config and len(user_config['init_path']) > 0:
            self.init_path = user_config['init_path']
            self.init_seqCnt = user_config['init_seqCnt']
            self.isUpdateActor = False

    def feedback(self, snr, done):
        isOverflow = 0
        self.SNR = snr

        # update the data buffer
        [data_t, data_p, data_r, over_power] = self.sampleData()

        # get the reward for the current slot
        self.Reward = -self.t_factor * np.sum(self.Power) * 10 - (1 - self.t_factor) * self.DataBuf

        # estimate the channel for next slot
        self.sampleCh()

        # update the actor and critic network
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        next_state = np.array([self.DataBuf, snr, channel_gain])

        self.agent.update(self.State, self.Power, self.Reward, done, next_state, self.isUpdateActor)

        # update system state
        self.State = next_state
        # return the reward in this slot
        sum_power = np.sum(self.Power) - over_power
        return self.Reward, sum_power, over_power, data_t, data_p, data_r, self.DataBuf, channel_gain, isOverflow

    def predict(self, isRandom):
        power, noise = self.agent.predict(self.State, self.isUpdateActor)
        self.Power = np.fmax(0, np.fmin(self.action_bound, power))

        return self.Power, noise


class MecSvrEnv(object):
    """"""
    Simulation environment
    """"""

    def __init__(self, user_list, num_att, sigma2, max_len):
        self.user_list = user_list
        self.num_user = len(user_list)
        self.num_att = num_att
        self.sigma2 = sigma2
        self.count = 0
        self.seqCount = 0
        self.max_len = max_len

        # specially designed for Greedy agent training

    #         self.data_set = []

    def init_target_network(self):
        for user in self.user_list:
            user.critic.init_target_network(path='data_set_OGD.npz')

    def plot_channel_gains_histogram(self):
        # Get the channel gains for all users
        channel_gains = [np.abs(user.getCh()) for user in self.user_list]

        # Flatten the channel gains to a 1D array
        flat_channel_gains = np.concatenate(channel_gains)

        # plot a histogram for the channel gains
        plt.hist(np.abs(flat_channel_gains), bins=20, edgecolor='black')
        plt.title(""Channel Gains Histogram"")
        plt.xlabel(""Channel Gain Magnitude"")
        plt.ylabel(""Frequency"")
        plt.show()

    def step_transmit(self, isRandom=True):
        # get the channel vectors
        channels = np.transpose([user.getCh() for user in self.user_list])
        # get the transmit powers
        powers = []
        noises = []

        for i in range(self.num_user):
            p, n = self.user_list[i].predict(isRandom)
            powers.append(p.copy())
            noises.append(n.copy())
        # compute the snr for each user

        powers = np.array(powers)
        noises = np.array(noises)
        snr_list = self.compute_snr(channels, powers[:, 0])

        rewards = np.zeros(self.num_user)
        powers = np.zeros(self.num_user)
        over_powers = np.zeros(self.num_user)
        data_ts = np.zeros(self.num_user)
        data_ps = np.zeros(self.num_user)
        data_rs = np.zeros(self.num_user)
        data_buf_sizes = np.zeros(self.num_user)
        next_channels = np.zeros(self.num_user)
        isOverflows = np.zeros(self.num_user)

        self.count += 1
        # feedback the snr to each user
        for i in range(self.num_user):
            [rewards[i], powers[i], over_powers[i], data_ts[i], data_ps[i], data_rs[i], data_buf_sizes[i],
             next_channels[i], isOverflows[i]] = self.user_list[i].feedback(snr_list[i], self.count >= self.max_len)

        return rewards, self.count >= self.max_len, powers, over_powers, noises, data_ts, data_ps, data_rs, data_buf_sizes, next_channels, isOverflows

    def compute_snr(self, channels, powers):
        # FDD - Computing SNR
        H_inv = np.linalg.pinv(channels)
        total_signal_power = np.power(np.linalg.norm(channels, axis=1), 2)
        noise = np.power(np.linalg.norm(H_inv, axis=1), 2) * self.sigma2
        snr_list = total_signal_power / noise

        return snr_list

    def reset(self, isTrain=True):
        self.count = 0

        if isTrain:
            init_data_buf_size = [user.reset(user.rate, self.seqCount) for user in self.user_list]
            # get the channel vectors
            channels = np.transpose([user.getCh() for user in self.user_list])
            # get the transmit powers to start
            powers = [np.random.uniform(0, user.action_bound) for user in self.user_list]
            # compute the snr for each user
            snr_list = self.compute_snr(channels, powers)
        else:
            init_data_buf_size = [0 for user in self.user_list]
            snr_list = [0 for user in self.user_list]

        for i in range(self.num_user):
            self.user_list[i].setSNR(snr_list[i])

        self.seqCount += 1
        return init_data_buf_size


# Create the environment
# def env():
#     envi = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
#     return envi

# env = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
# env.init_target_network()

train_config = {
    'sigma2': 0.01,
    'minibatch_size': 64,
    'actor_lr': 0.0001,
    'tau': 0.001,
    'critic_lr': 0.001,
    'gamma': 0.99,
    'buffer_size': 250000,
    'random_seed': int(time.perf_counter() * 1000 % 1000),
    'noise_sigma': 0.12
}

# Define user_list_info with user information
user_list_info = [
    {'state_dim': 3,
     'action_dim': 1,
     'id': '1',
     'action_bound': 1,
     'model': 'AR',
     'num_r': 4,
     'rate': 3.0,
     'dis': 100,
     'data_buf_size': 100,
     't_factor': 1.0,
     'penalty': 1000, }
]

# sess = tf.compat.v1.Session()
# Create instances of the User class from the dictionary in user_list
user_list = [
    MecTermRL(user_config=user_info, train_config=train_config)
    for user_info in user_list_info
]

# Initialize variables
for user in user_list:
    user.agent.init_target_network()
    # (
    #     path=""C:/Users/USER/PycharmProjects/mec_drl-masterr/mec_drl-master/mec_drl-master/data_set_OGD.npz""
    # )

@pytest.fixture
def env():
    # Create and return the environment object
    # Make sure to adjust this to properly create your environment instance
    return MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
```


### Relevant log output

```shell
WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'dense_5/bias:0' shape=(300,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
Traceback (most recent call last):
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py"", line 207, in get_json_type
    type_spec_name = type_spec_registry.get_name(type(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\type_spec_registry.py"", line 75, in get_name
    raise ValueError(""TypeSpec %s.%s has not been registered."" %
ValueError: TypeSpec tensorflow.python.ops.resource_variable_ops.VariableSpec has not been registered.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\\Users\\USER\\PycharmProjects\\mec_drl-masterr\\mec_drl-master\\mec_drl-master\\test.py"", line 303, in <module>
    user_list = [
                ^
  File ""C:\\Users\\USER\\PycharmProjects\\mec_drl-masterr\\mec_drl-master\\mec_drl-master\\test.py"", line 304, in <listcomp>
    MecTermRL(user_config=user_info, train_config=train_config)
  File ""C:\\Users\\USER\\PycharmProjects\\mec_drl-masterr\\mec_drl-master\\mec_drl-master\\test.py"", line 125, in __init__
    self.agent = DDPGAgent(user_config, train_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\PycharmProjects\\mec_drl-masterr\\mec_drl-master\\mec_drl-master\\conftest.py"", line 24, in __init__
    self.critic = CriticNetwork(self.state_dim, self.action_dim, float(train_config['critic_lr']),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\PycharmProjects\\mec_drl-masterr\\mec_drl-master\\mec_drl-master\\ddpg.py"", line 102, in __init__
    self.target_model = tf.keras.models.clone_model(self.model)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\cloning.py"", line 539, in clone_model
    return _clone_functional_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\cloning.py"", line 222, in _clone_functional_model
    model_configs, created_layers = _clone_layers_and_model_config(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\cloning.py"", line 298, in _clone_layers_and_model_config
    config = functional.get_network_config(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py"", line 1583, in get_network_config
    node_data = node.serialize(
                ^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\node.py"", line 219, in serialize
    kwargs = tf.nest.map_structure(_serialize_keras_tensor, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py"", line 624, in map_structure
    return nest_util.map_structure(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py"", line 1054, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py"", line 1094, in _tf_core_map_structure
    [func(*x) for x in entries],
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py"", line 1094, in <listcomp>
    [func(*x) for x in entries],
     ^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\node.py"", line 215, in _serialize_keras_tensor
    return (_COMPOSITE_TYPE, json_utils.Encoder().encode(t))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py"", line 55, in encode
    return super().encode(_encode_tuple(obj))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\json\\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\json\\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py"", line 52, in default
    return get_json_type(obj)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py"", line 225, in get_json_type
    ""spec"": get_json_type(spec),
            ^^^^^^^^^^^^^^^^^^^
  File ""C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py"", line 214, in get_json_type
    raise ValueError(
ValueError: Unable to serialize VariableSpec(shape=(300,), dtype=tf.float32, trainable=True, alias_id=None) to JSON, because the TypeSpec class <class 'tensorflow.python.ops.resource_variable_ops.VariableSpec'> has not been registered.
```
"
tensorflow/tensorflow,2023-08-20 18:45:47,bug,Activation function of a Dense hidden layer not getting invoked.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.4, MacBook Pro M2 Max

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Issue:
In the given auto-encoder setup, the encoder layers activation function (relu) is not getting invoked.

1. We create a simple auto-encoder, with Input size 3, hidden size 2, and output back to 3.
2. The activation function of the encoder layer is set a relu.
3. The weights of the encoder layers are all made negative. Idea is, if input is +ve, all the neutrons will have negative value and relu will o/p zero.
4. Give input as [1, 0, 0].
5. We expect the final decoder o/p layer, which has sigmoid activation, to o/p all [0.5, 0.5, 0.5] as the input to this layer from the encoder should have been [0, 0, 0]. 
6. But we find that is not the case, which clearly shows that 'relu' activation of the hidden layer is not getting invoked.

Installation:
pip install tensorflow-macos
pip install tensorflow-metal

### Standalone code to reproduce the issue

```shell
# https://colab.research.google.com/drive/14KKrdiBg8FT2cdUC5pjqJHi5S3BkTOHi?usp=sharing
# The above colab will run fine, but the same code on Mac with the said config has issue.
# Copying the code here for quick reference.

import tensorflow as tf    
import tensorflow.keras
import tensorflow as tf
import platform
import sys
from tensorflow.keras.layers import Input, Dense, Layer
from tensorflow.keras.models import Model

# Print versions:
print(f""Python {sys.version}"")
print(f""Python Platform: {platform.platform()}"")
print(f""Tensor Flow Version: {tf.__version__}"")
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")

# Setup input
import numpy as np
X_check = np.array([[1, 0, 0]])

# Setup autoencoder model
input_layer = Input(shape=(X_check.shape[1]))
bottleneck = Dense(2, activation='relu', name='bottleneck')(input_layer)
output = Dense(X_check.shape[1], activation='sigmoid', name='output')(bottleneck)
autoencoder = Model(input_layer, output)

# Set encoder layer weights to all negative.
layer = autoencoder.layers[1]
weights = np.array([[-1, -1],[-1, -1], [-1, -1]])
biases = np.array([0, 0])
layer.set_weights([weights, biases])

# create encoder model.
encoder = Model(input_layer, bottleneck)

# create decoder model.
decoder_input = Input(shape=(2,), name='decoder_input')
decoder_layer = autoencoder.layers[-1]
decoder = Model(decoder_input, decoder_layer(decoder_input))

# Run auto-encoder, with [1, 0, 0], since encoder has all negative weights,
# and has 'relu' activation o/p of enocder should all be zeros. And that being
# the input of next sigmod we should get output [0.5, 0.5, 0.5]
output_data = autoencoder.predict(X_check)
print(output_data)
```


### Relevant log output

```shell
Python 3.8.17 (default, Jul  5 2023, 15:45:03) 
[Clang 14.0.6 ]
Python Platform: macOS-13.4-arm64-arm-64bit
Tensor Flow Version: 2.13.0
GPU is available
1/1 [==============================] - 0s 38ms/step
[[0.287966   0.85427195 0.28276426]]
```
"
tensorflow/tensorflow,2023-08-19 23:04:06,bug,Check failure when running tf.config.experimental_connect_to_host,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

GTX 1660 TI

### Current behavior?

Due to feeding NaN input Argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-19 19:02:09.775956: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-19 19:02:10.305057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-19 19:02:10.742608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.829417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.838878: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:600] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0819 19:02:10.839114974  187448 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
"
tensorflow/tensorflow,2023-08-18 00:56:31,bug,Overflow when running tf.compat.v1.manip.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large element in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([1, 355, 768], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,]
  name = None
  out = tf.compat.v1.manip.tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 44407488056105 with 96070284019968, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
tensorflow/tensorflow,2023-08-18 00:52:39,bug,Overflow bug when running tf.compat.v1.image.resize,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  images_tensor = tf.constant(-15621075306911, shape=[218, 178, 3, 1], dtype=tf.int64,)
  images = tf.identity(images_tensor)
  size_0 = 8968073515812833920
  size_1 = 536870912
  size = [size_0,size_1,]
  method = ""nearest""
  align_corners = False
  preserve_aspect_ratio = False
  name = None
  out = tf.compat.v1.image.resize(images=images,size=size,method=method,align_corners=align_corners,preserve_aspect_ratio=preserve_aspect_ratio,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__ResizeNearestNeighbor_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 20527214848 with 536870912, result: -7426279517443850240 [Op:ResizeNearestNeighbor] name: 
```
```
"
tensorflow/tensorflow,2023-08-18 00:37:29,bug,Overflow bug when running tf.compat.v1.manip.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([1, 355, 768], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,]
  name = None
  out = tf.compat.v1.manip.tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 44407488056105 with 96070284019968, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
tensorflow/tensorflow,2023-08-18 00:29:46,bug,Overflow bug when running tf.compat.v1.keras.layers.ZeroPadding2D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding_0_0 = 125091515651
  padding_0_1 = 125091515651
  padding_0 = [padding_0_0,padding_0_1,]
  padding_1_0 = 125091515651
  padding_1_1 = 125091515651
  padding_1 = [padding_1_0,padding_1_1,]
  padding = [padding_0,padding_1,]
  data_format = None
  arg_class = tf.compat.v1.keras.layers.ZeroPadding2D(padding=padding,data_format=data_format,)
  arg_input_0_tensor = tf.random.uniform([3, 14, 14, 576], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 750549093948 with 250183031316, result: -1
	 [[{{node Pad}}]] [Op:Pad]

Call arguments received by layer 'zero_padding2d' (type ZeroPadding2D):
  • inputs=tf.Tensor(shape=(3, 14, 14, 576), dtype=float32)

```
```
"
tensorflow/tensorflow,2023-08-17 23:59:05,bug,Overflow bug when running tf.raw_ops.Tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input list

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([4, 1, 1, 20], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples_3 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,multiples_3,]
  name = None
  out = tf.raw_ops.Tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 500366062604 with 125091515651, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
tensorflow/tensorflow,2023-08-17 23:52:52,bug,Overflow bug when running tf.keras.layers.ZeroPadding3D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding = 1610612736
  arg_class = tf.keras.layers.ZeroPadding3D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([1, 1, 2, 2, 3], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:Exception encountered when calling layer 'zero_padding3d' (type ZeroPadding3D).

{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 3221225473 with 3221225474, result: -8070450522584252414 [Op:Pad]

Call arguments received by layer 'zero_padding3d' (type ZeroPadding3D):
  • inputs=tf.Tensor(shape=(1, 1, 2, 2, 3), dtype=float32)

```
```
"
tensorflow/tensorflow,2023-08-17 23:32:12,bug,Colab session crashes for unknown reasons when when running tf.raw_ops.ResizeBilinear on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  images_tensor = tf.random.uniform([1, 5, 5, 1], minval=-256, maxval=257, dtype=tf.int32)
  images = tf.identity(images_tensor)
  size_0 = 125091515651
  size_1 = True
  size = [size_0,size_1,]
  align_corners = False
  half_pixel_centers = False
  name = None
  out = tf.raw_ops.ResizeBilinear(images=images,size=size,align_corners=align_corners,half_pixel_centers=half_pixel_centers,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.593Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.592 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.594 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.600Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.603Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.602 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.604Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.605Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.604 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.605 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.608Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Searching ['/root/.jupyter', '/root/.local/etc/jupyter', '/usr/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files"",""time"":""2023-08-17T21:55:14.598Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.609 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.610 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.611Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.612 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.599Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.597 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.601Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.598 NotebookApp] Looking for jupyter_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.599 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter"",""time"":""2023-08-17T21:55:14.610Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.600 NotebookApp] Loaded config file: /etc/jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.601 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Loaded config file: /usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:14.612Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /usr/etc/jupyter"",""time"":""2023-08-17T21:55:14.613Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.local/etc/jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.603 NotebookApp] Looking for jupyter_notebook_config in /root/.jupyter"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""[D 21:55:14.606 NotebookApp] Loaded config file: /root/.jupyter/jupyter_notebook_config.py"",""time"":""2023-08-17T21:55:14.614Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.051Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.056Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.059Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.060Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.061Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.074Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.076Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.159Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.173Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json"",""time"":""2023-08-17T21:55:15.175Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.177Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/usr/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.179Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/root/.local/etc/jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.180Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    \\t/root/.jupyter/jupyter_notebook_config.json"",""time"":""2023-08-17T21:55:15.183Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret"",""time"":""2023-08-17T21:55:15.192Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Authentication of /metrics is OFF, since other authentication is disabled."",""time"":""2023-08-17T21:55:15.193Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""google.colab serverextension initialized."",""time"":""2023-08-17T21:55:15.213Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.313Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Serving notebooks from local directory: /"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.2:9000/"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.315Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Jupyter Notebook 6.5.5 is running at:"",""time"":""2023-08-17T21:55:19.314Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""http://172.28.0.12:9000/"",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."",""time"":""2023-08-17T21:55:19.316Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""Kernel started: 92d4365c-be07-4243-a024-4094c7317470, name: python3"",""time"":""2023-08-17T21:55:37.205Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bf475b0>"",""time"":""2023-08-17T21:55:52.871Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:56.928312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T21:55:56.928Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:55:58.601992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T21:55:58.602Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 21:56:04.276761: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at tile_ops.cc:193 : INVALID_ARGUMENT: Encountered overflow when multiplying 500366062604 with 125091515651, result: -1"",""time"":""2023-08-17T21:56:04.276Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Task exception was never retrieved"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""future: <Task finished name='Task-35' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /usr/local/lib/python3.10/dist-packages/tornado/websocket.py:1085> exception=WebSocketClosedError()>"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \\""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\\"", line 1087, in wrapper"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    await fut"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.iostream.StreamClosedError: Stream is closed"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""During handling of the above exception, another exception occurred:"",""time"":""2023-08-17T22:10:59.200Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Traceback (most recent call last):"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \\""/usr/lib/python3.10/asyncio/tasks.py\\"", line 232, in __step"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    result = coro.send(None)"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""  File \\""/usr/local/lib/python3.10/dist-packages/tornado/websocket.py\\"", line 1089, in wrapper"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""    raise WebSocketClosedError()"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""tornado.websocket.WebSocketClosedError"",""time"":""2023-08-17T22:10:59.201Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""Got events for closed stream <zmq.eventloop.zmqstream.ZMQStream object at 0x79d03bffc0d0>"",""time"":""2023-08-17T23:11:18.034Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:17:17.631931: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19327655268 exceeds 10% of free system memory."",""time"":""2023-08-17T23:17:17.634Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:17:28.209Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:54.827450: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T23:19:54.827Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:19:58.360271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T23:19:58.360Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:20:03.457377: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15342764032 exceeds 10% of free system memory."",""time"":""2023-08-17T23:20:03.457Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:20:40.231Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:20:40.234Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:19.461222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations."",""time"":""2023-08-17T23:25:19.461Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."",""time"":""2023-08-17T23:25:19.461Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:21.643520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT"",""time"":""2023-08-17T23:25:21.643Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""2023-08-17 23:25:25.628780: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2149856268 exceeds 10% of free system memory."",""time"":""2023-08-17T23:25:25.629Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":30,""msg"":""KernelRestarter: restarting kernel (1/5), keep random ports"",""time"":""2023-08-17T23:25:46.242Z"",""v"":0}
{""pid"":7,""type"":""jupyter"",""level"":40,""msg"":""WARNING:root:kernel 92d4365c-be07-4243-a024-4094c7317470 restarted"",""time"":""2023-08-17T23:25:46.242Z"",""v"":0}
```
```
"
tensorflow/tensorflow,2023-08-17 22:06:34,bug,Overflow bug when running tf.raw_ops.Pad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to the large list of elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([16, 16, 16, 512], dtype=tf.float32)
  input = tf.identity(input_tensor)
  paddings_0_0 = 125091515651
  paddings_0_1 = 125091515651
  paddings_0 = [paddings_0_0,paddings_0_1,]
  paddings_1_0 = 125091515651
  paddings_1_1 = False
  paddings_1 = [paddings_1_0,paddings_1_1,]
  paddings_2_0 = 125091515651
  paddings_2_1 = 125091515651
  paddings_2 = [paddings_2_0,paddings_2_1,]
  paddings_3_0 = 125091515651
  paddings_3_1 = 125091515651
  paddings_3 = [paddings_3_0,paddings_3_1,]
  paddings = [paddings_0,paddings_1,paddings_2,paddings_3,]
  name = None
  out = tf.raw_ops.Pad(input=input,paddings=paddings,name=name,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 250183031318 with 125091515667, result: -1
	 [[{{node Pad}}]] [Op:Pad]

```
```
"
tensorflow/tensorflow,2023-08-17 22:01:13,bug,Overflow bug when running tf.tile,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  arg_0_tensor = tf.random.uniform([452, 1, 768], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_0 = 125091515651
  arg_1_1 = 125091515651
  arg_1_2 = 125091515651
  arg_1 = [arg_1_0,arg_1_1,arg_1_2,]
  out = tf.tile(arg_0,arg_1,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 56541365074252 with 125091515651, result: -1
	 [[{{node Tile}}]] [Op:Tile]

```
```
"
tensorflow/tensorflow,2023-08-17 21:56:54,bug,Overflow bug when running tf.raw_ops.Tile on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  input_tensor = tf.random.uniform([4, 1, 1, 20], dtype=tf.float32)
  input = tf.identity(input_tensor)
  multiples_0 = 125091515651
  multiples_1 = True
  multiples_2 = 125091515651
  multiples_3 = 125091515651
  multiples = [multiples_0,multiples_1,multiples_2,multiples_3,]
  name = None
  out = tf.raw_ops.Tile(input=input,multiples=multiples,name=name,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__Tile_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 500366062604 with 125091515651, result: -1 [Op:Tile]
```
```
"
tensorflow/tensorflow,2023-08-17 21:51:46,bug,Overflow bug when running tf.keras.layers.ZeroPadding3D,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to the large integer value

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  padding = 1610612736
  arg_class = tf.keras.layers.ZeroPadding3D(padding=padding,)
  arg_input_0_tensor = tf.random.uniform([1, 1, 2, 2, 3], dtype=tf.float32)
  arg_input_0 = tf.identity(arg_input_0_tensor)
  arg_input = [arg_input_0,]
  out = arg_class(*arg_input)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:Exception encountered when calling layer 'zero_padding3d' (type ZeroPadding3D).

{{function_node __wrapped__Pad_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 3221225473 with 3221225474, result: -8070450522584252414 [Op:Pad]

Call arguments received by layer 'zero_padding3d' (type ZeroPadding3D):
  • inputs=tf.Tensor(shape=(1, 1, 2, 2, 3), dtype=float32)
{}

```
```
"
tensorflow/tensorflow,2023-08-17 18:37:54,bug,Issue with nightly-gpu docker image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230816

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using the tensorflow/tensorflow:nightly-gpu docker image I get an error saying the ""DNN library is not found""

However, when I change the base image to tensorflow/tensorflow:latest-gpu my code works fine.

Perhaps the nightly image broke something with the cuda / cudnn library paths?

### Standalone code to reproduce the issue

```shell
It seems that using a Conv1D layer is what causes the issue... see the log output below.
```


### Relevant log output

```shell
Detected at node 'peak_conv_1/Conv1D' defined at (most recent call last):
Node: 'peak_conv_1/Conv1D'
DNN library is not found.
         [[{{node peak_conv_1/Conv1D}}]] [Op:__inference_train_step_224831]
```
"
tensorflow/tensorflow,2023-08-17 15:19:19,bug,Integer overflow when running tf.compat.v1.matrix_diag on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in the input lists

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""None""
  k = 1610637938
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.compat.v1.matrix_diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Encountered overflow when multiplying 12885103520 with 1610637940, result: -1 [Op:MatrixDiagV3]
{}

```
```
"
tensorflow/tensorflow,2023-08-17 14:55:02,bug,Integer overflow when running tf.raw_ops.MatrixDiagV3 on colab,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to large elements in input lists

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0_0 = 1111
  diagonal_0_0_0_1 = 1112
  diagonal_0_0_0 = [diagonal_0_0_0_0,diagonal_0_0_0_1,]
  diagonal_0_0_1_0 = 1121
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = 2111
  diagonal_1_0_0_1 = 2112
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 35.0
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 2221
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  k = 3046875451
  num_rows = -1
  num_cols = -1
  padding_value = 0
  align = ""RIGHT_LEFT""
  name = ""diag_part""
  out = tf.raw_ops.MatrixDiagV3(diagonal=diagonal,k=k,num_rows=num_rows,num_cols=num_cols,padding_value=padding_value,align=align,name=name,)
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 9984734776 with 1248091847, result: -5984878005326580344
	 [[{{node MatrixDiagV3}}]] [Op:MatrixDiagV3]
{}
```
```
"
tensorflow/tensorflow,2023-08-17 14:22:48,bug,Integer overflow when running tf.linalg.diag,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

PRETTY_NAME=""Ubuntu 22.04.2 LTS"" NAME=""Ubuntu"" VERSION_ID=""22.04"" VERSION=""22.04.2 LTS (Jammy Jellyfish)"" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=jammy

### Mobile device

_No response_

### Python version

3.10.12 (main, Jun 11 2023, 05:26:28)

### Bazel version

_No response_

### GCC/compiler version

[GCC 11.4.0]

### CUDA/cuDNN version

[nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0](nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Wed_Sep_21_10:33:58_PDT_2022 Cuda compilation tools, release 11.8, V11.8.89 Build cuda_11.8.r11.8/compiler.31833905_0)

### GPU model and memory

T4

### Current behavior?

Due to the large list of elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
try:
  diagonal_0_0_0 = []
  diagonal_0_0_1_0 = True
  diagonal_0_0_1_1 = 1122
  diagonal_0_0_1 = [diagonal_0_0_1_0,diagonal_0_0_1_1,]
  diagonal_0_0 = [diagonal_0_0_0,diagonal_0_0_1,]
  diagonal_0_1_0_0 = 1211
  diagonal_0_1_0_1 = 1212
  diagonal_0_1_0 = [diagonal_0_1_0_0,diagonal_0_1_0_1,]
  diagonal_0_1_1_0 = 1221
  diagonal_0_1_1_1 = 1222
  diagonal_0_1_1 = [diagonal_0_1_1_0,diagonal_0_1_1_1,]
  diagonal_0_1 = [diagonal_0_1_0,diagonal_0_1_1,]
  diagonal_0 = [diagonal_0_0,diagonal_0_1,]
  diagonal_1_0_0_0 = True
  diagonal_1_0_0_1 = """"
  diagonal_1_0_0 = [diagonal_1_0_0_0,diagonal_1_0_0_1,]
  diagonal_1_0_1_0 = 2121
  diagonal_1_0_1_1 = 2122
  diagonal_1_0_1 = [diagonal_1_0_1_0,diagonal_1_0_1_1,]
  diagonal_1_0 = [diagonal_1_0_0,diagonal_1_0_1,]
  diagonal_1_1_0_0 = 2211
  diagonal_1_1_0_1 = 2212
  diagonal_1_1_0 = [diagonal_1_1_0_0,diagonal_1_1_0_1,]
  diagonal_1_1_1_0 = 30.0
  diagonal_1_1_1_1 = 2222
  diagonal_1_1_1 = [diagonal_1_1_1_0,diagonal_1_1_1_1,]
  diagonal_1_1 = [diagonal_1_1_0,diagonal_1_1_1,]
  diagonal_1 = [diagonal_1_0,diagonal_1_1,]
  diagonal = [diagonal_0,diagonal_1,]
  name = ""None""
  k = 1610612736
  padding_value = 0
  align = ""RIGHT_LEFT""
  out = tf.linalg.diag(diagonal=diagonal,name=name,k=k,padding_value=padding_value,align=align,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
Error:{{function_node __wrapped__MatrixDiagV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} Encountered overflow when multiplying 12884901888 with 1610612736, result: -1
	 [[{{node MatrixDiagV3}}]] [Op:MatrixDiagV3]
```
```
"
tensorflow/tensorflow,2023-08-17 08:51:23,bug,AttributeError: module 'tensorflow.python.pywrap_mlir' has no attribute 'experimental_convert_saved_model_v1',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-96406-gfa4d29bfef8 2.14.0-dev20230706

### Custom code

No

### OS platform and distribution

UIbuntu 20.04

### Mobile device

UIbuntu 20.04

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

-

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/iree/tools/tf/scripts/iree_import_tf/__main__.py"", line 54, in main
    import_saved_model(
  File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/iree/tools/tf/scripts/iree_import_tf/__main__.py"", line 102, in import_saved_model
    result = convert_saved_model_v1(
  File ""/home/fastDisk/jiahao/research/iree/.venv/lib/python3.8/site-packages/tensorflow/python/compiler/mlir/mlir.py"", line 141, in convert_saved_model_v1
    return pywrap_mlir.experimental_convert_saved_model_v1(
AttributeError: module 'tensorflow.python.pywrap_mlir' has no attribute 'experimental_convert_saved_model_v1'
```
"
tensorflow/tensorflow,2023-08-16 16:16:20,bug,TFLite GPU delegate: Broadcast output incorrect,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Nightly at 09cf1b2a39023e617e003a51be39d419702c2d36

### Custom code

Yes

### OS platform and distribution

Android 12 2023-03-01

### Mobile device

Vivo X80

### Python version

_No response_

### Bazel version

CMake 3.19.0

### GCC/compiler version

Android NDK r25

### CUDA/cuDNN version

_No response_

### GPU model and memory

Mali-G710 MC10

### Current behavior?

TFLite model file:  [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/12360271/model.tflite.zip)

The provided TFLite model contains a single broadcast operation, which when executed by the provided C++ program, should produce the following output:

```
1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  
2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  
3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  
4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  
5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  
6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  
7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  
8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 
```

However, when using the GPU delegate the following output is produced:

```
1  0  0  1  1  0  0  1  1  0  0  1  1  0  0  1  
2  0  0  1  2  0  0  1  2  0  0  1  2  0  0  1  
3  0  0  1  3  0  0  1  3  0  0  1  3  0  0  1  
4  0  0  1  4  0  0  1  4  0  0  1  4  0  0  1  
5  0  0  1  5  0  0  1  5  0  0  1  5  0  0  1  
6  0  0  1  6  0  0  1  6  0  0  1  6  0  0  1  
7  0  0  1  7  0  0  1  7  0  0  1  7  0  0  1  
8  0  0  1  8  0  0  1  8  0  0  1  8  0  0  1 
```

The correct output is produced when using the CPU and not the GPU delegate.  We are concerned that this could be a security issue if memory is being accessed incorrectly.

Content of `model.tflite`:

```
Your TFLite model has '1' subgraph(s). In the subgraph description below,
T# represents the Tensor numbers. For example, in Subgraph#0, the MUL op takes
tensor #0 and tensor #1 as input and produces tensor #2 as output.

Subgraph#0 main(T#0) -> [T#2]
  Op#0 MUL(T#0, T#1) -> [T#2]

Tensors of Subgraph#0
  T#0(serving_default_input:0) shape:[8, 1], type:FLOAT32
  T#1(BroadcastTo) shape:[8, 16], type:FLOAT32 RO 512 bytes, buffer: 2, data:[1, 1, 1, 1, 1, ...]
  T#2(PartitionedCall:0) shape:[8, 16], type:FLOAT32

---------------------------------------------------------------
Your TFLite model has '1' signature_def(s).

Signature#0 key: 'serving_default'
- Subgraph: Subgraph#0
- Inputs: 
    'input' : T#0
- Outputs: 
    'output' : T#2

---------------------------------------------------------------
              Model size:       1400 bytes
    Non-data buffer size:        780 bytes (55.71 %)
  Total data buffer size:        620 bytes (44.29 %)
    (Zero value buffers):          0 bytes (00.00 %)
```

### Standalone code to reproduce the issue

```shell
#include <memory>
#include <stdio.h>

#include ""tensorflow/lite/kernels/register.h""
#include ""tensorflow/lite/model.h""
#include ""tensorflow/lite/delegates/gpu/delegate.h""

int main() {
    std::unique_ptr<tflite::FlatBufferModel> model =
        tflite::FlatBufferModel::BuildFromFile(""model.tflite"");
    
    tflite::ops::builtin::BuiltinOpResolver resolver;
    tflite::InterpreterBuilder builder(*model, resolver);
    std::unique_ptr<tflite::Interpreter> interpreter;
    builder(&interpreter);

    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
    auto* delegate = TfLiteGpuDelegateV2Create(&options);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return 1;

    const TfLiteTensor *inTensor = interpreter->input_tensor(0);
    const TfLiteIntArray *inShape = inTensor->dims;
    float *input = inTensor->data.f;

    for (int i = 0; i < inShape->data[0]; i++) {
        input[i] = i + 1;
    }

    if (interpreter->Invoke() != kTfLiteOk) return 1;

    std::vector<unsigned long> outShapeVec;
    const TfLiteTensor *outTensor = interpreter->output_tensor(0);
    const TfLiteIntArray *outShape = outTensor->dims;
    const float *output = outTensor->data.f;

    const float *outPtr = output;
    for (size_t i = 0; i < outShape->data[0]; i++) {
        for (size_t j = 0; j < outShape->data[1]; j++) {
            printf(""%.2g  "", *(outPtr++));
        }
        printf(""\\n"");
    }
}
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-15 09:33:59,bug,Build failure on AARCH64 - undeclared identifier 'memset',"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build fails since commit https://github.com/tensorflow/tensorflow/commit/4993fb9fe4e4dbe26657b3bb88dab152ab397b8c

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/lite/kernels/internal/BUILD:448:11: Compiling tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/kernels/internal:optimized_4bit) 
  (cd /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\
  exec env - \\
    CACHEBUSTER=20220325 \\
    PATH=/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\
    PWD=/proc/self/cwd \\
    TF2_BEHAVIOR=1 \\
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.o' -DFC_4BIT_NEON '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/cpuinfo -iquote bazel-out/aarch64-opt/bin/external/cpuinfo -isystem external/cpuinfo/include -isystem bazel-out/aarch64-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/aarch64-opt/bin/external/cpuinfo/src -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' -DFARMHASH_NO_CXX_STRING -Wno-sign-compare -O3 -fno-exceptions -O3 '--sysroot=/dt10' -c tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/internal/_objs/optimized_4bit/neon_fully_connected.pic.o)
# Configuration: 70a2ceb8c9b79ab96bab8f0b73bbfb70969f7e2a66f605b1d1332a62f7eef342
# Execution platform: @local_execution_config_platform//:platform
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:284:3: error: use of undeclared identifier 'memset'
  memset(*dest, static_cast<uint8_t>(119), sizeof(uint8_t) * size);
  ^
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:313:3: error: use of undeclared identifier 'memset'
  memset(data, 0, sizeof(int8_t) * size);
  ^
tensorflow/lite/kernels/internal/optimized/4bit/neon_fully_connected.cc:314:3: error: use of undeclared identifier 'memset'
  memset(input_offsets, 0, sizeof(int32_t) * layout_rows);
  ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 23.741s, Critical Path: 7.00s
INFO: 439 processes: 339 internal, 100 local.
FAILED: Build did NOT complete successfully
```
"
tensorflow/tensorflow,2023-08-15 02:03:40,bug,Abort when running tensorflow.python.ops.nn_ops.pool,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Large list element

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  input_tensor = tf.constant(-8968073515812833920, shape=[2, 9, 10, 2], dtype=tf.float32,)
  input = tf.identity(input_tensor)
  window_shape_0 = 1e+38
  window_shape_1 = 536870912
  window_shape = [window_shape_0,window_shape_1,]
  padding = ""SAME""
  pooling_type = ""MAX""
  dilation_rate_0 = 1
  dilation_rate_1 = 1
  dilation_rate = [dilation_rate_0,dilation_rate_1,]
  strides_0 = 1
  strides_1 = 1
  strides = [strides_0,strides_1,]
  out = nn_ops.pool(input=input,window_shape=window_shape,padding=padding,pooling_type=pooling_type,dilation_rate=dilation_rate,strides=strides,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 22:03:21.255791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.273661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 22:03:21.274910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.275021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.275124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 22:03:21.328519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4361 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 22:03:21.392635: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 22:03:21.392690: F tensorflow/stream_executor/cuda/cuda_dnn.cc:886] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
tensorflow/tensorflow,2023-08-15 01:40:14,bug,Crash when running tensorflow.python.ops.gen_image_ops.resize_area,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to negative large tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_image_ops
try:
  arg_0_tensor = tf.constant(-1610612736, shape=[0, 6, 6, 1], dtype=tf.bfloat16,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-45932682421089, shape=[2], dtype=tf.int32,)
  arg_1 = tf.identity(arg_1_tensor)
  align_corners = False
  out = gen_image_ops.resize_area(arg_0,arg_1,align_corners=align_corners,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:39:36.565382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 21:39:37.126658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.144729: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:39:37.146083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.146208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.146312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.200954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:39:37.201045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4036 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
tensorflow/tensorflow,2023-08-15 01:34:29,bug,Abort when running tensorflow.python.ops.nn_ops.conv3d_transpose_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to invalid list elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.random.uniform([2, 4, 4, 4, 3], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.random.uniform([2, 2, 2, 5, 3], dtype=tf.float32)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 8
  arg_2_2 = 8
  arg_2_3 = 8
  arg_2_4 = False
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,arg_2_4,]
  arg_3 = 2
  out = nn_ops.conv3d_transpose_v2(arg_0,arg_1,arg_2,arg_3,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:33:38.109289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 21:33:38.668776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.686817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 21:33:38.687712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.687825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.687920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 21:33:38.749426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3978 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 21:33:38.812494: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600
2023-08-14 21:33:38.825580: F tensorflow/stream_executor/cuda/cuda_dnn.cc:804] Check failed: cudnnSetConvolutionGroupCount( handle_.get(), convolution_descriptor.group_count()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted

```
```
"
tensorflow/tensorflow,2023-08-15 01:29:57,bug,Crash when running tensorflow.python.framework.kernels.get_registered_kernels_for_op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to feeding None argument.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import kernels
try:
  arg_0 = None
  out = kernels.get_registered_kernels_for_op(arg_0,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 21:28:52.279973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
"
tensorflow/tensorflow,2023-08-14 23:40:39,bug,Abort when running tensorflow.python.ops.gen_array_ops.mirror_pad_grad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

due to NEGATIVE LARGE TENSOR

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_array_ops
try:
  arg_0_tensor = tf.constant(-17, shape=[1, 4, 7, 1], dtype=tf.int64,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-43871863081293, shape=[4, 2], dtype=tf.int32,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2 = ""REFLECT""
  out = gen_array_ops.mirror_pad_grad(arg_0,arg_1,arg_2,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:39:35.212387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.230641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.230799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.231097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:39:35.231945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.232079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.232187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.287820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.287970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.288088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:39:35.288181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4039 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:39:35.341105: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -1)
Aborted

```
```
"
tensorflow/tensorflow,2023-08-14 23:16:28,bug,Abort when running tensorflow.python.ops.gen_math_ops.sobol_sample,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Due to an empty input argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  arg_0 = 2
  arg_1 = 4
  arg_2 = [()]
  dtype = None
  out = gen_math_ops.sobol_sample(arg_0,arg_1,arg_2,dtype=dtype,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:14:54.117266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.134520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.134663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.135032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:14:54.135857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.135969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.136093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:14:54.189967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4373 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:14:54.237059: F tensorflow/core/framework/tensor.cc:733] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted

```
```
"
tensorflow/tensorflow,2023-08-14 23:08:43,bug,Abort when running tensorflow.python.ops.gen_sparse_ops.sparse_slice,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to Large List Elements

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  indices_0 = []
  indices = [indices_0,]
  values_0 = 0
  values = [values_0,]
  shape_0 = 1
  shape_1 = 1
  shape = [shape_0,shape_1,]
  start_0 = 4611686018427387904
  start_1 = -1
  start = [start_0,start_1,]
  size_0 = 4611686018427387904
  size_1 = 4611686018427387904
  size = [size_0,size_1,]
  out = gen_sparse_ops.sparse_slice(indices=indices,values=values,shape=shape,start=start,size=size,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 19:04:18.473689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.490933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.491133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.491455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 19:04:18.492102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.492212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.492308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 19:04:18.560508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4029 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-14 19:04:18.619230: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2023-08-14 19:04:18.619470: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
"
tensorflow/tensorflow,2023-08-14 22:25:16,bug,Crash when running tensorflow.python.ops.gen_data_flow_ops.record_input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to very large integer values

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_data_flow_ops
try:
  file_pattern = ""/tmp/record_input_testzsuyf9ap/tmpsqjnp5o1/basic.*""
  file_buffer_size = 1
  file_parallelism = 1676240524292489355
  file_shuffle_shift_ratio = 125091515651
  batch_size = 1
  file_random_seed = 125091515651
  compression_type = ""GZIP""
  out = gen_data_flow_ops.record_input(file_pattern=file_pattern,file_buffer_size=file_buffer_size,file_parallelism=file_parallelism,file_shuffle_shift_ratio=file_shuffle_shift_ratio,batch_size=batch_size,file_random_seed=file_random_seed,compression_type=compression_type,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-14 18:25:00.245344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 18:25:01.275751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.292863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.293001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.294465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 18:25:01.295740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.295849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.295947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.362610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 18:25:01.363266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4105 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
tensorflow/tensorflow,2023-08-14 21:19:25,bug,Segmentation fault when running tensorflow.python.ops.gen_math_ops._histogram_fixed_width,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to negative float argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_math_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_0_0 = -1.0
      arg_0_0_1 = 0.0
      arg_0_0_2 = 1.5
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1_0 = 2.0
      arg_0_1_1 = 5.0
      arg_0_1_2 = 15
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = -1.7976931348623157e+308
      arg_1_1 = -1.4013e-45
      arg_1 = [arg_1_0,arg_1_1,]
      arg_2 = 5
      dtype = tf.int32
      out = gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0_0 = [arg_0_0_0,arg_0_0_1,arg_0_0_2,]
      arg_0_1 = [arg_0_1_0,arg_0_1_1,arg_0_1_2,]
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1 = [arg_1_0,arg_1_1,]
      dtype = tf.int32
      gen_math_ops._histogram_fixed_width(arg_0,arg_1,arg_2,dtype=dtype,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-14 17:17:24.916189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-14 17:17:25.460939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.478922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 17:17:25.479696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.479802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.479896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-14 17:17:25.548456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3932 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault
(fuzzer_tf_2.11.0) n
```
```
"
tensorflow/tensorflow,2023-08-14 19:17:35,bug,Avoid partially saved ckpt from preempted device (e.g. TPU),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf.2.11

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I accidentally discovered from a TPU preemption that my ckpt cannot be used to resume training (the preemption occurs during the process of saving ckpt). The error is that some weights cannot be matched. This phenomenon is happening for the first time and has never happened before.

### Standalone code to reproduce the issue

```shell
It is very hard to reproduce because it must be preemption while saving the ckpt.
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-13 21:34:43,bug,I need TensorFlow 2.2.0 but it is removed how to find it?,"I need to install TensorFlow 2.2.0 

why? because this repo (https://github.com/GantMan/nsfw_model) is requesting it and now matter what I tried can't make it work with newer TensorFlows

How can I install TensorFlow 2.2.0  on Windows 10 and Python 3.10?

The error I am getting is and I am not able to fix it


```
(venv) G:\\nsfw_model>python a.py
2023-08-14 00:33:39.437427: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-14 00:33:40.148302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21643 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6
2023-08-14 00:33:40.149891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9603 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6
Traceback (most recent call last):
  File ""G:\\nsfw_model\\a.py"", line 13, in <module>
    print(predict.classify(model, 'test'))
  File ""G:\\nsfw_model\\nsfw_detector\\predict.py"", line 67, in classify
    probs = classify_nd(model, images, predict_args)
  File ""G:\\nsfw_model\\nsfw_detector\\predict.py"", line 77, in classify_nd
    model_preds = model.predict(nd_images, **predict_args)
  File ""G:\\nsfw_model\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""G:\\nsfw_model\\venv\\lib\\site-packages\\keras\\engine\\training.py"", line 1997, in predict
    raise ValueError('Unexpected result of `predict_function` '
ValueError: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```


predict.py

```
#! python

import argparse
import json
from os import listdir
from os.path import isfile, join, exists, isdir, abspath

import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_hub as hub


IMAGE_DIM = 299   # required/default image dimensionality

def load_images(image_paths, image_size, verbose=True):
    '''
    Function for loading images into numpy arrays for passing to model.predict
    inputs:
        image_paths: list of image paths to load
        image_size: size into which images should be resized
        verbose: show all of the image path and sizes loaded
    
    outputs:
        loaded_images: loaded images on which keras model can run predictions
        loaded_image_indexes: paths of images which the function is able to process
    
    '''
    loaded_images = []
    loaded_image_paths = []

    if isdir(image_paths):
        parent = abspath(image_paths)
        image_paths = [join(parent, f) for f in listdir(image_paths) if isfile(join(parent, f))]
    elif isfile(image_paths):
        image_paths = [image_paths]

    for img_path in image_paths:
        try:
            if verbose:
                print(img_path, ""size:"", image_size)
            image = keras.preprocessing.image.load_img(img_path, target_size=image_size)
            image = keras.preprocessing.image.img_to_array(image)
            image /= 255
            loaded_images.append(image)
            loaded_image_paths.append(img_path)
        except Exception as ex:
            print(""Image Load Failure: "", img_path, ex)
    
    return np.asarray(loaded_images), loaded_image_paths

def load_model(model_path):
    if model_path is None or not exists(model_path):
    	raise ValueError(""saved_model_path must be the valid directory of a saved model to load."")
    
    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer},compile=False)
    return model


def classify(model, input_paths, image_dim=IMAGE_DIM, predict_args={}):
    """"""
    Classify given a model, input paths (could be single string), and image dimensionality.
    
    Optionally, pass predict_args that will be passed to tf.keras.Model.predict().
    """"""
    images, image_paths = load_images(input_paths, (image_dim, image_dim))
    probs = classify_nd(model, images, predict_args)
    return dict(zip(image_paths, probs))


def classify_nd(model, nd_images, predict_args={}):
    """"""
    Classify given a model, image array (numpy)
    
    Optionally, pass predict_args that will be passed to tf.keras.Model.predict().
    """"""
    model_preds = model.predict(nd_images, **predict_args)
    # preds = np.argsort(model_preds, axis = 1).tolist()
    
    categories = ['drawings', 'hentai', 'neutral', 'porn', 'sexy']

    probs = []
    for i, single_preds in enumerate(model_preds):
        single_probs = {}
        for j, pred in enumerate(single_preds):
            single_probs[categories[j]] = float(pred)
        probs.append(single_probs)
    return probs


def main(args=None):
    parser = argparse.ArgumentParser(
        description=""""""A script to perform NFSW classification of images"""""",
        epilog=""""""
        Launch with default model and a test image
            python nsfw_detector/predict.py --saved_model_path mobilenet_v2_140_224 --image_source test.jpg
    """""", formatter_class=argparse.RawTextHelpFormatter)
    
    submain = parser.add_argument_group('main execution and evaluation functionality')
    submain.add_argument('--image_source', dest='image_source', type=str, required=True, 
                            help='A directory of images or a single image to classify')
    submain.add_argument('--saved_model_path', dest='saved_model_path', type=str, required=True, 
                            help='The model to load')
    submain.add_argument('--image_dim', dest='image_dim', type=int, default=IMAGE_DIM,
                            help=""The square dimension of the model's input shape"")
    if args is not None:
        config = vars(parser.parse_args(args))
    else:
        config = vars(parser.parse_args())

    if config['image_source'] is None or not exists(config['image_source']):
    	raise ValueError(""image_source must be a valid directory with images or a single image to classify."")
    
    model = load_model(config['saved_model_path'])    
    image_preds = classify(model, config['image_source'], config['image_dim'])
    print(json.dumps(image_preds, indent=2), '\\n')


if __name__ == ""__main__"":
	main()

```

"
tensorflow/tensorflow,2023-08-13 06:08:12,bug,Abort when running tensorflow.python.ops.gen_sparse_ops.sparse_split,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to zero integer argument. It would be best if you ran multiple times to see the abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_sparse_ops
try:
  arg_0 = 0
  arg_1_tensor = tf.random.uniform([14, 2], minval=-256, maxval=257, dtype=tf.int64)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_tensor = tf.random.uniform([14], minval=-256, maxval=257, dtype=tf.int64)
  arg_2 = tf.identity(arg_2_tensor)
  arg_3_tensor = tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64)
  arg_3 = tf.identity(arg_3_tensor)
  arg_4 = 2
  out = gen_sparse_ops.sparse_split(arg_0,arg_1,arg_2,arg_3,arg_4,)
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 02:06:08.283954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 02:06:09.159348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.181927: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 02:06:09.182527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.182681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.182809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.234678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.234880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.235018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 02:06:09.235129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 151 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 02:06:09.251615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 151.69M (159055872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 02:06:09.251958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 136.52M (143150336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-08-13 02:06:09.273191: E tensorflow/compiler/xla/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS: misaligned address
2023-08-13 02:06:09.273688: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Aborted

```
```
"
tensorflow/tensorflow,2023-08-13 05:22:51,bug,Abort when running tensorflow.python.eager.remote.connect_to_remote_host,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

NaN string argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.eager import remote
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      remote.connect_to_remote_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-13 01:22:37.499369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 01:22:38.459392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.480510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.480708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 01:22:38.481707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.481961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.536991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 01:22:38.537094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1725 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 01:22:38.546718: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:589] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0813 01:22:38.546961566 1686085 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
"
tensorflow/tensorflow,2023-08-13 04:15:00,bug,Abort when running tensorflow.python.ops.gen_nn_ops.conv3d_backprop_input_v2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Due to input tensor with zero shape

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  input_sizes_0 = 2
  input_sizes_1 = 8
  input_sizes_2 = 8
  input_sizes_3 = 8
  input_sizes_4 = 5
  input_sizes = [input_sizes_0,input_sizes_1,input_sizes_2,input_sizes_3,input_sizes_4,]
  filter_tensor = tf.random.uniform([0, 1, 2, 5, 3], dtype=tf.float32)
  filter = tf.identity(filter_tensor)
  out_backprop_tensor = tf.random.uniform([2, 4, 4, 4, 3], dtype=tf.float32)
  out_backprop = tf.identity(out_backprop_tensor)
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 2
  strides_4 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,strides_4,]
  padding = ""SAME""
  data_format = ""NDHWC""
  dilations_0 = 1
  dilations_1 = 1
  dilations_2 = 1
  dilations_3 = 1
  dilations_4 = 1
  dilations = [dilations_0,dilations_1,dilations_2,dilations_3,dilations_4,]
  out = gen_nn_ops.conv3d_backprop_input_v2(input_sizes=input_sizes,filter=filter,out_backprop=out_backprop,strides=strides,padding=padding,data_format=data_format,dilations=dilations,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-13 00:13:03.668988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-13 00:13:04.462547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.483905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-13 00:13:04.484753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.484944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.485057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-13 00:13:04.585565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 744 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-13 00:13:04.615688: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
"
tensorflow/tensorflow,2023-08-13 03:05:27,bug,Abort when running tensorflow.python.ops.linalg_ops.self_adjoint_eig,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

This behavior is very strange  and should not throw OOM error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import linalg_ops
try:
  arg_0_tensor = tf.random.uniform([1, 1], dtype=tf.float32)
  arg_0 = tf.identity(arg_0_tensor)
  out = linalg_ops.self_adjoint_eigvals(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 23:02:34.613725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 23:02:35.612147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.634199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.634612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 23:02:35.635637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.635948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 23:02:35.639564: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:370] A non-primary context 0x5a7ae50 for device 0 exists before initializing the StreamExecutor. The primary context is now 0x7ffd00000000. We haven't verified StreamExecutor works with that.
2023-08-12 23:02:35.639662: F tensorflow/tsl/platform/statusor.cc:33] Attempting to fetch value instead of handling error INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6216417280
Aborted

```
```
"
tensorflow/tensorflow,2023-08-12 19:32:57,bug,Abort when running tensorflow.python.ops.nn_ops.conv2d_transpose,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to the large input tensor

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.ops import nn_ops
try:
  arg_0_tensor = tf.constant(-1048576, shape=[2, 6, 4, 3], dtype=tf.float16,)
  arg_0 = tf.identity(arg_0_tensor)
  arg_1_tensor = tf.constant(-1250999896764, shape=[0, 3, 2, 3], dtype=tf.float16,)
  arg_1 = tf.identity(arg_1_tensor)
  arg_2_0 = 2
  arg_2_1 = 12
  arg_2_2 = 8
  arg_2_3 = 2
  arg_2 = [arg_2_0,arg_2_1,arg_2_2,arg_2_3,]
  strides_0 = 1
  strides_1 = 2
  strides_2 = 2
  strides_3 = 1
  strides = [strides_0,strides_1,strides_2,strides_3,]
  padding = ""SAME""
  out = nn_ops.conv2d_transpose(arg_0,arg_1,arg_2,strides=strides,padding=padding,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 15:32:22.021693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 15:32:22.879671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.899716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.899926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.900241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 15:32:22.900789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.900913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.901022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 15:32:22.952557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-12 15:32:22.974261: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted

```
```
"
tensorflow/tensorflow,2023-08-12 18:32:22,bug,segmentation fault when running tensorflow.python.eager.context.add_function,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to the NONE argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.eager import context
try:
  arg_0 = None
  out = context.add_function(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 14:32:03.270708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-08-12 14:32:04.485549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.505739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-12 14:32:04.506268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.506389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.506492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-08-12 14:32:04.556911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 739 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
Segmentation fault

```
```
"
tensorflow/tensorflow,2023-08-11 09:46:28,bug,tflite-rutime: RuntimeError: Encountered unresolved custom op: FarthestPointSample.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): 2.13.0


**Provide the text output from tflite_convert**
I did some test with pointnet++(https://github.com/charlesq34/pointnet2), and tried to inference with tf.lite or tflite-runtime, but both of them show the error message below:
```
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    predict = PointNetPredict('/kaggle/input/model-sign/model_sign.tflite')
  File ""/kaggle/working/pointnet3c1/models/pointnet_predict.py"", line 27, in __init__
    self.interpreter = self.init_model()
  File ""/kaggle/working/pointnet3c1/models/pointnet_predict.py"", line 39, in init_model
    interpreter.allocate_tensors()
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py"", line 513, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Encountered unresolved custom op: FarthestPointSample.
See instructions: https://www.tensorflow.org/lite/guide/ops_custom Node number 0 (FarthestPointSample) failed to prepare.Encountered unresolved custom op: FarthestPointSample.
See instructions: https://www.tensorflow.org/lite/guide/ops_custom Node number 0 (FarthestPointSample) failed to prepare.
```
I noticed there were some custom ops(tf_ops) in project pointnet2, but how to convert these ops to tflite-runtime operators?

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
tensorflow/tensorflow,2023-08-10 23:41:58,bug,AttributeError: can't set attribute in Plot or @property for example,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

Windows

### Mobile device

na

### Python version

3.8

### Bazel version

na

### GCC/compiler version

?

### CUDA/cuDNN version

?

### GPU model and memory

colab notebook

### Current behavior?

I'm running this tensorflow example: 

https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb

I wanted to add some additional models at the end and tried to create new data windows as done above.  I noticed the example already given, and my new code requires the following line to be run before the @property for ""example"" is created:

""  w2.example = example_inputs, example_labels  ""

else you get a vague error  ""AttributeError: can't set attribute "" but it looks like this has a setter?
  
This line is found under ""3. Plot"" and if moved to a later section after the 

@property 
def example

under section 4 this error occurs. 



### Standalone code to reproduce the issue

```shell
Move:
w2.example = example_inputs, example_labels

to under section 4 which should be ok. Can't set error occurs.
```


### Relevant log output

```shell
AttributeError: can't set attribute
```
"
tensorflow/tensorflow,2023-08-09 07:58:04,bug,Question about @tf.function,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.3.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4

### Current behavior?

After adding @tf.function, I found that each epoch only executes one batch_size, and this does not happen when @tf.function are removed

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D,GlobalAveragePooling2D, Flatten, Dense, Dropout
from tensorflow.keras import Model, Sequential
from tensorflow.keras.regularizers import L2
class ResNetBlock(Model):
    def __init__(self, filters=64, strides=1):
        super(ResNetBlock, self).__init__()
        self.strides = strides
        self.c1 = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')

        self.c2 = Conv2D(filters=filters, kernel_size=(3, 3), strides=1, padding='same')
        self.b2 = BatchNormalization()

        if(strides > 1):
            self.c3 = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')
            self.b3 = BatchNormalization()
        
        self.a2 = Activation('relu')

    def call(self, inputs):
        short_x = inputs
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)
        x = self.c2(x)
        y = self.b2(x)
        if(self.strides > 1):
            short_x = self.c3(short_x)
            short_x = self.b3(short_x)
        return self.a2(short_x + y)
    
class ResNet(Model):
    def __init__(self, model_lst, cur_filters = 64):
        super(ResNet, self).__init__()
        self.c1 = Conv2D(filters=cur_filters, kernel_size=(7, 7), strides=2, padding='same')
        self.b1 = BatchNormalization()
        self.a1 = Activation('relu')
        self.p1 = MaxPool2D((2, 2), 2)
        self.blocks = Sequential()
        for (i, lst) in enumerate(model_lst):
            for ids in range(lst):
                if(i != 0 and ids == 0):
                    block = ResNetBlock(cur_filters, strides=2)
                else:
                    block = ResNetBlock(cur_filters, strides=1)
                self.blocks.add(block)    
            cur_filters *= 2
        self.g1 = GlobalAveragePooling2D()
        self.d1 = Dense(10, activation='softmax', kernel_regularizer=L2())

    def call(self, inputs):
        x = self.c1(inputs)
        x = self.b1(x)
        x = self.a1(x)
        x = self.p1(x)

        x = self.blocks(x)
        x = self.g1(x)
        y = self.d1(x)
        return y


# ---------------------------------------------
# ResNet18
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# import matplotlib
# matplotlib.rcParams['font.family']=['SimHei', 'Arial']
from tensorflow.keras import *
from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Activation, MaxPool2D,GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Mean,SparseCategoricalAccuracy  
from tensorflow.keras.datasets.fashion_mnist import load_data 
batch_size = 64
epochs = 20
validation_freq = 2
(x_train, y_train), (x_test, y_test) = load_data()
x_train, x_test = x_train/255., x_test/255.
x_train = np.expand_dims(x_train, -1).astype(np.float32)
x_test = np.expand_dims(x_test, -1).astype(np.float32)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(len(x_test)).batch(batch_size)

model = ResNet([2, 2, 2, 2])
losses = SparseCategoricalCrossentropy(from_logits=False)
optimizer = Adam()
train_metrics_loss = Mean()
train_metrics_accuracy = SparseCategoricalAccuracy()
test_metrics_loss = Mean()
test_metrics_accuracy = SparseCategoricalAccuracy()

train_losses = []
train_accuracy = []
test_losses = []
test_accuracy = []
@tf.function
def train_step(model, input_images, y_real):
    with tf.GradientTape() as tape:
        y_pred = model(input_images, training=True)
        loss = losses(y_real, y_pred)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    train_metrics_loss.update_state(loss)
    train_metrics_accuracy.update_state(y_real, y_pred)
@tf.function
def test_step(model, input_images, y_real):
    with tf.GradientTape() as tape:
        y_pred = model(input_images, training=False)
        loss = losses(y_real, y_pred)
    test_metrics_loss.update_state(loss)
    test_metrics_accuracy.update_state(y_real, y_pred)

for epoch in range(epochs):
    train_metrics_loss.reset_states()
    train_metrics_accuracy.reset_states()
    test_metrics_accuracy.reset_states()
    test_metrics_loss.reset_states()
    for x_batch, y_batch in train_dataset:
        train_step(model, x_batch, y_batch)
    train_losses.append(train_metrics_loss.result())
    train_accuracy.append(train_metrics_accuracy.result())
    print(f""epoch={epoch}, train_loss={train_metrics_loss.result()}, train_accuracy={train_metrics_accuracy.result()}"")
    if(epoch % validation_freq == 0):
        for test_x_batch, test_y_batch in test_dataset:
            test_step(model, test_x_batch, test_y_batch)
        test_losses.append(test_metrics_loss.result())
        test_accuracy.append(test_metrics_accuracy.result())
        print(f""epoch={epoch}, test_loss={test_metrics_loss.result()}, test_accuracy={test_metrics_accuracy.result()}"")



plt.figure(figsize=(8, 5))
plt.subplot(1, 2, 1)
plt.title('损失值变化图')
plt.plot(test_losses, 'g-', label=""Test_Loss"")
plt.plot(train_losses, 'r-', label=""Train_Loss"")

plt.legend()

plt.subplot(1, 2, 2)
plt.title(""准确率变化图"")
plt.plot(train_accuracy, 'r-', label=""Train_Accuracy"")
plt.plot(test_accuracy, 'g-', label=""Test_Accuracy"")
plt.legend()

plt.show()
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-08-03 10:02:23,bug,"""load_model"" method causes operating system level user-interface freeze","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

 2.6.2

### Custom code

Yes

### OS platform and distribution

Windows : 10.0.17763 

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA : 11.2.0_460.89 /CUDNN : 8.1.0.77

### GPU model and memory

NVIDIA RTX A6000

### Current behavior?

There should not be any user interface freeze

### Standalone code to reproduce the issue

```shell
load_m = tf.keras.models.load_model('test.hdf5',custom_objects={'custom_loss':CustomLossFunction})
```


### Relevant log output

```shell
We just see operating system user interface freeze.
When the GPU (NVIDIA RTX A6000 ) mode is TCC we see that whole user interface of operating system ( not just the process which is execution this command ) is frozen for 10 seconds. Can this be fixed so that there is no freeze of user interface ? 
Same code when GPU mode is WDDM will not freeze the user interface.
```
"
tensorflow/tensorflow,2023-08-02 12:29:15,bug,tf 2.13 - tflite convert error  in topk when k is np.int64 ,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac and colab 
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13

### 2. Code

Colab code [here](https://colab.research.google.com/drive/163eKr3nkQM4vRqCnFu5U9K3QhgA5PWog?usp=sharing)

### 3. Bug
tf 2.13 model with `tf.math.top_k` error in tflite convert 

tf 2.12 - **pass**
`k` is numpy.int64 - **fail** 
`k` is numpy.int32 - **pass**
`k` is python int - **pass**  

### 4. Error logs
```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
[<ipython-input-6-2ef9a00e0912>](https://localhost:8080/#) in <cell line: 2>()
      1 # error
----> 2 create_model_and_convert(k=np.int64(5))

9 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py](https://localhost:8080/#) in convert(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)
    365               enable_mlir_converter,
    366           )
--> 367       raise converter_error
    368 
    369   return _run_deprecated_conversion_binary(

ConverterError: /usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py:1313:0: error: 'tf.TopKV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""PartitionedCall:"", ""PartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/save.py:1313:0: note: Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: TopKV2
Details:
	tf.TopKV2(tensor<?x29x7xf32>, tensor<i64>) -> (tensor<?x29x5xf32>, tensor<?x29x5xi32>) : {device = """", sorted = true}
```
"
tensorflow/tensorflow,2023-08-01 11:41:30,bug,try self.interpreter!.invoke() App got crashed on this line ,Swift 5 
tensorflow/tensorflow,2023-08-01 04:31:13,bug,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

N/A

### Python version

3.10(Microsoft Store)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda: 11.2

### GPU model and memory

RTX 3070 Ti 8GB
### Current behavior?

I installed CUDA 11.2 as recommended for tf 2.10.0, here's the install:
![Screenshot](https://github.com/tensorflow/tensorflow/assets/1494132/59352a2a-f90f-45bf-b8bd-861dc893a9ff)
At first, I thought it was a path issue, but after restarting my pc, I was able to access exe files in that folder:
![image](https://github.com/tensorflow/tensorflow/assets/1494132/5d9ccfca-4417-4045-ba74-fffde7b8a121)
If the files are in path, why can't tensorflow find them?
Many people say to use miniconda, so I did, but I got the same result. Other resolved issues were resolved as the OP's were using the wrong version of CUDA, I checked on the website and I can confirm that my version is the required one.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2023-07-31 18:56:25.098058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:25.098226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-07-31 18:56:26.164080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:26.164320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2023-07-31 18:56:26.164540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2023-07-31 18:56:26.164818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2023-07-31 18:56:26.368828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2023-07-31 18:56:26.369092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
```
"
tensorflow/tensorflow,2023-07-31 09:26:26,bug,Visual Studio 2022 / MingW64: cant find source files,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

8.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Cant write complete application

### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>

int main() {
    // Инициализация TensorFlow
    tensorflow::Scope root = tensorflow::Scope::NewRootScope();
    tensorflow::ClientSession session(root);

    // Входные данные (5 предыдущих OHLC свечей)
    std::vector<float> input_data = { /* Ваши значения OHLC свечей */ };
    tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 1, 5 }));
    auto input_tensor_mapped = input_tensor.tensor<float, 2>();
    for (int i = 0; i < 5; ++i) {
        input_tensor_mapped(0, i) = input_data[i];
    }

    // Загружаем модель или определяем свою модель для прогнозирования
    // tensorflow::GraphDef graph_def;
    // tensorflow::ReadBinaryProto(tensorflow::Env::Default(), ""path/to/model.pb"", &graph_def);
    // tensorflow::SessionOptions session_options;
    // tensorflow::ClientSession session(root, session_options);
    // session.Create(graph_def);

    // Выполняем прогноз на основе входных данных
    tensorflow::Tensor output_tensor;
    tensorflow::Status run_status = session.Run({ { ""input_tensor_name"", input_tensor } },
        { ""output_tensor_name"" }, {}, &output_tensor);

    if (!run_status.ok()) {
        std::cerr << ""Ошибка выполнения: "" << run_status.error_message() << std::endl;
        return 1;
    }

    // Обрабатываем результат прогноза
    auto output_tensor_mapped = output_tensor.tensor<float, 2>();
    // Выводим результаты прогноза OHLC свечи будущей

    return 0;
}
```


### Relevant log output

```shell
Серьезность	Код	Описание	Проект	Файл	Строка	Состояние подавления
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/ThreadPool""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\threadpool_interface.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\framework\\ops.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\framework\\ops.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/tensor.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\framework\\ops.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\framework\\scope.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/array_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/candidate_sampling_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/control_flow_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/data_flow_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/image_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/io_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/linalg_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/logging_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/lookup_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/math_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/nn_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/no_op.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/parsing_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/random_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/sparse_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/state_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/string_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/training_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	37	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/cc/ops/user_ops.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\cc\\ops\\standard_ops.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\common_runtime\\graph_constructor.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\allocator.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\allocator.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/macros.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\device_base.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\device_base.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/device_attributes.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\device_base.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\full_type_inference_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\full_type_util.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\full_type_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\full_type_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph_debug_info.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/variant.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/attr_value.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/function.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/optimized_function_graph.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	40	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	51	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/tsl/protobuf/error_codes.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	52	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/remote_tensor_handle.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\function.h	54	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_def_builder.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_def_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_def_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_def_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_def_util.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_properties.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\node_properties.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_def_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_def_builder.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/api_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_def_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_def_util.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/time/time.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/kernel_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/tensor_shape.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	44	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	47	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\op_kernel.h	59	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/registration/options.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\registration\\registration.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\resource_handle.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\tensor.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\tensor.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\tensor_shape.h	21	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\tensor_shape.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\tensor_types.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\types.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\types.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/types.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\framework\\types.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph.h	45	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/full_type.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph.h	46	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/node_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph.h	48	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/flat_hash_map.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/graph_debug_info.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\graph_debug_info_builder.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/framework/op_def.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\graph\\node_builder.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/span.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\lib\\gtl\\array_slice.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\platform\\errors.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\platform\\errors.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\platform\\threadpool.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/core/protobuf/config.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\public\\session_options.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/match.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\util\\managed_stack_trace.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_cat.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\util\\managed_stack_trace.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\util\\managed_stack_trace.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\core\\util\\tensor_format.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\framework\\allocator.h	25	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\framework\\allocator.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\framework\\device_type.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""Eigen/Core""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\framework\\fixedpoint_types.h	21	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/container/inlined_vector.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\lib\\gtl\\inlined_vector.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""third_party/eigen3/Eigen/Core""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\bfloat16.h	20	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\default\\cord.h	22	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/log_severity.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\default\\logging.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\default\\logging.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\default\\statusor.h	18	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/functional/any_invocable.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\env.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\errors.h	26	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\errors.h	27	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\errors.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\errors.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""include/float8.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\float8.h	19	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/descriptor.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/arena.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/descriptor.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/dynamic_message.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/coded_stream.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	34	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/tokenizer.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	35	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/zero_copy_stream.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	36	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/io/zero_copy_stream_impl_lite.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	37	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/map.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	38	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/message.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	39	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/repeated_field.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	40	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/text_format.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	41	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/field_comparator.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	42	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/json_util.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	43	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/message_differencer.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	44	
Ошибка (активно)	E1696	не удается открыть источник файл ""google/protobuf/util/type_resolver_util.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\protobuf.h	45	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	28	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/functional/function_ref.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/status.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	30	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/cord.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	31	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	32	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	33	
Ошибка (активно)	E1696	не удается открыть источник файл ""tensorflow/tsl/protobuf/error_codes.pb.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\status.h	39	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/base/attributes.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\statusor.h	71	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/status/statusor.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\statusor.h	72	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/string_view.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\stringpiece.h	29	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_join.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\str_util.h	23	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/strings/str_split.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\str_util.h	24	
Ошибка (активно)	E1696	не удается открыть источник файл ""absl/types/optional.h""	ai	C:\\Users\\User\\source\\repos\\ai\\include\\tensorflow\\tsl\\platform\\threadpool.h	22
```
"
tensorflow/tensorflow,2023-07-28 18:24:21,bug,Failed assertion in tf.linalg.sqrtm (and possibly other functions) crashes entire program instead of raising Exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.9.1

### Custom code

No

### OS platform and distribution

Macbook 2020 M1 air, Ventura 13.2

### Mobile device

-

### Python version

3.8.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently, providing a degenerate matrix to tf.linalg.sqrtm crashes the entire program, producing output: 

```
Assertion failed: (T(i,i) >= 0), function matrix_sqrt_quasi_triangular_diagonal, file external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixSquareRoot.h, line 128.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def demo_assertion_error_crashes_program():

    degenerate_matrix = tf.ones((3, 3), dtype=tf.float64)
    try:
        tf.linalg.sqrtm(degenerate_matrix)
        # tf.linalg.inv(degenerate_matrix)  # <- This also fails, but raises an actual exception
        print(""Calculated root"")  # This is never run
    except Exception as err:
        print(""Caught exception: "", err)  # Neither is this

if __name__ == '__main__':
    demo_assertion_error_crashes_program()
```


### Relevant log output

```shell
Assertion failed: (T(i,i) >= 0), function matrix_sqrt_quasi_triangular_diagonal, file external/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixSquareRoot.h, line 128.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```

### Workaround

One option is to add a small regularizing term to make it non-degenerate, but I don't (yet) know how to do this such that it always prevents the crash, and also does not significantly affect results then the matrix-square-root would have worked.

Instead, I now just use the Denmann-Beavers iteration to approximate the matrix square-root

```
def tf_denmann_beavers_sqrtm(matrix: tf.Tensor, n_iter=10):
    """"""
    Approximate the matrix-square-root by Denmann Beavers iteration
        https://en.wikipedia.org/wiki/Square_root_of_a_matrix#By_Denman%E2%80%93Beavers_iteration
    Convergence is not guaranteed.  Use at your own risk!
    This is handy for tflite, which does not yet support tf.linalg.sqrtm
        https://github.com/tensorflow/tensorflow/issues/60154
    Or for regular tensorflow, which crashes your entire program when input matrix is degenerate
        https://github.com/tensorflow/tensorflow/issues/61423
    """"""
    ym = matrix
    zm = tf.eye(tf.shape(matrix[0])[0], dtype=matrix.dtype)
    for i in range(n_iter):
        ym_ = 0.5 * (ym + tf.linalg.inv(zm))
        zm = 0.5 * (zm + tf.linalg.inv(ym))
        ym = ym_
    return ym
```
... obviously this is not ideal.
"
tensorflow/tensorflow,2023-07-26 21:56:14,bug,"model.fit() occur ""Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11, 2.12, 2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8 / 8.6 & 11.8 / 8.9.2

### GPU model and memory

RTX 3090 Ti & RTX 4090

### Current behavior?

This is first time experience to have such error message.

When I try
""model.fit()""
server stops with error message below

Tried cuDNN version 8.6 (as [tensorflow.org](https://www.tensorflow.org/install/pip) ) and 8.9.2 (lateset for CUDA 11.8)
Both have problem.

How can I solve the issue?
Thanks!

### Standalone code to reproduce the issue

```shell
gpu_id = ""2"" # 0 or 1
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = gpu_id

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
import tensorflow.keras.models as models

size_y = 256
size_x = 256

#--- load dataset
dic_path = './seg_dataset/train/dic'
msk_path = './seg_dataset/train/msk'

seed = 1004 # random number in your mind

dic_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2
    )
msk_datagen = keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2
    )

dic_train = \\
    dic_datagen.flow_from_directory( 
        dic_path,
        target_size=(size_y, size_x),
        class_mode=None, 
        seed=seed,
        subset='training'
    )

msk_train = \\
    msk_datagen.flow_from_directory( 
        msk_path,
        target_size=(size_y, size_x),
        class_mode=None,
        color_mode='grayscale',
        seed=seed,
        subset='training'
    )

dic_valid = \\
    dic_datagen.flow_from_directory( 
        dic_path,
        target_size=(size_y, size_x),
        class_mode=None, 
        seed=seed,
        subset='validation'
    )
msk_valid = \\
    msk_datagen.flow_from_directory( 
        msk_path,
        target_size=(size_y, size_x),
        class_mode=None,
        color_mode='grayscale',
        seed=seed,
        subset='validation'
    )

train_ds = zip(dic_train, msk_train)
valid_ds = zip(dic_valid, msk_valid)

f = [16, 32, 64, 128, 256]

kernel_size=(3,3)
padding='same'
strides=1


# number of filters at each level
inputs = layers.Input((size_y, size_x, 1))
p0 = inputs
# downblock 1
x  = layers.Conv2D(16,  kernel_size, padding=padding, strides=strides, activation=""relu"")(p0)
c1 = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c1)
# downblock 2
x  = layers.Conv2D(32,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c2 = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c2)
# downblock 3
x  = layers.Conv2D(64,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c3 = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c3)
# downblock 4
x  = layers.Conv2D(128,  kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
c4 = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.MaxPool2D((2, 2), (2, 2))(c4)
# bottle neck
x  = layers.Conv2D(256, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
x  = layers.Conv2D(256, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c4])
x  = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(128, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c3])
x  = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(64, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c2])
x  = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(32, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)
# up block 1
x  = layers.UpSampling2D((2, 2))(x)
concat = layers.Concatenate()([x, c1])
x  = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(concat)
x  = layers.Conv2D(16, kernel_size, padding=padding, strides=strides, activation=""relu"")(x)

# last convolution 1x1
outputs = layers.Conv2D(1, (1, 1), padding=""same"", activation=""sigmoid"")(x)
model = models.Model(inputs, outputs)



model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

path_checkpoint = './seg_checkpoint'
os.makedirs(path_checkpoint,exist_ok=True)

model_checkpointer = keras.callbacks.ModelCheckpoint(
    filepath = path_checkpoint,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose = 1
)

#--- additional
callbacks = [
        model_checkpointer,
        keras.callbacks.EarlyStopping(
            patience=50*3,
            monitor='val_loss',
            mode='min',
            verbose=1
            ),
]



#--- train start
EPOCH = 10

history = model.fit(
        train_ds,
        validation_data=valid_ds,
        validation_steps=15, 
        # Total number of steps (batches of samples) 
        # to draw before stopping when performing validation at the end of every epoch.
        batch_size=16,
        steps_per_epoch=50,
        epochs=EPOCH,
        callbacks=callbacks
    )
```


### Relevant log output

```shell
2023-07-26 14:46:27.667380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8942] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-07-26 14:46:27.667411: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-07-26 14:46:27.667426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-07-26 14:46:27.671343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-26 14:46:28.183018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-07-26 14:46:28.727203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.741660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.741864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.807913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:28.808057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6
2023-07-26 14:46:28.809796: I tensorflow/core/common_runtime/direct_session.cc:380] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6

Found 40000 images belonging to 1 classes.
Found 40000 images belonging to 1 classes.
Found 10000 images belonging to 1 classes.
Found 10000 images belonging to 1 classes.
2023-07-26 14:46:30.293048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.293974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-26 14:46:30.294450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:41:00.0, compute capability: 8.6
Epoch 1/10
2023-07-26 14:46:31.666051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8600
2023-07-26 14:46:31.674917: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_ops_fused_impl.h:625 : INTERNAL: Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4340): 'conv_op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed
Traceback (most recent call last):
  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(
  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node model/conv2d/Relu defined at (most recent call last):
  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations.py"", line 306, in relu
    return backend.relu(

  File ""/home/bootcamp/train_unet.py"", line 161, in <module>
    history = model.fit(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1783, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function
    return step_function(self, iterator)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step
    outputs = model.train_step(data)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1126, in train_step
    y_pred = self(x, training=True)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py"", line 589, in __call__
    return super().__call__(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 515, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 672, in _run_internal_graph
    outputs = node.layer(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/base_layer.py"", line 1149, in __call__
    outputs = call_fn(inputs, *args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 96, in error_handler
    return fn(*args, **kwargs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"", line 321, in call
    return self.activation(outputs)

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/activations.py"", line 306, in relu
    return backend.relu(

  File ""/home/bootcamp/miniconda3/envs/tf/lib/python3.10/site-packages/keras/src/backend.py"", line 5397, in relu
    x = tf.nn.relu(x)

Cudnn graph failed to build: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(4340): 'conv_op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed
         [[{{node model/conv2d/Relu}}]] [Op:__inference_train_function_4359]
```
"
tensorflow/tensorflow,2023-07-25 08:13:21,bug,OneDNN logs are not printing while building TF with --config=mkl_aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

6.3

### GCC/compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am expecting OneDNN logs should print while running deep learning model such as resnet50, if we export ONEDNN_VERBOSE=1

### Standalone code to reproduce the issue

```shell
To reproduce same, we have to build TF on Arm CPU, and use following command to build:
bazel build --config=mkl_aarch64 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-25 01:30:55,bug,"""ValueError: Cannot take the length of shape with unknown rank.""  when using MultiHeadRelativeAttention","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

mac M2 pro

### Mobile device

mac M2 pro

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

when using MultiHeadRelativeAttention from official.nlp.modeling.layers, I face on this error, ""ValueError: Cannot take the length of shape with unknown rank.""  I'm sorry  I'm not good at English. Thank you!


### Standalone code to reproduce the issue

```shell
from official.nlp.modeling.layers import MultiHeadRelativeAttention
import tensorflow as tf
vec= tf.constant([[[[0.1]*4]*3]*3])
layers=MultiHeadRelativeAttention(num_heads=4,key_dim=3)
output=layers(vec,vec,content_attention_bias=0.1, positional_attention_bias=0.1)
```


### Relevant log output

```shell
ValueError: Cannot take the length of shape with unknown rank.
```
"
tensorflow/tensorflow,2023-07-24 16:51:23,bug,Unable to save model when using EfficientNetB0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I’m trying to use EfficientNetB0 to create a model and save the model to my local disk. However, when saving it, it throws the error below.

> TypeError: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.

Also, I tried to downgrade tensorflow from V2.12.0 to V2.9.1, this works as expected. In other words, this is a bug in 2.12.0. Hope it helps and please fix this bug for V2.12.0

### Standalone code to reproduce the issue

```shell
model = tf.keras.applications.EfficientNetB0()
model.save(""model"")
```


### Relevant log output

```shell
TypeError: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.
```
"
tensorflow/tensorflow,2023-07-22 07:11:34,bug,TFLite Error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 11
- TensorFlow installed from (source or binary):source 
- TensorFlow version (or github SHA if from source):2.15.0


**Provide the text output from tflite_convert**
The below is the code, I am using to convert the deep learning model to tflite

converter = tf.lite.TFLiteConverter.from_keras_model(best_model)

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with open('compressed_model.tflite', 'wb') as f:
f.write(tflite_model)


**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1QlquN0xR94xMdiUNer00Nu0n5UDXdiWQ
![error](https://github.com/tensorflow/tensorflow/assets/107172150/19056701-1422-4ab2-939a-545f3f799f48)

"
tensorflow/tensorflow,2023-07-21 18:21:26,bug,Tensorflow Load Datasets Failure for Python 3.11.4 and Tensorflow 2.13.0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Rocky Linux 8.7

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.9.0.131-1

### GPU model and memory

_No response_

### Current behavior?

Dataset written and loaded in python 3.11.4 and tensorflow 2.12.1 should load in tensorflow 2.13

However, loading dataset in Tensorflow 2.13 with python 3.11.4 fails on Linux and windows: 

TensorFlow version: 2.13.0
Python version: 3.11.4
Download dev dataset...
Extract dev dataset...

Loading dev dataset...
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 14022746025082002701
Download train dataset...
Extract train dataset...

Loading train dataset...
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 10775564831112808841

### Standalone code to reproduce the issue

```shell
import io
import sys
from zipfile import ZipFile

import requests
import tensorflow as tf

print(""TensorFlow version:"", tf.__version__)
print(""Python version:"", sys.version.split()[0])

dev_url = (
    ""https://drive.google.com/uc?export=download&id=1-MJAgrTNZkaMpyBQLIgqqwM8gP9LKdDL""
)

print(""Download dev dataset..."")
r = requests.get(dev_url)
z = ZipFile(io.BytesIO(r.content))
print(""Extract dev dataset..."")
z.extractall()
print(""\\nLoading dev dataset..."")
ds_dev = tf.data.Dataset.load(""squadv2_dev_tf"")

train_url = (
    ""https://drive.google.com/uc?export=download&id=1-NWGcJz0ZaFGFeHOPG2PKvn8gmf3MwKn""
)
print(""Download train dataset..."")
r = requests.get(train_url)
z = ZipFile(io.BytesIO(r.content))
print(""Extract train dataset..."")
z.extractall()
print(""\\nLoading train dataset..."")
ds_train = tf.data.Dataset.load(""squadv2_train_tf"")
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-19 12:46:52,bug,tensorflow/core/common_runtime/gpu/gpu_util.cc:293] GPU->CPU Memcpy failed,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.15

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda version = 10.0
cudnn=7.6.4
_No response_

### GPU model and memory
Geforce RTX 4070 TI 12 GB
_No response_

### Current behavior?

I am using 

gpu geforce rtx 4070 ti 12 gb 

i add in my training file 

    config1 = tf.compat.v1.ConfigProto()
    config1.gpu_options.allow_growth = True
    session = tf.compat.v1.Session(config=config1)

But Nothing happen I get this issue 

### Standalone code to reproduce the issue

```shell
2023-07-19 13:40:37.744821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2023-07-19 13:40:37.744955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2023-07-19 13:40:37.745047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2023-07-19 13:40:37.745166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2023-07-19 13:40:37.745290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2023-07-19 13:40:37.745374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2023-07-19 13:40:37.745493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-07-19 13:40:37.745596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2023-07-19 13:40:37.745706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-07-19 13:40:37.745789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2023-07-19 13:40:37.745866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2023-07-19 13:40:37.746009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9)
WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:300: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:308: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

2023-07-19 13:40:40.254879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-07-19 13:43:12.728939: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2023-07-19 13:43:12.932855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2023-07-19 13:43:21.066511: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
Exception: Blas GEMM launch failed : a.shape=(2, 2048), b.shape=(2, 36), m=2048, n=36, k=2
         [[node gradients_1/dense_regress_10/MatMul_grad/MatMul_1 (defined at C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]

Original stack trace for 'gradients_1/dense_regress_10/MatMul_grad/MatMul_1':
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 262, in <module>
    train_kitti()
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 205, in train_kitti
    [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py"", line 1620, in train_on_batch
    self._make_train_function()
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py"", line 1002, in _make_train_function
    self.total_loss)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py"", line 381, in get_updates
    grads = self.get_gradients(loss, params)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\optimizers.py"", line 47, in get_gradients
    grads = K.gradients(loss, params)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py"", line 2138, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py"", line 679, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py"", line 350, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py"", line 679, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py"", line 1586, in _MatMulGrad
    grad_b = gen_math_ops.mat_mul(a, grad, transpose_a=True)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py"", line 6136, in mat_mul
    name=name)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'dense_regress_10/MatMul', defined at:
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 262, in <module>
    train_kitti()
  File ""c:/Users/user/Desktop/Binarios/keras_frcnn-master-atelier-B/keras_frcnn-master/train_frcnn_kitti.py"", line 88, in train_kitti
    classifier = nn.classifier(shared_layers, roi_input, cfg.num_rois, nb_classes=len(classes_count), trainable=True)
  File ""c:\\Users\\user\\Desktop\\Binarios\\keras_frcnn-master-atelier-B\\keras_frcnn-master\\keras_frcnn\\resnet.py"", line 270, in classifier
    name='dense_regress_{}'.format(nb_classes))(out)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\topology.py"", line 578, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\layers\\wrappers.py"", line 177, in call
    y = self.layer.call(inputs)  # (num_samples * timesteps, ...)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\layers\\core.py"", line 840, in call
    output = K.dot(inputs, self.kernel)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py"", line 848, in dot
    out = tf.matmul(x, y)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py"", line 2754, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py"", line 6136, in mat_mul
    name=name)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\framework\\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

2023-07-19 13:43:21.297408: I tensorflow/stream_executor/stream.cc:1990] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not wait for [stream=0000029EFE926CC0,impl=0000029EB937CF20]
2023-07-19 13:43:21.297815: I tensorflow/stream_executor/stream.cc:4925] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not memcpy device-to-host; source: 00000007129B6500
2023-07-19 13:43:21.298246: F tensorflow/core/common_runtime/gpu/gpu_util.cc:293] GPU->CPU Memcpy failed
2023-07-19 13:43:21.298255: I tensorflow/stream_executor/stream.cc:1990] [stream=0000029EFE927EC0,impl=0000029EB937CFB0] did not wait for [stream=0000029EFE926CC0,impl=0000029EB937CF20]
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-18 01:29:12,bug,KerasTensor和tf.tensor之间的转换,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.4

### Custom code

Yes

### OS platform and distribution

Window10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

KerasTensor没有numpy（）这种，应该如何转换

### Standalone code to reproduce the issue

```shell
File ""C:/Users/KM Group/Desktop/lmx/SemanticCompression-Speech/DeepSC-S-main/random_mask_training.py"", line 73, in <module>
    sem_dec = sem_dec_model(frame_length, stride_length, args)
  File ""C:\\Users\\KM Group\\Desktop\\lmx\\SemanticCompression-Speech\\DeepSC-S-main\\model_tfnn.py"", line 191, in sem_dec_model
    _output = sem_dec(_intput, batch_mean, batch_var)
  File ""C:\\Users\\KM Group\\Desktop\\lmx\\SemanticCompression-Speech\\DeepSC-S-main\\model_tfnn.py"", line 142, in __call__
    _input = tf.convert_to_tensor(keras.backend.get_value(_input))
  File ""C:\\Users\\KM Group\\Anaconda3\\envs\\speech-SC\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py"", line 3615, in get_value
    return x.numpy()
AttributeError: 'KerasTensor' object has no attribute 'numpy'
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-17 12:19:06,bug,tensorflow keras model.predict() is not thread safe,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux CentOS 7.9

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We executed model.predict() in multi-thread. And sometimes, the code raised the exception: Functional' object has no attribute 'predict_function.

### Standalone code to reproduce the issue

```shell
def predict(self, x, tf_server=False, port=8501, model_path='', step=0):
        if tf_server:
            return self.predict_tf_server_grpc(x, port, step)
        pred = None
        try:
            if not self.model_trained:
                print('try to load model ...\\n')
                self.load_model(model_path)
                self.model_trained = True
            if step == 0:
                pred = self.model.predict(x)
            else:
                pred = self.model.predict(x, steps=step)
        except Exception as ex:
            print(ex)
        return pred
```
While the exception raised, we executed the code ""self.model.predict(x)"" in debug window again, and it returned the correct prediction results.
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-15 13:33:28,bug,`tf.image.decode_jpeg` can not decode jpeg base64 encoded image,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.1

### Custom code

Yes

### OS platform and distribution

Win11 22H2

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It raise: `InvalidArgumentError: {{function_node __wrapped__DecodeJpeg_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeJpeg]`

![image](https://github.com/tensorflow/tensorflow/assets/4510984/342f8602-a2ba-48b7-bdaa-4d0684f9299f)

![image](https://github.com/tensorflow/tensorflow/assets/4510984/6177e9d3-90ed-42c8-881f-cc18bd045f39)


### Standalone code to reproduce the issue

```shell
import base64

from PIL import Image
import tensorflow as tf

img = Image.open('xxx.jpg')
base64str = base64.b64encode(img.tobytes()).decode()
tf.image.decode_jpeg(base64str, channels=3)
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-14 22:13:11,bug,Tensor dimension mismatch when `tf.keras.Input` is used as input,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0-dev20230602

### 2. Code
This is the minimized code to reproduce the issue:
```python
import tensorflow as tf
import numpy as np
input_shape = [1, 2]
x1 = tf.keras.Input(shape=input_shape, dtype=""float32"")

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.w1 = tf.Variable([[3., 4.], [5., 6.]])
    self.b1 = tf.Variable([7., 8.])
  @tf.function(input_signature=[tf.TensorSpec(x1.shape, x1.dtype)])
  def call(self, x1):
    return tf.matmul(x1, self.w1) + self.b1

m = Model()
converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    print(f'Keras input shape: {input_data[0].shape}') # print keras input shape
    print(f'Lite input shape: {input_details[0][""shape""]}') # print lite input shape
    
    for i in range(len(input_data)):
        interpreter.set_tensor(input_details[i]['index'], input_data[i])
    interpreter.invoke()
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data

x = tf.constant([1., 2.], shape=input_shape)
actual_value = _evaluateTFLiteModel(tflite_model,[x])
```
### 3. Failure after conversion
Output
```
Keras input shape: (1, 2)
Lite input shape: [1 1 2]
```
Error Message:
```
ValueError: Cannot set tensor: Dimension mismatch. Got 2 but expected 3 for input 0.
```
"
tensorflow/tensorflow,2023-07-13 13:03:01,bug,TFlite running interpreter->invoke() has failed - Segmentation fault,"In TFLite, I wrote a custom delegate in C++ and encountered an error: ""Segmentation fault"". This error occurs after the initialization is complete and specifically after the invocation of interpreter->invoke(). The custom delegate's prepare function is executed, but the eva function is not executed."
tensorflow/tensorflow,2023-07-13 03:41:33,bug,TypeError: _lookup_dependency() takes 2 positional arguments but 3 were given,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-dev20230712

### Custom code

Yes

### OS platform and distribution

Linux moe 5.10.0-12-amd64 #1 SMP Debian 5.10.103-1 (2022-03-07) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Can't load saved model

### Standalone code to reproduce the issue

```shell
Saved models can't be loaded:


model = tf.keras.models.Sequential([tf.keras.layers.Input((256,256,3)), tf.keras.layers.Dense(1)])
model.save(""../models/test"")
model = tf.keras.models.load_model(""../models/test/"")
```

same thing with more complex models

```
model = tf.keras.applications.efficientnet.EfficientNetB0()
model.save(""../models/test"")
model = tf.keras.models.load_model(""../models/test/"")
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[11], line 1
----> 1 model = tf.keras.models.load_model(""../models/test/"")

File ~/.local/lib/python3.9/site-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)
    254     return saving_lib.load_model(
    255         filepath,
    256         custom_objects=custom_objects,
    257         compile=compile,
    258         safe_mode=safe_mode,
    259     )
    261 # Legacy case.
--> 262 return legacy_sm_saving_lib.load_model(
    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs
    264 )

File ~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/.local/lib/python3.9/site-packages/tensorflow/python/checkpoint/restore.py:606, in _queue_children_for_restoration(checkpoint_position, visit_queue)
    604   continue
    605 child_position = checkpoint_position.create_child_position(child.node_id)
--> 606 local_object = trackable._lookup_dependency(child.local_name,
    607                                             trackable_children)
    608 child_proto = child_position.object_proto
    609 if local_object is None:
    610   # We don't yet have a dependency registered with this name. Save it
    611   # in case we do.

TypeError: _lookup_dependency() takes 2 positional arguments but 3 were given
```
"
tensorflow/tensorflow,2023-07-12 05:48:51,bug,ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.0.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

nvcc --version gives 10.1 nvidia-smi gives CUDA Version: 12.2 

### GPU model and memory

_No response_

### Current behavior?

ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory

### Standalone code to reproduce the issue

```shell
This error occured when I imported tensorflow
import tensorflow as tf

To resolve this issue I have set my $CUDA_HOME=/usr/lib/cuda/
and $LD_LIBRARY_PATH=usr/lib/cuda/lib64

But surprisingly usr/lib/cuda/lib64 is empty. I dont have cuda folder in usr/local directory.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/nkaushal/anaconda3/envs/lipnet3.6/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
tensorflow/tensorflow,2023-07-11 08:09:33,bug,"Different reference order may cause other modules to be unavailable, e.g. xgboost, sklearn.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

centos 7.6

### Mobile device

_No response_

### Python version

python 3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda=12.1

### GPU model and memory

32510MiB

### Current behavior?

I was trying to import xgboost(or sklearn) and tensorflow modules at same time, but when I  imported modules by different order, it just return me error message that I can not handle it, I don't whether it a bug or some issues that can be fixed by myself?  and I also search something resource, which said that it was a bug caused by glibc : https://sourceware.org/bugzilla/show_bug.cgi?id=17090. and then I was trying to reintstall glibc on my server, unfortunately, the plan finally failed and now I am just trying to rebuild my whole environment by rollbacking to previous mirror backup, sad.

### Standalone code to reproduce the issue

```shell
import numpy as np

## bad import order
import tensorflow as tf
from xgboost import XGBClassifier ## order # or import sklearn, may report different error messages.

## good import order
# from xgboost import XGBClassifier
# import tensorflow as tf

hparams = {
    'booster':'gbtree',
    'objective': 'binary:logistic',
    'eval_metric': 'aucpr',
    'max_depth': 10,
    'gamma': 4,
    'lambda':0.001,
    'subsample':0.7,
    'colsample_bytree':0.8,
    'colsample_bylevel':0.8,
    'colsample_bynode': 0.8,
    'min_child_weight':20,
    'eta': 0.03,
    'seed': 42,
    'nthread':15,
    'tree_method':'gpu_hist',
    'n_estimators': 350
}
estimator = XGBClassifier(**hparams)
X_train = np.random.rand(10000, 10)
y_train = np.random.randint(0, 2, (10000, 1))
X_eval = np.random.rand(1000, 10)
y_eval = np.random.randint(0, 2, (1000, 1))

estimator.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_eval, y_eval)])
```


### Relevant log output

```shell
2023-07-11 15:57:01.594620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/haojiaxiang/projects/test/test.py"", line 6, in <module>
    from xgboost import XGBClassifier ## order # or import sklearn, may report different error messages.
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/__init__.py"", line 7, in <module>
    from . import collective, dask, rabit
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/collective.py"", line 12, in <module>
    from .core import _LIB, _check_call, c_str, py_str, from_pystr_to_cstr
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/core.py"", line 264, in <module>
    _LIB = _load_lib()
  File ""/home/haojiaxiang/miniconda3/envs/gms/lib/python3.10/site-packages/xgboost/core.py"", line 216, in _load_lib
    raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (libxgboost.so) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

Error message(s): ['dlopen: cannot load any more object with static TLS']
```
"
tensorflow/tensorflow,2023-07-10 18:41:30,bug,Memory out of bounds in compiled tflite with emscripten.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have compiled tflite using cmake (without XNNPACK support) and emscripten (both latest 3.1.42 and 3.1.10). 
When trying to perform inference at the browser with my model I get the following error:

vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at img.onload (index.html:772:28)

This happens with all of my models at the very first operation (pad). When inspecting the .wasm file using chrome dev tools I see that the error happens at a ""memory.fill"" operation.

### Standalone code to reproduce the issue

```shell
I have compiled tflite with the following emcmake command:

cmake -DCMAKE_CXX_FLAGS=""-lpthread -pthread -lpthread -s USE_PTHREADS"" -DTFLITE_ENABLE_MMAP=OFF -DTFLITE_ENABLE_NNAPI=OFF -DTFLITE_ENABLE_RUY=ON -DTFLITE_ENABLE_XNNPACK=OFF ..

while when compiling my project with emscripten (including the above resulting libraries) I use the following flags:

	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s INITIAL_MEMORY=512MB"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_MEMORY_GROWTH=1"")
	SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -s ALLOW_TABLE_GROWTH=1"")
```


### Relevant log output

```shell
This is the output of PrintInterpreterState right before the first inference.

[WASM] === Pre-invoke Interpreter State ===
pre-vmt.js:11 [WASM] Interpreter has 1 subgraphs.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] -----------Subgraph-0 has 134 tensors and 49 nodes------------
pre-vmt.js:11 [WASM] 1 Inputs: [0] -> 602112B (0.57MB)
pre-vmt.js:11 [WASM] 1 Outputs: [122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Tensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  
pre-vmt.js:11 [WASM] Tensor   0  ��ʻ䯻9:󺂶*򨓮.. kTfLiteFloat32  kTfLiteArenaRw     602112   / 0.57 [1,224,224,3] [0, 602112)
pre-vmt.js:11 [WASM] Tensor   1 騅:��񛻺󿰻��m... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690960, 691024)
pre-vmt.js:11 [WASM] Tensor   2 r畼��匹��t;��... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [690864, 690928)
pre-vmt.js:11 [WASM] Tensor   3 jԐ;��4i;⦄;Q;箮. kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690688, 690848)
pre-vmt.js:11 [WASM] Tensor   4 究򑃷çc6ԯ#6ߗ<׹... kTfLiteFloat32  kTfLiteMmapRo      160      / 0.00 [40] [690512, 690672)
pre-vmt.js:11 [WASM] Tensor   5 :#��
𯫿��7tU��... kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690272, 690496)
pre-vmt.js:11 [WASM] Tensor   6 ƙl��7򣡷/򷕈4��.. kTfLiteFloat32  kTfLiteMmapRo      224      / 0.00 [56] [690032, 690256)
pre-vmt.js:11 [WASM] Tensor   7 ����뻪��3z}��.. kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689760, 690016)
pre-vmt.js:11 [WASM] Tensor   8 ᡁ������땐6ԝ... kTfLiteFloat32  kTfLiteMmapRo      256      / 0.00 [64] [689488, 689744)
pre-vmt.js:11 [WASM] Tensor   9 􌶄��5.8^L𷆄򷳱... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688896, 689472)
pre-vmt.js:11 [WASM] Tensor  10 ��""ۀ7��Ce����... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [688304, 688880)
pre-vmt.js:11 [WASM] Tensor  11 ģ
pre-vmt.js:11 [WASM] 7C^J𐬣𶓃6񪤷׾... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687712, 688288)
pre-vmt.js:11 [WASM] Tensor  12 ��ȏ6띓����8h... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [687120, 687696)
pre-vmt.js:11 [WASM] Tensor  13 ��A򪶚윶��䞏��.. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686816, 687104)
pre-vmt.js:11 [WASM] Tensor  14 ԙõ`򫷾F6򓕶O򙷳c... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686512, 686800)
pre-vmt.js:11 [WASM] Tensor  15 ū쵄񉸻罷{\\ѵfW𶯜... kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [686208, 686496)
pre-vmt.js:11 [WASM] Tensor  16 &ꋷ𷏸ᛸ6��ꮮ. kTfLiteFloat32  kTfLiteMmapRo      288      / 0.00 [72] [685904, 686192)
pre-vmt.js:11 [WASM] Tensor  17 |㗶񏍷򛀷��\\Y7... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [685312, 685888)
pre-vmt.js:11 [WASM] Tensor  18 蕷ΒU7��`&÷勸f+... kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [144] [684720, 685296)
pre-vmt.js:11 [WASM] Tensor  19 w춃֣74ٿ𣔝����.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [683552, 684704)
pre-vmt.js:11 [WASM] Tensor  20 ;""6!򑶷膷ڽ𷞮��... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [682384, 683536)
pre-vmt.js:11 [WASM] Tensor  21 󠼷YPQ������75... kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [681216, 682368)
pre-vmt.js:11 [WASM] Tensor  22 ��󰳳IB}������.. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [680048, 681200)
pre-vmt.js:11 [WASM] Tensor  23 򳤷񲂷wް5��^	8􊮮. kTfLiteFloat32  kTfLiteMmapRo      1152     / 0.00 [288] [678880, 680032)
pre-vmt.js:11 [WASM] Tensor  24 _��m󷟝򵳂󶬾𼾮.. kTfLiteFloat32  kTfLiteMmapRo      32       / 0.00 [8] [678832, 678864)
pre-vmt.js:11 [WASM] Tensor  25 P��&;𶿮����7#׮.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678752, 678816)
pre-vmt.js:11 [WASM] Tensor  26 ��5񺉷a꨷ᘑ��.. kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [678672, 678736)
pre-vmt.js:11 [WASM] Tensor  27 卞7󯉷򨮷��󠚷��. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678560, 678656)
pre-vmt.js:11 [WASM] Tensor  28 FR񷀃󷴫d������.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678448, 678544)
pre-vmt.js:11 [WASM] Tensor  29 򺜵W7𽒷񪶷̒#��   kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678336, 678432)
pre-vmt.js:11 [WASM] Tensor  30 $뀷��R𷸙u쭌7󿮮. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678224, 678320)
pre-vmt.js:11 [WASM] Tensor  31 \\~��ط$i򷮼
6҄*觮.. kTfLiteFloat32  kTfLiteMmapRo      96       / 0.00 [24] [678112, 678208)
pre-vmt.js:11 [WASM] Tensor  32 ��0��涂񖷝f5��. kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677904, 678096)
pre-vmt.js:11 [WASM] Tensor  33 ؉h������7˜𶠺... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677696, 677888)
pre-vmt.js:11 [WASM] Tensor  34 󚫷\\7͛$♩��7[... kTfLiteFloat32  kTfLiteMmapRo      192      / 0.00 [48] [677488, 677680)
pre-vmt.js:11 [WASM] Tensor  35 9X����������.. kTfLiteFloat32  kTfLiteMmapRo      1728     / 0.00 [16,3,3,3] [675744, 677472)
pre-vmt.js:11 [WASM] Tensor  36 jť<箯󧬍󘴙;Љ&󿈮.. kTfLiteFloat32  kTfLiteMmapRo      576      / 0.00 [1,3,3,16] [675152, 675728)
pre-vmt.js:11 [WASM] Tensor  37 􅻻λ""<����?<ɓ... kTfLiteFloat32  kTfLiteMmapRo      512      / 0.00 [8,1,1,16] [674624, 675136)
pre-vmt.js:11 [WASM] Tensor  38 ޛȻx׃<򁻝ě<u+��... kTfLiteFloat32  kTfLiteMmapRo      1280     / 0.00 [40,1,1,8] [673328, 674608)
pre-vmt.js:11 [WASM] Tensor  39 ��Ԍ󼲍׹󵏼Ϩ��... kTfLiteFloat32  kTfLiteMmapRo      1440     / 0.00 [1,3,3,40] [671872, 673312)
pre-vmt.js:11 [WASM] Tensor  40 ?ȑ򄏱;3d1󞻳��       kTfLiteFloat32  kTfLiteMmapRo      2560     / 0.00 [16,1,1,40] [669296, 671856)
pre-vmt.js:11 [WASM] Tensor  41 Cһ;��ٻ:뼩攻ŷ... kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [56,1,1,16] [665696, 669280)
pre-vmt.js:11 [WASM] Tensor  42 3������򥅡;b... kTfLiteFloat32  kTfLiteMmapRo      2016     / 0.00 [1,3,3,56] [663664, 665680)
pre-vmt.js:11 [WASM] Tensor  43 Mٚ<啤<j��8|��쮮. kTfLiteFloat32  kTfLiteMmapRo      3584     / 0.00 [16,1,1,56] [660064, 663648)
pre-vmt.js:11 [WASM] Tensor  44 ᦺ��򦖶:晼4櫻k... kTfLiteFloat32  kTfLiteMmapRo      4096     / 0.00 [64,1,1,16] [655952, 660048)
pre-vmt.js:11 [WASM] Tensor  45 轼\\<j4<Ӯ:𡻻ug... kTfLiteFloat32  kTfLiteMmapRo      2304     / 0.00 [1,3,3,64] [653632, 655936)
pre-vmt.js:11 [WASM] Tensor  46 ��৐��󶜿󄲢𠧮.. kTfLiteFloat32  kTfLiteMmapRo      6144     / 0.01 [24,1,1,64] [647472, 653616)
pre-vmt.js:11 [WASM] Tensor  47 ��񒘼Ѡj󫈞󭲐<�� kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [633632, 647456)
pre-vmt.js:11 [WASM] Tensor  48 '����󼰛��󋍮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [628432, 633616)
pre-vmt.js:11 [WASM] Tensor  49 `��𼢈<񗋻          kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [614592, 628416)
pre-vmt.js:11 [WASM] Tensor  50 ا""󖗽򫋺<󀮹͑x󪁮.. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [600752, 614576)
pre-vmt.js:11 [WASM] Tensor  51 ȷk󒏷:󁌝��󦦮.. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [595552, 600736)
pre-vmt.js:11 [WASM] Tensor  52 t=򜧨;6򌼾)𻀫��... kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [24,1,1,144] [581712, 595536)
pre-vmt.js:11 [WASM] Tensor  53 򚪼f񒻯��=弯P... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [574784, 581696)
pre-vmt.js:11 [WASM] Tensor  54 (����nۻ:,��󛥮.. kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [572176, 574768)
pre-vmt.js:11 [WASM] Tensor  55 ��뙿��󭩄;��?
pre-vmt.js:11 [WASM] 􄠫TfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [565248, 572160)
pre-vmt.js:11 [WASM] Tensor  56 ��̘<9D򆼇*t<'��... kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [72,1,1,24] [558320, 565232)
pre-vmt.js:11 [WASM] Tensor  57 𳼑sD󠺬;󋽦􊼠R... kTfLiteFloat32  kTfLiteMmapRo      2592     / 0.00 [1,3,3,72] [555712, 558304)
pre-vmt.js:11 [WASM] Tensor  58 mX��h:%4k<̫'<(ç<񝮮. kTfLiteFloat32  kTfLiteMmapRo      6912     / 0.01 [24,1,1,72] [548784, 555696)
pre-vmt.js:11 [WASM] Tensor  59 o󻛄񷖡��<𽤼񁮮. kTfLiteFloat32  kTfLiteMmapRo      13824    / 0.01 [144,1,1,24] [534944, 548768)
pre-vmt.js:11 [WASM] Tensor  60 Ⱥּ󏉼򘄼]��d_;�. kTfLiteFloat32  kTfLiteMmapRo      5184     / 0.00 [1,3,3,144] [529744, 534928)
pre-vmt.js:11 [WASM] Tensor  61 ֭C򢊛;󥙻怙<貦��.. kTfLiteFloat32  kTfLiteMmapRo      27648    / 0.03 [48,1,1,144] [502080, 529728)
pre-vmt.js:11 [WASM] Tensor  62 ��􊥺*?""��󸸔��.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [446768, 502064)
pre-vmt.js:11 [WASM] Tensor  63 򶼼1n⻗��-Ի񛹪�� kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [436384, 446752)
pre-vmt.js:11 [WASM] Tensor  64 ��ֻ𲰼􅛻o𣼣�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [381072, 436368)
pre-vmt.js:11 [WASM] Tensor  65 V;潛򠛮<ʻ㻅S;򮮮 kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [325760, 381056)
pre-vmt.js:11 [WASM] Tensor  66 ¿,<
pre-vmt.js:11 [WASM] 񏻥E����ݻE񮮮 kTfLiteFloat32  kTfLiteMmapRo      10368    / 0.01 [1,3,3,288] [315376, 325744)
pre-vmt.js:11 [WASM] Tensor  67 dճ<󋺑񢼍����Ү.. kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [48,1,1,288] [260064, 315360)
pre-vmt.js:11 [WASM] Tensor  68 ����𣺼d򻍪~;�� kTfLiteFloat32  kTfLiteMmapRo      55296    / 0.05 [288,1,1,48] [204752, 260048)
pre-vmt.js:11 [WASM] Tensor  69 冻񍖼𣄼𲿼󲎽��. kTfLiteInt32    kTfLiteMmapRo      8        / 0.00 [2] [204720, 204728)
pre-vmt.js:11 [WASM] Tensor  70 ~&��<��`<ݺNϘ;𫮮. kTfLiteInt32    kTfLiteMmapRo      12       / 0.00 [3] [204688, 204700)
pre-vmt.js:11 [WASM] Tensor  71 ��%ɻ	`޻󴵹ϛ;t񮮮 kTfLiteFloat32  kTfLiteMmapRo      708      / 0.00 [177] [203968, 204676)
pre-vmt.js:11 [WASM] Tensor  72 <񌹂L��;ԅ򻬰A󾫮.. kTfLiteInt32    kTfLiteMmapRo      32       / 0.00 [4,2] [203920, 203952)
pre-vmt.js:11 [WASM] Tensor  73 Ё򻙃ٻ:
ԺV۪;��
pre-vmt.js:11 [WASM] ;Ŗ... kTfLiteFloat32  kTfLiteMmapRo      203904   / 0.19 [177,288] [0, 203904)
pre-vmt.js:11 [WASM] Tensor  74 2
pre-vmt.js:11 [WASM] 纝S
󊺅򚋟��<... kTfLiteFloat32  kTfLiteArenaRw     612912   / 0.58 [1,226,226,3] [2759680, 3372592)
pre-vmt.js:11 [WASM] Tensor  75 ⻒����S󁟮󼘮.. kTfLiteFloat32  kTfLiteArenaRw     802816   / 0.77 [1,112,112,16] [1956864, 2759680)
pre-vmt.js:11 [WASM] Tensor  76 z򛼷x0<ʬ𠻗��߮.. kTfLiteFloat32  kTfLiteArenaRw     831744   / 0.79 [1,114,114,16] [602112, 1433856)
pre-vmt.js:11 [WASM] Tensor  77 󥺗��Ї􆛁����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,56,56,16] [1433856, 1634560)
pre-vmt.js:11 [WASM] Tensor  78 ½ẇ                     kTfLiteFloat32  kTfLiteArenaRw     100352   / 0.10 [1,56,56,8] [602112, 702464)
pre-vmt.js:11 [WASM] Tensor  79 𖒽񅬽Ɓּ򽮨=ڝ... kTfLiteFloat32  kTfLiteArenaRw     501760   / 0.48 [1,56,56,40] [1140352, 1642112)
pre-vmt.js:11 [WASM] Tensor  80 󦜼󅯻牪=𞁼{Jk<󯮮. kTfLiteFloat32  kTfLiteArenaRw     538240   / 0.51 [1,58,58,40] [602112, 1140352)
pre-vmt.js:11 [WASM] Tensor  81 )a˼q󀀀
pre-vmt.js:11 [WASM] ������<蚮.. kTfLiteFloat32  kTfLiteArenaRw     125440   / 0.12 [1,28,28,40] [1140352, 1265792)
pre-vmt.js:11 [WASM] Tensor  82 ⰾ󝔦<񃼄h��8��.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [953344, 1003520)
pre-vmt.js:11 [WASM] Tensor  83 ��į<򑁼Q󬽴߼6)... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [602112, 777728)
pre-vmt.js:11 [WASM] Tensor  84 ��s��񌽌𱽬��*... kTfLiteFloat32  kTfLiteArenaRw     175616   / 0.17 [1,28,28,56] [777728, 953344)
pre-vmt.js:11 [WASM] Tensor  85 ��La޻��4<zL󼚨... kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [602112, 652288)
pre-vmt.js:11 [WASM] Tensor  86 <򔼌��󰽊V-��<��. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,28,28,16] [652288, 702464)
pre-vmt.js:11 [WASM] Tensor  87 DZQ��󿒇<;zZ����.. kTfLiteFloat32  kTfLiteArenaRw     200704   / 0.19 [1,28,28,64] [832512, 1033216)
pre-vmt.js:11 [WASM] Tensor  88 ^��j􌧁;E����... kTfLiteFloat32  kTfLiteArenaRw     230400   / 0.22 [1,30,30,64] [602112, 832512)
pre-vmt.js:11 [WASM] Tensor  89 ��󛶼+)������.. kTfLiteFloat32  kTfLiteArenaRw     50176    / 0.05 [1,14,14,64] [832512, 882688)
pre-vmt.js:11 [WASM] Tensor  90 ༟<cdͻYϑ������.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [882688, 901504)
pre-vmt.js:11 [WASM] Tensor  91 ;��༕󗼫}<~_.��.. kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  92 덒;T@ë<ˠ��v<5�� kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  93 򣀼򧂼4񥻬SN����.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  94 ��6𢺊$@<󶢺T>ֺ舮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [846720, 865536)
pre-vmt.js:11 [WASM] Tensor  95 i粼**&<ļ򅫈;B򻤺... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [602112, 715008)
pre-vmt.js:11 [WASM] Tensor  96 S{.ZR����<9��1... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [715008, 827904)
pre-vmt.js:11 [WASM] Tensor  97 Pw􀗃􈨉��㢥󧂮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [827904, 846720)
pre-vmt.js:11 [WASM] Tensor  98 ߴüs��9󢠠           kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor  99 t#򼇟񼻺<􊚻󣫚򡋮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 100 ��ݿ������9\\r... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 101 ��ڛ��𼮲��N􏕮.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 102 򝷺iܨ<<<򾃆;󵮮. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [733824, 752640)
pre-vmt.js:11 [WASM] Tensor 103 ��hԂ<o񉼿hἑݎ<f�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 104 󻦨𼯄[<��W~J󗩮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,14,14,72] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 105 0»Lv<ǡF򟻸;jV��... kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [715008, 733824)
pre-vmt.js:11 [WASM] Tensor 106 s	��W򈤋񊗺9fZ��.. kTfLiteFloat32  kTfLiteArenaRw     18816    / 0.02 [1,14,14,24] [602112, 620928)
pre-vmt.js:11 [WASM] Tensor 107 wŔ;􃋼��~<��<ڷ... kTfLiteFloat32  kTfLiteArenaRw     112896   / 0.11 [1,14,14,144] [749568, 862464)
pre-vmt.js:11 [WASM] Tensor 108 KԽ;㚹֛!;񬻠         kTfLiteFloat32  kTfLiteArenaRw     147456   / 0.14 [1,16,16,144] [602112, 749568)
pre-vmt.js:11 [WASM] Tensor 109 𜭼艺<ﻨ9刼;
pre-vmt.js:11 [WASM] *<􉮮. kTfLiteFloat32  kTfLiteArenaRw     28224    / 0.03 [1,7,7,144] [749568, 777792)
pre-vmt.js:11 [WASM] Tensor 110 0��ĺ;ۚ<��ϭܻI󮮮 kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 111 sl$􀍋;OQֺ<��𻺔ޮ.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 112 2\\𼀄��;ဥ;��Ȉ... kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 113 󂆅��􎙧<񶧻񍮮. kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [602112, 611520)
pre-vmt.js:11 [WASM] Tensor 114 ꥩ8^񑻢����         kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [724416, 733824)
pre-vmt.js:11 [WASM] Tensor 115 𛁺����[񁺼��߮.. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 116 /ӎ􀀀
pre-vmt.js:11 [WASM] 껵򀼏��B�� kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [658560, 715008)
pre-vmt.js:11 [WASM] Tensor 117 ϣ󺕠����;f򜺹
pre-vmt.js:11 [WASM] ... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [715008, 724416)
pre-vmt.js:11 [WASM] Tensor 118 ᳠:$p溓l��㻂	𺩯... kTfLiteFloat32  kTfLiteArenaRw     9408     / 0.01 [1,7,7,48] [658560, 667968)
pre-vmt.js:11 [WASM] Tensor 119 ��򭶹��Cျꬓ:��. kTfLiteFloat32  kTfLiteArenaRw     56448    / 0.05 [1,7,7,288] [602112, 658560)
pre-vmt.js:11 [WASM] Tensor 120 SW��Y��;��e��... kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [1,1,1,288] [659712, 660864)
pre-vmt.js:11 [WASM] Tensor 121 ��Ę;ᑩ򘯱󨞇󎓮.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,177] [602112, 602820)
pre-vmt.js:11 [WASM] Tensor 122 긟𔎴;ڗs񡮠����.. kTfLiteFloat32  kTfLiteArenaRw     708      / 0.00 [1,59,3] [602880, 603588)
pre-vmt.js:11 [WASM] Tensor 123 (nil)                     kTfLiteInt32    kTfLiteArenaRw     16       / 0.00 [4] [660864, 660880)
pre-vmt.js:11 [WASM] Tensor 124 (nil)                     kTfLiteInt32    kTfLiteArenaRw     8        / 0.00 [2] [660928, 660936)
pre-vmt.js:11 [WASM] Tensor 125 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1152     / 0.00 [288] [658560, 659712)
pre-vmt.js:11 [WASM] Tensor 126 (nil)                     kTfLiteInt32    kTfLiteDynamic     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 127 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 128 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 129 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 130 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 131 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 132 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)
pre-vmt.js:11 [WASM] Tensor 133 (nil)                     kTfLiteFloat32  kTfLiteArenaRw     1354752  / 1.29 [1,112,112,27] [602112, 1956864)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRw Info: 
pre-vmt.js:11 [WASM] Tensor 133 has the max size 1354752 bytes (1.292 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x15e33b0, 0x12abd80), taking 3372592 bytes (3.216 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 0 -> 133 -> 75 -> 74.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteArenaRwPersistent Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteMmapRo Info: 
pre-vmt.js:11 [WASM] Tensor 73 has the max size 203904 bytes (0.194 MB).
pre-vmt.js:11 [WASM] This memory arena is estimated as[0x10dde70, 0x1035320), taking 691024 bytes (0.659 MB).
pre-vmt.js:11 [WASM] One possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:
pre-vmt.js:11 [WASM] Tensor 73 -> 72 -> 71 -> 70 -> 69 -> 68 -> 67 -> 66 -> 65 -> 64 -> 63 -> 62 -> 61 -> 60 -> 59 -> 58 -> 57 -> 56 -> 55 -> 54 -> 53 -> 52 -> 51 -> 50 -> 49 -> 48 -> 47 -> 46 -> 45 -> 44 -> 43 -> 42 -> 41 -> 40 -> 39 -> 38 -> 37 -> 36 -> 35 -> 34 -> 33 -> 32 -> 31 -> 30 -> 29 -> 28 -> 27 -> 26 -> 25 -> 24 -> 23 -> 22 -> 21 -> 20 -> 19 -> 18 -> 17 -> 16 -> 15 -> 14 -> 13 -> 12 -> 11 -> 10 -> 9 -> 8 -> 7 -> 6 -> 5 -> 4 -> 3 -> 2 -> 1.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] kTfLiteDynamic Info: not holding any allocation.
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Node   0 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[0,72] -> 602144B (0.57MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[74] -> 612912B (0.58MB)
pre-vmt.js:11 [WASM] Node   1 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[74,35,1] -> 614704B (0.59MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[75] -> 802816B (0.77MB)
pre-vmt.js:11 [WASM]   1 Temporary Tensors:[133] -> 1354752B (1.29MB)
pre-vmt.js:11 [WASM] Node   2 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[75,72] -> 802848B (0.77MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[76] -> 831744B (0.79MB)
pre-vmt.js:11 [WASM] Node   3 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[76,36,2] -> 832384B (0.79MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[77] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node   4 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[77,37,24] -> 201248B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[78] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM] Node   5 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[78,38,3] -> 101792B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[79] -> 501760B (0.48MB)
pre-vmt.js:11 [WASM] Node   6 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[79,72] -> 501792B (0.48MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[80] -> 538240B (0.51MB)
pre-vmt.js:11 [WASM] Node   7 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[80,39,4] -> 539840B (0.51MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[81] -> 125440B (0.12MB)
pre-vmt.js:11 [WASM] Node   8 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[81,40,25] -> 128064B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[82] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node   9 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[82,41,5] -> 53984B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[83] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  10 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[83,42,6] -> 177856B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[84] -> 175616B (0.17MB)
pre-vmt.js:11 [WASM] Node  11 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[84,43,26] -> 179264B (0.17MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[85] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  12 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[85,82] -> 100352B (0.10MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[86] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  13 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[86,44,7] -> 54528B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[87] -> 200704B (0.19MB)
pre-vmt.js:11 [WASM] Node  14 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[87,72] -> 200736B (0.19MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[88] -> 230400B (0.22MB)
pre-vmt.js:11 [WASM] Node  15 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[88,45,8] -> 232960B (0.22MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[89] -> 50176B (0.05MB)
pre-vmt.js:11 [WASM] Node  16 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[89,46,27] -> 56416B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[90] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  17 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[90,47,9] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[91] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  18 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[91,48,10] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[92] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  19 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[92,49,28] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[93] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  20 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[93,90] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[94] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  21 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[94,50,11] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[95] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  22 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[95,51,12] -> 118656B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[96] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  23 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[96,52,29] -> 126816B (0.12MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[97] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  24 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[97,94] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[98] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  25 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[98,53,13] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[99] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  26 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[99,54,14] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[100] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  27 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[100,55,30] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[101] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  28 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[101,98] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[102] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  29 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[102,56,15] -> 26016B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[103] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  30 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[103,57,16] -> 59328B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[104] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  31 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[104,58,31] -> 63456B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[105] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  32 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[105,102] -> 37632B (0.04MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[106] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM] Node  33 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[106,59,17] -> 33216B (0.03MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[107] -> 112896B (0.11MB)
pre-vmt.js:11 [WASM] Node  34 Operator Builtin Code  34 PAD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[107,72] -> 112928B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[108] -> 147456B (0.14MB)
pre-vmt.js:11 [WASM] Node  35 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[108,60,18] -> 153216B (0.15MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[109] -> 28224B (0.03MB)
pre-vmt.js:11 [WASM] Node  36 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[109,61,32] -> 56064B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[110] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  37 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[110,62,19] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[111] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  38 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[111,63,20] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[112] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  39 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[112,64,33] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[113] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  40 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[113,110] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[114] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  41 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[114,65,21] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[115] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  42 Operator Builtin Code   4 DEPTHWISE_CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[115,66,22] -> 67968B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[116] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  43 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[116,67,34] -> 111936B (0.11MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[117] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  44 Operator Builtin Code   0 ADD (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[117,114] -> 18816B (0.02MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[118] -> 9408B (0.01MB)
pre-vmt.js:11 [WASM] Node  45 Operator Builtin Code   3 CONV_2D (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[118,68,23] -> 65856B (0.06MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[119] -> 56448B (0.05MB)
pre-vmt.js:11 [WASM] Node  46 Operator Builtin Code  40 MEAN (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[119,69] -> 56456B (0.05MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[120] -> 1152B (0.00MB)
pre-vmt.js:11 [WASM]   4 Temporary Tensors:[123-126] -> 1176B (0.00MB)
pre-vmt.js:11 [WASM] Node  47 Operator Builtin Code   9 FULLY_CONNECTED (not delegated)
pre-vmt.js:11 [WASM]   3 Input Tensors:[120,73,71] -> 205764B (0.20MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[121] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] Node  48 Operator Builtin Code  22 RESHAPE (not delegated)
pre-vmt.js:11 [WASM]   2 Input Tensors:[121,70] -> 720B (0.00MB)
pre-vmt.js:11 [WASM]   1 Output Tensors:[122] -> 708B (0.00MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Execution plan as the list of 49 nodes invoked in-order: [0-48]
pre-vmt.js:11 [WASM] --------------Subgraph-0 dump has completed--------------
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] --------------Memory Arena Status Start--------------
pre-vmt.js:11 [WASM] Total memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total arena memory usage: 3372848 bytes (3.217 MB)
pre-vmt.js:11 [WASM] - Total dynamic memory usage: 0 bytes (0.000 MB)
pre-vmt.js:11 [WASM] 
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Normal)        3372720 (100.00%)
pre-vmt.js:11 [WASM] Subgraph#0   Arena (Persistent)        128 (0.00%)
pre-vmt.js:11 [WASM] --------------Memory Arena Status End--------------


vmt.wasm:0x31cff Uncaught RuntimeError: memory access out of bounds
    at vmt.wasm:0x31cff
    at vmt.wasm:0x1f7a94
    at vmt.wasm:0x3c4910
    at vmt.wasm:0x65ace
    at vmt.wasm:0x231c3e
    at vmt.wasm:0x458a49
    at vmt.wasm:0x517c60
    at Module._landmarkDetection (vmt.js:6100:85)
    at VmtHelper.cycleForSingleImage (vmt-helper.js:115:29)
    at img.onload (index.html:772:28)
```
"
tensorflow/tensorflow,2023-07-10 09:15:00,bug,ctc_ops.py deprecation warning,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.

### Standalone code to reproduce the issue

```shell
I suspect any call to tf.nn.ctc_loss
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-08 04:32:10,bug,"savedmodel convert to tflite and merge labels.  tx to new tflite model,but resulte is error by Xcode,  use python api is correct","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

mac os 12.6

### Mobile device

ios 16.1

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

convert savedmodel to tflite in ios is error

model download  url  https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5

convert steps

1.
<img width=""996"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/1a8312fc-7d45-4e6e-97c2-d5ef02979082"">

2.
<img width=""530"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/5116798e-cb0d-40f5-b937-f964bb15e284"">

script is use  offical
metadata_writer_for_image_classifier.py


use convert savedmodel to convert tflite （merge  labels.txt and  tflite）

python api  result  is correct
<img width=""569"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/6b4ebb76-99a9-4d8f-8bcf-e87192bf80a8"">

ios is error  （self-converted）

<img width=""925"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/0bfe3490-c3bf-4a49-9696-1b3be7363e30"">

ios is correct （download tflite is correct ）

<img width=""796"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/c974e3d1-6582-4818-b50e-ec62548fc7e5"">




### Standalone code to reproduce the issue

```shell
ios code：

<img width=""1075"" alt=""image"" src=""https://github.com/tensorflow/tensorflow/assets/7971897/c3618d30-3dae-4a85-a3d2-dcf0f340afe3"">


python code：


def classify_image_tflite_no_sin(model_path, predicted_image_path, labels_path):


    TF_MODEL_FILE_PATH = model_path
    interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)
    interpreter.allocate_tensors()
    
    # 加载标签文件
    with open(labels_path, 'r') as f:
        labels = f.read().splitlines()

    # 读取和预处理图像
    image_path = predicted_image_path
    image = Image.open(image_path).resize((224, 224))  # 调整图像大小
    image = np.array(image)  # 将图像转换为NumPy数组
    image = image / 255.0  # 归一化图像
    image = np.expand_dims(image, axis=0).astype(np.float32)  # 添加批次维度并转换为float32

    # 设置模型输入和输出张量
    input_tensor_index = interpreter.get_input_details()[0]['index']
    output_tensor_index = interpreter.get_output_details()[0]['index']

    # 设置输入张量的值
    interpreter.set_tensor(input_tensor_index, image)

    # 执行推断
    interpreter.invoke()

    # 获取输出张量的结果
    output_data = interpreter.get_tensor(output_tensor_index)
    
    score_lite = tf.nn.softmax(output_data)

    # 获取预测类别索引
    class_index = np.argmax(score_lite)
    predicted_label = labels[class_index]
    confidence = score_lite[0][class_index]

    # 输出预测结果
    print('预测类别：', predicted_label)
    print('预测准确度：', confidence)
```
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-07 07:27:24,bug,tf.keras.models.model_from_json() missing a safe_mode parameter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When loading a model with a lambda layer from a json model config, TensorFlow provides the following error:

`
ValueError: Requested the deserialization of a Lambda layer with a Python 'lambda' inside it. This carries a potential risk of arbitrary code execution and thus it is disallowed by default. If you trust the source of the saved model, you can pass 
'safe_mode=False' to the loading function in order to allow Lambda layer loading.
`

However `tf.keras.models.model_from_json()` does not have a `safe_mode` parameter. So there does not seem to be a way to load models with lamda layers using a json config. This issue does not appear in TensorFlow 2.12



### Standalone code to reproduce the issue

```shell
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input, Lambda

inputs = Input(shape=(1,))
x = Lambda(lambda x: x*2)(inputs)
out = Dense(1)(x)
model = Model(inputs=inputs,outputs=out)

model_config = model.to_json()
tf.keras.models.model_from_json(model_config)
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-06 12:56:59,bug,Issue with Reproducible Results: Inconsistent Behavior of Random Seeds in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8-3.9-3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would like to report an issue regarding the reproducibility of results in TensorFlow. Currently, in order to achieve consistent and deterministic results, it seems necessary to set both random.seed(42) and tf.random.set_seed(1) together.

Expected Behavior:
Setting tf.random.set_seed(1) alone should be sufficient to ensure reproducible results across different runs.

Observed Behavior:
Without setting random.seed(42) alongside tf.random.set_seed(1), the results obtained from TensorFlow exhibit inconsistency and do not remain fixed between runs.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import random

tf.random.set_seed(1)
random.seed(42) # This line seems to be redundant

x = tf.constant(tf.random.uniform([2, 3, 2]), dtype=tf.float32)
```


### Relevant log output

_No response_"
tensorflow/tensorflow,2023-07-05 15:38:42,bug,ValueError: Checkpoint was expecting to be a trackable object (an object derived from `Trackable`),"### Issue type

Bug

### Source

binary

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Enterprise

### Python version

3.9.16

### Current behavior?

I'm receiving an error when I try to restore the model checkpoint. I've seen a posting on here that's similar, but I think my case is different. Help is very much appreciated!

I'm using a pre-trained object detection model called SSD MobileNet V2 FPNLite 320x320 


### Standalone code to reproduce the issue

```python
import os
import tensorflow as tf
import pandas as pd
import openpyxl
import cv2 
import numpy as np

from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder
from object_detection.utils import config_util
from matplotlib import pyplot as plt
from pathlib import Path


os.chdir(r""C:\\Users\\mill286"")

CUSTOM_MODEL_NAME = 'my_ssd_resnet50_v1_fpn' # *** Enter here the name of the model you trained. ***
files = {
    'PIPELINE_CONFIG':os.path.join('tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config')
}
# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])
detection_model = model_builder.build(model_config=configs['model'], is_training=False)
# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-54.index')).expect_partial() # *** Replace the number in 'ckpt-XX' with the checkpoint you want to use. ***
```


### Relevant log output

```shell
ValueError                                Traceback (most recent call last)
Cell In[18], line 2
      1 # Restore checkpoint
----> 2 ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
      3 ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-54.index')).expect_partial()

File ~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:2142, in Checkpoint.__init__(self, root, **kwargs)
   2140 if isinstance(converted_v, weakref.ref):
   2141   converted_v = converted_v()
-> 2142 _assert_trackable(converted_v, k)
   2144 if root:
   2145   # Make sure that root doesn't already have dependencies with these names
   2146   child = trackable_root._lookup_dependency(k)

File ~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:1562, in _assert_trackable(obj, name)
   1559 def _assert_trackable(obj, name):
   1560   if not isinstance(
   1561       obj, (base.Trackable, def_function.Function)):
-> 1562     raise ValueError(
   1563         f""`Checkpoint` was expecting {name} to be a trackable object (an ""
   1564         f""object derived from `Trackable`), got {obj}. If you believe this ""
   1565         ""object should be trackable (i.e. it is part of the ""
   1566         ""TensorFlow Python API and manages state), please open an issue."")

ValueError: `Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x000001E163D93910>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.
```
"
tensorflow/tensorflow,2023-07-02 10:13:52,bug,tf.image.extract_patches error for tf.RaggedTensor inputs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.image.extract_patches` should be able to extract patches from `ragged `tensors.

### Standalone code to reproduce the issue

```shell
def build_model():
  input = tf.keras.Input([None, None, 3], ragged=True, name=""image"")
  patches = tf.image.extract_patches(
              images=input,
              sizes=[1, 4, 4, 1],
              strides=[1, 4, 4, 1],
              rates=[1, 1, 1, 1],
              padding=""SAME"",
          )
  
  return tf.keras.Model(
            inputs=input,
            outputs=patches)
  

model = build_model()
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-48-0f2eac36aeae> in <cell line: 16>()
     14 
     15 
---> 16 model = build_model()

3 frames
/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer ""tf.image.extract_patches_3"" (type TFOpLambda).

Failed to convert elements of tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(""Placeholder:0"", shape=(None, 3), dtype=float32), row_splits=Tensor(""Placeholder_1:0"", shape=(None,), dtype=int64)), row_splits=Tensor(""Placeholder_2:0"", shape=(None,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.

Call arguments received by layer ""tf.image.extract_patches_3"" (type TFOpLambda):
  • images=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(""Placeholder:0"", shape=(None, 3), dtype=float32), row_splits=Tensor(""Placeholder_1:0"", shape=(None,), dtype=int64)), row_splits=Tensor(""Placeholder_2:0"", shape=(None,), dtype=int64))
  • sizes=['1', '4', '4', '1']
  • strides=['1', '4', '4', '1']
  • rates=['1', '1', '1', '1']
  • padding='SAME'
  • name=None
```
"
tensorflow/tensorflow,2023-06-30 16:50:24,bug,mutiple issues with the new parameter server strategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.1

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We meet multiple issues in using the Tensorflow 2.x strategy:
1. how to shard data by files, how to use the later binding mechanism to shard data by files, which is critical for high performance training using tf.data.Dataset apis, all workers reading the total data is not scalable.
2. how to control the placement of ops, in one attempt, we build dataset using tf.data.Dataset.from_tensor_slices and.     
     dataset = dataset.interleave(lambda x: tf.data.TextLineDataset(x).skip(1),        cycle_length=15,       num_parallel_calls=15)
   with parameter server strategy, the dataset related ops runs on workers, however, if I add one more op dataset.repeat(), the dataset ops all runs on chief, which is surprising.
4. how to ensure even the workers are with uneven amount of data, the training process with model.fit could end elegantly instead of having to using try catch or data.repeat, as recommendation models generally assume one epoch training.

[yuefengz@google.com](mailto:yuefengz@google.com)
[rchao@google.com](mailto:rchao@google.com)

### Standalone code to reproduce the issue

```shell
# for chief: 
file_list = tf.io.gfile.glob(input_pattern)
dataset = tf.data.Dataset.from_tensor_slices(file_list)
dataset = dataset.shard(worker_num, worker_id)  # how to later bind worker_id?
dataset = dataset.interleave(lambda x: tf.data.TextLineDataset(x).skip(1),        cycle_length=15,       num_parallel_calls=15)

def _parse_csv(line):
  record_defaults = []
  for x in all_columns:
    record_defaults.append(0)
  with tf.control_dependencies([tf.print(tf.shape(line), line[0], output_stream=sys.stderr)]):
    fields = tf.io.decode_csv(
        line,
        field_delim=',',
        record_defaults=record_defaults,
        name='decode_csv')
    return fields

dataset = dataset.map(_parse_csv, num_parallel_calls=8)
dataset = dataset.repeat()

strategy = tf.distribute.experimental.ParameterServerStrategy(
      cluster_resolver, variable_partitioner=variable_partitioner)

with strategy.scope():
  model = build_model(...)

model.fit(dataset, epochs=num_epoch,
     callbacks=[
         tf.keras.callbacks.ProgbarLogger(count_mode='steps'),
         tf.keras.callbacks.TensorBoard(log_dir='/train/tensorboard/', histogram_freq=0)
     ],
     steps_per_epoch=steps_per_epoch)

# for workers and ps:
server = tf.distribute.Server(
      cluster_resolver.cluster_spec(),
      job_name=cluster_resolver.task_type,
      task_index=cluster_resolver.task_id,
      protocol=cluster_resolver.rpc_layer or 'grpc',
      start=True)
    server.join()
```


### Relevant log output

```shell
# tf.print outputs are all on chief stdout, and no improvements if we place the dataset built process under strategy.scope() or using ops.device() operations.
```
</details>"
tensorflow/tensorflow,2023-06-28 15:59:05,bug,//tensorflow/python/data/kernel_tests:snapshot_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/kernel_tests:snapshot_test sometimes fails

x86 log
https://source.cloud.google.com/results/invocations/8b60bfba-b6b6-4503-aa43-62e8bbe1a094/log

AARCH64 log


### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
ERROR: testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2 (__main__.SnapshotTest)
SnapshotTest.testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2
testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 63, in tearDown
    shutil.rmtree(self._snapshot_dir)
  File ""/usr/lib/python3.9/shutil.py"", line 734, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File ""/usr/lib/python3.9/shutil.py"", line 673, in _rmtree_safe_fd
    onerror(os.rmdir, fullname, sys.exc_info())
  File ""/usr/lib/python3.9/shutil.py"", line 671, in _rmtree_safe_fd
    os.rmdir(entry.name, dir_fd=topfd)
OSError: [Errno 39] Directory not empty: '5643068742232426178'

======================================================================
FAIL: testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2 (__main__.SnapshotTest)
SnapshotTest.testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2
testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 318, in testWriteSnapshotDatasetSameFingerprintIncompleteRunRestart
    self.assertSnapshotDirectoryContains(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/snapshot_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/snapshot_test.py"", line 108, in assertSnapshotDirectoryContains
    self.assertLen(run_dirlist, num_snapshot_shards_per_run)
AssertionError: ['00000000.shard', '00000001.shard', '00000002.shard', '00000003.shard', '00000004.shard', '00000005.shard', '00000006.shard', '00000007.shard', '00000008.shard', '00000009.shard', '00000010.shard', '00000011.shard', '00000012.shard', '00000013.shard', '00000014.shard', '00000015.shard', '00000016.shard', '00000017.shard', '00000018.shard', '00000019.shard', '00000020.shard', '00000021.shard', '00000022.shard', '00000023.shard', '00000024.shard', '00000025.shard', '00000026.shard', '00000027.shard', '00000028.shard', '00000029.shard', '00000030.shard', '00000031.shard', '00000032.shard', '00000033.shard', '00000034.shard', '00000035.shard', '00000036.shard', '00000037.shard', '00000038.shard', '00000039.shard', '00000040.shard', '00000041.shard', '00000042.shard', '00000043.shard', '00000044.shard', '00000045.shard', '00000046.shard', '00000047.shard', '00000048.shard', '00000049.shard', '00000050.shard', '00000051.shard', '00000052.shard', '00000053.shard', '00000054.shard', '00000055.shard', '00000056.shard', '00000057.shard', '00000058.shard', '00000059.shard', '00000060.shard', '00000061.shard', '00000062.shard', '00000063.shard', '00000064.shard', '00000065.shard', '00000066.shard', '00000067.shard', '00000068.shard', '00000069.shard', '00000070.shard', '00000071.shard', '00000072.shard', '00000073.shard', '00000074.shard', '00000075.shard', '00000076.shard', '00000077.shard', '00000078.shard', '00000079.shard', '00000080.shard', '00000081.shard', '00000082.shard', '00000083.shard', '00000084.shard', '00000085.shard', '00000086.shard', '00000087.shard', '00000088.shard', '00000089.shard', '00000090.shard', '00000091.shard', '00000092.shard', '00000093.shard', '00000094.shard', '00000095.shard', '00000096.shard', '00000099.shard', '00000101.shard', '00000103.shard', '00000104.shard', '00000105.shard', '00000108.shard', '00000110.shard', '00000115.shard', '00000116.shard', '00000121.shard'] has length of 107, expected 128.

----------------------------------------------------------------------
Ran 20 tests in 5.563s
```
</details>"
tensorflow/tensorflow,2023-06-27 17:09:43,bug,UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

0.0.0-nightly-SNAPSHOT

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Android Samsung Galaxy J5

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Fatal Exception: java.lang.UnsatisfiedLinkError: Failed to load native TensorFlow Lite methods. Check that the correct native libraries are present, and, if using a custom native library, have been properly loaded via System.loadLibrary():
  java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""__register_atfork"" referenced by ""libtensorflowlite_jni.so""
  
This is reproducible on lot of android devices running android version 5,6,7
This is happening in the nightly snapshots from probably last 2 weeks. Did not encounter this in previous nightly snapshots.


### Standalone code to reproduce the issue

```shell
val nnApiOption = NnApiDelegate.Options()
nnApiOption.setUseNnapiCpu(true)
val nnApiDelegate = NnApiDelegate(nnApiOption)
```


### Relevant log output

2023-06-28 10:52:35.629 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni
2023-06-28 10:52:35.637 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni_stable
2023-06-28 10:52:35.638 29536-29620 InterpreterApi                      I  Didn't load native library: tensorflowlite_jni_gms_client

</details>"
tensorflow/tensorflow,2023-06-27 09:22:41,bug,The relationship between the parameters of Conv2D is unclear,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


```
ValueError: `strides > 1` not supported in conjunction with `dilation_rate > 1`. Received: strides=[2, 2] and dilation_rate=[4, 5]
```

The relationship between these two parameters is not clearly defined in the documentation, and it is not certain. Be unaware of that  `strides > 1` not supported in conjunction with `dilation_rate > 1`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.keras.layers import Conv2D

input_tensor = tf.random.normal(shape=(1, 32, 32, 3))

x = Conv2D(filters=2, kernel_size=(1,1), strides=(2,2), padding=""same"", use_bias=False, dilation_rate=(4, 5))(input_tensor)

print(x.shape)
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-26 17:39:13,bug,Not initialized delegate kernel after tflite conversion,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 11
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
TensorFlow 2.10.0

### 2. Code

Code to reproduce my issue is attached to this issue.
[tf_issue.zip](https://github.com/tensorflow/tensorflow/files/11871912/tf_issue.zip)

### 3. Failure after conversion

```
File ""tf_issue\\test_ocr.py"", line 62, in __call__
    self._interpreter.invoke()
File ""...\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: Current implementation only supports equal length strides in the row and column dimensions.Delegate kernel was not initializedNode number 510 (TfLiteFlexDelegate) failed to prepare.
```


### 5. (optional) Any other info / logs
Hello, 
I downloaded the OCR model called **en_PP-OCRv3_rec_infer** from the [Paddle repository](https://github.com/PaddlePaddle/PaddleOCR). To prepare it for my purposes, I converted it into the ONNX format and optimized it, following the guidelines provided [here](https://github.com/PaddlePaddle/Paddle2ONNX/blob/develop/README_en.md#command-line-conversion). To ensure compatibility, I defined a static input/output size for the model.

Subsequently, I proceeded to convert the ONNX format to TFLite using this [repository](https://github.com/sithu31296/PyTorch-ONNX-TFLite/tree/master#onnx-to-tf). Once the conversion was complete, I loaded the resulting .tflite model into [Neutron](https://netron.app/) without any issues, as it successfully read and visualized the model.

However, the problem arises when I attempt to test this model using Python (3.10). The attached zip file contains the code, the model itself, and a sample testing image (It also contain a requirements file with all the packages of my environment).

In my case, I utilized an input_shape_dict of ""{'x': [1, 3, 48, 320]}"" and exported the model in both fp16 and fp32 formats. I also experimented with opset_versions 10 and 16. However, despite these attempts, I encountered the reported failure repeatedly.
"
tensorflow/tensorflow,2023-06-24 15:13:55,bug,Requested feature_data_ size 536907080 doesn't match 1960; Feature generation failed;,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

V2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hello Together,

I'm having a Problem with the micro_speech example for arduino from this repo: https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/micro_speech

When trying to use this example with a new trained model from this jupyter noteboobk: https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech/train

I always get the same error message:
Requested feature_data_ size 536907080 doesn't match 1960
Feature generation failed

The only thing i changed in the notebook was the tensorflow version. This is because this notebook was using 1.x Version which is no longer supported by colab and i changed it to work with the latest 2.x version

Can anyone help here?

Greetings,
Patrick

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tflite-micro-arduino-examples/tree/main/examples/micro_speech

 https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech/train
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-23 08:51:18,bug,"Model runs without error in Tensorflow, but crashes with a segmentation fault in TFLite","### 1. System information

- OS Platform and Distribution: Ubuntu 20.04.4 LTS
- TensorFlow installation: pip
- TensorFlow library: 2.12.0
- TFLite runtime: 2.12.0

### 2. Code

The model is exported from PyTorch using ONNX. I have not included the PyTorch code below for brevity's sake (and because it is used for an active Kaggle competition); you can download the saved Keras model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm). The TFLite conversion code is given below, but you can also download the TFLite model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm) (same link).

Below is the code to create and save the Keras model from two PyTorch models `feat_gen` and `model`, converted using ONNX:

```python
class TFInferModel(tf.Module):
    def __init__(self):
        super(TFInferModel, self).__init__()
        self.feat_gen = tf.saved_model.load(""feat_gen.pb"")
        self.model = tf.saved_model.load(""model.pb"")

        self.feat_gen.trainable = False
        self.model.trainable = False

    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 126], dtype=tf.float32, name=""inputs"")])
    def call(self, inputs):
        output_tensors = {}

        # Add batch dimension.
        inputs = inputs[None]

        # Process using ported PyTorch model.
        features = self.feat_gen(inputs=inputs)[""outputs""]
        outputs = self.model(inputs=features)[""outputs""]

        # Remove batch dimension.
        outputs = outputs[0]

        output_tensors[""outputs""] = outputs
        return output_tensors

tf_model = TFInferModel()
tf.saved_model.save(tf_model, ""tf_model"", signatures={""serving_default"": tf_model.call})
```

The model can be loaded in Keras and run:

```python
model = tf.saved_model.load(""tf_model"")
inputs = tf.zeros((100, 126), dtype=tf.float32)
output = model.call(inputs=inputs)
```

It can also be converted to TFLite:

```python
converter = tf.lite.TFLiteConverter.from_saved_model(""tf_model"")

tf_lite_model = converter.convert()
output_path = ""model.tflite""
with open(output_path, ""wb"") as f:
    f.write(tf_lite_model)
```

And finally the code for TFLite inference:

```python
interpreter = tflite.Interpreter(model_path=""model.tflite"")
prediction_fn = interpreter.get_signature_runner(""serving_default"")
inputs = np.zeros((100, 126), dtype=np.float32)
output = prediction_fn(inputs=inputs)
```

### 3. Failure after conversion

The Keras inference code runs without issue. The TFLite inference code crashes immediately with a segmentation fault (no further info is given)."
tensorflow/tensorflow,2023-06-22 22:29:39,bug,"Crashes in model.save, wrapt error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.12

### Custom Code

No

### OS Platform and Distribution

Fodera Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Crashed when calling model.save()

See log below.

Worked after deinstalling tensorflow and wrapt. wrapt was 1.15.x
and installing tensorflow and wrapt==1.14.1

The problem is that when installing tensorflow, the wrapt 1.15.x is installed automatically and this is not playing with tensorflow.


### Standalone code to reproduce the issue

```shell
model.save('Modelname')

Causes the problem for any trained network.
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""ModelPredictorTraining.py"", line 1415, in <module>
    run_hparam_on_grid(branched_model_1,
  File ""ModelPredictorTraining.py"", line 1403, in run_hparam_on_grid
    fitted_model.save('Model-name')
  File ""/anaconda3/envs/tf2/lib/python3.11/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/anaconda3/envs/tf2/lib/python3.11/site-packages/tensorflow/python/trackable/data_structures.py"", line 823, in __getattribute__
    return super().__getattribute__(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
```
</details>"
tensorflow/tensorflow,2023-06-21 16:46:50,bug,tf.mul after tf.split + tf.sigmoid produces wrong numerical results with MKL enabled,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

intel-tensorflow 2.8 - 2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When running with an MKL enabled tensorflow (e.g. intel-tensorflow from pypi) (or self-compiled with `--config=mkl`). Starting with tensorflow 2.8.0 up until 2.12.0 The attached code produces the wrong numerical result. (1.7615 vs expected 2.6439).

If line 25 is changed to `m = sig * (b + 0.0)` one can get the correct result.
This issue does not occur if installing ""vanilla"" tesorflow from pip with `pip install tensorflow`.

This issue also does not occur if one uses `tf.exp` or `tf.log` instead of `tf.sigmoid`.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python3

import math

import tensorflow as tf
import numpy as np

def sigmoid(x):
  return 1 / (1 + math.exp(-x))

data = [[[2.0, 3.0]]]

tf.compat.v1.disable_eager_execution()

s = tf.compat.v1.Session()
p = tf.compat.v1.placeholder(dtype=tf.float32)
a, b = tf.split(p, 2, axis=2)
sig = tf.sigmoid(a)
m = sig * b

out = s.run([p, m], feed_dict={p: data})
print(out)

print('computed: ', out[-1][0,0,0])
print('expected: ', sigmoid(data[0][0][0]) * data[0][0][1])
```


### Relevant log output

```shell
2023-06-21 18:37:35.713027: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-21 18:37:35.715264: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 
[array([[[2., 3.]]], dtype=float32), array([[[1.7615942]]], dtype=float32)]
computed:  1.7615942
expected:  2.642391233933647
```
</details>"
tensorflow/tensorflow,2023-06-20 19:30:00,bug,fit() fails with CUDNN_STATUS_BAD_PARAM when using Conv3D and multi-GPU MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-95675-g47602c0bad8 2.14.0-dev20230620

### Custom Code

Yes

### OS Platform and Distribution

Rocky Linux release 8.6 (Green Obsidian)

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8600

### GPU model and memory

4 NVIDIA A100s w/ 80GB each

### Current Behaviour?

When executing a model fit that includes a `Conv3D` layer on multiple GPUs, I'm encountering a `CUDNN_STATUS_BAD_PARAM` error in the gradient computation step. No errors occur when running on a single GPU, nor when I swap out `Conv3D` with `AveragePooling3D` or `Conv2D`. However, `Conv3DTranspose` also fails.

```none
  (0) UNKNOWN:  CUDNN_STATUS_BAD_PARAM
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(3549): 'tensor' CUDNN_BACKEND_TENSOR_DESCRIPTOR: Check and Set the CUDNN_ATTR_TENSOR_DIMENSIONS Correctly
         [[{{node gradient_tape/replica_2/model/conv3d/Conv3D/Conv3DBackpropFilterV2}}]]
         [[div_no_nan/ReadVariableOp_1/_52]]
         [[group_deps/_95]]
         [[Adam/update_2_2/AssignAddVariableOp/_119]]
         [[group_deps/_103]]
```

With the `Graph execution error` traceback:
```none
Traceback (most recent call last):
  File ""conv3_multi_gpu_fail_repro.py"", line 25, in <module>
    model.fit(x_data, y_data, batch_size=1, epochs=1, verbose=1)
  File ""/***/env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/***/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:

Detected at node gradient_tape/replica_3/model/conv3d/Conv3D/Conv3DBackpropFilterV2 defined at (most recent call last):
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
    grads_and_vars = self.compute_gradients(loss, var_list, tape)
  File ""/usr/lib/python3.8/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/usr/lib/python3.8/threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1348, in run_step
    outputs = model.train_step(data)
  File ""/***/env/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1129, in train_step
    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
    grads_and_vars = self.compute_gradients(loss, var_list, tape)
  File ""/***/env/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
    grads = tape.gradient(loss, var_list)
```

I'm running from the `tensorflow/tensorflow:nightly-gpu` docker image.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers, models

input_shape = (28, 28, 28, 1)
num_samples = 10

x_data = tf.random.uniform((num_samples, *input_shape), 0, 1)
y_data = tf.random.uniform((num_samples, *input_shape), 0, 1)

multi_gpu=True # <== fails
#multi_gpu=False # <== works
devices = [] if multi_gpu else ['/gpu:0']

mirrored_strategy = tf.distribute.MirroredStrategy(devices=devices)
print(f""{mirrored_strategy.num_replicas_in_sync} replica(s)"")

with mirrored_strategy.scope():
    inputs = layers.Input(shape=input_shape)
    outputs = layers.Conv3D(1, 1)(inputs) # <== fails
    #outputs = layers.AveragePooling3D(1)(inputs) # <== works
    #outputs = layers.Conv2D(1, 1)(inputs) # <== works
    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy')

model.fit(x_data, y_data, batch_size=1, epochs=1, verbose=1)
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-19 19:28:28,bug,FFT produces wrong results when using multiple GPUs with MirroredStrategy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0 2.14.0-dev20230619

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8.0 / 8.6.0.163

### GPU model and memory

_No response_

### Current Behaviour?

Using TensorFlow FFT in a Keras model will produce incorrect results when using MirroredStrategy and multiple GPUs.
This is not an accuracy issue. The results of consecutive calls seem to be either correct or garbage.

I created a test Keras model that has one layer that does FFT. There is also a reference model using a DFT layer that is used to verify that incorrect behavior only happens when using tf.signal.fft.
Attached is a test application that runs both models in different combinations of MirroredStrategy/default strategy and eager/graph execution.
MirroredStrategy and graph execution is the combination that produces the error. At least two GPUs are required to reproduce the problem.
The output MAE loss is around 6.5, which translates to 650% error. (The absolute value of each entry in the correct output is 1.0.)

I think it's not a user error, but if it is, there should be an error or warning instead of incorrect results.

I was able to reproduce the issue with all TF fft variants (tf.signal.fft, tf.signal.rfft, tf.signal.stft, tf.signal.fft2d)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from scipy.linalg import dft
from math import sqrt

# Layer that does tf.signal.fft operation
class FFTLayer(tf.keras.layers.Layer):

    def call(self, x):
      fx = tf.signal.fft(x)
      return fx

# Layer that returns same results as tf.signal.fft op, but
# uses slower direct computation of DFT, implemented as matrix multiply.
class MatrixDFTLayer(tf.keras.layers.Layer):
    def __init__(self):
      super().__init__()
      self.dft = tf.cast(dft(1024), tf.complex64)

    def call(self, x):
        fx = self.dft @ tf.transpose(x)
        return tf.transpose(fx)


def create_model(use_mirrored_strategy: bool = True,
                              run_eagerly: bool = True,
                              layer_to_use: tf.keras.layers.Layer = FFTLayer) -> None:
    print(f""\\ncreate model with: use_mirrored_strategy: {use_mirrored_strategy}, "",
          f""run_eagerly: {run_eagerly}, "",
          f""layer_to_use: {layer_to_use}"")

    if use_mirrored_strategy:
        distribution_strategy = tf.distribute.MirroredStrategy()
    else:
        distribution_strategy = tf.distribute.get_strategy()
    with distribution_strategy.scope():

        ins = tf.keras.layers.Input([1024], dtype=tf.complex64)
        x = layer_to_use()(ins)
        model = tf.keras.Model(inputs=ins, outputs=x)

        model.compile(
            loss=tf.keras.losses.MeanAbsoluteError(),
            run_eagerly=run_eagerly
        )
    return model


def create_data(fft_size, batch_size, num_steps):
    num_examples = num_steps * batch_size

    # y data is a complex vector of all (1/sqrt(2), (1/sqrt(2)j)
    train_y = np.ones([fft_size], np.float32)
    train_y = (1/sqrt(2))*train_y + (1/sqrt(2))*1j*train_y
    # abs mean is 1 -> MAE magnitude should be compared to 1
    print(""train_y mean: "", tf.reduce_mean(tf.abs(train_y)))

    # use inverse transform to create input data
    # fft(train_x) will produce train_y
    train_x = tf.signal.ifft(train_y)

    # clone data to get larger training set
    train_y = train_y[tf.newaxis, ...]
    train_x = train_x[tf.newaxis, ...]

    train_x = tf.tile(train_x, [num_examples, 1])
    train_y = tf.tile(train_y, [num_examples, 1])
    
    return train_x, train_y


fft_size = 1024
batch_size = 9
num_steps = 100
train_x, train_y = create_data(fft_size, batch_size, num_steps)

# Test cases with MatrixDFTLayer
# These are all ok, MAE close to 0.0
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=True, run_eagerly=False, layer_to_use=MatrixDFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")

# Test Cases using TF FFT. These fail when using MirroredStrategy.
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=False, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# ok
model = create_model(use_mirrored_strategy=False, run_eagerly=True, layer_to_use=FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
# fail, 
model = create_model(use_mirrored_strategy=True, run_eagerly=False,layer_to_use= FFTLayer)
loss = model.evaluate(train_x, train_y, batch_size=batch_size, verbose=0)
print(f""loss: {loss}"")
```


### Relevant log output

```shell
train_y mean:  tf.Tensor(1.0, shape=(), dtype=float32)

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.MatrixDFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: False,  run_eagerly: True,  layer_to_use: <class '__main__.FFTLayer'>
loss: 0.0

create model with: use_mirrored_strategy: True,  run_eagerly: False,  layer_to_use: <class '__main__.FFTLayer'>
loss: 6.451958656311035
```
</details>"
tensorflow/tensorflow,2023-06-19 11:45:23,bug,F1 score error on multi class data,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v1.12.1-95639-g08bd7e1a8e5 2.14.0-dev20230618

### Custom Code

Yes

### OS Platform and Distribution

OS Ventura 13.0.1

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Implementing the F1 score available in the nightly builds on multi-class data such as below:

```
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics= tf.keras.metrics.F1Score())

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))
```

triggers the following error:

```
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[8], line 5
      1 model.compile(optimizer='adam',
      2               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      3                 metrics= tf.keras.metrics.F1Score())
----> 5 history = model.fit(train_images, train_labels, epochs=10, 
      6                     validation_data=(test_images, test_labels))

File /opt/homebrew/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File /var/folders/f5/mkqkf_0d42qcsqc37hd_y0hm0000gn/T/__autograph_generated_fileb8tcgui2.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

ValueError: in user code:

    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1085, in train_step
        return self.compute_metrics(x, y, y_pred, sample_weight)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1179, in compute_metrics
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/engine/compile_utils.py"", line 605, in update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/utils/metrics_utils.py"", line 77, in decorated
        update_op = update_state_fn(*args, **kwargs)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/metrics/base_metric.py"", line 140, in update_state_fn
        return ag_update_state(*args, **kwargs)
    File ""/opt/homebrew/lib/python3.8/site-packages/keras/src/metrics/f_score_metrics.py"", line 176, in update_state  **
        y_true = tf.convert_to_tensor(y_true, dtype=self.dtype)

    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype uint8: <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=uint8>
```

I've tried with multiple multi-class datasets and the same error is returned. The F1 score page says it should work with multi-class data https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score. Is there something I've missed regarding its implementation for multi-class data (such as somewhere to specify the number of classes?) or is this a bug?



### Standalone code to reproduce the issue

```shell
Here is a Jupyter notebook with some example data from https://www.tensorflow.org/tutorials/images/cnn

https://drive.google.com/file/d/1tExJ80AktA87EmsExOPEMWsevoiQz4VX/view?usp=share_link
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-18 22:44:17,bug,Uncaught exception in ZMQStream callback when running your example notebooks using latest or nightly docker image,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux gpu02 6.2.11-2-pve #1 SMP PREEMPT_DYNAMIC PVE 6.2.11-2 (2023-05-10T09:13Z) x86_64 x86_64 x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

python3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Occurs when running any of your example notebooks:

```
[E 22:36:50.295 NotebookApp] Uncaught exception in ZMQStream callback
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
        self._handle_write()
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
[E 22:36:50.297 NotebookApp] Uncaught exception in zmqstream callback
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
        self._handle_write()
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
Exception in callback BaseAsyncIOLoop._handle_events(33, 1)
handle: <Handle BaseAsyncIOLoop._handle_events(33, 1)>
Traceback (most recent call last):
  File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py"", line 206, in _handle_events
    handler_func(fileobj, events)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
    f = callback(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
    return callback(self, msg)
  File ""/usr/local/lib/python3.8/dist-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
    super()._on_zmq_reply(stream, msg)
  File ""/usr/local/lib/python3.8/dist-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
    self.write_message(msg, binary=isinstance(msg, bytes))
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 339, in write_message
    return self.ws_connection.write_message(message, binary=binary)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1086, in write_message
    fut = self._write_frame(True, opcode, message, flags=flags)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/websocket.py"", line 1061, in _write_frame
    return self.stream.write(frame)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 546, in write
    self._handle_write()
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 976, in _handle_write
    self._write_buffer.advance(num_bytes)
  File ""/usr/local/lib/python3.8/dist-packages/tornado/iostream.py"", line 182, in advance
    assert 0 < size <= self._size
AssertionError
```

### Standalone code to reproduce the issue

```shell
Run any of your Jupyter example in your docker image.
```


### Relevant log output

_No response_</details>"
tensorflow/tensorflow,2023-06-13 19:41:41,bug,W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) 

### Bazel version

bazel 5.3.2

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0

### CUDA/cuDNN version

11.2/8 in conda env

### GPU model and memory

laptop 3080 RTX

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
I have tensorflow 2 installed and also from the code below I see cudnn 8 is found. 


(samurai) mona@ard-gpu-01:~/samurai$ cat cudnn_test.py 
import tensorflow as tf

sys_details = tf.sysconfig.get_build_info()
cuda_version = sys_details[""cuda_version""]
print(cuda_version)

cudnn_version = sys_details[""cudnn_version""]
print(cudnn_version)


cuda_compute_capabilities = sys_details[""cuda_compute_capabilities""]
print(cuda_compute_capabilities)
(samurai) mona@ard-gpu-01:~/samurai$ python cudnn_test.py 
11.2
8
['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']
```

However, when I run the following command, I get an error that cudnn 8 is not found.

```
(samurai) mona@ard-gpu-01:~/samurai$ python train_samurai.py --config configs/samurai/samurai.txt --datadir data/duck/ --basedir . --expname duck_test --gpu 0
Namespace(config=None, basedir='.', expname='duck_test', batch_size=1024, learning_rate=0.0001, epochs=150, steps_per_epoch=2000, gpu='0', tpu=None, debug=False, profile=False, perturb=1.0, raw_noise_std=0.0, coarse_samples=64, linear_disparity_sampling=False, fine_samples=128, fourier_frequency=10, direction_fourier_frequency=4, random_encoding_offsets=True, fine_net_width=128, fine_net_depth=8, coarse_net_width=128, coarse_net_depth=6, appearance_latent_dim=32, diffuse_latent_dim=24, fix_diffuse=True, camera_distribution='sphere', use_fully_random_cameras=False, random_cameras_per_view=4, min_softmax_scaler=1.0, max_softmax_scaler=10.0, camera_weight_update_lr=0.3, camera_weight_update_momentum=0.75, bounding_size=0.5, resolution_factor=4, advanced_loss_done=80000, network_gradient_norm_clipping=0.1, camera_gradient_norm_clipping=-1, not_learn_r=False, not_learn_t=False, not_learn_f=False, edge_align_step=200, num_edge_align_steps=50, pretrained_camera_poses_folder=None, start_f_optimization=90000, start_fourier_anneal=0, finish_fourier_anneal=50000, slow_scheduler_decay=100000, brdf_schedule_decay=40000, lambda_smoothness=0.01, smoothness_bound_dividier=200, coarse_distortion_lambda=0.001, fine_distortion_lambda=0, normal_direction_lambda=0.005, mlp_normal_direction_lambda=0.0003, disable_posterior_scaling=False, disable_mask_uncertainty=True, lambda_brdf_decoder_smoothness=0.1, lambda_brdf_decoder_sparsity=0.01, camera_lr=0.003, camera_lr_decay=70, camera_regularization=0.1, aim_center_regularization=10.0, camera_rotation='lookat', learn_camera_offsets=True, basecolor_metallic=True, skip_decomposition=False, compose_on_white=True, rotating_object=False, single_env=False, brdf_preintegration_path='data/neural_pil/BRDFLut.hdr', illumination_network_path='data/neural_pil/illumination-network', datadir='data/duck/', max_resolution_dimension=400, test_holdout=16, dataset='samurai', load_gt_poses=False, canonical_pose=0, log_step=100, weights_epoch=5, validation_epoch=5, testset_epoch=150, video_epoch=50, lrate_decay=300, render_only=False)
2023-06-13 15:35:10.002485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-06-13 15:35:10.022702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/home/mona/MVTec/HALCON-23.05-Progress//lib/x64-linux:/usr/local/cuda-11.7/lib64:/home/mona/onnx-tensorrt/build:
2023-06-13 15:35:10.022715: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Utilizing 0 GPUs for training.
2023-06-13 15:35:11.092766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
(70, 3)
Model: ""sequential_12""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 MappingNetwork/Layer_0 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Layer_1 (Den  (None, 128)              16512     
 se)                                                             
                                                                 
 MappingNetwork/Final (Dense  (None, 768)              99072     
 )                                                               
                                                                 
 reshape_1 (Reshape)         (None, 2, 3, 128)         0         
                                                                 
=================================================================
Total params: 132,096
Trainable params: 132,096
Non-trainable params: 0
_________________________________________________________________
Model: ""sequential_13""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 ConditionalNetwork/Dense1 (  (None, 32)               192       
 Dense)                                                          
                                                                 
 ConditionalNetwork/DenseFin  (None, 256)              8448      
 al (Dense)                                                      
                                                                 
 reshape_2 (Reshape)         (None, 2, 128)            0         
                                                                 
=================================================================
Total params: 8,640
Trainable params: 8,640
Non-trainable params: 0
_________________________________________________________________
Found ckpts []
Starting training in epoch 0 at step 0
Start Training...
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-bottom_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_right/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
/home/mona/anaconda3/envs/samurai/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape_1:0"", shape=(1024,), dtype=int32), values=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Reshape:0"", shape=(1024, 1), dtype=float32), dense_shape=Tensor(""gradients/interpolate_bilinear/gather-top_left/GatherV2_grad/Cast:0"", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  warnings.warn(
  25/2000 [..............................] - ETA: 42:41 - loss: 1.8824 - loss_camera: 7.2076 - fine_loss: 1.8019   

```
```


### Relevant log output

```shell
(samurai) mona@ard-gpu-01:~/samurai$ lsb_release -a
LSB Version:	core-11.1.0ubuntu4-noarch:security-11.1.0ubuntu4-noarch
Distributor ID:	Ubuntu
Description:	Ubuntu 22.04.2 LTS
Release:	22.04
Codename:	jammy
(samurai) mona@ard-gpu-01:~/samurai$ uname -a
Linux ard-gpu-01 5.19.0-43-generic #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon May 22 13:39:36 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

```
(samurai) mona@ard-gpu-01:~/samurai$ nvidia-smi
Tue Jun 13 15:38:44 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3080 L...    On | 00000000:01:00.0 Off |                  N/A |
| N/A   49C    P8               17W /  90W|    102MiB / 16384MiB |     21%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      2549      G   /usr/lib/xorg/Xorg                           95MiB |
|    0   N/A  N/A      2983      G   ...libexec/gnome-remote-desktop-daemon        3MiB |
+---------------------------------------------------------------------------------------+
```
```
(samurai) mona@ard-gpu-01:~/samurai$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Jun__8_16:49:14_PDT_2022
Cuda compilation tools, release 11.7, V11.7.99
Build cuda_11.7.r11.7/compiler.31442593_0

```


The code is from this repo: https://github.com/google/samurai
```
</details>"
tensorflow/tensorflow,2023-06-12 18:07:28,bug,"Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1) Node number 0 (PAD) failed to prepare.","I have converted my DenseNet-121 model to model.tflite and when i am loading it to android app and trying to make predictions, it's giving following errors : java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (4 != 1)
Node number 0 (PAD) failed to prepare.
at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)
at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensorsIfNeeded(NativeInterpreterWrapper.java:308)
at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:248)
at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:101)
at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:77)
at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:94)
at org.tensorflow.lite.Interpreter.run(Interpreter.java:77)
at com.example.appleleafdiseasedetection.DiseaseDetector$2.onClick(DiseaseDetector.java:72)
at android.view.View.performClick(View.java:7743)
at android.view.View.performClickInternal(View.java:7720)
at android.view.View.access$3700(View.java:854)
at android.view.View$PerformClick.run(View.java:29111)
at android.os.Handler.handleCallback(Handler.java:938)
at android.os.Handler.dispatchMessage(Handler.java:99)
at android.os.Looper.loopOnce(Looper.java:210)
at android.os.Looper.loop(Looper.java:299)
at android.app.ActivityThread.main(ActivityThread.java:8309)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:556)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1038) how can i solve it?
"
tensorflow/tensorflow,2023-06-12 10:32:27,bug,Documentation Bug：the description of padding,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.12.0

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### Output

```
ValueError: The `padding` argument must be a tuple of 2 integers. Received: {'padding': 2}
```

#### Document

| `padding` | Int, or tuple of int (length 2), or dictionary. |
| --------- | ----------------------------------------------- |




### Standalone code to reproduce the issue

```shell
input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
x = ZeroPadding1D({'padding':2})(x)
print(x)
```


### Relevant log output
_No response_
</details>"
tensorflow/tensorflow,2023-06-11 00:21:12,bug,Tensorflow Lite on Raspberry Pi,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tflite_runtime-2.12.0-cp39-cp39-manylinux2014_armv7l.whl

### Custom Code

Yes

### OS Platform and Distribution

Linux raspbari14 6.1.32-v7+ #1656 SMP Wed Jun  7 11:31:19 BST 2023 armv7l GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.9.2 (default, Mar 12 2021, 04:06:34) 

### Bazel version

_No response_

### GCC/Compiler version

[GCC 10.2.1 20210110] on linux

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Not working as documented:

`import tflite_runtime.interpreter as tflite`

How to import Tensorflow Lite in python scripts?

### Standalone code to reproduce the issue

```shell
$ python3 -m pip install tflite-runtime
Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple
Collecting tflite-runtime
  Downloading tflite_runtime-2.12.0-cp39-cp39-manylinux2014_armv7l.whl (1.8 MB)
     |████████████████████████████████| 1.8 MB 2.6 MB/s 
Requirement already satisfied: numpy>=1.19.2 in /home/chowkidar/.local/lib/python3.9/site-packages (from tflite-runtime) (1.23.1)
Installing collected packages: tflite-runtime
Successfully installed tflite-runtime-2.12.0
$ python3
Python 3.9.2 (default, Mar 12 2021, 04:06:34) 
[GCC 10.2.1 20210110] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/chowkidar/.local/lib/python3.9/site-packages/tflite_runtime/interpreter.py"", line 33, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /home/chowkidar/.local/lib/python3.9/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so)
>>>
```


### Relevant log output

_No response_</details>"
microsoft/vscode,2023-09-29 07:30:00,bug,Give ms-python.debugpy access to portsAttributes proposal,The Python debugger extension has adopted the portsAttributes API to help us gain confidence in the API before finalization. Not granting that extension access to the API was an oversight.
microsoft/vscode,2023-09-28 21:18:51,bug,Verify fix/revision to Code Actions on Save,"tests https://github.com/microsoft/vscode/issues/194031 and new changes from https://github.com/microsoft/vscode/pull/194409. related to https://github.com/microsoft/vscode/issues/194397

added back old boolean values (since we reverted the changes for migration) in addition to supporting new enum values.

This is for both Code Actions in notebooks and in the editor:

1. Find `notebook.codeActionsOnSave` or  `editor.codeActionsOnSave`
2. test each enum's behavior. note that `always` currently does not support code actions on auto save after delay. (easiest code actions to test would be `source.fixAll` and `source.organizeImports`. other code actions like `source.fixAll.eslint` and `source.removeUnusedImports` are supported as well.
3. test boolean behavior. (true and `explicit` should be the same, `false` and `never` should be the same. there are descriptions about future deprecation and behavior as well!)"
microsoft/vscode,2023-09-28 18:11:54,bug,Comments editor height is incorrect when window is resized to be small,"1. Resize window
2. Start a comment
3. :bug: as you can see from the scrollbar, the comment text doesn't fill the full editor

![Image](https://github.com/microsoft/vscode/assets/30305945/a2899e98-d88d-4f1b-9cb1-b6b6dac33a2d)

"
microsoft/vscode,2023-09-28 12:05:26,bug,[regression] CodeAction on save is broken,"Version: 1.83.0-insider
Commit: aad333b878b4cfce2f4152d48552fb6f980d7daf

* have the user setting spec'd below
* open `src/vs/editor/contrib/stickyScroll/test/browser/stickyScroll.test.ts`
* remove the semicolon on line 29
* save via `Cmd+S`, semi column isn't inserted

```
  ""editor.codeActionsOnSave"": {
    ""source.fixAll.eslint"": ""explicit"",
    ""source.removeUnusedImports"": ""explicit""
  },
```"
microsoft/vscode,2023-09-28 08:26:59,bug,"Chat view  `ERR Error: Invalid range: [1, 0)`","In one of my workspaces I see no more chat. The console shows the stacktraces below, the debugger stops like this

<img width=""1302"" alt=""Screenshot 2023-09-28 at 10 26 44"" src=""https://github.com/microsoft/vscode/assets/1794099/0940db42-526f-4ac8-9066-78bf2ce5035c"">


```
ERR Error: Invalid range: [1, 0)
    at new k (offsetRange.ts:48:10)
    at c.G (chatModel.ts:538:42)
    at chatModel.ts:525:65
    at Array.map (<anonymous>)
    at c.F (chatModel.ts:524:19)
    at new c (chatModel.ts:504:39)
    at g.j (instantiationService.ts:119:18)
    at g.createInstance (instantiationService.ts:85:18)
    at O (chatServiceImpl.ts:325:43)
    at O.getOrRestoreSession (chatServiceImpl.ts:404:15)
    at s.U (chatViewPane.ts:114:54)
    at s.render (paneview.ts:267:9)
    at s.render (viewPane.ts:286:9)
    at R.rb (viewPaneContainer.ts:778:9)
    at u.value (viewPaneContainer.ts:532:87)
    at c.z (event.ts:1138:13)
    at c.A (event.ts:1149:9)
    at c.fire (event.ts:1173:9)
    at b.I (viewContainerModel.ts:661:41)
    at b.H (viewContainerModel.ts:656:8)
    at u.value (viewContainerModel.ts:361:118)
    at c.z (event.ts:1138:13)
    at c.fire (event.ts:1169:9)
    at u.value (event.ts:152:97)
    at l.z (event.ts:1138:13)
    at l.A (event.ts:1149:9)
    at l.fire (event.ts:1173:9)
    at l.fire (event.ts:1335:11)
    at s.setContext (contextKeyService.ts:345:29)
    at u.reset (contextKeyService.ts:214:18)
    at new u (contextKeyService.ts:203:8)
    at s.createKey (contextKeyService.ts:290:10)
    at w (contextKeyService.ts:599:20)
    at g.invokeFunction (instantiationService.ts:68:11)
    at y.n (commandService.ts:95:46)
    at y.executeCommand (commandService.ts:60:17)
    at p.$executeCommand (mainThreadCommands.ts:91:31)
    at m.S (rpcProtocol.ts:456:17)
    at m.Q (rpcProtocol.ts:441:32)
    at m.M (rpcProtocol.ts:371:19)
    at m.L (rpcProtocol.ts:297:10)
    at u.value (rpcProtocol.ts:161:42)
    at c.z (event.ts:1138:13)
    at c.fire (event.ts:1169:9)
    at r.fire (ipc.net.ts:650:19)
    at Y.onmessage (localProcessExtensionHost.ts:581:40)
log.ts:441   ERR Invalid range: [1, 0): Error: Invalid range: [1, 0)
    at new k (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:95:32662)
    at c.G (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1575:1173)
    at vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1575:827
    at Array.map (<anonymous>)
    at c.F (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1575:779)
    at new c (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1575:326)
    at g.j (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:640:1241)
    at g.createInstance (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:640:733)
    at O (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1576:8061)
    at O.getOrRestoreSession (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1576:9336)
    at s.U (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2551:20103)
    at s.render (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:227:65895)
    at s.render (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1669:11445)
    at R.rb (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2537:37565)
    at u.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2537:34403)
    at c.z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1902)
    at c.A (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1972)
    at c.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:2188)
    at b.I (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2321:122034)
    at b.H (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2321:122006)
    at u.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2321:116441)
    at c.z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1902)
    at c.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:2119)
    at u.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:85:49477)
    at l.z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1902)
    at l.A (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1972)
    at l.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:2188)
    at l.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:3317)
    at s.setContext (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:638:35504)
    at u.reset (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:638:33917)
    at new u (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:638:33812)
    at s.createKey (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:638:34811)
    at w (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:638:38446)
    at g.invokeFunction (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:640:326)
    at y.n (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1528:12391)
    at y.executeCommand (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1528:11979)
    at p.$executeCommand (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1537:552)
    at m.S (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1551:18856)
    at m.Q (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1551:18622)
    at m.M (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1551:17715)
    at m.L (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1551:16794)
    at u.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1551:15597)
    at c.z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1902)
    at c.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:2119)
    at r.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:616:14335)
    at Y.onmessage (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:1652:1077)
```"
microsoft/vscode,2023-09-27 11:01:28,bug,[web][ff] web socket breaks when changing network,"At Gitpod we received bug requests that in FireFox (117-118) VS Code Web (1.82) sometimes became unresponsive. See [Loom](https://www.loom.com/share/c028fe3b10814f5f90b00255625eaac6?sid=5aa5fdaa-1d65-4581-bc45-5a1153dcafda) for what unresponsive means.

It is hard to reproduce, but I was able with following:

1. Leave FF tab in the background.
2. Trigger reconnects by toggling your wifi for instance.
3. After a while come back to FF tab.

Debugging web part showed that frontend is keep asking to replay missing messages, but server keeps replaying with the same message. Unfortunately I was not able to debug the server part.

"
microsoft/vscode,2023-09-27 08:29:52,bug,Extension magically got reenabled,"* I have rust-analyzer installed but disabled
* It's now enabled without me doing something
* I blame it on settings sync because yesterday I reenabled that and I have onboarded a bunch of clients (vscode.dev, separate mac, etc)

## Steps to Repro

- Open VS Code
- Install an extension A and disable it
- Install older version of the extension using `Install Another Version...` action
- Open VS Code using different user data directory but same extensions directory - `code --user-data-dir <folder>`
- Update the extension from this instance
🐛 Extension is enabled in other instance

Note that this is nothing to do with Settings Sync"
microsoft/vscode,2023-09-26 22:04:46,bug,Toggling file node in Comments view focuses comment,"Testing #194012

1. Checkout https://github.com/microsoft/vscode/pull/193812
2. Open comments view
3. Have `debug.contribution.ts` open and scrolled to the top of the file
4. Hit enter in the `debug.contribution.ts` node
5. :bug: I get scrolled to the first comment"
microsoft/vscode,2023-09-26 21:44:38,bug,Debug console with DWARF debugging isn't printing the expected value,"Testing #194071

See top left `a` and debug console `a`:

![Image](https://github.com/microsoft/vscode/assets/2193314/5c6ca24a-73e7-4424-8373-1d73ac57ce91)

"
microsoft/vscode,2023-09-26 21:18:55,bug,Specify what is in the event data for `env.onDidChangeShell`,"Testing #194028

'env.onDidChangeShell' tsdoc does not specify what the event data string will contain. 
It looks like it's the path to the newly selected default shell."
microsoft/vscode,2023-09-26 18:10:14,bug,Seeing two action bars,"Testing #193991



![Image](https://github.com/microsoft/vscode/assets/900690/e1f21316-35ee-4c88-b1ef-320c66078411)


I am still trying to recap how I got there...

**Update:**
* have some pinned and non-pinned tabs
* open an empty group to the right
* drag the entire group to the right by dragging from empty space after the last tab
* you end up with 2 action bars visible"
microsoft/vscode,2023-09-26 18:04:35,bug,Debug toolbar with commandCenter limits command center search ,"Testing #194001

Followed the steps to turn on the debug toolbar with commandCenter options.
However, when the debugging started, clicking the area around the directory name did not trigger 'search files'. (the blank space at the right side of 'vscode-python' in terms of the attached screenshot.
I was able to trigger 'search files' only when pressing specifically on the directory name, in this case, 'vscode-python.

++ Nice feature by the way!

<img width=""983"" alt=""Screenshot 2023-09-26 at 10 42 11 AM"" src=""https://github.com/microsoft/vscode/assets/62267334/a5970348-52eb-4014-a197-1b3c24309f36"">
<img width=""512"" alt=""Screenshot 2023-09-26 at 11 02 46 AM"" src=""https://github.com/microsoft/vscode/assets/62267334/95245cf3-147c-420f-8166-d0be5ce90f3d"">
"
microsoft/vscode,2023-09-26 17:51:18,bug,Enable Source Mapped Stepping command is registered twice,"Testing #194071

1. Run Disable Source Mapped Stepping
2. Open command palette
3. :bug: there are two Enable Source Mapped Stepping commands (there's only one Disable Source Mapped Stepping command)

![Image](https://github.com/microsoft/vscode/assets/30305945/d4b152ad-684c-4af2-8733-80e820d1855c)

"
microsoft/vscode,2023-09-26 16:58:49,bug,insertFinalNewline breaks typings in first line in a new cell,"Re #194078, I was editing the endgame notebook in VS Code repo but it keeps moving the cursor to the next line, which breaks typing.


https://github.com/microsoft/vscode/assets/876920/be93070f-c41a-42ac-9afb-b0bdb050b552

It's very likely an issue with these two settings combined

* `""files.insertFinalNewline"": true`
* ""files.autoSave"": ""afterDelay""
 
We seems to have the same issue for files in text editor but untitled file doesn't suffer from this as it can't be auto saved. I wonder if we should have some special treatment for notebook cells, as it's very usual to create new cells, which are always empty at the beginning, typing in the first line is making it almost impossible. Maybe we could keep the cursor position for this scenario.
"
microsoft/vscode,2023-09-26 15:35:14,bug,Views entry is only present when English is the display language,"https://github.com/microsoft/vscode/issues/192271

English:

![Image](https://github.com/microsoft/vscode/assets/2193314/2622f6bd-11fc-413b-8aec-816b4e034fc1)

Korean:

![Image](https://github.com/microsoft/vscode/assets/2193314/46b0c4b1-d910-419f-a558-9e4f7194d182)

Daniel:

😕  "
microsoft/vscode,2023-09-26 14:07:28,bug,Settings Sync is uninstalling the extension that is installed from sources,"Settings Sync is uninstalling the extension that is installed from sources.

It needs complex steps to setup and reproduce. @hediet has this setup and is seeing this issue."
microsoft/vscode,2023-09-26 13:42:32,bug,Problematic Theme Styles,"White on gray is not readable. I think the explorer tree has a different behavior here.

![Image](https://github.com/microsoft/vscode/assets/2931520/d2a424a2-93c8-4a90-942c-b95ecda099b4)

"
microsoft/vscode,2023-09-26 13:06:15,bug,SCM Sync: Multiple select is enabled,"Testing #194016

It seems like multiple selection is enabled in this list/tree, though without any value. Let's disable it for now."
microsoft/vscode,2023-09-26 12:49:20,bug,Arrow up while editing comment goes to previous comment,"Testing #194011

Editing the second of two comment, the arrow up key navigates to the first comment instead of the previous line in the comment being edited.


https://github.com/microsoft/vscode/assets/9205389/39d6dc8d-e06b-4cd4-8468-cbd56a6ed2aa

"
microsoft/vscode,2023-09-26 09:38:25,bug,Settings description is wrong/swapped,"Testing #194077

* configure  `""notebook.codeActionsOnSave"": {    ""notebook.source.normalizeVariableNames"": ""explicit""},`
* 🐛 the description of the value is wrong

<img width=""417"" alt=""Screenshot 2023-09-26 at 11 37 31"" src=""https://github.com/microsoft/vscode/assets/1794099/fb5d43c7-7f8e-42f1-af14-67d8189d0371"">
"
microsoft/vscode,2023-09-26 08:31:03,bug,Remove xtermTerminal.clearActiveSearchDecoration,"Version: 1.83.0-insider (user setup)
Date: 2023-09-26T06:31:13.416Z

![1](https://github.com/microsoft/vscode/assets/48614781/b0eceb8d-cbf9-43b8-b66a-847570b5f99a)
"
microsoft/vscode,2023-09-25 18:52:55,bug,icon aria label isn't updated/correct,"Testing #194040

It says settings gear regardless, making it seem to a screen reader user like the icon hasn't been successfully changed.

https://github.com/microsoft/vscode/assets/29464607/9bd2fbff-518c-432a-a1d5-b8f8f27a818e

"
microsoft/vscode,2023-09-25 14:52:55,bug,File > Open Recent > recent workspace no longet opens in new window when Ctrl or Shift used in VS Code Insiders,"
Type: <b>Bug</b>

Opening in new window straing from File > Open Recent menu is great tool to quickly get to another workspace wiithout loosing curren open one, and much faster then New Window and searching same workspace in new window.
So I hope this change is not intentional.

Steps to reproduce:
1. Have history of several opened workspaces
1. Open File > Open Recent submenu.
1. Press <kbd>Ctrl</kbd> or <kbd>Shift</kbd> and click on some of recent workspaces
1. Workspace in active window is replaces with selection from recent

Expecte behavior:
New window should be opened with new workspace instead of replacing workspace in active window.

This is behavior is reproducible in standalone(zip) Insiders intallation but not not reprodusible in standalone(zip) Stable intallation. However in stable <kbd>Ctrl</kbd>(<kbd>Shift</kbd>) sometimes does nothing, but never replaces current workspace.

VS Code version: Code - Insiders 1.83.0-insider (109e1f8d8afb754ed31317f79937a44e98d5063b, 2023-09-25T10:41:26.295Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz (8 x 2996)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|31.50GB (15.50GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details>Extensions: none
<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-25 13:55:47,bug,[Accessibility] Focus doesn't move to the symbol in terminal accessible view,"
Type: <b>Bug</b>

1. Open terminal in editor area.

1. Type `echo hello` and hit enter.

1. Type `echo world` and hit enter

1. Press alt+F2 to open accessible view

1. Press ctrl+shift+O to open symbol list and select `echo hello` and hit enter

* Note: the focus doesn't move to `echo hello`

VS Code version: Code - Insiders 1.83.0-insider (109e1f8d8afb754ed31317f79937a44e98d5063b, 2023-09-25T10:41:26.295Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.71GB (6.53GB free)|
|Process Argv|--crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (91)</summary>

Extension|Author (truncated)|Version
---|---|---
android-dev-ext|ade|1.3.2
aiprm-lang|AIP|0.0.2
Bookmarks|ale|13.4.1
openscad|Ant|1.2.1
spellright|ban|3.0.118
zoterolatex|bna|0.4.1
mermaid-markdown-syntax-highlighting|bpr|1.5.2
doxdocgen|csc|1.4.0
vscode-markdownlint|Dav|0.52.0
vscode-eslint|dba|2.4.2
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.23.1
gitlens|eam|14.3.0
EditorConfig|Edi|0.16.4
prettier-vscode|esb|10.1.0
vscode-google-translate|fun|1.4.13
codespaces|Git|1.15.3
copilot|Git|1.115.437
copilot-chat|Git|0.8.2023092501
remotehub|Git|0.60.0
vscode-github-actions|git|0.26.2
vscode-pull-request-github|Git|0.72.0
overleaf-workshop|iam|0.2.4
cslpreview|igo|0.2.2
easy-snippet|inu|0.6.3
path-autocomplete|ion|1.25.0
latex-workshop|Jam|9.14.0
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
zotero|mbl|0.1.10
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.1
black-formatter|ms-|2023.4.1
flake8|ms-|2023.6.0
isort|ms-|2023.11.12681021
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.20
jupyter|ms-|2023.8.1002501831
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.312.0
remote-ssh|ms-|0.106.4
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.4
vscode-remote-extensionpack|ms-|0.24.0
azure-repos|ms-|0.36.0
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.9.2117
powershell|ms-|2023.6.0
remote-repositories|ms-|0.38.1
vscode-github-issue-notebooks|ms-|0.0.129
vscode-selfhost-test-provider|ms-|0.3.18
vscode-serial-monitor|ms-|0.10.0
vsliveshare|ms-|1.0.5883
autodocstring|njp|0.6.1
pandocciter|not|0.10.3
shiny-python|Pos|0.1.4
shinyuieditor|pos|0.4.3
quarto|qua|1.100.0
r-debugger|RDe|0.5.4
java|red|1.22.1
vscode-xml|red|0.26.1
r|REd|2.8.1
multi-command|ryu|1.6.0
vscode-deepl|soe|1.0.6
abc-music|sof|0.4.0
lua|sum|3.7.0
latex-utilities|tec|0.4.10
cmake|twx|0.0.17
errorlens|use|3.13.0
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-arduino|vsc|0.6.0
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.40.0
vscode-maven|vsc|0.42.0
markdown-all-in-one|yzh|3.5.1
grammarly|znc|0.22.1

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012:30540252
pynewext54:30618038
a9j8j154:30646983
showlangstatbar:30737417
ecj1e332:30687743
pythonfmttext:30716741
fixshowwlkth:30771523
showindicator:30805243
pythongtdpath:30726887
i26e3531:30792625
welcomedialog:30812478
pythonnosmt12:30779711
pythonidxpt:30768918
pythonnoceb:30776497
copilotsettingt:30808721
asynctok:30821568
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206
pythonlinttype:30823781
pythonmpsinfo:30842935
dsvsc015:30821418
pythontestfixt:30826906
pythonfb280951:30830809
pythonregdiag:30842812

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-25 09:35:53,bug,Cannot read properties of undefined (reading 'id'),"* open a file from a pull request
* get an error saying id cannot be read from undefined


```
ERR Cannot read properties of undefined (reading 'id'): TypeError: Cannot read properties of undefined (reading 'id')
    at vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:765:140066
    at Object.h [as map] (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:12:10432)
    at h.next (<anonymous>)
    at P (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:7:2275)
    at r.setActions (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:765:140001)
    at d.Z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2412:20205)
    at h.value (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:2412:20897)
    at b.z (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:1902)
    at b.fire (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:2119)
    at b.resume (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:3188)
    at vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/workbench/workbench.desktop.main.js:87:3469
```"
microsoft/vscode,2023-09-22 08:28:04,bug,Cannot read properties of null (reading 'b'),"```
ERR [uncaught exception in main]: Cannot read properties of null (reading 'b'): TypeError: Cannot read properties of null (reading 'b')
    at h (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:103:24182)
    at d.value (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:33:35781)
    at $.z (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1913)
    at $.A (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1983)
    at $.fire (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:2199)
    at d.value (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:106:42564)
    at $.z (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1913)
    at $.fire (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:2130)
    at J.setReady (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:105:100714)
    at p.notifyReady (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:105:81450)
    at Object.call (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:42:5018)
    at I.s (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:40:5116)
    at I.q (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:40:4639)
    at d.value (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:40:4050)
    at $.z (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1913)
    at $.A (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1983)
    at $.fire (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:2199)
    at d.value (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:33:35891)
    at $.z (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1913)
    at $.fire (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:2130)
    at d.value (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:33:36099)
    at $.z (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:1913)
    at $.fire (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:35:2130)
    at me (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:33:38543)
    at IpcMainImpl.h (/Applications/Visual Studio Code - Insiders.app/Contents/Resources/app/out/vs/code/electron-main/main.js:36:96476)
    at IpcMainImpl.emit (node:events:513:28)
```"
microsoft/vscode,2023-09-21 17:55:36,bug,active editor has commenting range is true even when it should not be,"I don't have a PR checked out and yet, it's `true`



![Image](https://github.com/microsoft/vscode/assets/29464607/54f4bb9b-d128-40ee-90cd-a83ca34e4e60)



![Image](https://github.com/microsoft/vscode/assets/29464607/e241bb2f-ffaa-43ed-8b3d-c70e440c01dd)


"
microsoft/vscode,2023-09-21 14:03:26,bug,Toggling tab pin row setting hides actions for me,Going from multi-row tabs to single-row tabs shows no editor actions.
microsoft/vscode,2023-09-20 18:36:34,bug,SCM Sync view: focus lost from the tree on mouse click,"Nice view 👍 

Noticed that when you click on a file with the mouse, focus moves into the editor immediately. Maybe connect with @alexr00 and custom trees to learn how you can get opening behaviour that is consistent for free."
microsoft/vscode,2023-09-20 13:27:41,bug,Open editors view does not close editor that is selected anymore,"Type: <b>Bug</b>

On the right panel, where the opened files are listed, when you have a file selected and hover on another file it appears a close(x) icon, if you click that (x) the editor window that is closed is the file that is selected and not the file where you clicked (x). I keep repoen closed editor because of this recent issue.

VS Code version: Code 1.82.2 (Universal) (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:59:47.790Z)
OS version: Darwin arm64 22.1.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (12 x 24)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|3, 4, 5|
|Memory (System)|16.00GB (0.04GB free)|
|Process Argv|--crash-reporter-id a4b55f5e-5134-402c-85da-b605e22d73f5|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (61)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
alpine-js-intellisense|adr|1.2.0
laravel-extra-intellisense|ami|0.6.3
sidebar-markdown-notes|ass|1.2.0
laravel-blade-spacer|aus|2.1.3
laravel-docs|aus|1.10.0
tailwind-docs|aus|2.1.0
vscode-intelephense-client|bme|1.9.5
vscode-tailwindcss|bra|0.10.0
vscode-coloured-status-bar-problems|bra|0.2.0
vscode-better-align|cho|1.4.2
laravel-goto-view|cod|1.3.9
save-commands|dee|0.5.1
vscode-notes|dio|1.1.0
githistory|don|0.6.20
gitlens|eam|14.3.0
vscode-html-css|ecm|1.13.1
prettier-vscode|esb|10.1.0
restore-terminals|Eth|1.1.8
php-intellisense|fel|2.3.14
auto-rename-tag|for|0.1.10
code-runner|for|0.12.0
vscode-google-translate|fun|1.4.13
html-preview-vscode|geo|0.2.5
copilot|Git|1.113.423
copilot-chat|Git|0.7.1
copilot-labs|Git|0.15.1019
gitlab-workflow|Git|3.77.1
githd|hui|2.3.3
vscode-peacock|joh|4.2.2
vscode-inline-svg|kon|0.9.4
rainbow-csv|mec|3.7.1
php-namespace-resolver|Meh|1.1.9
git-graph|mhu|1.30.0
dotenv|mik|1.0.1
theme-monokai-pro-vscode|mon|1.2.1
empty-directory-extension|MRK|0.0.6
test-adapter-converter|ms-|0.1.8
resourcemonitor|mut|1.0.7
laravel-goto-components|nao|1.2.0
sftp|Nat|1.16.3
vscode-configurable-shortcuts|nor|1.0.3
laravel-blade|one|1.34.0
laravel5-snippets|one|1.17.0
material-icon-theme|PKi|4.30.1
vscode-thunder-client|ran|2.12.0
vscode-yaml|red|1.14.0
laravel-artisan|rya|0.0.31
vue-vscode-snippets|sdr|3.1.1
vscode-blade-formatter|shu|0.23.1
svg-preview|Sim|2.8.3
vscode-fileutils|sle|3.10.3
git-prefix|srm|1.3.1
workspace-explorer|tom|2.3.0
luna-paint|Tyr|0.16.0
remove-empty-lines|use|1.0.1
vscode-icons|vsc|12.5.0
volar|Vue|1.8.11
vscode-typescript-vue-plugin|Vue|1.8.11
vuetify-vscode|vue|0.2.0
php-debug|xde|1.33.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
89544117:30613380
showlangstatbar:30737416
0bi6i642:30835152
03d35959:30757346
ecj1e332:30736112
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-19 16:27:29,bug,Terminal PS1 double,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.2
- OS Version: Fedora 38 Workstation ediditon

### My bashrc :
```bash
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

# Branch name

parse_git_branch() {
    git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/'
}

# User specific environment
if ! [[ ""$PATH"" =~ ""$HOME/.local/bin:$HOME/bin:"" ]]
then
    PATH=""$HOME/.local/bin:$HOME/bin:$PATH""
fi
export PATH

PS1=""[\\u@\\h \\W]\\[\\e[91m\\]\\$(parse_git_branch)\\[\\e[00m\\]$ ""
# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=

# User specific aliases and functions
if [ -d ~/.bashrc.d ]; then
        for rc in ~/.bashrc.d/*; do
                if [ -f ""$rc"" ]; then
                        . ""$rc""
                fi
        done
fi

unset rc
```


Separate terminal/vscode.
Lower(vscode terminal) : folder and branch is updating in the both cases (left and right)
![image](https://github.com/microsoft/vscode/assets/115103276/7e19c79a-f996-401d-9a5d-be4996ee63f0)
![Screenshot from 2023-09-19 21-40-59](https://github.com/microsoft/vscode/assets/115103276/1d593092-96e7-499f-bc95-5bd450b49578)
![Screenshot from 2023-09-19 21-41-04](https://github.com/microsoft/vscode/assets/115103276/e7a0c3ca-a0c5-4d36-b46c-36739f96b4b6)
"
microsoft/vscode,2023-09-19 14:54:31,bug,"Tasks using `""runOn"": ""folderOpen""` are broken","* have a task with `""runOn"": ""folderOpen""`, like https://github.com/microsoft/vscode-github-issue-notebooks/blob/e017704a5eb4bb6080322a3d489c76d421089111/.vscode/tasks.json#L18
* make sure it ran once
* reload/reopen folder
* 🐛 task doesn't run"
microsoft/vscode,2023-09-19 11:10:15,bug,[Accessibility] Display terminal-specific help in Accessible View for terminal buffer,"
Type: <b>Bug</b>

Currently, when alt+f1 is pressed in the accessible terminal buffer, it displays the following info:

> In the accessible view, you can:
> - Show the next (Alt+]) or previous (Alt+[) item
> - Navigate to the toolbar (Shift+Tab))

The above instruction is not applicable for the terminal buffer, and the original terminal buffer help content needs to be displayed here.


VS Code version: Code - Insiders 1.83.0-insider (7c7f7eee860e299499a3bd2915ad716f09f2d6a6, 2023-09-19T08:56:35.775Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.71GB (6.06GB free)|
|Process Argv|C:\\\\Users\\\\jseo1005\\\\OneDrive - University of Illinois - Urbana\\\\Desktop\\\\source.R --crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (91)</summary>

Extension|Author (truncated)|Version
---|---|---
android-dev-ext|ade|1.3.2
aiprm-lang|AIP|0.0.2
Bookmarks|ale|13.4.1
openscad|Ant|1.2.1
spellright|ban|3.0.118
zoterolatex|bna|0.4.1
mermaid-markdown-syntax-highlighting|bpr|1.5.2
doxdocgen|csc|1.4.0
vscode-markdownlint|Dav|0.51.0
vscode-eslint|dba|2.4.2
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.22.0
gitlens|eam|14.3.0
EditorConfig|Edi|0.16.4
prettier-vscode|esb|10.1.0
vscode-google-translate|fun|1.4.13
codespaces|Git|1.15.3
copilot|Git|1.112.422
copilot-chat|Git|0.8.2023091901
remotehub|Git|0.60.0
vscode-github-actions|git|0.26.2
vscode-pull-request-github|Git|0.72.0
overleaf-workshop|iam|0.1.5
cslpreview|igo|0.2.2
easy-snippet|inu|0.6.3
path-autocomplete|ion|1.25.0
latex-workshop|Jam|9.13.4
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
zotero|mbl|0.1.10
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.0
black-formatter|ms-|2023.4.1
flake8|ms-|2023.6.0
isort|ms-|2023.11.12061012
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.10
jupyter|ms-|2023.8.1002501831
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.311.0
remote-ssh|ms-|0.106.4
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.3
vscode-remote-extensionpack|ms-|0.24.0
azure-repos|ms-|0.36.0
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.9.1317
powershell|ms-|2023.6.0
remote-repositories|ms-|0.38.1
vscode-github-issue-notebooks|ms-|0.0.129
vscode-selfhost-test-provider|ms-|0.3.18
vscode-serial-monitor|ms-|0.10.0
vsliveshare|ms-|1.0.5883
autodocstring|njp|0.6.1
pandocciter|not|0.10.3
shiny-python|Pos|0.1.4
shinyuieditor|pos|0.4.3
quarto|qua|1.98.0
r-debugger|RDe|0.5.4
java|red|1.22.1
vscode-xml|red|0.26.1
r|REd|2.8.1
multi-command|ryu|1.6.0
vscode-deepl|soe|1.0.6
abc-music|sof|0.4.0
lua|sum|3.7.0
latex-utilities|tec|0.4.10
cmake|twx|0.0.17
errorlens|use|3.13.0
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-arduino|vsc|0.6.0
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
markdown-all-in-one|yzh|3.5.1
grammarly|znc|0.22.1

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012:30540252
pynewext54:30618038
a9j8j154:30646983
showlangstatbar:30737417
ecj1e332:30687743
pythonfmttext:30716741
fixshowwlkth:30771523
showindicator:30805243
pythongtdpath:30726887
i26e3531:30792625
welcomedialog:30812478
pythonnosmt12:30779711
pythonidxpt:30768918
pythonnoceb:30776497
copilotsettingt:30808721
asynctok:30821568
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206
pythonlinttype:30823781
pythonmpsinfo:30815194
dsvsc015:30821418
pythontestfixt:30826906
pythonfb280951:30830809

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-19 01:19:55,bug,"Extension development：Frequent calls to _onDidChangeTreeData.fire(), resulting in memory leakage","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: V1.81.1
- OS Version: Linux kkuser-virtual-machine 5.15.0-83-generic #92~20.04.1-Ubuntu SMP Mon Aug 21 14:00:49 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

Steps to Reproduce:

1. There are constant changes in attribute states in the treeview
2. Frequent calls to this._onDidChangeTreeData.fire() functions result in memory leakage
"
microsoft/vscode,2023-09-18 13:32:35,bug,Bad Diff,"Notice the confusing import:

![Image](https://github.com/microsoft/vscode/assets/2931520/714c15c5-801d-4a48-90b0-8801263fac66)


[Monaco Editor Repro](https://microsoft.github.io/monaco-editor/playground.html?source=v0.44.0-dev-20230918#XQAAAAKuAwAAAAAAAABBqQkHQ5NjdMjwa-jY7SIQ9S7DNlzs5W-mwj0fe1ZCDRFc9ws9XQE0SJE1jc2VKxhaLFIw9vEWSxW3yscw4m0lEfGetoVyYgiB3WIAyGCL4buFPFOLc1lAy95RUKo06z2t3AsctLSHAcjORfYRN1yg400RgiDpSRk10aTwvQv6zU0xdTpaj6zSTK7O5w1Odpy7ndjoSj3ma3YVF3_FmBOgOwiopFHNNBPqMbq-v0RvCX2wcDNfJrORPwG1VK7vtKAvoDnL6nqm0qcLeCox2Vcd-zngBrgzSqiTgEXQEGrD3g7gonOqEAOByAGd_oCugkOt3ao7jXzQd3w4Fy-4lLimVF3_GvmNjyR7YPne-K0XCQvOZEr9A5yeBu954aGEYPtU0P7tS7ZI7XdPVWp_7Vx2LCz8OoMuuTAYZTfq_ZGyKuBTW_m2-yc_3ERGu8lnSsV9_kZXkpX9zk4UDmZemygf6VkJ5wo45Sstklpfb8Z8bTlqETeOfyLOVckLmu1MlZ26HwaiabpFkfqVf5VDfHsrfXn5RGiDRliyDdZrtECrEiRwdjhhRx-4_z-Exv_uPW5o)"
microsoft/vscode,2023-09-17 20:40:50,bug,Terminal context menu not hiding after running action,"<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: insiders and from sources

Steps to Reproduce:

1. Create terminal
2. Right click to show context menu
3. Run `Copy` or `Clear` action
4. :bug: Context menu still visible

Regression from https://github.com/microsoft/vscode/pull/192809

cc @Tyriar 
"
microsoft/vscode,2023-09-15 18:04:59,bug,[Accessibility] accessible-buffer is not auto-focused,"
Type: <b>Bug</b>

I think there was an inadvertent regression in recent accessibility patches. {""terminal.integrated.focusAfterRun"": ""accessible-buffer""} setting does not take  any effect. Tested on Windows with NVDA and JAWS. The focus does not move to the accessible buffer.

VS Code version: Code - Insiders 1.83.0-insider (bccfade64adb249f57c8fcf03cba41609f76ce5c, 2023-09-15T05:35:16.508Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.71GB (6.19GB free)|
|Process Argv|C:\\\\Users\\\\jseo1005\\\\OneDrive - University of Illinois - Urbana\\\\Desktop\\\\source.py --crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (90)</summary>

Extension|Author (truncated)|Version
---|---|---
android-dev-ext|ade|1.3.2
aiprm-lang|AIP|0.0.2
Bookmarks|ale|13.4.1
openscad|Ant|1.2.1
spellright|ban|3.0.118
zoterolatex|bna|0.4.1
mermaid-markdown-syntax-highlighting|bpr|1.5.2
doxdocgen|csc|1.4.0
vscode-markdownlint|Dav|0.51.0
vscode-eslint|dba|2.4.2
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.22.0
gitlens|eam|14.3.0
EditorConfig|Edi|0.16.4
prettier-vscode|esb|10.1.0
vscode-google-translate|fun|1.4.13
codespaces|Git|1.15.2
copilot|Git|1.111.414
copilot-chat|Git|0.8.2023091501
remotehub|Git|0.60.0
vscode-github-actions|git|0.26.2
vscode-pull-request-github|Git|0.72.0
cslpreview|igo|0.2.2
easy-snippet|inu|0.6.3
path-autocomplete|ion|1.25.0
latex-workshop|Jam|9.13.4
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
zotero|mbl|0.1.10
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.0
black-formatter|ms-|2023.4.1
flake8|ms-|2023.6.0
isort|ms-|2023.11.12061012
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.10
jupyter|ms-|2023.8.1002501831
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.311.0
remote-ssh|ms-|0.106.4
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.3
vscode-remote-extensionpack|ms-|0.24.0
azure-repos|ms-|0.36.0
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.9.1317
powershell|ms-|2023.6.0
remote-repositories|ms-|0.38.1
vscode-github-issue-notebooks|ms-|0.0.129
vscode-selfhost-test-provider|ms-|0.3.18
vscode-serial-monitor|ms-|0.10.0
vsliveshare|ms-|1.0.5883
autodocstring|njp|0.6.1
pandocciter|not|0.10.3
shiny-python|Pos|0.1.4
shinyuieditor|pos|0.4.3
quarto|qua|1.98.0
r-debugger|RDe|0.5.4
java|red|1.22.1
vscode-xml|red|0.26.1
r|REd|2.8.1
multi-command|ryu|1.6.0
vscode-deepl|soe|1.0.6
abc-music|sof|0.4.0
lua|sum|3.7.0
latex-utilities|tec|0.4.10
cmake|twx|0.0.17
errorlens|use|3.13.0
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-arduino|vsc|0.6.0
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
markdown-all-in-one|yzh|3.5.1
grammarly|znc|0.22.1

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012:30540252
pynewext54:30618038
a9j8j154:30646983
showlangstatbar:30737417
ecj1e332:30687743
pythonfmttext:30716741
fixshowwlkth:30771523
showindicator:30805243
pythongtdpath:30726887
i26e3531:30792625
welcomedialog:30812478
pythonnosmt12:30779711
pythonidxpt:30768918
pythonnoceb:30776497
copilotsettingt:30808721
asynctok:30821568
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206
pythonlinttype:30823781
pythonmpsinfo:30815194
dsvsc015:30821418
pythontestfixt:30826906
pythonfb280951:30830809

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-15 10:43:39,bug,TypeError: Cannot read properties of undefined (reading 'trim'),"The diff editor was actually unable to compute the diff and just hangs. 

* original file: 

[original.txt](https://github.com/microsoft/vscode/files/12618480/original.txt)


* modified file: 



[modified.txt](https://github.com/microsoft/vscode/files/12618481/modified.txt)



```
ERR Cannot read properties of undefined (reading 'trim'): TypeError: Cannot read properties of undefined (reading 'trim')
    at $ (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:37:3518)
    at e (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:37:3086)
    at b (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:36:3677)
    at E.h (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:37:6960)
    at E.computeDiff (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:37:6268)
    at v.l (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:41:59363)
    at v.computeDiff (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:41:59226)
    at i.d (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:41:45244)
    at Object.handleMessage (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:41:44965)
    at A.k (vscode-file://vscode-app/Applications/Visual%20Studio%20Code%20-%20Insiders.app/Contents/Resources/app/out/vs/base/worker/workerMain.js#editorWorkerService:41:42344)
```"
microsoft/vscode,2023-09-14 18:48:12,bug,Top padding is wrong on comment view zone,"Notice the hover feedback is cut off vertically and should have additional spacing.

![Image](https://github.com/microsoft/vscode/assets/2193314/0602b10f-9761-48f6-a320-02ca7b57a4e0)

Setup:

1. Korean language pack (may impact?)
2. `""window.zoomLevel"": 2

It also happens with other zoom levels:



![Image](https://github.com/microsoft/vscode/assets/2193314/f9ebf622-f4fd-49db-8ec6-f790c25f7ea2)

"
microsoft/vscode,2023-09-14 09:34:02,bug,Sash variables end up on the HTML element,"I would have expected maybe the workbench element if this needs to be global?

![Image](https://github.com/microsoft/vscode/assets/900690/345163aa-9713-4525-bcd5-6ffbeb7bb5bb)

"
microsoft/vscode,2023-09-13 16:33:54,bug,F9 removes existing breakpoint instead of adding one,"- Add a breakpoint on line 1
- Move cursor to line 2
- Press F9- removes the breakpoint on line 1 instead of adding one on line 2


@connor4312 this is from https://github.com/microsoft/vscode/pull/192483/files#diff-46dcc835c545807c00c0fc7a1797c010f84d51fac120e4205fb68cf114c3d9fdR84-R86, why did that change?"
microsoft/vscode,2023-09-13 00:47:57,bug,code CLI update mechanism previous version renaming anomaly ,"- VS Code Version: 1.82
- OS Version: Linux

When the vscode CLI update mechanism updates the code binary, the process preserves the outdated version by appending the executable with .old.  At present, the process appends a second period, resulting in code..old.  This occurs through `code update` or when it is updated through the Notification Pop-Up when Connected to Tunnel on vscode.dev 


```
localhost /usr/local/bin # ls -la code*
-rwxr-xr-x. 1 root root 17981560 Sep 12 16:59 code
-rwxr-xr-x. 1 root root 17981560 Sep  8 04:59 code..old
localhost /usr/local/bin # ./code --version
code 1.82.1 (commit 6509174151d557a81c9d0b5f8a5a1e9274db5585)
localhost /usr/local/bin # ./code..old --version
code 1.82.0 (commit 8b617bd08fd9e3fc94d14adb8d358b56e3f72314)
```


Steps to Reproduce:

1.  Invokde code CLI update
2. Examine path where code CLI lives"
microsoft/vscode,2023-09-12 20:44:29,bug,Unbounded keybindings are presented in empty workbench hints,"Find in Files and Show Settings provide no value, just noise:



![Image](https://github.com/microsoft/vscode/assets/2193314/d5b0a683-7fb9-4569-a794-7eafd2aea8a7)

Not sure who owns this, I remember Christof but maybe not?"
microsoft/vscode,2023-09-12 16:44:00,bug,quick search: preserveInput priority over selected text,"We added support for auto-populating selected text to quick search (https://github.com/microsoft/vscode/issues/191513), but setting `preserveInput` overrides this.

This is because `preserveInput` takes precedence over any default string (even if `defaultFilterValue` is defined).

https://github.com/microsoft/vscode/blob/26f41a49948c6da4eec5c93836c1c331364b54f2/src/vs/platform/quickinput/browser/quickAccess.ts#L83-L87

(from https://github.com/microsoft/vscode/pull/191956)"
microsoft/vscode,2023-09-12 13:58:53,bug,"When re-requesting a copilot inline chat answer, pressing escape accepts the current solution","* trigger inline chat and send a command
* press the reload button in the chat
* press escape
* notice that it keeps the last result (and does not restore the initial document)

When I don't click the reload button, pressing escape restores the initial document.

![Image](https://github.com/microsoft/vscode/assets/2931520/7b712edf-55f9-4edd-adb3-9c21107c8713)

Context:
```ts
// Remove short suffixes/prefixes
	for (let i = 0; i < diffs.length; i++) {
		const cur = diffs[i];

		let newDiff = cur;

		const fullRange1 = sequence1.extendToFullLines(cur.seq1Range);
		const prefix = sequence1.getText(new OffsetRange(fullRange1.start, cur.seq1Range.start));
		if (prefix.length > 0 && prefix.trim().length <= 3 && cur.seq1Range.length + cur.seq2Range.length > 100) {
			newDiff = newDiff.deltaStart(-prefix.length);
		}

		const suffix = sequence1.getText(new OffsetRange(cur.seq1Range.endExclusive, fullRange1.endExclusive));
		if (suffix.length > 0 && (suffix.trim().length <= 3 && cur.seq1Range.length + cur.seq2Range.length > 100)) {
			newDiff = newDiff.deltaEnd(suffix.length);
		}

		while (true) {
			const prevDiff = lastOrDefault(newDiffs);
			if (prevDiff) {
				if (newDiff.intersectsOrTouches(prevDiff)) {
					newDiff = newDiff.join(prevDiff);
					newDiffs.pop();
					continue;
				}
			}
			break;
		}
		
		newDiffs.push(newDiff);
	}
```"
microsoft/vscode,2023-09-11 19:07:30,bug,go to symbol in the terminal's accessible view doesn't contain all commands after the first invocation,"1. with screen reader mode enabled, run some commands in the terminal
2. `ctrl/cmd+up arrow` to open the accessible view
3. `ctrl/cmd+shift+o` to go to symbol
4. ✅  the commands are there
5. `Escape` then `ctrl/cmd+shift+o` again
6. 🐛 some commands are missing (only the most recent one is there)"
microsoft/vscode,2023-09-11 14:55:26,bug,Test runner hangs with global `teardown` throwing,"Steps to Reproduce:

1. `git co ben/eventual-earthworm` or make sure https://github.com/microsoft/vscode/pull/192774 has landed
2. in `test/unit/electron/renderer.js` make sure to make `_allowedTestsWithUnhandledRejections` and empty `Set`
3. open `src/vs/workbench/services/lifecycle/test/electron-sandbox/lifecycleService.test.ts`
4. click on `suite('Lifecycleservice...` for running the suite

=> 🐛 the suite never finishes



![Image](https://github.com/microsoft/vscode/assets/900690/db3f786c-9edb-4cbb-9600-61483d956a48)

![Recording 2023-09-11 at 16 56 17](https://github.com/microsoft/vscode/assets/900690/ade08c9e-7e69-4190-ac3a-0090bd01ebc7)

"
microsoft/vscode,2023-09-09 16:05:16,bug,Problems with minimumContrastRatio inverse/selection edge cases,"Upstream: https://github.com/xtermjs/xterm.js/issues/4759

To verify:

1. On linux/macOS/wsl
2. Run `echo 'normal \\x1b[7minverse\\x1b[0m'`
3. Test various values of `terminal.integrated.minimumContrastRatio` (eg. 1, 4.5, 10) first with no selection and second with a selection. You may need to change your theme to see the differences"
microsoft/vscode,2023-09-09 12:01:13,bug,Terminal: Invisible text is visible in the DOM renderer,"Upstream: https://github.com/xtermjs/xterm.js/issues/4758

Repro:

1. On Linux/macOS/WSL
2. Run `echo -e '\\x1b[8minvisible'`, it should not show invisible in the output"
microsoft/vscode,2023-09-09 06:38:13,bug,Debug console is not working,"Type: <b>Bug</b>

After updating yesterday, I encountered an issue with my VS code where I set a breakpoint and attempted to enter a variable in the debug console, but the console remained empty.

VS Code version: Code 1.82.0 (8b617bd08fd9e3fc94d14adb8d358b56e3f72314, 2023-09-06T22:07:07.438Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.79GB (3.02GB free)|
|Process Argv|--file-uri file:///d%3A/Work/InstachatAI/InstachatAIApi.code-workspace --crash-reporter-id 6da24a21-c0cd-41c8-b3e7-ed9a77b716df|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (52)</summary>

Extension|Author (truncated)|Version
---|---|---
codesnap|adp|1.3.4
TabOut|alb|0.2.2
Bookmarks|ale|13.4.1
ng-template|Ang|16.1.8
vscode-django|bat|1.10.0
vscode-opennewinstance|chr|0.0.12
fastapi-snippets|dam|0.0.2
dart-code|Dar|3.72.2
flutter|Dar|3.72.0
gitlens|eam|14.3.0
prettier-vscode|esb|10.1.0
remotehub|Git|0.60.0
vscode-pull-request-github|Git|0.72.0
gc-excelviewer|Gra|4.2.58
todo-tree|Gru|0.0.226
vscode-drawio|hed|1.6.6
git-graph|mhu|1.30.0
dotenv|mik|1.0.1
vscode-docker|ms-|1.26.0
csharp|ms-|2.1.2
vscode-dotnet-runtime|ms-|1.7.2
vscode-edge-devtools|ms-|2.1.3
autopep8|ms-|2023.6.0
isort|ms-|2023.10.1
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.10
jupyter|ms-|2023.8.1002501831
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.309.0
remote-ssh|ms-|0.106.2
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.2
vscode-remote-extensionpack|ms-|0.24.0
remote-explorer|ms-|0.4.1
remote-repositories|ms-|0.38.1
remote-server|ms-|1.4.3
vsliveshare|ms-|1.0.5883
autodocstring|njp|0.6.1
material-icon-theme|PKi|4.30.1
material-product-icons|PKi|1.6.0
sqlite-viewer|qwt|0.3.13
vscode-thunder-client|ran|2.11.4
vscode-yaml|red|1.14.0
errorlens|use|3.13.0
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscodeintellicode-completions|Vis|1.0.22
change-case|wma|1.0.0
material-theme|zhu|3.16.0

(3 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627cf:30244335
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931:30280409
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
f6dab269:30613381
2i9eh265:30646982
showlangstatbar:30737416
a2ce3375:30757347
7ij38806:30736111
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-08 20:54:02,bug,Port forwarding for remote-ssh doesn't notice closed socket on server side,"Type: <b>Bug</b>

Since the 1.82.0 update, using port forwards for remote SSH development has been extremely difficult for me, as the port forwarding system doesn't seem to notice when a socket is closed on the server side.

This leads to my client application running on the local side (generally Chrome) sending packets (e.g. GET requests) into the port forward (which shows up in `netstat` as `ESTABLISHED` still from chrome to Code, but those packets seem to be dropped into the bit bucket as the socket from the vscode-server to my application is closed (in `TIME_WAIT`) on the server (remote development machine). This causes HTTP requests to time out (very ... verry ......... sloooooooooowly) which severely hampers being able to get anything done.

VS Code version: Code 1.82.0 (8b617bd08fd9e3fc94d14adb8d358b56e3f72314, 2023-09-06T22:07:18.759Z)
OS version: Linux x64 6.2.0-32-generic
Modes:
Remote OS version: Linux x64 5.10.0-25-cloud-amd64

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-9700 CPU @ 3.00GHz (8 x 900)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off|
|Load (avg)|1, 1, 1|
|Memory (System)|31.18GB (22.54GB free)|
|Process Argv|--unity-launch --crash-reporter-id f628f52a-51ea-40c8-aedd-0af9d4a6bd9f|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu-xorg|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu-xorg|
|XDG_SESSION_TYPE|x11|

|Item|Value|
|---|---|
|Remote|SSH: gdm.mgabeler-lee|
|OS|Linux x64 5.10.0-25-cloud-amd64|
|CPUs|AMD EPYC 7B13 (16 x 2449)|
|Memory (System)|31.36GB (17.60GB free)|
|VM|0%|
</details><details><summary>Extensions (26)</summary>

Extension|Author (truncated)|Version
---|---|---
Bookmarks|ale|13.4.1
vscode-peacock|joh|4.2.2
remote-ssh|ms-|0.106.1
remote-ssh-edit|ms-|0.86.0
remote-explorer|ms-|0.4.1
rewrap|stk|1.16.3
errorlens|use|3.13.0
github-markdown-preview|bie|0.3.0
markdown-checkbox|bie|0.4.0
markdown-emoji|bie|0.3.0
markdown-footnotes|bie|0.1.1
markdown-mermaid|bie|1.19.0
markdown-preview-github-styles|bie|2.0.2
markdown-yaml-preamble|bie|0.1.0
vscode-markdownlint|Dav|0.51.0
vscode-eslint|dba|2.4.2
EditorConfig|Edi|0.16.4
vscode-typescript-exportallmodules|eli|2.6.0
prettier-vscode|esb|10.1.0
terraform|has|2.27.2
bash-ide-vscode|mad|1.39.0
uuid-generator|net|0.0.5
vscode-commons|red|0.0.6
vscode-yaml|red|1.14.0
vscode-workspace-switcher|sad|1.15.3
code-spell-checker|str|3.0.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
3biah626:30602489
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-08 18:43:50,bug,"""Press ctrl+I to ask Copilot ...` message showing in empty output channel","Steps:
1. Open an empty output channel. To do this, you can open an HTML file from a fresh reload and then check the HTML language server output channel.
2. The output channel says ""Press `CTRL`+`I` to ask Copilot to do something. 

![Image](https://github.com/microsoft/vscode/assets/31675041/129778b6-a686-4336-9ce2-9347c09bd0da)
"
microsoft/vscode,2023-09-08 14:56:51,bug,Disposable leaks cause unit test failures,"Build: https://dev.azure.com/monacotools/a6d41577-0fa3-498e-af22-257312ff0545/_build/results?buildId=231184
Changes: https://github.com/Microsoft/vscode/compare/8971983...35c2ce3

```

  (shared with 1/2 leaks) at $Fg (out-build/vs/base/common/async.js:389:16)
  (shared with 1/2 leaks) at /mnt/vss/_work/1/s/out-build/vs/base/common/async.js:387:33
  (shared with 1/2 leaks) at $sg (out-build/vs/base/common/async.js:15:26)
      - stacktraces of 1 other leaks continue with $sg (out-build/vs/base/common/async.js:16:25)
  (shared with 2/2 leaks) at Object.$Fg (out-build/vs/base/common/async.js:387:20)
  (shared with 2/2 leaks) at Context.<anonymous> (out-build/vs/base/test/common/async.test.js:613:45)
  (shared with 2/2 leaks) at process.processImmediate (node:internal/timers:476:21)
  ============================================================
  
  
  
  
  ==================== Leaking disposable 2/2: Object ====================
  (shared with 1/2 leaks) at $8S.trackDisposable (out-build/vs/base/test/common/utils.js:56:21)
  (shared with 1/2 leaks) at $db (out-build/vs/base/common/lifecycle.js:58:28)
  (shared with 1/2 leaks) at $kb (out-build/vs/base/common/lifecycle.js:143:22)
  (shared with 1/2 leaks) at MutableToken.q (out-build/vs/base/common/event.js:822:52)
  (shared with 1/2 leaks) at /mnt/vss/_work/1/s/out-build/vs/base/common/async.js:17:47
  (shared with 1/2 leaks) at new Promise (<anonymous>)
  (shared with 1/2 leaks) at $sg (out-build/vs/base/common/async.js:16:25)
      - stacktraces of 1 other leaks continue with $sg (out-build/vs/base/common/async.js:15:26)
  (shared with 2/2 leaks) at Object.$Fg (out-build/vs/base/common/async.js:387:20)
  (shared with 2/2 leaks) at Context.<anonymous> (out-build/vs/base/test/common/async.test.js:613:45)
  (shared with 2/2 leaks) at process.processImmediate (node:internal/timers:476:21)
  ============================================================
  
  
      at $8S.ensureNoLeakingDisposables (out-build/vs/base/test/common/utils.js:145:19)
      at Context.<anonymous> (out-build/vs/base/test/common/utils.js:170:25)
      at process.processImmediate (node:internal/timers:476:21)

```"
microsoft/vscode,2023-09-08 08:22:20,bug,Sticky scroll: folding icons missing from some sticky headers,"Type: <b>Bug</b>

> Issue troubleshooting has identified that the issue is with Visual Studio Code - Insiders. 

If the sticky scroll header shows only one line, that line has no folding indicator.

If it shows 2 or more lines, only the last line has the indicator.

![junk](https://github.com/microsoft/vscode/assets/6726799/e76de0d7-e8f0-4b86-bea5-4b96c3bdb4d2)

This is working correctly in 1.82.0 Stable.

VS Code version: Code - Insiders 1.83.0-insider (5a400e53e985dc5e24f6ee574b07ab23943841c5, 2023-09-07T17:35:42.740Z)
OS version: Windows_NT x64 10.0.22621
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-08 06:03:56,bug,1.82 cannot select text in integrated terminal,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.0
- OS Version: Ubuntu 22.04

Steps to Reproduce:

1. Try to select text in the integrated terminal by shift + select text. Doesn't work.
2. Last working in 1.81.*
"
microsoft/vscode,2023-09-07 22:33:02,bug,Tab separator setting refers to the same setting twice,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.0
- OS Version: Windows 10

Steps to Reproduce:

1. Go to settings
2. Find `terminal.integrated.tabs.separator`
3. It refers to one setting twice, it should refer to `title` and `description`:
![image](https://github.com/microsoft/vscode/assets/3606072/4ffa9275-d8ab-4d1f-bd73-2b175d108b58)

"
microsoft/vscode,2023-09-07 19:58:51,bug,VSCode warning 64 bit Windows users about deprecated 32 bit Windows support,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.0
- OS Version: Windows 10 Pro - 64 bit

My VSCode just updated this morning to 1.82.0 and now it's warning me that I won't be able to use it on Windows 32 bit anymore. Except I'm _not_ using 32 bit Windows, I'm using 64 bit Windows.

![image](https://github.com/microsoft/vscode/assets/298883/c6898685-2868-4345-bf26-7045c383acc9)

![image](https://github.com/microsoft/vscode/assets/298883/b1ef7306-b901-4f22-bf7f-603d598a80c6)
![image](https://github.com/microsoft/vscode/assets/298883/938e717f-ddc3-4692-8ba6-86ba072801c5)
![image](https://github.com/microsoft/vscode/assets/298883/49e4bbdc-4f72-4515-9279-172ee4f4e590)
"
microsoft/vscode,2023-09-07 15:40:03,bug,Why doesn't reflow work in test view output?,"When I resize this down and back up again:



![Image](https://github.com/microsoft/vscode/assets/2193314/1d75b734-3f0a-4a83-b9a9-f036e4693fa7)


I see this:



![Image](https://github.com/microsoft/vscode/assets/2193314/40421dee-e3a6-4344-9619-a87e70956bed)

Why isn't reflow working here?

Does the terminal not have scrollback by chance as it will be disabled if so:

https://github.com/xtermjs/xterm.js/blob/19c760a20470c26666efad770fe82496c4a9bf1e/src/common/buffer/Buffer.ts#L292-L294"
microsoft/vscode,2023-09-07 13:12:58,bug,Editor scrollbar markers update only after cursor move,"1. In a TS file, use the keyboard and type out a syntax error.

🐛 The scrollbar error marker doesn't show up.

2. Move the text cursor to another location using the mouse. The scrollbar error marker shows up then.


https://github.com/microsoft/vscode/assets/22350/ce7eea5f-7c2d-4bc9-ab40-f6760c05a09a



@hediet says:

> I think you need eslint [to repro]"
microsoft/vscode,2023-09-07 12:55:24,bug,Web: cannot right click into custom title,"Steps to Reproduce:

1. open https://insiders.vscode.dev/ 
2. notice the custom title shows by default (probably because of command center enabled by default?)
3. right click into empty space of custom title

=> 🐛 the menu does not open (or quickly closes)



![Image](https://github.com/microsoft/vscode/assets/900690/d7eadf67-588e-43c9-8601-7e967cc227c5)


"
microsoft/vscode,2023-09-07 04:43:00,bug,Forders not opening,"
Type: <b>Bug</b>

When I open new window VS Code, showing me ""Recent"" part. In this part has a menu More.... When I press More... menu nothing is happend. This problem has been for 2-3 month

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.65GB (8.85GB free)|
|Process Argv|--crash-reporter-id ff9a8db9-c9cf-42a6-a0e2-5dc8ac461d11|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (13)</summary>

Extension|Author (truncated)|Version
---|---|---
vite|ant|0.2.5
vscode-apollo|apo|1.19.11
vscode-eslint|dba|2.4.2
gitlens|eam|14.2.1
prettier-vscode|esb|10.1.0
vscode-highlight|fab|1.9.0
vue-snippets|hol|1.0.4
graphql|mqu|0.1.2
vetur|oct|0.37.3
material-icon-theme|PKi|4.30.1
tabnine-vscode|Tab|3.8.13
volar|Vue|1.8.10
vscode-typescript-vue-plugin|Vue|1.8.10

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vscaac:30438847
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
282f8724:30602487
f6dab269:30613381
2i9eh265:30646982
showlangstatbar:30737416
962ge761:30823813
a2ce3375:30757347
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-06 18:54:49,bug,Cannot read properties of null (reading 'uri'),"```javascript
TypeError: Cannot read properties of null (reading 'uri')
at b.D in src/vs/workbench/contrib/comments/browser/commentThreadZoneWidget.ts:222:24
at b.create in src/vs/editor/contrib/zoneWidget/browser/zoneWidget.ts:246:8
at new b in src/vs/workbench/contrib/comments/browser/commentThreadZoneWidget.ts:148:8
at f.j in src/vs/platform/instantiation/common/instantiationService.ts:119:18
at f.createInstance in src/vs/platform/instantiation/common/instantiationService.ts:85:18
at M.O in src/vs/workbench/contrib/comments/browser/commentsController.ts:749:48
at <anonymous> in src/vs/workbench/contrib/comments/browser/commentsController.ts:1002:10
at Array.forEach (<anonymous>)
at <anonymous> in src/vs/workbench/contrib/comments/browser/commentsController.ts:987:17
at Array.forEach (<anonymous>)
at M.Z in src/vs/workbench/contrib/comments/browser/commentsController.ts:983:22
at <anonymous> in src/vs/workbench/contrib/comments/browser/commentsController.ts:478:9
```
[Go to Errors Site](https://errors.code.visualstudio.com/card?ch=6c3e3dba23e8fadc360aed75ce363ba185c49794&bH=6d61ff0f-3543-017f-ba38-130325f5832c)"
microsoft/vscode,2023-09-06 17:04:47,bug,Notebook: run button is rendered when revealing cell into view when hitting breakpoint,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. Add a breakpoint in a cell
2. Debug the cell
3. Scroll the cell out of view
4. Reveal the cell
5. Now the run button is rendered, other than the stop button

Digging into this a bit and I found that the context key service/update is correct, the problem is we receive an execution `complete` message from the extension host right after initializing the debug session so the cell execution state is `idle` instead of `pending` or `executing`. That's why when we re-render the cell, the run button is rendered.
"
microsoft/vscode,2023-09-05 19:12:57,bug,code cli shouldn't panic if it can't bind a port,"```
❯ ./code-insiders serve-web
*
* Visual Studio Code Server
*
* By using the software, you agree to
* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and
* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).
*
Web UI available at http://127.0.0.1:8000?tkn=a1c7a8b1-bfc2-4b91-a60e-e18fac626dad
thread 'main' panicked at 'error binding to 127.0.0.1:8000: error creating server listener: Address in use (os error 98)', /home/cloudtest/.cargo/registry/src/index.crates.io-6f17d22bba15001f/hyper-0.14.26/src/server/server.rs:79:13
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
```

Probably should print an error message instead of panicking, and maybe do that before printing the URL to connect to."
microsoft/vscode,2023-09-05 09:36:05,bug,Accept button is not visible in inline chat,"* generate an answer
* tweak it
* press Send again (by accident)
* press Stop generating
* the Accept button is no longer visible


![Image](https://github.com/microsoft/vscode/assets/5047891/34ac892d-ed31-4f11-b10d-ee92fa3518d8)


```
Version: 1.82.0-insider
Commit: f1302be1e67e3af5fbeb8bbb2ea784de7bc96150
Date: 2023-09-01T11:13:22.523Z
Electron: 25.8.0
ElectronBuildId: 23503258
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Darwin arm64 22.6.0
```"
microsoft/vscode,2023-09-04 13:09:38,bug,Terminal: White bar on top of terminal,"Version: 1.82.0-insider (user setup)
Commit: f1302be1e67e3af5fbeb8bbb2ea784de7bc96150
Date: 2023-09-01T11:08:40.414Z
Electron: 25.8.0
ElectronBuildId: 23503258
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Windows_NT x64 10.0.22631

 A white bar is hiding the first characters of the last line:

![Recording 2023-09-04 at 15 18 55](https://github.com/microsoft/vscode/assets/6461412/cce87b8a-a1d6-4dc7-8d50-ded57b604a24)

![Image](https://github.com/microsoft/vscode/assets/6461412/ce15937d-78b0-40ef-a465-24c78d40bf38)

Last week I tested some terminal accessibility issues, using the terminal buffer, maybe that's related."
microsoft/vscode,2023-09-04 08:48:52,bug,Don't detect aligned code as moved,"Don't show non-moves:

![Image](https://github.com/microsoft/vscode/assets/2931520/6a80bb42-7bd8-433b-a41f-af99d44560f5)



[Monaco Editor Repro](https://microsoft.github.io/monaco-editor/playground.html?source=v0.42.0-dev-20230904#XQAAAAK3JAAAAAAAAABBqQkHQ5NjdMjwa-jY7SIQ9S7DNlzs5W-mwj0fe1ZCDRFc9ws9XQE0SJE1jc2VKxhaLFIw9vEWSxW3yscyIQRnzlomSBcA3EzPkMOWcvqy7fj-WpAGlaTunV0ezgt0-lrqbxDLBHp40VgoG1h0bYn33Z5W8JwKP8faa2U12FLGnWghxFOkfPhEOEevGj81IbTtL6Ld4LFL-hBvbp0k-XSoxa7IRE-EmrVD6gb_EMvQHtDePYSE1RfALToyLD1JPlY6IbqOY2EAdk26JxkYD43vdAVrCgTuddhyHQfardBF5ljU86TWzXewJ2H5jS-Hl4PNJF5i_cMbqKfevhmFDIbD7Q3aD6hChhhyJTcImz3ASkmLluTbYFygXKpXoeJp570vHkx0ZchqGGEetOXSI8h4cG__BgUbusCGIHKdewGYW3CHjfbY64q7cktl7OQyDLtb5hLRxlm-ULDNGqt06SYuMgqnavqrW42s273OUZIQ4bY7L25mPIKex9yRqTRidVwLB3BvFAk_3xnTVMzDlYLClhuy5EFaytECcEbyP8SOf6mdRcJ0Lf2eyUHaBXb7Xv6jrmlHVP3TDOaD5e1icb1_n_k03_G5C57d5FPlO6cIrShF4EwABjIBb_rECvlWibgSdyUk2zJ-nwakgt_GtTI313hO6WopDQIPsF03LR3ZbAgRCGLUOdNNQCUhbf99n8Saum4nYlYVf8F0e47zs3VbZdW_hyv0pUv28AMe7x-w1QtVBFAjISmFUnFTC7anISph8irYSdRKqM23pSXBBHk0__81vgwSUCPxAy72rTwTzv3QnCk0t4RtNhM1IBUHqfkoYuFycQw9Qcie-jdF4YuCQlPVcqWnrF8TGNz-WfeaucrXPo6cxXieTc6ynWHcNwkGPrIGM25Zdo04jzBDBpQNsh1cqXICLx0FDOqMe2PtRZO3QWYxsnWq71ZTdlx5jy4K_mm8DfQWL0-8QCWjpMJBCkgAbEhsc5HXwLFn54zblz162W1nEaQ1YUE-Uw1VI1XM-XPIUGQmXv85eTcF8eYg1QphNLpV98bek-0EBYtVKeCADi9F_y3EKr-JhbngkGsTIB7OEuW70Ma0M8MMRDWj65KgZEnVjXBnJEgn5RhUbMgWMfQsj1ADtB4mV85yGzAwqQr4HMFhBG6_-loXEbVRCfs2fA0l3OAL2nicZI3OnQ7SOWAM2IXvgN87UJ1xHrkHbre3v5q_iFvVgD3m0BGnWs9ZBR2ZLIhxg670IOHR3SSebcJhreoAcMRmE_zPrMa3UCZT4c5_tH-jFPK4Oa1GqiMWlAjKPJUlEZFr9j917M9wZ82jZ9y_pwZnW8sJGF_s6FGxtQ4D1KEbW1mZo6vYsZnH5U8X14N_6y2OzwLCIX-6ws1h8aj-3AiKoTc9mOsVpFshV6ZKIF0SsM5rjwgNhBf7CNkpvOhMreugXs7i1f5SSe2_iOWFWgx7Ix-VnBbHT-0co27hkT7pG2k_agfwJ0gRsNYrl5JfPT6eMSxhKEHGQxMF0GF6l3by8kTw7n485SiS1ErzSzklkoknQwi19izjQidVa1xFpMdfrL9dr0FRh6kGR6pHqOyob_iWUfYUuJJBJA6bsr27LiVI1P0GcsITTsMpyAsEW8RAIajvwLfQm0GVOBzbPcnRLGMaOvX5YYW59SrMadg6N-8sPLjf0mdmF7rwnEDLX01UERzzz0oZLdRcikQK8tI6EvStPe5S8SXtMuhMPmoeHj16fe0Z5IFW99GJr8_RGinXCk1RPhowmnSxua3IuJwJcXBR8pJrbzwcubfzqdCCeUd_p4myCNUMRJze8_TgPLlJWjEYyP0_d9JREaCx585qwtgNQkb1mvfj38sDw3HOh3ZxdfUiSxcBC_rc_nT5tsyYPDGtmFuO8h-GqDLAJ_aY0iC84cD3stwJ7VPZvs8UNPt5caJkaQ4ZPcVfE3LVZ_tCI0OHv1qydAaUm4Q1iRCtpxVwp1C9EjFnktxDny6vlAn4hRuzze4KjCpJ8RgYwaXOBW1RgdE2FasQj1B2L-dKVH7mVqbcp_ABkmRMz53hnmOltl0M4b84-DjSdIk3wQKAetegw8MPL-lMTEdQYwu5Zy0FOzFdpma-0u-v4lDCMiqU_gICJfT24GT6U1XSUZcX1Y46xK7II-tzPod52YdEYLBuWeKBs7u3vu5aDBJEflKqKtVvSlZd0UmyM0B94U3wwLvKj06qB4bILkylyuhL4gHzvyT6iab64Iagfqh7dpyQx43HNrgsMI-4vGPUuxKBH4r9WHLTV8ZN2gRwabvcfFzzC6n1e3LoX1SImXU9nFj01pDwhoqUwin-acYfFRQLs8c4UfU1-dXFRkEteaQafCsZv6gBLfDu758R8l7w7T8GsgjXYed-wb6mJ31ZCVIPYH28dPAfe2kzHH7f7QVUPaYBIfiA1agvKDyKJlWH_7soN6LGvCWejfWDT_N4_-Uod0Kvxz2A05CUgWMMdCZuv2teMzZSoApmvu2nR6UlLNEFeyCPhAUbq13oYR5fRc84dOXBqmM41MBMq3txeTGXOX0KPzc0qzuRQUE5qyfQnjImtoULFIdIxFEY9S92mqgsb9gn1Qu77LsDv1HuobqQcFqedNbGqxGq0MUrzThF_fRXy7vipPbXQsB6a0MzRyVdtk8C7CvnyBa5H9vzIX-2Z4IU5JS7FklNUJI8OJ2BbbXOv7MwogNh9s07aI_i6i2kxiN63G__-iEpEw)"
microsoft/vscode,2023-09-01 16:34:50,bug,Compress single test messages in the Test Results tree view,"              @connor4312 testing this out and it looks better to me - although is it the case that now there will always be only a single child in the tree below the test?

![image](https://github.com/microsoft/vscode/assets/1078012/996afa2d-231b-4e0e-9d42-aa1fa69deff0)

If so, could the output not be shown against the test node and the child removed? Currently it seems like clicking on the test name doesn't do anything (except maybe expand/collapse) which I find confusing. I feel like clicking on the test name and seeing the output (and it having no children) would be better - but I'm not sure if there are cases where there are more children.

Thanks!

_Originally posted by @DanTup in https://github.com/microsoft/vscode/issues/187104#issuecomment-1700873018_
            "
microsoft/vscode,2023-08-31 23:04:51,bug,SSH/DevContainer Port Forwarding broken,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
```
Version: 1.82.0-insider (system setup)
Commit: 3cd6f481266dcbd2ca2fcff43b4465d747c78e2f
Date: 2023-08-31T17:34:55.916Z
Electron: 25.7.0
ElectronBuildId: 23434598
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Windows_NT x64 10.0.22621
```

Issue similar to #190859 - this time it affects **Devcontainers** running on **remote ssh hosts** ⚠️ 

Client: Windows 11
Remote-Host: Ubuntu 22.04
Container: Debian 11
Repo for reproduction: https://github.com/gecio/gecio.github.io

Steps to Reproduce:

1. Connect to ssh host
2. Clone [Repo](https://github.com/gecio/gecio.github.io)
3. Open in devcontainer when asked
4. Launch Task ""Serve"" to start webserver
5. Open http://localhost:4000 and see indefinite loading.

I can't test it without the remote ssh host :/

<details><summary>Shared-Log</summary>
<p>

```
2023-09-01 00:51:07.658 [info] [SharedProcessTunnelService] Created tunnel 1: 127.0.0.1:64076 (local) to 127.0.0.1:7863 (remote).
2023-09-01 00:51:07.673 [info] [SharedProcessTunnelService] Created tunnel 2: 127.0.0.1:33761 (local) to 127.0.0.1:33761 (remote).
2023-09-01 00:51:07.833 [info] [SharedProcessTunnelService] Created tunnel 3: localhost:4000 (local) to localhost:4000 (remote).
2023-09-01 00:51:07.834 [info] [SharedProcessTunnelService] Created tunnel 4: localhost:35729 (local) to localhost:35729 (remote).
2023-09-01 00:51:09.216 [info] Getting Manifest... ms-vscode-remote.remote-containers
2023-09-01 00:51:09.313 [info] Installing extension: ms-vscode-remote.remote-containers
2023-09-01 00:51:10.103 [info] Extension signature is verified: ms-vscode-remote.remote-containers
2023-09-01 00:51:10.426 [info] Extracted extension to file:///c%3A/Users/max06/.vscode-insiders/extensions/ms-vscode-remote.remote-containers-0.308.0: ms-vscode-remote.remote-containers
2023-09-01 00:51:10.432 [info] Renamed to c:\\Users\\max06\\.vscode-insiders\\extensions\\ms-vscode-remote.remote-containers-0.308.0
2023-09-01 00:51:10.439 [info] Extracting extension completed. ms-vscode-remote.remote-containers
2023-09-01 00:51:10.463 [info] Extension installed successfully: ms-vscode-remote.remote-containers
2023-09-01 00:51:10.469 [info] Marked extension as uninstalled ms-vscode-remote.remote-containers-0.307.0
2023-09-01 00:51:12.628 [info] [SharedProcessTunnelService] Created tunnel 5: 127.0.0.1:42025 (local) to 127.0.0.1:42025 (remote).
2023-09-01 00:51:30.294 [info] Creating a socket (renderer-Tunnel-f2cc70c7-d306-4708-b542-3bfdd2cb513f)...
2023-09-01 00:51:30.416 [info] Creating a socket (renderer-Tunnel-f2cc70c7-d306-4708-b542-3bfdd2cb513f) was successful after 123 ms.
2023-09-01 00:51:30.957 [info] Creating a socket (renderer-Tunnel-57cbefc6-cb29-42ef-89ac-2ff5873ad23a)...
2023-09-01 00:51:31.049 [info] Creating a socket (renderer-Tunnel-57cbefc6-cb29-42ef-89ac-2ff5873ad23a) was successful after 93 ms.
2023-09-01 00:51:31.223 [info] Creating a socket (renderer-Tunnel-d3fcfdea-adbf-44f9-a6da-2d1089f1c7fd)...
2023-09-01 00:51:31.311 [info] Creating a socket (renderer-Tunnel-d3fcfdea-adbf-44f9-a6da-2d1089f1c7fd) was successful after 90 ms.
2023-09-01 00:56:31.298 [info] Creating a socket (renderer-Tunnel-ec10ac1a-bbec-46d3-bb84-2667f176bf2f)...
2023-09-01 00:56:31.400 [info] Creating a socket (renderer-Tunnel-ec10ac1a-bbec-46d3-bb84-2667f176bf2f) was successful after 101 ms.
2023-09-01 00:58:16.519 [info] Creating a socket (renderer-Tunnel-bebfb153-c346-414c-bdcf-e567fb8e8bf6)...
2023-09-01 00:58:16.623 [info] Creating a socket (renderer-Tunnel-bebfb153-c346-414c-bdcf-e567fb8e8bf6) was successful after 104 ms.
2023-09-01 00:58:16.783 [info] Creating a socket (renderer-Tunnel-54b3ca0c-f89e-4827-bd48-d42869c6094f)...
2023-09-01 00:58:16.891 [info] Creating a socket (renderer-Tunnel-54b3ca0c-f89e-4827-bd48-d42869c6094f) was successful after 109 ms.
```

</p>
</details> 

Dev console doesn't contain anything related.

The extension host shows: 

```
2023-09-01 00:53:21.861 [error] Error: read ECONNRESET
    at TCP.onStreamRead (node:internal/stream_base_commons:217:20)
2023-09-01 00:53:21.861 [error] Error: read ECONNRESET
    at TCP.onStreamRead (node:internal/stream_base_commons:217:20)
2023-09-01 00:53:21.985 [error] Error: read ECONNRESET
    at TCP.onStreamRead (node:internal/stream_base_commons:217:20)
2023-09-01 00:53:22.031 [error] Error: read ECONNRESET
    at TCP.onStreamRead (node:internal/stream_base_commons:217:20)
2023-09-01 00:53:22.451 [error] Error: read ECONNRESET
    at TCP.onStreamRead (node:internal/stream_base_commons:217:20)
``` 

Although I'm not sure if that's related.

The server output is interesting: 

```
2023-08-31 23:03:58.609 [error] Error: connect ECONNREFUSED ::1:4000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
2023-08-31 23:03:58.870 [error] Error: connect ECONNREFUSED ::1:4000
    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16)
```

This is directly related - it happens right/shortly after opening the url in the browser."
microsoft/vscode,2023-08-31 19:49:05,bug,Can't change local address port in port forwarding,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: Todays insider
- OS Version: Win11

Steps to Reproduce:

![image](https://github.com/microsoft/vscode/assets/7556827/d8f0cd2d-8664-4060-b93a-889dbe978855)

Be in a **devcontainer** (here on a remote ssh host, if that matters).
Try to set a new local port for an existing port forwarding.
Enter `localhost:5000` -> does not work
Enter `5000` -> does work.
"
microsoft/vscode,2023-08-31 14:14:09,bug,xterm live region isn't present unless output spans full viewport,noticed by @rperez030
microsoft/vscode,2023-08-31 12:46:34,bug,"`app#resolveInitialProtocolUrls()` does not remove `?windowId=_blank""` part of the URL with `vscode:` schema","Type: <b>Bug</b>

I'm trying to open workspaces with links like this: `vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank`

When VS Code is launched before opening the link, everything working as intended: the new window opens, connects to the server and loads working workspace.

But if VS Code was not running, loaded workspace has some problems: almost every internal URI contains `?windowId=_blank` in `external` and this breaks many extensions. I think this is effect of passing ""query"":""windowId=_blank"" to the Remote-SSH extension.

I enabled trace logging and found different behaviour in handling URL when running vs. not running VS Code process:

<details>
  <summary>Launching VS Code with link</summary>

```
2023-08-31 06:53:59.892 [trace] app#resolveInitialProtocolUrls() protocol urls from macOS 'open-url' event: [""vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank""]
2023-08-31 06:53:59.893 [trace] app#resolveInitialProtocolUrls() protocol url will be handled as window to open: vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank {""workspaceUri"":{""$mid"":1,""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net"",""query"":""windowId=_blank""}}
2023-08-31 06:53:59.893 [trace] ElectronURLListener initialUrisToHandle: []
2023-08-31 06:53:59.893 [trace] ElectronURLListener: waiting for window to be ready to handle URLs...
2023-08-31 06:53:59.893 [trace] lifecycle (main): phase changed (value: 2)
2023-08-31 06:53:59.893 [trace] windowsManager#open
2023-08-31 06:53:59.893 [trace] windowsManager#open pathsToOpen [{""workspace"":{""id"":""805a4cf8f6335103243bb008f275ef99"",""configPath"":{""$mid"":1,""external"":""vscode-remote://ssh-remote%2Bviknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId%3D_blank"",""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net"",""query"":""windowId=_blank""}},""remoteAuthority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}]
2023-08-31 06:53:59.894 [trace] windowsManager#doOpenFolderOrWorkspace {""folderOrWorkspace"":{""workspace"":{""id"":""805a4cf8f6335103243bb008f275ef99"",""configPath"":{""$mid"":1,""external"":""vscode-remote://ssh-remote%2Bviknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId%3D_blank"",""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net"",""query"":""windowId=_blank""}},""remoteAuthority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}}
```
</details>
<details>
  <summary>Opening link when VS Code is running</summary>

```
2023-08-31 06:58:06.231 [trace] app#handleProtocolUrl(): vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank {""originalUrl"":""vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank""}
2023-08-31 06:58:06.232 [trace] app#handleProtocolUrl() found 'windowId=_blank' as parameter, setting shouldOpenInNewWindow=true: vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace?windowId=_blank
2023-08-31 06:58:06.232 [trace] app#handleProtocolUrl() opening protocol url as window: {""workspaceUri"":{""$mid"":1,""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}} vscode://vscode-remote/ssh-remote+viknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace
2023-08-31 06:58:06.232 [trace] windowsManager#open
2023-08-31 06:58:06.232 [trace] windowsManager#open pathsToOpen [{""workspace"":{""id"":""f7c3787ac45e98797d40d7528f7be026"",""configPath"":{""$mid"":1,""external"":""vscode-remote://ssh-remote%2Bviknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace"",""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}},""remoteAuthority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}]
2023-08-31 06:58:06.232 [trace] windowsManager#doOpenFolderOrWorkspace {""folderOrWorkspace"":{""workspace"":{""id"":""f7c3787ac45e98797d40d7528f7be026"",""configPath"":{""$mid"":1,""external"":""vscode-remote://ssh-remote%2Bviknet-bionic.sas.yp-c.yandex.net/home/viknet/arc-project/arc-project.code-workspace"",""path"":""/home/viknet/arc-project/arc-project.code-workspace"",""scheme"":""vscode-remote"",""authority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}},""remoteAuthority"":""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""}}
```
</details>

And `TRACE workbench#open(): with configuration` looks like this:
```
{
    ...
    ""workspace"":
    {
        ""id"": ""805a4cf8f6335103243bb008f275ef99"",
        ""configPath"":
        {
            ""$mid"": 1,
            ""path"": ""/home/viknet/arc-project/arc-project.code-workspace"",
            ""scheme"": ""vscode-remote"",
            ""authority"": ""ssh-remote+viknet-bionic.sas.yp-c.yandex.net"",
            ""query"": ""windowId=_blank""
        }
    },
    ...
}
```
vs.
```
{
    ...
    ""workspace"":
    {
        ""id"": ""f7c3787ac45e98797d40d7528f7be026"",
        ""configPath"":
        {
            ""$mid"": 1,
            ""path"": ""/home/viknet/arc-project/arc-project.code-workspace"",
            ""scheme"": ""vscode-remote"",
            ""authority"": ""ssh-remote+viknet-bionic.sas.yp-c.yandex.net""
        }
    },
    ...
}
```

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:40:25.698Z)
OS version: Darwin arm64 22.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 24)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|1, 2, 2|
|Memory (System)|32.00GB (7.32GB free)|
|Process Argv|--enable-proposed-api jeanp413.open-remote-ssh|
|Screen Reader|no|
|VM|0%|
</details>Extensions: none
<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-31 08:23:21,bug,Open dialog filter does not work for multiple extension files (e.g. .tar.gz) when working with Remote-SSH,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.77.3
- OS Version: Ubuntu 22.04.2 LTS

Steps to Reproduce:

1. set the open dialog options filter to accept .tar.gz files:
 const openDialogOptions: vscode.OpenDialogOptions = {
        canSelectFiles: true,
        canSelectFolders: false,
        canSelectMany: false,
        openLabel: 'Select',
        filters: { 'targz': ['tar.gz'] }
    };
 await vscode.window.showOpenDialog(openDialogOptions);

2. Run the extension in Remote-SSH mode and try to select a .tar.gz file
"
microsoft/vscode,2023-08-31 06:14:32,bug,Cannot select a path outside the workspace to save the file,"ADD ISSUE DESCRIPTION HERE

Version: 1.81.1
Commit: 6c3e3dba23e8fadc360aed75ce363ba185c49794
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36
Embedder: vscode.dev

<!-- generated by web issue reporter -->

![bug1](https://github.com/microsoft/vscode/assets/11473889/29f86a05-8305-4511-a1f1-3df582d21324)

Steps:
1. Open a repo on vscode.dev
2. Open a file in any directory
3. Double-click the blank space to create a new file and use the shortcut key to save it
4. You can find the path to save the file can directly click on the previous level

Although the path beyond the current workspace is not saved successfully, in theory, it should not be possible to choose such a path."
microsoft/vscode,2023-08-30 21:21:21,bug,Bracket colorization for markdown links with mismatched parenthesizes ,"![image](https://github.com/microsoft/vscode/assets/26030610/2fe78b57-ffcb-4c35-9a80-72e3cef54c97)

issue surfaced when testing: https://github.com/microsoft/vscode/issues/188867

It correctly displays with <> but the rightmost parenthesis is still highlighted red. Likely the bracket pair colorization isn't working properly in this case."
microsoft/vscode,2023-08-30 15:52:20,bug,change default value of `focusAfterRun` to be `none` for screen reader users,"Changing this behavior could be jarring, so instead, we will set it to `none` by default and include a hint about this in the terminal accessibility help dialog. "
microsoft/vscode,2023-08-30 07:44:13,bug,Unable to forward port due to tunnel limit ,"Testing #191540

* Using latest insiders 
```
Version: 1.82.0-insider
Commit: 35be9bf683eace09796e59d54f1f225bbc3a7866
Date: 2023-08-30T06:26:06.760Z
Electron: 25.7.0
ElectronBuildId: 23434598
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Linux x64 6.2.0-31-generic snap
```
* Initiate forward port and after successful sign in flow, following error is seen

```
2023-08-30 16:40:00.726 [info] [forwarding] starting CLI
2023-08-30 16:40:00.810 [info] [forwarding] [2023-08-30 16:40:00] debug No code server tunnel found, creating new one

2023-08-30 16:40:00.810 [info] [forwarding] [2m[2023-08-30 16:40:00] trace Found token in keyring

2023-08-30 16:40:03.915 [info] [forwarding] [0m[2023-08-30 16:40:03] info Creating tunnel with the name: parallels-parallels-virtual-platform

2023-08-30 16:40:03.915 [info] [forwarding] [2m[2023-08-30 16:40:03] trace Found token in keyring

2023-08-30 16:40:06.699 [info] [forwarding] [0m[2m[2023-08-30 16:40:06] trace Found token in keyring

2023-08-30 16:40:07.720 [info] [forwarding] [0m[2m[2023-08-30 16:40:07] trace Tunnel limit hit, trying to recycle an old tunnel

2023-08-30 16:40:07.720 [info] [forwarding] [0m[2m[2023-08-30 16:40:07] trace Found token in keyring

2023-08-30 16:40:10.487 [info] [forwarding] [0m[2m[2023-08-30 16:40:10] trace No tunnels available to recycle

2023-08-30 16:40:10.489 [info] [forwarding] [0m[2023-08-30 16:40:10] error Could not create tunnel with name: parallels-parallels-virtual-platform

2023-08-30 16:40:10.489 [info] [forwarding] Reason: The request was denied because it would exceed the limit for 'TunnelsPerUserPerLocation' (5).

2023-08-30 16:40:10.492 [info] [forwarding] exited with code 1
```"
microsoft/vscode,2023-08-30 01:10:04,bug,Wrong extension name reported as installed when installing extension pack,"1. Install this vsix https://github.com/microsoft/vscode-jupyter/files/12454365/ms-toolsai-jupyter-hub-insiders.vsix.zip
2. :bug: VS Code shows this success notification 

![Image](https://github.com/microsoft/vscode/assets/30305945/ddf59f13-c2cd-4b1f-a661-b11727e42d86)

"
microsoft/vscode,2023-08-29 21:34:35,bug,`x` button in accessibility help view triggers error,"Type: <b>Bug</b>

1. Run `Open accessibility help...`
2. Try clicking on the `x` in the help

**bug**
See the error:

```
workbench.desktop.main.js:sourcemap:765 Unable to write to User Settings because accessibility.verbosity.editor is not a registered configuration.
```

VS Code version: Code - Insiders 1.82.0-insider (Universal) (ebd67244fb2da33ab078bb2baa96106fda29f336, 2023-08-29T05:34:04.713Z)
OS version: Darwin x64 22.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz (16 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|3, 9, 10|
|Memory (System)|32.00GB (0.92GB free)|
|Process Argv|--crash-reporter-id 48781ca2-1705-4f64-9bab-325055aab55d|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-29 20:07:40,bug,Linux: Notifications Accessible View only opens when focusing with mouse,"Testing #191344

After focusing the notification by keyboard I cannot open the Accessible View. Only after clicking into it with the mouse cursor does the Accessible View open.

(Today's Insiders build on Linux.)"
microsoft/vscode,2023-08-29 19:59:10,bug,Padding is off on the quick question `x` button,"Testing #191496

There's not gap between the x button and the right side, hovering it makes it really clear:

![Image](https://github.com/microsoft/vscode/assets/2193314/493238bc-980d-456c-ae2f-eacfd07f959f)



![Image](https://github.com/microsoft/vscode/assets/2193314/2ae6be83-c1d7-4e5c-9b29-8e62ac866e28)



![Image](https://github.com/microsoft/vscode/assets/2193314/44acbdd2-c61c-4ddd-bb44-e558e03f82ae)

"
microsoft/vscode,2023-08-29 18:08:46,bug,Use better codicon for action to disable verbosity hint,"We use an X now, which to me feels more like a 'dismiss accessible help' button. Maybe `bell-slash` would be more appropriate https://github.com/microsoft/vscode-codicons/blob/main/src/icons/bell-slash.svg, or even `circle-slash` https://github.com/microsoft/vscode-codicons/blob/main/src/icons/circle-slash.svg

![Image](https://github.com/microsoft/vscode/assets/30305945/dc0762b1-8ca1-40f0-9d57-74fed2d9edb9)

"
microsoft/vscode,2023-08-29 17:46:50,bug,Clicking into notebook markdown search result clears result,"Testing #191488

1. Create a notebook with a markdown cell
2. Close the notebook
3. Search for some text in the notebook markdown
4. Click on search result for the markdown content

**Bug**
The search results view is cleared


![Image](https://github.com/microsoft/vscode/assets/12821956/e8ff52f7-6a4c-4f67-8f9c-e1acba16d03a)


Maybe because the markdown cell is now in edit mode?"
microsoft/vscode,2023-08-29 17:25:47,bug,Copy image output doesn't work on Linux,"Testing #191502

1. Have a cell with an image
2. Copy the cell output
3. Paste in something like Gimp
4. 🐛 Nothing happens

Both `xclip` and Gimp don't see anything written to the clipboard. Copying images from a browser works so I know the clipboard can be written with images."
microsoft/vscode,2023-08-29 15:18:51,bug,"[Bug] When the window is downsized, icons to expand the hidden lines move to the left","This behavior is seen in the following GIF. Perhaps the icons should remain on the same position:

https://github.com/microsoft/vscode/assets/61460952/e344ce73-9696-4539-a3f8-bda01f4d6782

"
microsoft/vscode,2023-08-29 14:04:30,bug,Error: Throttler is disposed in serve-web,"Testing #191542 

- VS Code Version: 
Version: 1.82.0-insider
Commit: ebd67244fb2da33ab078bb2baa96106fda29f336
Date: 2023-08-29T05:03:17.701Z
Browser: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36

- OS Version: Arch Linux

Steps to Reproduce:

1. Download latest insider vscode cli
2. Run `code-insider serve-web` , open the browser and wait for vscode server to open
3. Open and close some workspaces and folders
4. The following error lines appear in terminal where cli is run:

```
[2023-08-29 16:57:20] info [ebd6724 stderr]: [16:57:20] Error: Throttler is disposed
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at c.queue (/<path-to-vscode-cli>/cli/serve-web/ebd67244fb2da33ab078bb2baa96106fda29f336/out/vs/server/node/server.main.js:70:13632)
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at /<path-to-vscode-cli>/cli/serve-web/ebd67244fb2da33ab078bb2baa96106fda29f336/out/vs/server/node/server.main.js:70:15216
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at /<path-to-vscode-cli>/cli/serve-web/ebd67244fb2da33ab078bb2baa96106fda29f336/out/vs/server/node/server.main.js:70:14852
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at runNextTicks (node:internal/process/task_queues:60:5)
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at listOnTimeout (node:internal/timers:538:9)
[2023-08-29 16:57:20] info [ebd6724 stderr]:     at process.processTimers (node:internal/timers:512:7)
```
"
microsoft/vscode,2023-08-29 13:33:28,bug,Settings feedback,"Testing #191527

Two polish items:
* the link to the setting does not seem to work
* you talk about ""making the focused view more obvious"" but that is not true, this feature only works for ""text editors"" and ""terminals"", so I would clarify that

![Image](https://github.com/microsoft/vscode/assets/900690/082040dc-9868-455e-ac17-82d5d77835ed)

"
microsoft/vscode,2023-08-29 13:23:07,bug,Editor placeholder (error case) does not dim,"Testing #191527



![Image](https://github.com/microsoft/vscode/assets/900690/1e98226a-3bbc-4f78-ac3b-9d8e913effd1)

"
microsoft/vscode,2023-08-29 13:21:06,bug,Keybindings editor does not dim,"Testing #191527



![Image](https://github.com/microsoft/vscode/assets/900690/530bc461-27b3-41b8-822b-1fac1aba955c)

"
microsoft/vscode,2023-08-29 13:11:54,bug,Remove `envCollectionOptions` from product.json,"```
Via 'product.json#extensionEnabledApiProposals' extension 'ms-python.python' wants API proposal 'envCollectionOptions' but that proposal DOES NOT EXIST. Likely, the proposal has been finalized (check 'vscode.d.ts') or was abandoned.
```"
microsoft/vscode,2023-08-29 12:11:18,bug,[Accessibility] Alt+F1 does not work in terminal,"
Type: <b>Bug</b>

This issue is found on Windows.

1. Open terminal.

2. Press Alt+F1.

It says:

> TypeError: Cannot read properties of undefined (reading 'getAriaLabel')

VS Code version: Code - Insiders 1.82.0-insider (ebd67244fb2da33ab078bb2baa96106fda29f336, 2023-08-29T05:32:55.965Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.71GB (6.49GB free)|
|Process Argv|--crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (89)</summary>

Extension|Author (truncated)|Version
---|---|---
android-dev-ext|ade|1.3.2
Bookmarks|ale|13.4.1
openscad|Ant|1.2.1
spellright|ban|3.0.116
zoterolatex|bna|0.4.1
mermaid-markdown-syntax-highlighting|bpr|1.5.2
doxdocgen|csc|1.4.0
vscode-markdownlint|Dav|0.51.0
vscode-eslint|dba|2.4.2
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.20.0
gitlens|eam|14.2.1
EditorConfig|Edi|0.16.4
prettier-vscode|esb|10.1.0
vscode-google-translate|fun|1.4.13
codespaces|Git|1.14.16
copilot|Git|1.105.366
copilot-chat|Git|0.7.2023082902
remotehub|Git|0.60.0
vscode-github-actions|git|0.26.1
vscode-pull-request-github|Git|0.70.0
cslpreview|igo|0.2.2
easy-snippet|inu|0.6.3
path-autocomplete|ion|1.24.1
latex-workshop|Jam|9.13.4
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
zotero|mbl|0.1.10
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.0
black-formatter|ms-|2023.4.1
flake8|ms-|2023.6.0
isort|ms-|2023.11.12061012
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.40
jupyter|ms-|2023.7.1002162226
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.307.0
remote-ssh|ms-|0.105.1
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.0
vscode-remote-extensionpack|ms-|0.24.0
azure-repos|ms-|0.36.0
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.8.2817
powershell|ms-|2023.6.0
remote-repositories|ms-|0.38.1
vscode-github-issue-notebooks|ms-|0.0.129
vscode-selfhost-test-provider|ms-|0.3.16
vscode-serial-monitor|ms-|0.10.0
vsliveshare|ms-|1.0.5883
autodocstring|njp|0.6.1
pandocciter|not|0.10.3
shiny-python|Pos|0.1.2
shinyuieditor|pos|0.4.3
quarto|qua|1.95.1
r-debugger|RDe|0.5.4
java|red|1.21.0
vscode-xml|red|0.26.1
r|REd|2.8.1
multi-command|ryu|1.6.0
vscode-deepl|soe|1.0.6
abc-music|sof|0.4.0
lua|sum|3.7.0
latex-utilities|tec|0.4.10
cmake|twx|0.0.17
errorlens|use|3.13.0
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-arduino|vsc|0.6.0
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.13
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
markdown-all-in-one|yzh|3.5.1
grammarly|znc|0.22.1

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012:30540252
pynewext54:30618038
a9j8j154:30646983
showlangstatbar:30737417
ecj1e332:30687743
pythonfmttext:30716741
fixshowwlkth:30771523
showindicator:30805243
pythongtdpath:30726887
i26e3531:30792625
gsofa:30797620
welcomedialog:30812478
pythonnosmt12:30779711
pythonidxpt:30768918
pythonnoceb:30776497
copilotsettingt:30808721
asynctok:30821568
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206
pythonlinttype:30823781
pythonmpsinfo:30815194
dsvsc015:30821418

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-29 09:47:23,bug,Debug: Open Link created chrome config when only msedge is installed,"Testing #191545

I don't have Chrome installed, but Edge. If Open Link would detect which browsers are available, this would have worked out-of-the-box."
microsoft/vscode,2023-08-28 23:43:23,bug,quick search - file highlight decoration isn't showing up on search,"1. use quick search for something like `% dispose(` that is in the current file.
2. Notice that, while the picker is open, the matches aren't highlighted :bug:

You should see it like this on Dark Modern 


![Image](https://github.com/microsoft/vscode/assets/31675041/c99d40d9-a1c7-4cfe-a9c4-f9610faec23b)

"
microsoft/vscode,2023-08-28 18:27:09,bug,exec server port forwarding fails for certain cases,"              I still have this issue in the latest version:
```
Version: 1.82.0-insider
Commit: 083fca132543aa91a7e1de2dc23857d70ea56dd3
Date: 2023-08-25T05:44:25.625Z (23 hrs ago)
```


To reproduce:
1. Use node script [from here](https://gist.github.com/Dador/746e9b7806f2d3b0f4e7d6913950fc00).
2. Do a request from client side:
```
head -c 10000 /dev/urandom | curl 'http://127.0.0.1:9090/'  -X POST --data-binary @-
```
3. Server side receives only the first chunk:
```
on req
on data 3726
```
After this, the request gets stuck and never ends.

In my case, the issue mostly affects small requests. A 10kb request always has the issue, but a 100kb request seems to be consistently fine. Size of the response also seems to be important.

I connected to a distant server (with a ping of ~300 ms), but there doesn't seem to be any problem with the connection itself.

_Originally posted by @Dador in https://github.com/microsoft/vscode/issues/190859#issuecomment-1694181472_
            

---

I have discovered this to be an issue in the CLI or SDK's compression handling. It seems like something is not flushing or decompressing completely. Disabling connection compression fixes, it but this is not a good solution."
microsoft/vscode,2023-08-28 17:39:17,bug,Context menu for Quick Search appearing in search view,"1. Right-click in the search view.
2. See this option, which brings you to the quick search :bug: This shouldn't be here.

![Image](https://github.com/microsoft/vscode/assets/31675041/95a7c11a-0dbe-4d01-91e7-abde633e8e55)




"
microsoft/vscode,2023-08-27 23:14:27,bug,[Bug] Tela preta no terminal do VS Code ,"Type: <b>Bug</b>

o meu terminal do vscode ficou todo preto derrepente, ele esta normal e quando fui abri-lo estava preto, nao consigo resolver e nao posso voltar a minha rotina por causa disso.

My vscode terminal suddenly went all black, is that normal and when I opened it was black, I can't resolve it and I can't get back to my routine because of this.
![2023-08-23](https://github.com/microsoft/vscode/assets/90806102/43143f05-3de3-445d-9bba-bf31db8cb0ca)


VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5600X 6-Core Processor              (12 x 3700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.89GB (6.67GB free)|
|Process Argv|--crash-reporter-id 74ce9143-439b-4406-a24f-d6f86ccdd59a|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (8)</summary>

Extension|Author (truncated)|Version
---|---|---
turbo-console-log|Cha|2.9.6
gitlens|eam|14.2.1
auto-rename-tag|for|0.1.10
copilot|Git|1.105.350
prettify-json|moh|0.0.3
vscode-language-pack-pt-BR|MS-|1.81.2023081609
vscode-thunder-client|ran|2.10.5
vscode-icons|vsc|12.5.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
282f8724:30602487
89544117:30613380
showlangstatbar:30737416
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-25 16:55:53,bug,Terminal multiple action icons overlap,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version:  Insiders
- OS Version: macOS




![Image](https://github.com/microsoft/vscode/assets/876920/421798b0-dd84-4399-b7eb-ceab65ffdb0a)

"
microsoft/vscode,2023-08-25 16:04:19,bug,Moved Code Detection: Don't Detect Moves When Nothing Moved,"![Image](https://github.com/microsoft/vscode/assets/2931520/4d930f3a-2d6b-4396-a18d-2ecbb474392d)
"
microsoft/vscode,2023-08-25 07:41:47,bug,The markdown link in TEST RESULTS is hard to read,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
Version: 1.82.0-insider (user setup)
Commit: a0377f0c51dbb2d3188565cdf35e89929f864e65
Date: 2023-08-24T05:32:35.024Z
Electron: 25.5.0
ElectronBuildId: 23084831
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Windows_NT x64 10.0.19045

![image](https://github.com/microsoft/vscode/assets/6193897/a3fdd456-0169-4baf-8ea9-22a48da54c60)


Especially in the dark theme, the font color makes it difficult to read.
"
microsoft/vscode,2023-08-24 22:26:31,bug,Search view not showing warnings,"1. Open search view and search for something that isn't in your workspace. Enable `Use Exclude Files and Ignore Files` button that is within the files to exclude input.
2. You result should say something like ""No results found. Review your settings for configured ...."", but it says ""0 results in 0 files"".

Buggy behavior:
![Image](https://github.com/microsoft/vscode/assets/31675041/f95c76e5-6272-42b7-8413-4aab010a7992)

Correct behavior:
![Image](https://github.com/microsoft/vscode/assets/31675041/f27c584e-061a-4545-b8a2-4dc1a4a4694b)

"
microsoft/vscode,2023-09-27 19:25:34,feature,Smooth scrolling on tabs,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I'd like to have smooth scrolling for window tabs (when you scroll horizontally across open tabs up top)"
microsoft/vscode,2023-09-25 16:48:45,feature,Introduce a concept of similar commands in Core,We can use an implementation of TF-IDF to provide quick local similarity search for commands.
microsoft/vscode,2023-09-15 22:49:07,feature,Allow text in error message popups to be copied or at least selected,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

The possibility to copy the message in an error popup (the one with a red X in a circle. This would be nice for:

* searching error messages
* getting key info to resolve the error oneself

Below is my specific usecase:
![Untitled](https://github.com/microsoft/vscode/assets/4111/f2d3a726-6c89-4d2d-bba6-aa2feb345449)

The server I work on does not have ftp access, so I need to download that file on my workstation and scp it onto the server.

As it is I cannot select that text to copy the url, so I'm left with typing a 100 character ftp url by hand. Which is painful enough that I'm writing this issue."
microsoft/vscode,2023-09-14 07:51:02,feature,Add a 'wordwise' option for the diff inline view,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
As kind of requested in issue #103285 and visible in the mentioned issues attached image I'd like the option to only display the changed part of the line.

The idear is to not have a read line and a green line underneeth each other but rather a single line where only the parts are highlighted that have acually changed.

Maybe the option could be added to the ""More Actions..."" drop down menu as ""Inline Word View"", ""Wordwise View"" or ""Word Diff View""
"
microsoft/vscode,2023-09-13 15:05:18,feature,Extension tree that use `vscode.open` and `vscode.diff` commands should respect enter vs. space,"When discussing with @meganrogge how we're going to improve accessibility for GitHub Pull Requests and Issues when opening a PR description we found that the built in Extensions view does the following:
- Arrow keys are used to navigate between list/tree items
- Enter opens the extension item and moves focus to the extension description webview editor
- Space opens the extension item and keeps focus in the tree

Extension trees don't work the same way:
- Arrow keys are used to navigate between list/tree items (same as above)
- Both enter and space run the tree item command and keep focus in the tree

We already have special handling for `vscode.open` and `vscode.diff` command run from the tree view so that we can keep things ctrl+click as open to the side. We should also handle enter and space as the Extensions view does."
microsoft/vscode,2023-09-11 09:59:46,feature,Diff Editor: Mark the full block as being replaced,"## Description

Mark the full block as being replaced

![Image](https://github.com/microsoft/vscode/assets/2931520/976bf5e8-dc2b-42d4-9fd7-e1aeec87a650)

## Playground Example

[Monaco Editor Playground Repro](https://microsoft.github.io/monaco-editor/playground.html?source=v0.42.0-dev-20230911#XQAAAAItBgAAAAAAAABBqQkHQ5NjdMeMm-jY7SIQ9S7DNlzs5W-mwj0fe1ZCDRFc9ws9XQE0SJE1jc2VKxhaLFIw9vEWdz-byd2-yxN8X4gTjZnHx_R_ugxZ6xrWgz8k0rlYVGZfwQ3QjH2GDw7bI_8paqOoYtw1AXFwYvJQ2FfcOWSRahkBHGIpvphFsp0dv_kujP7FSM9eFraIq3yjHeuGUpEofHF_Yz0qHNPPGbV1wfm52_Xt4W3OK2ChtRL6Eixci58SKGibR-tiOIpi4GiIjKYunEEy_De2QgmQzTrvm9ZcLoL07vWl7EwJ0YRaZbNIyb4__bTWr7m1zwDI-050XjyflhFYiKy7DNB3IH3VYJ0gV00H2dKdptYHQQUD2fJoaPw7TA95hnEIo2KQllGMkZBjhzxSyUfLQZznlY-eDiyxYyoQgH8Y0BZMSPEBCbKPu0DDk1naT_20zIUJ0PMcLYf6KYIHO08EDzvQqJwuN_dxt6k9ZkKq3QXlmVr0hMnUo7CPnMrtllKe-oSFihNiofQ8a0FYt3QfuSWw9TQ2JGwPnlDRJv5AwcHcX2rhgrcniUn7eMSoPWTLJFHGkwYv68VHwKfSFcIoJh7X3nmm9Hx_t8i-OgJ29wGuRBmYBgbJs9wIFgMSz9Ihp-BqmxQDqJ7bpNAfkwjGanqJYAIetU6mXrIpeyNKZHg3TP1Qd-3jqj20kAIlRCXfrG3O1W7a3y6BbtpFRIULsrzZTIougs_8hID4bv2o4lmt_62tZBQHwXEyEA-oJTx8iICIC3cPqxv3g-2eBthtZFYw6Fa6uE5BPANXUCvPKZoeDIHJUY-w7jIBgZPOz6B-qxfTBaexmNjFwOiO93hwlKdmrjkUymWI0paUkEnHRMP_6jBQt3P1DuENKLdIQ_QA0nTeLWxNmmUT1T8Dkg828N-vk10b9LjxNZxut16McLml7yq-WZiam0M07Co1e8CVuEWLUDJgLMtsdkPm7FuvgpcO7x8x3u9Mo6Qnf8-lhK_s3xdcuRv-b4Vd) (click on ""compare withlatest dev"" to verify a future bug-fix)
"
microsoft/vscode,2023-09-08 12:48:03,feature,Debug toolbar and CC,"This PR adds the option to let the debug toolbar show in the command center. While it's there, it also updates the background color


https://github.com/microsoft/vscode/assets/1794099/cb85e68e-747a-4181-8a8e-2bfd658bb5ce

"
microsoft/vscode,2023-09-07 07:38:19,feature,Improve Comments accessibility ,"- [x] Indicate when a document has commentable ranges
  - Aria status when a document is opened
- [x] Add keyboard shortcut for adding a comment
- [x] Provide commands to go to next and previous commentable range
- [x] Add an accessible help menu to the comment widget
  - Esc dismisses widget
  - Lists go to next/previous commentable range commands + keyboard shortcuts
  - Lists command and keyboard shortcut to add a comment
  - Lists keyboard shortcut to execute the comment primary action and what the primary action is
- [x] The Comments view should respect the enter/space = reveal+focus/reveal
- [x] Make the comment widget toolbar always visible when in screen reader mode
"
microsoft/vscode,2023-09-06 07:49:11,feature,Improve Settings Sync diagnostics tooling,"The settings sync tooling that allows to inspect what happened must be improved. I do understand that bugs like https://github.com/microsoft/vscode/issues/192267 happen but I see myself being unable to file good issues (and as a consequence I observe that things don't get better). The sync view with all its viewlets, diff editors, and logs is overwhelming, esp when you must use in an unpleasant situation. There should be a single command which collects all the information needed (it can ask for my input) so that it can create a (zip)-file which allows the respective owners to investigate issues"
microsoft/vscode,2023-09-05 17:54:42,feature,"Add functionality that copies command and output, as opposed to just copying command, or just copying output","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->
<!-- Describe the feature you'd like. -->"
microsoft/vscode,2023-08-31 22:45:48,feature,"[FR] Notebook, Markdown cells: please allow to generate a line break using '\\n' and/or '<br>' in the link tooltips","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Hello, 👋

While I don't think it's ""standard"", some Markdown renderers show **link tooltips on multiple lines** when **'\\n'** or '**\\<br\\>'** are used in the link tooltip code. In some cases they require that it be preceded by two spaces.

Where support exists, the tooltips in the examples I add below are seen on multiple lines, **however that's not the case in VSCode's Markdown cells**, and for that matter on GitHub.

Code:
```markdown
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode: \\nVisual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode:  \\nVisual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode: <br>Visual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode:  <br>Visual Studio Code')  
```
Example:
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode: \\nVisual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode:  \\nVisual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode: <br>Visual Studio Code')  
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ 'microsoft/vscode:  <br>Visual Studio Code')  

The only way I've found to achieve what I'm aiming for is to write the tooltip spreading multiple lines but I'd want to avoid having to resort to that method.
Below I show an example of the successful case with the unwanted method:

Code:
```markdown
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ ' microsoft/vscode: 
Visual Studio Code
       GitHub')  
```
Example:
[microsoft/vscode: Visual Studio Code](https://github.com/microsoft/vscode/ ' microsoft/vscode: 
Visual Studio Code
       ​​​GitHub')  

Please add support for **'\\n'** and/or '**\\<br\\>'** in **link tooltips**.

Kind regards.
Claudio Salvio

P.S.: 🙏 Thank you for the useful work you are doing in the field of notebook support in VSCode!
"
microsoft/vscode,2023-08-31 13:14:11,feature,Does package.json configuration properties pattern regex support unicode?,"I have a configuration property in my extension's package.json with the following definition:

```json
""objectscript.unitTest.autoload.folder"": {
  ""markdownDescription"": ""When running client-side test classes, automatically load the contents of sub-directories with this name. See the [%UnitTest /autoload qualifier documentation](https://docs.intersystems.com/irislatest/csp/documatic/%25CSP.Documatic.cls?LIBRARY=%25SYS&CLASSNAME=%25UnitTest.Manager#RunTest) for details."",
  ""type"": ""string"",
  ""default"": ""_autoload"",
  ""scope"": ""resource"",
  ""pattern"": ""^[\\\\p{L}\\\\d_. -]*$""
}
```

That regex is a valid unicode regex that works in the node REPL, but it doesn't work properly in the VS Code settings editor:

<img width=""474"" alt=""Screenshot 2023-08-31 at 9 11 30 AM"" src=""https://github.com/microsoft/vscode/assets/44776135/2df4e58b-29bd-41d7-a892-e520133abc02"">

Are these patterns evaluated with the unicode flag on? If not, can that feature be added or enabled?
"
microsoft/vscode,2023-08-30 18:02:22,feature,Add new scope to VS Code settings for only workspace configurable,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I was hoping we could add a new scope to VS Code settings. Currently, VS Code settings have the following scopes:
 
![image](https://github.com/microsoft/vscode/assets/5290572/1c1dfac4-1ea7-4e4b-a494-845de31ca18b)

None of these scopes describe a setting that is only workspace configurable. We were hoping we could have a scope that would only show up in the workspace and not in Global and User settings.

For context as to why we want this: we use vscode settings to help save default Azure resources that users want to deploy that specific project to. This setting doesn't really make sense to show as a user setting since what resource you want to deploy to is generally always going to be tied to the workspace/project.

We're worried that users may end up in a scenario where they have accidentally altered their user setting rather than their workspace setting, and now they will deploy every new project to whatever that resource is"
microsoft/vscode,2023-08-29 01:22:30,feature,Enable Command Center by default,We've seen great responses and results of testing with experimentation that the command center is a net-positive feature. So let's turn it on my default.
microsoft/vscode,2023-08-28 18:19:34,feature,Rename `--disable-keytar` to `--disable-persisted-secrets` or similar,"Follow up from https://github.com/microsoft/vscode/issues/188432

We don't use `keytar` anymore, so we should rename this flag to something more generic:
* `--disable-persisted-secrets`
* `--disable-secrets-storage`"
microsoft/vscode,2023-08-28 17:44:57,feature,Add Quick Chat to the Command Center,"If a Chat provider is contributed, we should offer a way to start a Quick Chat session from the Command Center so that Quick Chat is easier to discover."
microsoft/vscode,2023-08-25 01:42:20,feature,testing.openTesting is not working for testing explorer now,"Version: 1.82.0-insider (user setup)
Commit: a0377f0c51dbb2d3188565cdf35e89929f864e65
Date: 2023-08-24T05:32:35.024Z
Electron: 25.5.0
ElectronBuildId: 23084831
Chromium: 114.0.5735.289
Node.js: 18.15.0
V8: 11.4.183.29-electron.0
OS: Windows_NT x64 10.0.19045

Noticed that this setting is now controlling the `TEST RESULT` panel. Some users hoping that this setting can still work for the testing explorer: https://github.com/microsoft/vscode-java-test/issues/1597"
microsoft/vscode,2023-08-24 15:21:27,feature,Support Profile Inheritance,"Can something like inheritance or import be added?
```plain text
global
├── web dev
│ ├── vue
│ ├── react
│ └── vanilla
├── python
│ ├── data mining
│ └── deep learning
└── c/cpp
```
When adding a plugin to `global`, it means that a plugin is installed globally.
When certain settings are added in `web dev`, this applies to all web development.

_Originally posted by @tbontb-iaq in https://github.com/microsoft/vscode/issues/190856#issuecomment-1685975772_
            "
microsoft/vscode,2023-08-23 15:23:11,feature,Add quick search to command center,"As mentioned in title. add the text quick access menu to this list:


![Image](https://github.com/microsoft/vscode/assets/31675041/66639311-3bce-49fa-aba2-fe4fc8478a07)

"
microsoft/vscode,2023-08-22 15:55:05,feature,builtin command executeInlineValueProvider does not exist,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.81.1
- OS Version: any

Steps to Reproduce:

1. in extension code, call
vscode.commands.executeCommand('vscode.executeInlineValueProvider',vscode.Uri.parse('c:/foo'),new vscode.Range(0, 0, 1, 1));
2. returned promise is rejected with Error: command '_executeInlineValueProvider' not found

I am developing an extension, and was not interested in this specific command, but noticed it was missing when I was looking for an example of a debug related built-in command.  
I have no use for the command, I am just reporting it, because its in the the built-in commands wiki.
"
microsoft/vscode,2023-08-22 10:13:37,feature,Diff Editor Improve Alignment,"![Image](https://github.com/microsoft/vscode/assets/2931520/f52a3453-adf2-4e6f-9d81-0b42b5016000)

[Repro](https://microsoft.github.io/monaco-editor/playground.html?source=v0.42.0-dev-20230809#XQAAAALoCgAAAAAAAABBqQkHQ5NjdMjwa-jY7SIQ9S7DNlzs5W-mwj0fe1ZCDRFc9ws9XQE0SJE1jc2VKxhaLFIw9vEWSxW3yscxJt3-RhsAFQjfi1GSTeRDChAtr8RC8jhkfedYn4TxJcJIcii5J3w75DBaWbOhZVfaDoM0qNlMfjIIi_pOZYLX8hUC9KEglwGK1Zx62ZzqjEL1FXgB-5hVRcUYpIQxJHVMk6DsoODVXD_zW6G5yAc4hAJqEAZiCsze4ySI8URz0mzOCEjiILxd8pTxZp9fnkMXF3JYiuv9w5V9DLi0uL-Vg-L4Z5BmibEgIuj0cg2_6Felz9MsVJMDIXp0xAYYsBjDBxGZ9XiLNhBz8heIt936L8CFCazGpFfLfp2hZjM67tOfTNJv5oplz5m4sJxZdkgaSkrFbOkDrwF11izXX-nB3yKVHp2YGXRlENt-M3p_8GWiJPFPVJRkug1XByBxx8ipiy5Z8CK2pN456VLQePZS4L_-mvRdZdnP8mIPGds4dCZpxenxMmW2sbRFAg6aRz7xd_WDzjqrrAzAQo0-_ZiQrYRWzYPeEcUOu_8AgiXyc9Bte59oPLe6LBZfiodJa66knqiOqwGUZyDTRubvumWT3UyaNXohUSgb_HsKuAAq55vJL0BlxKInzVT4c3E4zLlQt8PKHtN4GzeG_q4pmNLd5j9F3uTsQkOB9A2itpoNf3qENHDmz7QzIijQMqK323gfqkekmjiWaRYBjuTyrR6jvCxl8W27RYvMw0Q88JAM-SQ5LXkxyOo0GosZXHcv8TRS1i8q5ubBrPhCeTxNvjXOFSDv6VfbRGuATvwVvRQ91sklqEZQCq-pDsp2wobp--Fa4gyE4TdLzVscgceidCjSbcIjUjS_JH5Maet2R6aj6K2Wfg_0ap_NJi-93Nci5kmOip1-94ydbe7dhpZWD22Wu13KxWVGvTdE6R56OweQbtCbIGUDhz3UqmIhTI72Egol-SY_XdaFbR6559gLxD-t8E-kh7wqMCjZsxrU80JB0w94IVkeCntt7S24Q5j8T7Gl00qqagmRN2VcK0f1qhG8uLj1TQpcaPMTAImlSk5w2NY8OGCx_eNPcmB5vCbQx81x9J2uCMpjxNr2GF8xNto9ijkdQNofRKW9VpCBM9jpAwX8kBP6aLaBMO1tO38Hg4RukmjZyA1XQ2PBVhSBgauK8dIk0wLPYqLMTmwwPuwHZvpE_94lDtM)

The const line should be aligned (try latest dev)."
microsoft/vscode,2023-08-21 14:20:33,feature,Diff Editor v2: setting for number of expanded lines when clicking top/bot,"There is now `diffEditor.hideUnchangedRegions.revealLineCount`.

Verification steps:
Set it to a value (e.g. 7), open a diff where there are many unchanged lines, enable collapsing unchanged lines (click the map symbol in the editor toolbar) and click the border of the folded lines indicator. Notice that the specified amount of lines are revealed."
microsoft/vscode,2023-08-20 08:54:02,feature,Integrated terminal code folding,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
when running a Node.js script that logs extensive data, it becomes challenging to find and focus on specific sections of the output. Code folding in the terminal would significantly improve the user experience and productivity when dealing with such situations.

This feature would be a valuable addition to VS Code, enhancing its capabilities for working with the integrated terminal."
microsoft/vscode,2023-08-18 03:00:16,feature,[Accessibility] consider adding default keybinding to accept completion in inline accessible view,"
Type: <b>Feature Request</b>

In Accessible View (Alt+F2) for inline suggestion, users can activate some action buttons via keybindings, such as Alt+F6, Alt+[, Alt+]. However, ""Accept Completion"" does not have its default keybinding so users have to press Shift+Tab and hit Enter. Please add default keybinding for this, such as Ctrl+/ or Ctrl+Enter to accept the current suggestion.

VS Code version: Code - Insiders 1.82.0-insider (ccb95fd921349023027a0df25ed291b0992b9a18, 2023-08-17T05:33:29.141Z)
OS version: Windows_NT x64 10.0.22621
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-15 22:46:05,feature,Add SmartSelect / CamelHumps caret browsing for the F2 rename symbol feature,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Add SmartSelect / CamelHumps caret browsing for the F2 rename symbol feature"
microsoft/vscode,2023-08-14 15:44:14,feature,Use a single action bar,"![image](https://github.com/microsoft/vscode/assets/2931520/17e56b44-e3f4-4eea-9b15-6a496d1b14e1)

These are currently individual action bars, which makes navigation harder.
"
microsoft/vscode,2023-08-13 20:59:07,feature,DOM renderer does not show selection over regular background colors,"When gpuAcceleration = 'off', the yellow text background should be blue here:

![image](https://user-images.githubusercontent.com/2193314/188175658-0b95600f-70fb-4702-bcfc-de788b0f8b54.png)
"
microsoft/vscode,2023-08-13 09:32:52,feature,Support GNU style file:line.column links,"The [Sail compiler]() outputs file:line links that follow [this GNU convention](https://www.gnu.org/prep/standards/html_node/Errors.html):

```
Warning: Redundant case sail-riscv/model/riscv_sys_control.sail:206.6-7:
206 |      _ => false
    |      ^
```

This doesn't currently work in VSCode. It does support a very wide range of formats and I don't recall ever seeing this format before (even from GNU tools) so I suspect nobody else uses it. Nonetheless it's easy to add support in VSCode.

See https://github.com/rems-project/sail/issues/287"
microsoft/vscode,2023-08-11 12:01:05,feature,Adopt xterm.js' cursorStyleInactive option,See https://github.com/xtermjs/xterm.js/issues/4566
microsoft/vscode,2023-08-09 21:10:03,feature,Placeholder text for Ports tab should be different for local port forwarding,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Currently, it says ""No forwarded ports. Forward a port to access your running services locally."".

But when using the Ports tab locally (no remote), this doesn't have the same meaning because I can already access the port locally.

The suggestion is to change the text in the local port forwarding case to show something more meaningful.

Just as an example, something like ""No forwarded ports. Forward a port to securely access your locally running services over the Internet."""
microsoft/vscode,2023-08-08 21:37:51,feature,add title bar to accessible view / help,to align with quickpick
microsoft/vscode,2023-08-08 18:52:11,feature,Explore showing workspace search results in the command center,"We want to start to explore what it'd be like to have a quickpick that shows text results. 

For example, if we search `activate` in the `vscode-livepreview` repo, we don't get anything because it isn't in any filenames.

![Image](https://github.com/microsoft/vscode/assets/31675041/ca4ab35f-5dd2-4071-867f-e63e389b15bb)

It would be nice if vscode knew that `activate` was in the `extension.ts` file and showed that file. 

![Image](https://github.com/microsoft/vscode/assets/31675041/68acd14f-5005-4837-96ef-5517854e69ce)



"
microsoft/vscode,2023-08-06 16:34:24,feature,Ignore a Collapsed Code Block While Copying,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.80.1 (Universal)
- OS Version: Darwin arm64 22.6.0


I'm working on a Vue.js project, and I have a rather large array of data within one of my components that takes up a lot of space in my file. I often find myself wanting to copy various parts of the code from this component, but I don't want this array to be copied.

In VS Code, I have the ability to collapse this array, but when I try to copy the code, the collapsed array is still being copied. Is this intended behaviour? Is there any way to tell the editor to ignore the collapsed code block when copying?

Steps to Reproduce:
1. copy-paste the array to vscode,
2. collapse the array in the editor
3. copy the collapsed array
4. paste the collapsed array anywhere else
5. the collapsed array will appear full size
```
const categoryTitles = {
  'choroby-wewnetrzne': 'Choroby Wewnętrzne',
  'chirurgia': 'Chirurgia Ogólna',
  'pediatria': 'Pediatria',
  'poloznictwo-ginekologia': 'Położnictwo i Ginekologia',
};
```



"
microsoft/vscode,2023-08-05 10:14:20,feature,Allow to theme foreground color of status bar entries on hover,"Hi. i am creating a theme pack for VSCode. 
I have discovered that there is a  ` ""statusBarItem.hoverBackground"" ` to customize the background color of a status bar hovered item but there is no option to customize the color of the hovered item foreground color. Something like ` ""statusBarItem.hoverForeground"" `

![image](https://github.com/microsoft/vscode/assets/55595063/b014bcce-fc58-4979-a332-b31c9b42b4f5)
"
microsoft/vscode,2023-08-04 19:35:26,feature,Improve discoverability of accessibility verbosity settings,"I imagine the accessible view hints (and others) that we have around the workbench are annoying if a user doesn't know they can be disabled. Should we include info about disabling at the end of the hint?

For example, ""Use Tab+Shift to access the terminal accessible buffer, disable this hint with the `accessibility.verbosity.terminal` setting

cc @jooyoungseo, @rperez030 

Someone just emailed me that they didn't know where this was coming from and were annoyed by it."
microsoft/vscode,2023-08-03 09:07:27,feature,Diff Editor: Detect Exact Moves,"When a code fragment of at least 3 lines of code is deleted somewhere in inserted somewhere else without modifications, this move should be detected.

[In this example, moves should be detected](https://microsoft.github.io/monaco-editor/playground.html?source=v0.41.0-dev.20230727#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). (select latest -dev version to verify)"
microsoft/vscode,2023-08-02 16:32:18,feature,Support actions in the accessible view,"For features like notifications, a user might want to tab to actions directly instead of having to go back to the item to take action.

_Originally posted by @meganrogge in https://github.com/microsoft/vscode/issues/188325#issuecomment-1656364314_
            "
microsoft/vscode,2023-08-01 15:14:41,feature,Adopt new DOM renderer performance changes,Upstream: https://github.com/xtermjs/xterm.js/pull/4605
microsoft/vscode,2023-08-01 10:26:23,feature,Less aggressive `comments.openView` setting,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

With #147365 we got the `firstFile` setting to open the comments panel when opening the first file with comments. This is a nice improvement over the more aggressive `file` as setting. However users will still have the comments panel focused when there are only resolved comments and therefore no action needs to be taken. This isn't optimal, especially in setups where the comments panel hides panels like source control, which might be more relevant in these situations.

That's why we propose to add an additional option to the `comments.openView` setting which behaves like `firstFile` but will only consider unresolved comments. Naming wise this might be tricky but something among the lines of `firstFileUnresolved` might work.

Cc @alexr00 @laurentlb @hermannloose"
microsoft/vscode,2023-07-31 21:49:47,feature,[Accessibility] Change command history default keybindings in terminal buffer on Windows to align with Mac,"Type: <b>Feature Request</b>

I am educating blind folks how to use VSCode with screen readers. It is confusing to have two different keys. On Mac, Alt+Up/DownArrow are used in terminal buffer to navigate executed commands whereas Windows uses Ctrl+Up/DownArrow.

I suggest using Alt+Up/DownArrow on Windows by default to align with Mac keybindings. There will be another benefit of saving Ctrl+Up/DownArrow for Windows that will be described in a separate feature request.

VS Code version: Code - Insiders 1.81.0-insider (9800cf6dd6bf4634889d60720ef46a400f3a7298, 2023-07-28T12:08:04.472Z)
OS version: Windows_NT x64 10.0.22621
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-30 06:18:13,feature,Always on top window,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Requst to have an always on top feature for window especially within Mac and Windows, as of now Linux provides a OS level always on top window, which has it's own use cases especially while multitasking and not wanting to loose the focus upon the coding window.

The implementation details which might help could be found over [here](https://stackoverflow.com/a/39844471/9928212)."
microsoft/vscode,2023-07-27 04:58:33,feature,Tunnel factory to provide error message for notification toast,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

We currently can contribute a tunnel factory through the embedder so that VS Code can open tunnel connections.
The problem is that if something goes wrong within that factory we have no control on what we show to the user because every error falls back to [this error handler](https://github.com/microsoft/vscode/blob/28849849a9e28f2abcecb5fad62564942821a333/src/vs/workbench/contrib/remote/browser/tunnelView.ts#L1152).

It would be nice if the tunnel factory could provide a custom error to show as part of the notification to the user and fallback to the default in case an uncaught error was thrown.

cc @alexr00 "
microsoft/vscode,2023-07-26 11:33:05,feature,"When no text is selected, CTRL-C should not overwrite the current buffer with nothing","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->
<!-- Describe the feature you'd like. -->
Sometimes, when it's my intention to paste a piece of text into VSCode, I accidentally hit CTRL-C instead of CTRL-V.
This overwrites the cut/paste buffer with whatever is currently selected in VSCode.
However, if nothing is selected then it effectively erases the current contents of the cut/paste buffer.
I cannot think of a scenario where this is desired functionality.

So: if no text is selected in VSCode, entering CTRL-C should have no effect at all (thus leaving the cut/paste buffer intact)."
microsoft/vscode,2023-07-25 16:49:43,feature,Should it be possible to use ctrl+up/ctrl+down to navigate within a comment thread,"Testing #188536

1. Open a file in a PR that has comments
2. Set focus to the comment widget input
3. :bug: can't ctrl+up/ctrl+down to move focus to the comments within that thread
"
microsoft/vscode,2023-07-25 11:10:22,feature,Navigating notification a11y view with alt+]/[ should announce where you are in the list,"Testing #188528

I'm expecting something like ""notification x of y"", instead it just announces the next focused notification, even if it's the same one:

![image](https://github.com/microsoft/vscode/assets/2193314/929ae59f-a833-44c8-952e-7b69dc528c21)
"
microsoft/vscode,2023-07-25 03:37:33,feature,[Feature] Support Sticky display the code stack of the cursor line,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Sticky currently only supports displaying the stack structure of the first line of code in the editor.
Can it add feature to let sticky shows the code stack of the cursor line?"
microsoft/vscode,2023-07-24 18:14:01,feature,Support go to symbol in the accessible view,"ATM, this is opening the symbols for the focused editor pane. We will also want to make sure that the accessible view remains open despite the `blur` event that will occur on quick pick open. Also check the `zIndex` as I think it's currently set to = that of the quickpick, but will need to be < than it"
microsoft/vscode,2023-07-24 12:19:33,feature,.ipynb wrap cell output at fixed character limit,"
Type: <b>Bug</b>

```python
'1' * 1000
```
The output is a single line that goes off my screen to the right.
I would prefer to have options:
- single line, no wrap
- fixed margin (e.g. 80)
- wrap to visible width (so if I resize the window it adjusts)


VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:20:23.298Z)
OS version: Darwin x64 22.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8559U CPU @ 2.70GHz (8 x 2700)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|2, 2, 2|
|Memory (System)|16.00GB (0.02GB free)|
|Process Argv|--crash-reporter-id ffe7017d-9a68-4026-a96e-3eb92191e23c|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (25)</summary>

Extension|Author (truncated)|Version
---|---|---
project-manager|ale|12.7.0
gitignore|cod|0.9.0
vscode-office|cwe|3.1.6
git-extension-pack|don|0.1.3
githistory|don|0.6.20
gitlens|eam|14.1.1
copilot|Git|1.98.275
vscode-pull-request-github|Git|0.68.1
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.0
isort|ms-|2023.10.1
python|ms-|2023.12.0
vscode-pylance|ms-|2023.7.30
jupyter|ms-|2023.6.1101941928
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.299.0
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
makefile-tools|ms-|0.7.0
cmake|twx|0.0.17
vscode-open-in-github|ziy|1.3.6

(1 theme extensions excluded)

</details>
<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-21 21:22:34,feature,Improve Quick Question experience for conversational chat,"Right now, the Quick Question experience only allows for one question and one answer.

![image](https://github.com/microsoft/vscode/assets/2644648/e3726d4d-3492-4dbb-ac21-6c1e64c8cfb0)

We should explore alternatives to this UX that allows for full conversations."
microsoft/vscode,2023-07-20 23:28:37,feature,Support a `--password-store=inmemory` or similar,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
The main purpose of this would be for running in CI when you need _some_ sort of secret storage store, but it doesn't have to live on. 

Right now `--password-store=basic` allows the SecretStorage API to work, but it still stores things on disk which isn't needed in CI scenarios... and it's probably better to store it in memory than weakly on disk in CI anyway."
microsoft/vscode,2023-07-20 19:15:53,feature,Behaviour of the Search-Box,"Type: <b>Feature Request</b>

 
Dear Sirs
As an older Developer using a lot of Editors during Time i find VS really, really good - so thanks a lot.
 
There is one Issue with the Search-Box, i often found crazy while in 'my Workflow':
Simply searching while working with 'Ctrl-F', so focusing the Search-Box, all Functions for moving in the Code are disrupted.
So, no Page-Up, no Page-Down, no 'Home'/'End' and, and , and are'nt working as long as the Search-Box is focused - after years i cant get used to - perhaps too old and to much experience while decades with other Editors...
 
Ok, sorry for my poor English, perhaps somebody will think about changing...
 
Eckart Bechler, Dortmund, Germany
 

VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-20 02:30:58,feature,[Accessibility] Cannot use Shift+Tab as an input key in terminal,"Type: <b>Bug</b>


## Repro

1. Configure the settings like below:

`settings.json`:

``` json
{
  ""terminal.integrated.tabFocusMode"": false
}
```

1. Create new terminal via ctrl+`

1. Press `Shift+Tab` from terminal input.

## Current Behavior

Pressing Shift+Tab moves to the terminal buffer.

## Expected Behavior

Shift+Tab should be passed to terminal as an input key. The focus needs to remain in the terminal input field.

We need `Shift+Tab` when cycling back through shell auto-suggestion or ipython completion.

VS Code version: Code - Insiders 1.81.0-insider (c85bf61a82b0c39886b032d2634108782a55c637, 2023-07-19T05:34:51.441Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.71GB (6.44GB free)|
|Process Argv|--crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (90)</summary>

Extension|Author (truncated)|Version
---|---|---
android-dev-ext|ade|1.3.2
Bookmarks|ale|13.4.1
openscad|Ant|1.1.1
spellright|ban|3.0.116
zoterolatex|bna|0.4.1
mermaid-markdown-syntax-highlighting|bpr|1.5.2
doxdocgen|csc|1.4.0
dscodegpt|Dan|2.1.14
vscode-markdownlint|Dav|0.51.0
vscode-eslint|dba|2.4.2
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.19.1
gitlens|eam|14.1.1
EditorConfig|Edi|0.16.4
prettier-vscode|esb|9.19.0
vscode-google-translate|fun|1.4.13
codespaces|Git|1.14.12
copilot|Git|1.97.271
copilot-chat|Git|0.5.2023071901
remotehub|Git|0.60.0
vscode-github-actions|git|0.25.8
vscode-pull-request-github|Git|0.68.1
easy-snippet|inu|0.6.3
path-autocomplete|ion|1.24.1
latex-workshop|Jam|9.13.1
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
zotero|mbl|0.1.10
git-graph|mhu|1.30.0
vscode-docker|ms-|1.26.0
black-formatter|ms-|2023.4.1
flake8|ms-|2023.6.0
isort|ms-|2023.11.11921012
python|ms-|2023.12.0
vscode-pylance|ms-|2023.7.30
jupyter|ms-|2023.6.1101941928
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.301.0
remote-ssh|ms-|0.102.0
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.80.2
vscode-remote-extensionpack|ms-|0.24.0
azure-repos|ms-|0.36.0
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.7.1717
remote-repositories|ms-|0.38.1
vscode-github-issue-notebooks|ms-|0.0.129
vscode-selfhost-test-provider|ms-|0.3.15
vscode-serial-monitor|ms-|0.10.0
vsliveshare|ms-|1.0.5873
resourcemonitor|mut|1.0.7
autodocstring|njp|0.6.1
pandocciter|not|0.10.2
shiny-python|Pos|0.1.2
shinyuieditor|pos|0.4.3
quarto|qua|1.90.0
r-debugger|RDe|0.5.4
java|red|1.20.0
vscode-xml|red|0.26.1
r|REd|2.8.1
multi-command|ryu|1.6.0
vscode-deepl|soe|1.0.6
abc-music|sof|0.4.0
lua|sum|3.6.23
latex-utilities|tec|0.4.10
chatgpt|tim|1.1.2
cmake|twx|0.0.17
errorlens|use|3.12.0
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-arduino|vsc|0.6.0
vscode-java-debug|vsc|0.52.0
vscode-java-dependency|vsc|0.23.0
vscode-java-pack|vsc|0.25.12
vscode-java-test|vsc|0.39.0
vscode-maven|vsc|0.41.0
markdown-all-in-one|yzh|3.5.1
grammarly|znc|0.22.1

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012:30540252
pynewext54:30618038
pylantcb52:30590116
a9j8j154:30646983
showlangstatbar:30737417
ecj1e332:30687743
pythonfmttext:30716741
pythoncmvfstr:30726892
fixshowwlkth:30771523
hideindicator:30766887
pythongtdpath:30726887
i26e3531:30792625
gsofa:30778558
pythonnosmt12:30779711
pythonidxpt:30768918
pythondjangots:30768917
pythonnoceb:30776497
copilotsettingt:30767686
e537b577:30772214
h0f32768:30792099
synctok:30783813
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-19 14:50:19,feature,there should be a shortcut for jumping between new and old files,"Use ""Diff Editor: Switch Side"" command to jump from original to modified and vice versa. Notice how selections are mapped.

There is no keybinding for it, but users can configure their own.
"
microsoft/vscode,2023-07-19 04:29:33,feature,Trigger IntelliSense(code completions) after paste or delete?,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
"
microsoft/vscode,2023-07-18 21:48:38,feature,Only show top-level variables in `Outline: Show Variables`,"* Related to https://github.com/microsoft/vscode/issues/146937

For our JS/TS projects, we are declaring functions and components as variables (eg. `const foo = () => {...}`). For the Outline view to be useful, we have `Outline: Show Variables` enabled. 

However, this means a file like:
```typescript
const componentA = () => {
  const state = null;
  const localVar = 1;
};

const componentB = () => {
  return;
};

const funcAsVar = () => {
  const localVar2 = 1;
};
```
outlines like:

<img width=""240"" alt=""Screenshot 2023-07-18 at 2 43 51 PM"" src=""https://github.com/microsoft/vscode/assets/29215801/a36fb3bc-8630-494c-8ea9-8e5c48d0d04c"">

It would be useful if there was a way to only show top-level variables
```
componentA
componentB
funcAsVar
```
May also be useful for other languages that allow similar function-as-variable declaration
"
microsoft/vscode,2023-07-18 19:13:44,feature,add info about go to next / previous accessible view to help menu,This is currently supported in notifications and in the chat responses
microsoft/vscode,2023-07-17 19:28:57,feature,Show more terminal links by default,"Context: https://github.com/microsoft/vscode/issues/188101#issuecomment-1638740467

Let's explore pulling all the links when you open it, with some reasonable timeout. We could also do this lazily by showing just the buffer initially but when you filter we backfill the results."
microsoft/vscode,2023-07-17 12:36:20,feature,Use Find (search text in files) WITHOUT expanding already-collapsed sections,"
Type: <b>Feature Request</b>

When I'm using Find/Replace, the Find is way too aggressive at expanding all my collapsed areas.  It seems to do so on a per-typed-letter basis so depending what the first few letters are, the experience is way more or less annoying.

VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 10.0.19044
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-17 10:24:52,feature,Support link detection in test results terminal,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

- VS Code Version: VSCode 1.80.1, 1.81.0-insider; plugin playwright test for VSCode 1.0.14
- OS Version: Windows 10

Steps to Reproduce:
- Run playwright TS test throwing an error. 
- Open test results viewer in vs code (see picture)
- Try Ctrl+Click on call stack item for the error 

**Expected**
We jump to source code line

**Actual**
Nothing happens.

It stopped to work ~ a week ago, I guess. But used to work in  vs code insider build. But today is stopped to work in the insider build too. (monday, 17.07.2023)
Doesn't reproduce in VSCode  1.79.2.

![Скриншот 17-07-2023 105240](https://github.com/microsoft/vscode/assets/11839300/b4debe61-81f9-45e0-892b-91bb1a211fcc)

"
microsoft/vscode,2023-07-15 21:01:11,feature,Feature Request: Introduce $TM_SUGGESTED_TEXT Variable for Enhanced Snippet Creation,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I'm a devoted user of VSCode and appreciate the powerful snippets feature it offers. I would like to propose adding a new variable, $TM_SUGGESTED_TEXT, to enhance snippet creation.

The $TM_SUGGESTED_TEXT variable would capture the prefix or suggested text provided by VSCode's IntelliSense. This would simplify creating dynamic snippets that adapt to user input.

For instance, using the TODO Tree extension, accessing the suggested text within snippets would greatly improve workflow, enabling more contextual and efficient code templates.

I believe this feature would benefit the VSCode community, empowering users to create more sophisticated and personalized snippets.

Thank you for considering my suggestion to enhance the already remarkable snippets functionality in VSCode!"
microsoft/vscode,2023-07-14 07:13:00,feature,In debug session: Add items to WATCH section by drag-and-drop them from VARIABLES section,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
 Add a feature which will allow to drag-and-drop items from VARIABLES debug section to WATCH section.

<img width=""325"" alt=""image"" src=""https://github.com/microsoft/vscode/assets/13915612/4094a0d9-8b45-402c-ac59-5782cd21babb"">
"
microsoft/vscode,2023-07-13 16:25:58,feature,Add info about sticky scroll to editor accessibility help menu,related to #186659
microsoft/vscode,2023-07-13 00:22:55,feature,Debugger: copy value from hover,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

As far as I understand to copy variables I have to either:

- Find them in variables view
- Type them in debugger -> right click copy

Ultimatively for greater user experience it would be better to add copy button to the hover (I see there is a space on the right near `Hold Alt key to switch to editor language hover`)

And for advanced users add command to copy value, so they can assign it to keybinding and click mouse less, they just need to hover over variable and thats it! (really want this so bad)

![image](https://github.com/microsoft/vscode/assets/46503702/c954c3ec-f50d-4474-8406-e7641ae4a249)
"
microsoft/vscode,2023-07-12 11:14:29,feature,[Feature] Extension hover labelling,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Some extensions provide content on hover in the editor window. Sometimes, some extensions provide similar content on hover - leading to duplicated content.

A (debug?) feature that can be enabled in settings that prints name of extension contributing a hover content. This name can be printed just below the hover content provided by said extension.
"
microsoft/vscode,2023-07-11 23:10:58,feature,Speed up creating troubleshooting profile,Speed up creating troubleshooting profile by copying extensions instead of installing them
microsoft/vscode,2023-07-11 15:49:22,feature,"When restoring a file editor on a UNC path, the security error message should include an option to allow the host.","<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.80.0
- OS Version: Windows 10 22H2

Steps to Reproduce:

1. Have a VSCode session or workspace with open editors hosted on UNC paths (pre 1.78)
2. Upgrade VSCode to a version after the [GHSA-mmfh-4pv3-39hr](https://github.com/microsoft/vscode/security/advisories/GHSA-mmfh-4pv3-39hr) fix
3. Reopen the workspace and hit this error message:
![error](https://github.com/microsoft/vscode/assets/7613032/aa85a237-bccc-42cc-a79b-bbc8132796d9)

What should happen:

3. The error message should provide an option to add the server to the `security.allowedUNCHosts` list, like you get when reopening the file manually:
![dialog](https://github.com/microsoft/vscode/assets/7613032/498d57ef-54dc-447c-94e5-d4b577eb371e)


I have dozens of files on several servers open, so reopening them manually to get this fixed is rather annoying. Integrating that allow dialog into the error message would make for a smoother transition experience for users getting blindsided by this upgrade."
microsoft/vscode,2023-07-10 18:17:58,feature,Pick up TS  5.1.6,Track picking up https://github.com/microsoft/typescript/issues?q=is%3Aissue+milestone%3A%22TypeScript+5.1.6%22+is%3Aclosed
microsoft/vscode,2023-07-10 15:44:01,feature,[Feature Request] Option to enable `remote.SSH.defaultExtensions` for all currently installed extensions.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
To enable a more seamless transition to remote development we'd like to have users extensions transparently enabled on the remote.

I have a script that uses `code --list-extensions` to grab the current set of extensions and add it to the user's local `remote.SSH.defaultExtensions`, but this is cumbersome and error prone.  Ideally we'd have an option to install all existing extensions on the remote by default instead of having to sync them periodically."
microsoft/vscode,2023-07-10 15:03:10,feature,Add accessible view hint,"cc @isidorn 

when a user focuses an item with an accessible view, we should tell them how to access it in the aria label and have a setting to disable that"
microsoft/vscode,2023-07-10 12:59:38,feature,"Add option to Allow configuration / customization of ""Open with VSCode"" in Windows Explorer context menu","I encountered an issue during the installation of Visual Studio Code (VSCode) on Windows where there is an option to add an ""Open with VSCode"" entry to the context menu in Windows Explorer for folders and files. Unfortunately, once this option is selected during installation, there is no built-in way to undo or remove it later within VSCode. Additionally, there is no option to add or enable this feature after the installation.

# Workarounds
Workarounds found online suggest modifying the Windows Registry to remove or add the ""Open with VSCode"" entry. While this solution may work, it involves manual registry editing. Since these workarounds can be found online, this indicates that there is a solution needed for a problem, and the correct solution should be a setting within VSCode.

It would be beneficial to have an option within the VSCode settings or installer to easily enable or disable the ""Open with VSCode"" entry in the Windows Explorer context menu, without the need for manual registry modifications.

# Steps to reproduce:

Install VSCode on Windows.
During the installation process, select the option to add ""Open with VSCode"" to the context menu in Windows Explorer for folders and files.
After installation, observe that there is no built-in option within VSCode to remove or disable this feature.
# Expected behavior:
There should be an option within the VSCode settings or installer to enable or disable the ""Open with VSCode"" entry in the Windows Explorer context menu, allowing users to control this feature without the need for manual registry modifications.

# Environment:

Operating System: Windows"
microsoft/vscode,2023-07-10 09:00:15,feature,An small feature to improve the program efficiently,"Hi
I have an small improvement for vs code.
It will be really efficient in manage files and projects.

Today I wanted to write a new plugin using other plugins. Imagine I have this structure:

plugins folder
>plugin one
>plugin two
>plugin three

In each plugin I have to open some files and all files should be seperated.
What if I can group each plugin opened files? there is a way? yes
I want to group opened files by 3 tabs in the top-top-top of vs code window. If I click on each of those items (tabs), in vs code I can see just opened files for that plugin(folder), and I simply can go to other plugin opened files by choosing right plugin from initial tabs section ( in the top-top-top of vs code window)


Best
Iman Ghorbani
UI/UX Designer and Developer"
microsoft/vscode,2023-07-10 07:39:46,feature,Use other extensions to enhance the emmet extension,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I want to add emmet functionality to my extension, but I don't want to add duplicate code😄. 
Is it possible to define a configuration so that emmet gets the classnames and idnames from other extensions to enhance autocomplete?"
microsoft/vscode,2023-07-07 05:02:45,feature,Rounded corner interface request,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I'm using Windows 10, but the pages of VS Code are more suitable for Windows 11, is it possible to add a theme suitable for Windows 10 at the theme color?
Thanks!"
microsoft/vscode,2023-07-06 19:39:33,feature,Independent options to enable/disable preview for the editor and source control management,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
My personal preference is to disable the default option of opening files in ""preview"" mode, requiring the user to double-click the file in the explorer to ensure that opening another file does not do so in the same tab. By disabling this option, files open in new tabs without the need to double-click them in the explorer.

The default behaviour of opening files in preview mode is great when looking at diffs in the SCM view. However, currently, it is only possible to globally enable and disable opening files in preview mode.

I would like to request adding a separate option to enable/disable opening files in preview mode for SCM. 

Adding a reference to a previous request here: #149891"
microsoft/vscode,2023-07-06 15:22:18,feature,copy notebook output command for built in renderers,"The Jupyter renderer extension provides `copy image` through an icon for images, but the builtin renderers do not support any copying.
We can provide a context menu and toolbar locations for this command"
microsoft/vscode,2023-07-06 12:57:50,feature,allow extensions to update configuration enums ,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->

Extensions can contribute configuration items with enum values. These enum values currently have to be hard-coded in `package.json`. Extensions do not have a way to set these enum values dynamically.

Use case: an extension might want to offer the user a configuration to choose a certain version of an installed software. The list of installed software versions is determined on extension start and cannot be hard-coded.

https://github.com/microsoft/vscode/issues/120940 added something similar, but not exposed to extensions."
microsoft/vscode,2023-07-06 04:09:04,feature,[Accessibility] Make Alt+F2 work in notification area,"
Type: <b>Feature Request</b>

When focused in the notification area and moving up and down each notification item in the list view, it would be so instrumental if Alt+F2 can augment the content in the accessible Monaco view.

VS Code version: Code - Insiders 1.80.0-insider (660393deaaa6d1996740ff4880f1bad43768c814, 2023-07-04T10:57:02.727Z)
OS version: Windows_NT x64 10.0.22621
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-04 11:17:32,feature,Provide a way to manually reload an opened file.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

https://github.com/microsoft/vscode/issues/17643#issue-196856396

<blockquote>

* VSCode Version:1.8.1
* *OS Version:Windows 7 64 bit

I am trying to analyze the log files. It would be very helpful if i have a feature like a button click or a command to reload a file from disk. I can see the file being reloaded automatically but i am expecting a command to reload the file manually/only when needed.

Any suggestion or help on this feature please?

Thanks,
@jai1122.

</blockquote>

was closed prematurely by

https://github.com/microsoft/vscode/issues/17643#issuecomment-350696652


<blockquote>

`File > Revert File` allows to do so. It will fetch the contents of the file from disk even if not dirty.

</blockquote>

Reverting is not reloading. I don't want to revert certain files, even temporarily. I want to *reload* them."
microsoft/vscode,2023-07-03 10:17:11,feature,"Preserve sidebar view sizes when resizing, if they are at their minimum","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
when minimizing side tabs like open editors and time line the bottom one is always enlarged to maximum height, the editor should remember manual resizing so when opening and closing side tabs you go back to how you resized the tabs originally
"
microsoft/vscode,2023-07-02 15:57:23,feature,Add EnvironmentVariableCollection.description to the environment variables explanation,"Repro:

1. Create 2 terminals
2. Hover one tab
3. Click `Show environment contributions`

This should show something like ""Enables the following features: git auth provider""

![image](https://github.com/microsoft/vscode/assets/2193314/96a5c83e-bf66-4986-8732-cb0dbc8aed41)
"
microsoft/vscode,2023-06-30 15:46:38,feature,data science audio and text graph for visually impaired person,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
as we know VScode is most accessible code editor with screen readers.
however, I use matplotlib with this but unfortunately, I didn't find any accessibility with graphs.
in Google Colab with third parti library called audio-plot-lib I can access the graph with audio and screen readers but it's not work with VSCode can you do something for that library, or you can provide separate  facility for all graphes
for more detail you can visit ""https://a11y-ds-intro.hassaku-labs.com/""
"
microsoft/vscode,2023-06-29 21:38:01,feature,Change accessible buffer command navigation keybinding for screen reader users,"From a screen reader user's perspective, the terminal accessible buffer is an editor and should behave as such. 

`Ctrl/Cmd+Up/DownArrow` should jump to the top and bottom line. 

We should keep it as is to align with terminals for non screen reader users and provide a new command navigation keybinding for screen reader users.

cc @jooyoungseo

"
microsoft/vscode,2023-06-29 21:25:23,feature,have accessible view for ghost text completions,@jooyoungseo suggested that it would be great to be able to review the text of the suggestion character by character. We could do this using an accessible view
microsoft/vscode,2023-06-29 18:27:33,feature,Sticky scroll for screen reader users,"Sticky scroll is a feature that allows sighted users to understand the nested context that they are in. 

@kieferrm suggested it would be cool if we had this for screen reader users. 

Imagine you jump to a line where there's a problem reported. You invoke a command which provides context - what class you are in, the function signature, the conditional, etc. 

cc @rperez030 and @jooyoungseo"
microsoft/vscode,2023-06-29 06:19:02,feature,stickyScroll sticks curly brackets instead of class/function if bracket in new line ,"Type: <b>Bug</b>

###  Steps to repoduce

- In settings.json:
```json
    ""editor.stickyScroll.enabled"": true,
    ""editor.stickyScroll.defaultModel"": ""indentationModel""
```
- or alternatively:
```json
    ""editor.stickyScroll.defaultModel"": ""foldingProviderModel""
```
- Create an example file containing a function or class with the opening brace on a new line.
- Scroll down the file.

###  Current Behaviour

When using stickyScroll in the `outlineModel`, it functions as expected. However, I would prefer to use the `indentationModel` because it allows me to see all indentation levels at the top, which I find useful. Unfortunately, in this mode (indentationModel) and also in the `foldingProviderModel`, the curly brackets stick to the screen instead of the class/function when the bracket is on a new line.

![Code_CMOoY7ApCy](https://github.com/microsoft/vscode/assets/136900941/6d1061a8-7f60-445b-be2f-13a81638c4ea)

###  Expected Behaviour

It would be great if both the `indentationModel` and `foldingProviderModel` treated opening brackets on a new line the same way as the `outlineModel` does. This means that the respective function should stick to the screen instead of the bracket.

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-28 16:38:51,feature,add an accessibility verbosity setting for notebooks so users can discover the notebook help menu,"alt+F1 can be used in notebooks to open the accessibility help, but I don't think that is noted anywhere."
microsoft/vscode,2023-06-28 09:00:45,feature,Scrolling at the edges of the reference view editor will scroll the outer editor,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.79.2
- OS Version: Windows_NT x64 10.0.19045

Steps to Reproduce:

1. Use go to references/definitions to open a reference view window
2. Scroll to the bottom of the embedded editor
3. Keep scrolling will scroll the outer editor

It's quite annoying sometimes, make it behaves like the file tree in the right side would be nice

https://github.com/microsoft/vscode/assets/68118705/1ae23b05-4ddb-46ef-965b-2f06d5cf0107

But I saw this, so is this behavior intended? If so may I ask why?

https://github.com/microsoft/vscode/blob/efb49cc271d6eea5634ac395e36e1e4cd108a447/src/vs/editor/contrib/gotoSymbol/browser/peek/referencesWidget.ts#L308
"
microsoft/vscode,2023-06-27 21:52:22,feature,Enabling tag telemetry for Go,"<!-- Thank you for submitting a Pull Request. Please:
* Read our Pull Request guidelines:
  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests
* Associate an issue with the Pull Request.
* Ensure that the code is up-to-date with the `main` branch.
* Include a description of the proposed changes and how to test them.
-->
"
microsoft/vscode,2023-06-27 18:14:34,feature,"Let me click on ""hidden lines"" text to unfold","Testing #186213

I was trying to double-click on the the ""123 Hidden Lines"" text to unfold that region. I would love to be able to do that if possible

<img width=""262"" alt=""image"" src=""https://github.com/microsoft/vscode/assets/323878/9bd04b99-b999-4d84-b390-7b74b5ed5d35"">
"
microsoft/vscode,2023-06-27 18:02:15,feature,Peek Call/Type Hierarchy not in command palette,"Testing #186213

<img width=""497"" alt=""image"" src=""https://github.com/microsoft/vscode/assets/323878/ba07c303-4a1a-460f-9da5-d582c3b43d85"">

<img width=""667"" alt=""image"" src=""https://github.com/microsoft/vscode/assets/323878/436aab1c-305a-4b2d-aab2-a32c9d9f6d6b"">
"
microsoft/vscode,2023-06-27 06:55:27,feature,Can't view embedded html PDF files in vscode jupyter,"### Discussed in https://github.com/microsoft/vscode-jupyter/discussions/13769

<div type='discussions-op-text'>

<sup>Originally posted by **MikeLemo1** June 27, 2023</sup>
I'm trying to reference an internal PDF file to view a single page from it with embedded HTML in VSCode Jupyter plugin with no luck as it just displays nothing or blank rectangles with no luck(just for reference the same method works in mkdocs)
Here is what I tried to do it with assuming the PDF file is sitting in the same folder as the .ipynb file:

```py
from IPython.display import IFrame

# display(HTML('<embed id=""myPDF2"" src = ""STM32F302xD_E_MCU.pdf#page=34&zoom=150&toolbar=1&statusbar=1&viewrect=100%,100%,50%,50%"" type=""application/pdf"" width=""100%"" height=800px />'))

IFrame( src = 'STM32F302xD_E_MCU.pdf#page=34&zoom=150&toolbar=1&statusbar=1', width=700, height=600)
``` 

Also tried: 

```
%%HTML
<embed id=""myPDF2"" src = ""STM32F302xD_E_MCU.pdf#page=34&zoom=150&toolbar=1&statusbar=1&viewrect=100%,100%,50%,50%"" type=""application/pdf"" width=""100%"" height=800px />
```

Any idea what can be done to help it work?</div>"
microsoft/vscode,2023-06-26 18:13:04,feature,Support creating a new task of a particular type programmatically,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

In our extension we would like to guide the user to create a task of a specific type. It's already possible to read all the tasks and determine whether one of the desired type exists, however the programmatic flow seems to require executing `workbench.action.tasks.configureTaskRunner` which exposes all task types.

Would it be possible to expose a function in the API to create a specific task (e.g. by executing [_configureTask](https://github.com/microsoft/vscode/blob/main/src/vs/workbench/contrib/tasks/browser/abstractTaskService.ts#L3223)) or accepting a filter in `workbench.action.tasks.configureTaskRunner` similar to [runTask](https://github.com/microsoft/vscode/blob/main/src/vs/workbench/contrib/tasks/browser/abstractTaskService.ts#L2758)?
"
microsoft/vscode,2023-06-25 12:25:18,feature,Small UI Improvement ,"Type: <b>Bug</b>

Looks like the tooltip (when hovering on file tabs) utilizes a light theme approach, meanwhile all the app is used a dark theme. Appearing this tooltip when you work in the dark theme for a while is painful for the eyes and difficult to read. So, would be great to have it in a dark color and there will not be such a gap in contrast difference, it will be more pleasant to eyes. 

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz (8 x 1800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.89GB (1.12GB free)|
|Process Argv|--crash-reporter-id d513e5e9-a478-4d2f-b140-dc7d93a863a3|
|Screen Reader|no|
|VM|0%|
</details>Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627cf:30244335
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30773604
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
2e4cg342:30602488
pyind779:30671433
f6dab269:30613381
pythonsymbol12:30671437
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
pythongtdpath:30769146
bgfeh915:30769767
gsof1:30774496
dh2dc718:30770000
pythonidxpt:30772539
pythondjangotscf:30772537

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-24 09:28:32,feature,Increase the width of breadcrumb box to support longer function name indexing.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Hello VSCode Team,
Breadcrumb box is quite a good feature for me when indexing functions in a larger file.

In a large project, I see lots of functions named like this: same long prefix but different short suffix, this makes troubles in finding them in breadcrumb box with a fixed width. I wonder if you can consider increase it, thanks a lot:)

XXXXXXXXXXXXXXXXXXXXXXXXXXX_YYYYYYYYY_AAAAA...
XXXXXXXXXXXXXXXXXXXXXXXXXXX_YYYYYYYYY_BBBBB...
XXXXXXXXXXXXXXXXXXXXXXXXXXX_YYYYYYYYY_CCCCC...

Best Regards,
Mingliang"
microsoft/vscode,2023-06-22 20:46:44,feature,"Picture-in-Picture for terminal, using new Google Chrome API","As seen [in this tweet](https://twitter.com/antfu7/status/1671974341032935424?s=20) by @antfu, Google Chrome mentioned in a blog post [they are planning to add a new PiP mode for html elements](https://developer.chrome.com/docs/web-platform/document-picture-in-picture/)

This would be extremely useful for the terminal in vscode.dev and github codespaces, and also on local (if electron supports it). Is there any plan to add support?

https://github.com/microsoft/vscode/assets/13242392/45f989d9-5fe2-4a26-8bc3-fbc4e5a284e2

"
microsoft/vscode,2023-06-21 14:52:48,feature,Diff Editor: Collapse Unchanged Code - Show Context Header,"It would be very helpful to include the current symbol name in the collapsed unmodified code indicator (see `DiffEditorWidget2` and `constructor`):

![chrome_BE4rflvO6W](https://github.com/microsoft/vscode/assets/2931520/97dcd4ee-7b98-44fa-9ed0-71770cbb9963)

Verification steps:
* Open a diff in vscode (not monaco editor playground)
* Enable collapsing unchanged regions (map icon in editor titlebar)
* Observe that the collapsed code shows a header indicating which symbol started inside of the unchanged code but ended outside of it.
* Verify that clicking on it reveals the symbol"
microsoft/vscode,2023-06-21 09:14:29,feature,Disable alt toggle of menubar altogether,"Since `alt` is used to `move lines up and down`, if you press it in a certain way (accidentally) it will toggle the menu bar, which I prefer to be hidden, always. Would be nice if there was a way to never show the menu bar even if `alt` is pressed, because it is disrupting."
microsoft/vscode,2023-06-20 21:12:25,feature,Consider providing screen reader with the chat response for inline chat,"as a screen reader user:

1. Start code chat
2. Type a request and hit enter
3. 🐛 tab 7 times to focus response

Perhaps we should align with the chat view and update via `status` with the response when it is ready

"
microsoft/vscode,2023-06-20 16:49:46,feature,Consider adding audio cues for inline chat,Now we have audio cues - which are off by default atm - in the chat view. We might want these also in the inline chat. 
microsoft/vscode,2023-06-20 14:30:22,feature,Improve presentation of startup perf raw marks,"We should put these in a table:

![image](https://github.com/microsoft/vscode/assets/2193314/3dbeb9d5-ea68-4479-832f-f7c67bb5fa1d)

Something like this:

## Raw Perf Marks: main

| Name | Timestamp | Delta | Total
|---|---|---|---
| code/timeOrigin | 1687187191725.414 | 0 | 0
| code/didStartMain | 1687187191797 | 71.5859375 | 71.5859375
| code/willStartCrashReporter | 1687187191812 | 15 | 86.5859375
| code/didStartCrashReporter | 1687187191838 | 26 | 112.5859375
| code/willGenerateNls | 1687187191841 | 3 | 115.5859375
| code/mainAppReady | 1687187191872 | 31 | 146.5859375
| code/willLoadMainBundle | 1687187191872 | 0 | 146.5859375
| code/fork/willLoadCode | 1687187191885 | 13 | 159.5859375
| code/registerFilesystem/file | 1687187191998 | 113 | 272.5859375
| code/didLoadMainBundle | 1687187192018 | 20 | 292.5859375

cc @bpasero, @jrieken "
microsoft/vscode,2023-09-28 20:59:06,question,Breadcrumbs are limited to 6 items within a file,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: recent main - 8b5719b3b65cc5ffd9145f5fca98ab225fd717d6 (also v1.82.2)
- OS Version: Windows 11

Steps to Reproduce:

1. Just open any deeply nested file. Personally, I used a JSONified version of the [Docker API specs](https://docs.docker.com/engine/api/v1.43/).
2. You'll see that the breadcrumbs only go to a certain level:

![Screenshot 2023-09-24 234426](https://github.com/microsoft/vscode/assets/16936908/4a925184-d429-4a4d-bab9-3f835b26c3ca)

Given that the breadcrumbs container is scrollable, I assume that this is a bug, not an intended limitation.

I traced it until `outlineModel.ts:OutlineGroup._getItemEnclosingPosition()`, because I was hoping that I could maybe see whether it is intended after all. But once I got there and saw that it seems to be caused by an incomplete tree (`children` are empty from that 6th level onwards), I couldn't justify sinking more time into this."
microsoft/vscode,2023-09-28 15:53:17,question,autocomplete style  in javascript,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Good morning, I recently started a JavaScript course using the self-taught official documentation from Firefox. First and foremost, I'd like to express my gratitude to all the developers who contribute to this project, both voluntarily and professionally.

I'm new to this, so please forgive me if I may be taking up your time, but I couldn't find a solution to the problem I encountered.

I kindly request that you add the "".style"" object to the default autocompletion provided by VS Code for JavaScript. You know, the one that allows you to modify certain CSS properties of HTML from JavaScript. I haven't come across any extensions that achieve this.

The solution suggested by artificial intelligences was for me to either create an extension myself or attempt to configure autocompletion suggestions from the ""settings.json"" file in VS Code. Unfortunately, neither approach worked.

Every time I write, for example:

element.style.

The autocompletion changes it to:

element.computedStyleMap.

What I end up doing is pressing space to prevent it from autocompleting.

I apologize if there was already a previous solution that I couldn't find, and I appreciate you taking the time to respond.


"
microsoft/vscode,2023-09-28 04:13:06,question,VS code stops running react code,"Type: <b>Bug</b>

Hi,

VS code stops automatically while running react code, this happens frequently. After this every time npm start command has to be given. This issue is very annoying. Kindly check the issue and resolve asap.

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.73GB (0.65GB free)|
|Process Argv|--crash-reporter-id 232489fa-7944-4d11-b31b-582170cfc1a7|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (16)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-postgres|cko|1.4.3
prettier-vscode|esb|10.1.0
fabric8-analytics|red|0.7.0
java|red|1.22.1
LiveServer|rit|5.7.9
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-boot-dev-pack|vmw|0.2.1
vscode-spring-boot|vmw|1.49.0
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.40.0
vscode-maven|vsc|0.42.0
vscode-spring-boot-dashboard|vsc|0.13.1
vscode-spring-initializr|vsc|0.11.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
282f8724:30602487
89544117:30613380
showlangstatbar:30737416
0bi6i642:30841073
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
copilotsettingc:30839828
synctok:30821570
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30821571
dsvsc015:30845448

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-26 20:41:36,question,Unable to deny Built-in Port Forwarding,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.2
- OS Version: Windows 11 22H2 

Issue: [Release notes](https://code.visualstudio.com/updates/v1_82) specify the new ""Built-in port forwarding"" feature that allows users to forward port from within VS Code and make it available publicly. 
The [documentation](https://code.visualstudio.com/docs/editor/port-forwarding) specifies that we can allow/deny access to domain `global.rel.tunnels.api.visualstudio.com` to be able to control this feature. 
During our testing it seems the feature still works even after we denied the domain `global.rel.tunnels.api.visualstudio.com` on our enterprise DNS secure gateway Cisco Umbrella. 

Steps to Reproduce:

1. Denied domain `global.rel.tunnels.api.visualstudio.com` through our enterprise DNS secure gateway Cisco Umbrella. 
2. Validated that the domain `global.rel.tunnels.api.visualstudio.com` is blocked by trying to browse to it - shows the blocked message from opendns. 
3. Tried steps to 'Forward a Port' and it still allows the user to forward port. 
"
microsoft/vscode,2023-09-26 05:14:01,question,Sponsored issue: Support Request - Infinite Scrolling Web App,"Description:
Hello,
I hope this message finds you well. I'm currently working on implementing infinite scrolling for a web project and have run into an issue that I could use some assistance with.

Issue:
I have set up infinite scrolling on my website to load additional content as users scroll down the page. However, I've noticed that the new content is not loading as expected when users reach the bottom of the page. Instead, the page remains static, and no new data is loaded.
I'm using a React-based frontend with a Node.js backend.
I've followed tutorials and documentation to set up the infinite scrolling feature, but I seem to have missed something.
I can share relevant code snippets or configurations if needed.
I would greatly appreciate your guidance on resolving this issue and getting infinite scrolling to work correctly on my website.
Thank you for your assistance, and I look forward to your response.

Best regards,
Aniket Mandloi

## Priority Support

- @aniketmandloi is using  [Mintycode](https://mintycode.io) to fund this issue.
- If you would like to accept ![amount](https://imageupload.io/ib/zwkd2EaAY6YpPsg_1693209112.png) bounty for solving this issue join [Mintycode](https://mintycode.io/profile?action=CREATOR_SUPPORT_MODAL&requestId=aae585dd-6912-4162-8133-7e3005ba1a67&owner=microsoft&name=vscode).
- Thank you in advance for helping.

[![mintycode](https://imageupload.io/ib/LvxMwrPxam1JW9e_1693210482.png)](https://mintycode.io)
"
microsoft/vscode,2023-09-25 13:44:18,question,Terminal process failed to launch,"Type: <b>Bug</b>

""The terminal process failed to launch: Starting dierectory(cwd) ""C:/Program Files/--------/-------"" does not exist"" How do we solve this terminal issue?

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz (8 x 2803)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.68GB (3.74GB free)|
|Process Argv|--crash-reporter-id 75eb889d-432f-4f14-a87e-2fbb546928cd|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (9)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-ros|ms-|0.9.2
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.20
remote-containers|ms-|0.309.0
cpptools|ms-|1.17.5
java|red|1.22.1
vscode-xml|red|0.26.1
vscode-java-debug|vsc|0.54.0
vscode-java-test|vsc|0.40.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
962ge761:30841074
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
copilotsettingc:30839828
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-25 02:47:11,question,problema con idioma vscode,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:
no puedo descargar la extensión spanish en vscode  y ya probe todo que puedo hacer este es el error que me tira lo desinstale vscode en limpio 3 veces y lo volvi a instalar y me sigue tirando error tengo la versio 1.82.2 nesesito ayuda en windows 10 64 bits
 2023-09-24 23:33:30.931 [info] [perf] Render performance baseline is 391ms
2023-09-24 23:34:44.868 [error] Error: Cannot read the extension from /c:/Users/Lenovo/.vscode/extensions/ms-ceintl.vscode-language-pack-es-1.82.2023091309
    at W.w (c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\out\\vs\\code\\node\\sharedProcess\\sharedProcessMain.js:92:15050)
    at async B.u (c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\out\\vs\\code\\node\\sharedProcess\\sharedProcessMain.js:92:18230)
1. 
2. 
"
microsoft/vscode,2023-09-23 04:41:43,question,Terminal problem,"Type: <b>Performance Issue</b>

when I installed node js I could not type in  my terminal

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 6.0.6002
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 3 2200U with Radeon Vega Mobile Gfx   (4 x 2495)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|3.66GB (0.63GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   139	 11216	code main
    2	   118	  2200	window [2] (Issue Reporter)
    0	    43	  4488	fileWatcher [1]
    0	    81	  5936	extensionHost [1]
    0	    45	  4948	     electron-nodejs (""C:\\Users\\Hello\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\Hello\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\html-language-features\\server\\dist\\node\\htmlServerMain"" --node-ipc --clientProcessId=5936)
    0	    36	  9408	     electron-nodejs (""C:\\Users\\Hello\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\Hello\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\json-language-features\\server\\dist\\node\\jsonServerMain"" --node-ipc --clientProcessId=5936)
    0	    46	  7192	ptyHost
    0	    20	 10164	     winpty-agent
    1	     7	  7816	       C:\\Windows\\system32\\conhost.exe 0x4
    0	    51	  8024	       C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe
    0	    20	 10412	     winpty-agent
    0	     7	  1612	       C:\\Windows\\system32\\conhost.exe 0x4
    0	    56	 11188	       C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe
    0	    31	  9448	   utility-network-service
    0	    58	  9484	shared-process
    1	    69	 10460	   gpu-process
    0	   196	 10852	window [1] (index.html - Import and Exports - Visual Studio Code)
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (index.html - Import and Exports - Visual Studio Code)
|    Folder (Import and Exports): 3 files
|      File types: js(2) html(1)
|      Conf files:;
```

</details>
Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
282f8724:30602487
89544117:30613380
vscrp:30673768
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
copilotsettingc:30839828
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-22 14:38:12,question,pow() function not working in c++,"#include <iostream>
#include <math.h>
using namespace std;

int main()
{
    int n;
    cin >> n;

    int ans = 0;
    int i = 0;

    while (n != 0)
    {
        int bit = n & 1;
        ans = (bit * pow(10, i)) + ans;
        n = n >> 1;
        i++;
    }
    cout << ans << endl;
   
}

"
microsoft/vscode,2023-09-22 01:45:20,question,Calculation is not being done in higher datatype for cpp code,"
Type: <b>Bug</b>

in .cpp file just cout<<5/2.0; It's answer shoulde be 2.5 but in vs code it is showing 2 which is wrong.

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3-7020U CPU @ 2.30GHz (4 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.90GB (1.29GB free)|
|Process Argv|C:\\\\Users\\\\dell\\\\Desktop\\\\D --crash-reporter-id 8ac51244-69d0-49d5-8323-8dee75c6270b|
|Screen Reader|no|
|VM|67%|
</details><details><summary>Extensions (18)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-tailwindcss|bra|0.10.0
bracket-pair-toggler|dzh|0.0.3
chatgpt-gpt4-gpt3-vscode|Eas|1.1.8
auto-rename-tag|for|0.1.10
code-runner|for|0.12.0
c-cpp-runner|fra|8.1.0
vscode-pull-request-github|Git|0.73.2023091209
vscode-language-babel|mgm|0.0.39
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
vscode-thunder-client|ran|2.12.1
LiveServer|rit|5.7.9
es7-react-js-snippets|rod|1.9.3
cmake|twx|0.0.17
vscode-lldb|vad|1.10.0
vscode-icons|vsc|12.5.0
JavaScriptSnippets|xab|1.8.0

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
copilotsettingc:30839828
synctok:30821570
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-21 16:14:30,question,Issues with source control,"
Type: <b>Performance Issue</b>

i have over 937 changes to sync and it is not working, i just completed cs50p
and most of my folders are not updated to my github
how do i push these changes to my repository?
how can i clone this repository (it is private and id like it to  be public)
please help
thank you

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.19045
Modes:
Remote OS version: Linux x64 5.15.0-1041-azure

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz (8 x 2691)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|3.91GB (0.68GB free)|
|Process Argv|--crash-reporter-id ab4cdca6-5af7-4e51-8543-bccccc3656ae|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|Codespaces: studious space couscous|
|OS|Linux x64 5.15.0-1041-azure|
|CPUs|AMD EPYC 7763 64-Core Processor (2 x 3242)|
|Memory (System)|7.75GB (5.72GB free)|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	    74	 10840	code main
    0	    89	  8800	window [3] (Issue Reporter)
    0	   142	 10532	window [1] (seasons.py - 135445414 [Codespaces: studious space couscous] - Visual Studio Code)
    0	    44	 14476	shared-process
    0	    34	 38960	fileWatcher [1]
    0	   136	 46116	extensionHost [1]
    0	    12	 47004	   crashpad-handler
    0	    16	 47196	   utility-network-service
    0	    77	 49332	   gpu-process

Remote: Codespaces: studious space couscous
CPU %	Mem MB	   PID	Process
    0	     0	   282	remote agent
    0	     0	   306	   fileWatcher
    0	     0	   677	   ptyHost
    0	     0	 12579	     /usr/bin/bash --login
    0	     0	 12014	   extension-host
    0	     0	 12589	     /vscode/bin/linux-x64/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/node /home/ubuntu/.vscode-remote/extensions/ms-python.vscode-pylance-2023.9.20/dist/server.bundle.js --cancellationReceive=file:e60d5c3ae6864b329640c8625645e31d2b124ef03b --node-ipc --clientProcessId=12014
    0	     0	 13281	     /vscode/bin/linux-x64/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/node /vscode/bin/linux-x64/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=12014
    0	     0	 12018	   fileWatcher
    0	     0	 15315	   /bin/sh -c /usr/bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	     0	 15316	     /usr/bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
```

</details>
<details>
<summary>Workspace Info</summary>

```

|  Remote: Codespaces: studious space couscous|    Folder (135445414): 192 files|      File types: py(55) gitignore(9) TAG(8) md(8) csv(4) jpg(4) json(2)
|                  pdf(2) png(2) pub(1)
|      Conf files: settings.json(1);
```

</details>
<details><summary>Extensions (34)</summary>

Extension|Author (truncated)|Version
---|---|---
codespaces|Git|1.15.3
cs50|CS5|0.0.1
ddb50|CS5|2.0.0
explain50|CS5|1.0.0
extension-uninstaller|CS5|1.0.7
phpliteadmin|CS5|0.0.1
style50|CS5|0.0.1
codespaces|Git|1.15.3
vscode-pull-request-github|Git|0.72.0
prettier-sql-vscode|inf|1.6.0
vscode-pdf|mat|0.0.6
vscode-docker|ms-|1.26.1
vscode-language-pack-bg|MS-|1.48.3
vscode-language-pack-cs|MS-|1.82.2023091309
vscode-language-pack-de|MS-|1.82.2023091309
vscode-language-pack-es|MS-|1.82.2023091309
vscode-language-pack-fr|MS-|1.82.2023091309
vscode-language-pack-hu|MS-|1.48.3
vscode-language-pack-it|MS-|1.82.2023091309
vscode-language-pack-ja|MS-|1.82.2023091309
vscode-language-pack-ko|MS-|1.82.2023091309
vscode-language-pack-pl|MS-|1.82.2023091309
vscode-language-pack-pt-BR|MS-|1.82.2023091309
vscode-language-pack-ru|MS-|1.82.2023091309
vscode-language-pack-zh-hans|MS-|1.82.2023091309
vscode-language-pack-zh-hant|MS-|1.82.2023091309
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.20
cpptools|ms-|1.17.5
hexeditor|ms-|1.9.12
vsliveshare|ms-|1.0.5883
java|red|1.22.1
vscode-java-debug|vsc|0.54.0
gitdoc|vsl|0.1.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyonecf:30548226
3biah626:30602489
89544117:30613380
vscrpc:30673769
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
copilotsettingc:30839828
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015cf:30829746

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-20 16:28:33,question,[Feature Request] Add workbench action to split editor terminal below,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I currently have the following actions available:

```
workbench.action.createTerminalEditor
workbench.action.createTerminalEditorSameGroup
workbench.action.createTerminalEditorSide
```

I'd like to be able to split an editor terminal below. It seems like the UI can do this, because I can drag a terminal editor below another and have a horizontal split:

<img width=""1624"" alt=""image"" src=""https://github.com/microsoft/vscode/assets/15133041/9694066a-e177-404b-8b1e-e3337a57fc90"">

So I would love to have a `workbench.action.createTerminalEditorBelow` action to split it below 😊"
microsoft/vscode,2023-09-18 18:20:14,question,Giveing wrong output in C programming compilation,"Type: <b>Bug</b>

Mathematical calculation of some number having 5 in its digit, gives worng output which is decremented by 1 from original result.
The code which is wrote is of Armstrong Number, and i mentioning it below-

#include<stdio.h>
#include<math.h>
int main() {
    int num,s,arm=0,n,count=0,r;
    printf(""Enter any number\\n"");
    scanf(""%d"",&num);
    n=num;
    s=num;
    while (n!=0)
    {
        n=n/10;
        count++;
    }
    printf(""DIGIT\\t= %d\\n"",count);
    while (s!=0)
    {
        r=s%10;
        arm+=pow(r,count);
        s=s/10;
    }
    printf(""arm = %d"",arm);
    
}

In this code after compilation when we give input of any number having 5 in its digit for example 25, 55, 153, 50, 457, etc. It gives wrong output.
The output result decremented by 1 from original result.

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.76GB (2.22GB free)|
|Process Argv|--crash-reporter-id 2a3cab1c-42c1-4a04-8b5e-f30b9eb5e3ad|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (15)</summary>

Extension|Author (truncated)|Version
---|---|---
code-runner|for|0.12.0
python|ms-|2023.16.0
vscode-pylance|ms-|2023.9.10
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
java|red|1.22.1
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492:30256859
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
282f8724:30602487
f6dab269:30613381
vscrp:30673768
2i9eh265:30646982
showlangstatbar:30737416
962ge761:30835153
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
copilotsettingc:30834057
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-17 07:02:24,question,Terminal Issue.,"
Type: <b>Performance Issue</b>

Intergrated Terminal exiting improperly after running the code ny giving the command to run C code , even if the code is correct.

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.75GB (1.34GB free)|
|Process Argv|--crash-reporter-id ed0fb855-e722-4c59-b225-db7fc284d4eb|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    1	   100	 10088	code main
    1	    94	  2408	window [2] (Issue Reporter)
    0	    75	  9752	ptyHost
    0	     7	 14636	     conpty-agent
    0	    66	 19292	     C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -noexit -command ""try { . \\""d:\\Microsoft VS Code\\resources\\app\\out\\vs\\workbench\\contrib\\terminal\\browser\\media\\shellIntegration.ps1\\"" } catch {}""
    0	    72	 11352	fileWatcher [1]
    0	   216	 13604	window [1] (str_problems.c - Untitled (Workspace) - Visual Studio Code)
    0	    37	 16664	   utility-network-service
    0	    26	 17176	   crashpad-handler
    0	   168	 17528	   gpu-process
    0	    88	 20340	shared-process
    0	   136	 21976	extensionHost [1]
    0	    35	  3108	     c:\\Users\\RAKSHIT\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64\\bin\\cpptools.exe
    0	     4	 10368	       ""c:\\Users\\RAKSHIT\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64\\bin\\cpptools.exe""
    0	    18	 11952	         c:\\Users\\RAKSHIT\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64/bin/cpptools-srv.exe 3108 {135BBCE3-1572-41DA-9897-0C35EB0DB490}
    0	     6	 13620	           C:\\WINDOWS\\system32\\conhost.exe 0x4
    0	     6	 14544	       C:\\WINDOWS\\system32\\conhost.exe 0x4
    0	    71	 11764	     electron-nodejs (""D:\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""d:\\Microsoft VS Code\\resources\\app\\extensions\\json-language-features\\server\\dist\\node\\jsonServerMain"" --node-ipc --clientProcessId=21976)
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (str_problems.c - Untitled (Workspace) - Visual Studio Code)
|    Folder (Online-Recruitment): 25 files
|      File types: jpg(8) css(4) html(4) ico(1) jpeg(1) js(1) md(1)
|      Conf files:
|    Folder (My_Portfolio): 5 files
|      File types: html(1) yaml(1) xlsx(1) css(1)
|      Conf files:
|    Folder (DSA-practice-problems): 33 files
|      File types: c(13) exe(12) json(4) c++(1) md(1)
|      Conf files: launch.json(1) settings.json(1) tasks.json(1)
|      Launch Configs: cppdbg(3)
|    Folder (Java_Practice): 42 files
|      File types: class(26) java(16)
|      Conf files:;
```

</details>
<details><summary>Extensions (31)</summary>

Extension|Author (truncated)|Version
---|---|---
project-manager|ale|12.7.0
blackbox|Bla|1.1.58
gitignore|cod|0.9.0
composer-php-vscode|DEV|1.38.13918
phptools-vscode|DEV|1.38.13918
profiler-php-vscode|DEV|1.38.13918
githistory|don|0.6.20
vscode-html-css|ecm|1.13.1
code-runner|for|0.12.0
kotlin|fwc|0.2.31
Kotlin|mat|1.7.1
jupyter|ms-|2023.8.1002501831
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
java|red|1.22.1
LiveServer|rit|5.7.9
open-in-browser|tec|2.0.0
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.14
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
cors-browser|Wsc|1.0.11
php-debug|xde|1.33.0
vscode-open-in-github|ziy|1.3.6

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30792227
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
2e4cg342:30602488
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
0bi6i642:30835152
03d35959:30757346
pythonfmttext:30731395
9b8hh234:30694863
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-16 11:14:59,question,Please provide a complete list of files and folders generated by VSCode and their locations,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
On re-opening a project and navigating to the terminal via `Ctrl + backtick` (Toggle Terminal), I notice that the terminal states:

` *  History restored `

This would mean that there is some file/folder on disk somewhere which tracks this project and stores this data. 

Can a complete list of files and folders generated by VSCode and their locations be documented and put up on the web? My hard disk space is running out and I would like to keep deleting such extra folders routinely so that it does not eat into the remaining space.

In other words, what files and folders can one safely delete so as to restore VSCode to the status it would have been under right after the very first time it has been freshly installed on a new machine without any extensions, cache files, etc. without affecting its functionality."
microsoft/vscode,2023-09-15 06:09:43,question,Where are the Extensions Stored?,"
Type: <b>Performance Issue</b>

I need to know where are the VS Code Extensions stored.  It is not in the VS Code subdirectory.

VS Code version: Code 1.82.2 (abd2f3db4bdb28f9e95536dfa84d8479f1eb312d, 2023-09-14T05:55:25.390Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.77GB (8.81GB free)|
|Process Argv|--crash-reporter-id d28106b3-cd04-490a-b194-f819821f7d80|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

"
microsoft/vscode,2023-09-14 12:06:09,question,Please make sure vscode's window title bar changes color when the Windows focus is acquired.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Please make sure vscode's window title bar changes color when the Windows focus is acquired.
I will appreciate seeing the top bar changing its background color to blue (in my case) when the windows focus is on vscode.
Thanks!"
microsoft/vscode,2023-09-13 13:17:38,question,Terminate batch job issue,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.82.0
- OS Version: 

Steps to Reproduce:

1. press ctrl+c for terminate project
2. press N for no. But still project terminate if we select No.
"
microsoft/vscode,2023-09-13 00:27:38,question,please allow for multiple tunnels on same machine,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
It would be amazing if remote tunneling didn't force us into one server per machine (i.e. allow optional work around of https://github.com/microsoft/vscode/issues/171520)

An amazing feature of tunnel is to be able to use it in HPC environments, for example starting up a tunnel through an sbatch job with specific allocated resources. This allows users to connect to the newly created tunnel and use vscode's debugging with specific resources managed through cluster management systems like slurm. 

The issue is that if this happens more than once on a machine, any slurm allocation after the first will just point users to the tunnel of the first allocation, leading to conflict of resources. This would be an easy solution to workflow issues described in https://github.com/microsoft/vscode-remote-release/issues/1722 . 

Could supporting multiple tunnels on a machine be brought back?"
microsoft/vscode,2023-09-11 07:33:42,question,Open files from different folders in one workspace without having to split editor,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
When adding a folder to a workspace and opening a file from the new folder, it automatically opens **instead** of an already opened file in the workspace, closing the file from the other active folder in the workspace. The only workaround seems to be to ""open to the side"" which is quite tedious after a while. 
So it would be nice if the files could all automatically open besides each other in the same editor without having to split."
microsoft/vscode,2023-09-09 13:37:00,question,Add terminal highlighting support,"
Type: <b>Feature Request</b>

The reason is that when I used WSL to compile the opencv library, the information displayed by the terminal was all white font, and it was difficult to capture some key information. Can I add a highlight effect like mobaxterm? However, I remember that GitHub's codespace had some highlighting, but as a user there was no more convenient way to set it up, or plugins.

VS Code version: Code 1.82.0 (8b617bd08fd9e3fc94d14adb8d358b56e3f72314, 2023-09-06T22:07:07.438Z)
OS version: Windows_NT x64 10.0.22621
Modes:
Connection to 'wsl+Ubuntu-22.04' could not be established


<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-09 03:29:09,question,"logging into cs50.dev is ok, but VS code space does not load ","ADD ISSUE DESCRIPTION HERE:
logging into cs50.dev is ok, but VS code space does not load , it lasts forever but never ends. 
I have reloaded, logout - login, restarted browser, rebooted my windows lap.
I can not continue with the course, really sad :(

Version: 1.82.0
Commit: 8b617bd08fd9e3fc94d14adb8d358b56e3f72314
User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->"
microsoft/vscode,2023-09-08 11:38:19,question,Search files (advanced search - filter a particular extension),"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Is it possible to search file by keyword only for a particular extension. For example, I want to search that lists all files that have extension `md` and the search term `time`"
microsoft/vscode,2023-09-07 17:47:37,question,grille.py,"Type: <b>Bug</b>

I cannot be successfull when I am trying to execute my programms. But, before my programms were running without problem. I use Python language.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:18:39.991Z)
OS version: Linux x64 5.15.0-83-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-4670K CPU @ 3.40GHz (4 x 3600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off|
|Load (avg)|1, 2, 1|
|Memory (System)|7.63GB (5.98GB free)|
|Process Argv|--no-sandbox --force-user-env --unity-launch --crash-reporter-id e9151a7f-cd85-47e8-9186-149c97afed6b|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-language-pack-fr|MS-|1.81.2023081609
python|ms-|2023.14.0
vscode-pylance|ms-|2023.9.10


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscoreces:30445986
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
0bi6i642:30823812
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30821571
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-07 14:40:11,question,My terminal keeps giving me errors.I can't run my codes in the terminal.,"Type: <b>Bug</b>

Hello, I have been using visual studio code for a long time, but recently I can not run my code in the terminal, no matter what language. When I run a code in Python language, it gives the following error in the terminal:

""c:\\Users\\LENOVO-GAMNG\\Desktop\\C\\hey.py : The term 'c:\\Users\\LENOVO-GAMNG\\Desktop\\C\\hey.py' is not recognised as the name 
of a cmdlet, function, script file, or operable programme. Check the spelling of the name, or if a path was included, verif
y that the path is correct and try again.
At line:1 char:1
+ c:\\Users\\LENOVO-GAMNG\\Desktop\\C\\hey.py
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo : ObjectNotFound: (c:\\Users\\LENOVO-GAMNG\\Desktop\\C\\hey.py:String) [], CommandNotFoundExcepti  
   on
    + FullyQualifiedErrorId : CommandNotFoundException""

When I run a code in C in the terminal, it gives the following error:

""cd : Cannot find path 'C:\\Users\\LENOVO-GAMNG\\Desktop\\C\\' because it does not exist.
At line:1 char:1
+ cd ""c:\\Users\\LENOVO-GAMNG\\Desktop\\C\\"" ; if ($?) { gcc 9_From_ User_ ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo : ObjectNotFound: (C:\\Users\\LENOVO-GAMNG\\Desktop\\C\\:String) [Set-Location], ItemN 
   otFoundException
    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.SetLocationCommand""

because of these errors, I can not write code in the terminal in any way, what is the solution to this, please urgently.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz (4 x 2904)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|11.88GB (6.93GB free)|
|Process Argv|--crash-reporter-id aabdbfeb-c1b6-4fbe-8753-d6b16ef5132e|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
prettier-vscode|esb|10.1.0
code-runner|for|0.12.0
vscode-language-pack-tr|MS-|1.81.2023081609
python|ms-|2023.15.12301911
vscode-pylance|ms-|2023.9.10
jupyter|ms-|2023.7.1002162226
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
live-server|ms-|0.4.9
JavaScriptSnippets|Tem|0.5.0
cmake|twx|0.0.17
vscode-lldb|vad|1.9.2
vscode-icons|vsc|12.5.0

(3 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vscaac:30438847
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
2e4cg342:30602488
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-06 00:39:35,question,It compiles but the console instantly closes,"Type: <b>Bug</b>

/*14. Realice una función que reciba como parámetros una matriz de enteros, la cantidad de
filas, la cantidad de columnas y un valor a buscar. La función debe devolver – POR
PARÁMETRO – la fila y la columna donde se encuentra el valor buscado. En el NOMBRE debe
devolver verdadero si lo encontró o falso si no lo hizo.
*/
#include <iostream>
using namespace std;

bool busquedas(int mat[][999],int &filas, int &columnas,int dato);
int main(int argc, char const *argv[])
{
    int matriz[999][999], fila, columna, dat;
    cout << ""Ingrese cantidad de filas y columnas de la matriz "";
    cin >> fila >> columna;
    for (int i = 0; i < fila; i++)
    {
        for (int j = 0; j < columna; j++)
        {
            cout << ""Ingrese el numero de la fila: "" << i << "" columna :"" << j << ""   "";
            cin >> matriz[i][j];
        }
    }
    cout << ""Ingrese el dato que quiere buscar: "";
    cin >>  dat;
    bool encontro = busquedas(matriz,fila,columna,dat);
    if (encontro)
    cout << ""El dato fue encontrado en la fila "" << fila << "" columna: "" << columna;
    else cout << ""El dato no fue encontrado"";
    return 0;
}
bool busquedas(int mat[][999],int &filas, int &columnas,int dato)
{
    for (int i = 0; i < filas; i++)
    {
        for (int j = 0; j < columnas; j++)
        {
         if (mat[i][j] == dato)
            {
                filas = i;
                columnas = j;
                return true;
            }
        }
    }
    return false;
} This is the full code i dont see any problems at all, but in this only exercise i have that problem

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 1600 Six-Core Processor             (12 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.93GB (8.91GB free)|
|Process Argv|--crash-reporter-id a0003ca4-e173-4674-9b53-5ef381a1fb2a|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (19)</summary>

Extension|Author (truncated)|Version
---|---|---
c-cpp-compile-run|dan|1.0.50
c-cpp-runner|fra|8.1.0
codespaces|Git|1.15.0
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.50
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
java|red|1.21.0
cmake|twx|0.0.17
vscode-lldb|vad|1.9.2
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.54.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.13
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
material-theme|zhu|3.16.0

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30821571
dsvsc015:30829745

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-04 06:36:37,question,PIL library encountering error,"Type: <b>Bug</b>

i am getting this error while i am writing the python code in VS code library for Image PIL Image.        PS C:\\Users\\musta\\Desktop\\Visual Studio> & C:/Users/musta/AppData/Local/Microsoft/WindowsApps/python3.11.exe ""c:/Users/musta/Desktop/Visual Studio/main.py""
Traceback (most recent call last):
  File ""c:\\Users\\musta\\Desktop\\Visual Studio\\main.py"", line 4, in <module>
    from PIL import Image, ImageTk 
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'PIL'
PS C:\\Users\\musta\\Desktop\\Visual Studio> 

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.22000
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz (4 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|5.90GB (0.94GB free)|
|Process Argv|--crash-reporter-id 9b270cbb-942f-45c7-b9a2-157b6afee602|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (13)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.108.376
gc-excelviewer|Gra|4.2.58
bash-ide-vscode|mad|1.39.0
rainbow-csv|mec|3.7.0
csharp|ms-|2.0.436
vscode-dotnet-runtime|ms-|1.7.2
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.50
cpptools|ms-|1.17.5
platformio-ide|pla|3.3.1
intellicode-api-usage-examples|Vis|0.2.8
vscodeintellicode|Vis|1.2.30
console-ninja|Wal|0.0.215


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
962ge761:30823813
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30821571
dsvsc015cf:30823818

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-03 12:17:46,question,wont run the program,"Type: <b>Bug</b>

Whenever i try to run the program on terminal in my mac and write - 'gcc Hello.c'(Hello is the file name) it wont run and show ""no such file as 'Hello.c' ""
I am new to coding and dont know much about visual code thus please help me. 

VS Code version: Code 1.81.1 (Universal) (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:20:33.924Z)
OS version: Darwin arm64 22.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 Pro (10 x 24)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|1, 1, 2|
|Memory (System)|16.00GB (0.09GB free)|
|Process Argv|--crash-reporter-id da2b00bf-03ff-4eb9-9a7d-922fa4284720|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (7)</summary>

Extension|Author (truncated)|Version
---|---|---
c-cpp-runner|fra|8.1.0
cpptask|kay|0.0.1
lldb-vscode|lan|0.2.3
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
o-language-support|ora|0.0.6
vscode-lldb|vad|1.9.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
282f8724:30602487
89544117:30613380
showlangstatbar:30737416
962ge761:30823813
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-02 18:37:59,question,Autoformat stopped working for Javascript,"
Type: <b>Bug</b>

I keep getting `There is no  formatter installed for ""Javascript""` even though I tried ESLint and native autoformatter. 

""[javascript][javascriptreact][typescript]"": {
    ""editor.defaultFormatter"": ""vscode.typescript-language-features""
},r:

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz (16 x 2304)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|31.79GB (16.84GB free)|
|Process Argv|--crash-reporter-id 767b857d-6081-42c7-a168-353955dd0e41|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (15)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
project-manager|ale|12.7.0
iconify|ant|0.7.0
turbo-console-log|Cha|2.9.6
vscode-eslint|dba|2.4.2
githistory|don|0.6.20
todo-tree|Gru|0.0.226
vscode-peacock|joh|4.2.2
json-to-ts|Mar|1.7.5
vetur|oct|0.37.3
markdown-preview-enhanced|shd|0.7.1
vscode-icons|vsc|12.5.0
volar|Vue|1.8.8
gitblame|wad|10.5.0
vscode-import-cost|wix|3.3.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
a2ce3375:30757347
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-09-01 12:26:02,question,code is not run prperly,"Type: <b>Performance Issue</b>

code is not properly please helpo me


VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3-1005G1 CPU @ 1.20GHz (4 x 1190)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.77GB (2.00GB free)|
|Process Argv|--crash-reporter-id 84f717d1-674b-4f6f-9880-86fab8d2c667|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   104	 13636	code main
    0	   188	  3060	window [1] (Extension: Code Runner - practice c lang - partik - Visual Studio Code)
    1	   155	  4616	   gpu-process
    0	   103	  4920	shared-process
    0	    76	  5064	fileWatcher [1]
    0	    43	  7708	   utility-network-service
    0	   147	  8140	extensionHost [1]
    0	     4	  1820	     C:\\Windows\\system32\\cmd.exe /d /s /c ""cd ""c:\\Users\\91701\\Desktop\\practice c lang\\"" && gcc tempCodeRunnerFile.c -o tempCodeRunnerFile && ""c:\\Users\\91701\\Desktop\\practice c lang\\""tempCodeRunnerFile""
    0	     4	  2120	       ""c:\\Users\\91701\\Desktop\\practice c lang\\""tempCodeRunnerFile
    0	    11	 14364	       C:\\Windows\\system32\\conhost.exe 0x4
    0	    25	 15712	     c:\\Users\\91701\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64\\bin\\cpptools.exe
    0	     4	 10840	       ""c:\\Users\\91701\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64\\bin\\cpptools.exe""
    0	    14	 15600	         c:\\Users\\91701\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64/bin/cpptools-srv.exe 15712 {28F83285-AD47-482A-9208-DF9450D4B259}
    0	    11	 16112	           C:\\Windows\\system32\\conhost.exe 0x4
    0	    17	 16760	         c:\\Users\\91701\\.vscode\\extensions\\ms-vscode.cpptools-1.17.5-win32-x64/bin/cpptools-srv.exe 15712 {5255AD30-BE63-4EBF-B7D8-89BF7F1F0A41}
    0	    11	 18176	           C:\\Windows\\system32\\conhost.exe 0x4
    0	    11	 18264	       C:\\Windows\\system32\\conhost.exe 0x4
    0	    80	 15944	     electron-nodejs (""C:\\Users\\91701\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\91701\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\json-language-features\\server\\dist\\node\\jsonServerMain"" --node-ipc --clientProcessId=8140)
    0	    27	  8272	   crashpad-handler
    1	    91	  9112	window [2] (Issue Reporter)
    0	    95	 14248	   window
    0	    88	 16140	ptyHost
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Extension: Code Runner - practice c lang - partik - Visual Studio Code)
|    Folder (practice c lang): 51 files
|      File types: c(26) exe(11) json(5) txt(4) C(1) h(1) html(1)
|      Conf files: launch.json(1) settings.json(1) tasks.json(1)
|      Launch Configs: cppdbg(3) cppvsdbg;
```

</details>
<details><summary>Extensions (5)</summary>

Extension|Author (truncated)|Version
---|---|---
code-runner|for|0.12.0
cmake-tools|ms-|1.15.31
cpptools|ms-|1.17.5
cpptools-extension-pack|ms-|1.3.0
cmake|twx|0.0.17

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30792227
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
f6dab269:30613381
2i9eh265:30646982
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30821572

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-31 16:13:32,question,local host ,"Type: <b>Bug</b>

hello dear ,
actually am faceing a problem during debuging, crome gives only local host error

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-31 02:41:06,question,"vscode: PERM: operation not permitted, copyfile","Type: <b>Bug</b>

This issue is detailed here:  https://stackoverflow.com/questions/77010682/vscode-perm-operation-not-permitted-copyfile

1. Install Google Drive. this should install a G: drive on your windows 11 system
     -When setting this up, select 'streaming' instead of mirrored
2. Create project folder somewhere on the G drive.
3. Open project in vscode and create a dev container.
4. Im thinking now we have to wait for the files to be 'unmirrored'
5.  Re-open the project and then the devcontainer
6. Try to cut and paste a file or folder from one location to another folder.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.22621
Modes:
Connection to 'dev-container+7b22686f737450617468223a222f686f6d652f6d6168656e6472612f70726f6a656374732f72616173222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6e727a646f636b657274657374227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d6168656e6472612f70726f6a656374732f726161732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d' could not be established
Remote OS version: Linux x64 5.15.90.1-microsoft-standard-WSL2

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 9 4900HS with Radeon Graphics          (16 x 2994)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|39.42GB (21.81GB free)|
|Process Argv|--crash-reporter-id a94c5544-2680-458f-803c-0aecde90a1fc|
|Screen Reader|no|
|VM|0%|

Connection to 'dev-container+7b22686f737450617468223a222f686f6d652f6d6168656e6472612f70726f6a656374732f72616173222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6e727a646f636b657274657374227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d6168656e6472612f70726f6a656374732f726161732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d' could not be established

|Item|Value|
|---|---|
|Remote|Dev Container: redacted|
|OS|Linux x64 5.15.90.1-microsoft-standard-WSL2|
|CPUs|AMD Ryzen 9 4900HS with Radeon Graphics (16 x 2994)|
|Memory (System)|19.25GB (16.93GB free)|
|VM|0%|
</details><details><summary>Extensions (35)</summary>

Extension|Author (truncated)|Version
---|---|---
terraform|4op|0.2.5
vscode-azurevirtualmachines|ms-|0.6.5
remote-containers|ms-|0.304.0
remote-ssh|ms-|0.102.0
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.0
vscode-remote-extensionpack|ms-|0.24.0
remote-explorer|ms-|0.4.1
remote-server|ms-|1.4.3
terraform|4op|0.2.5
vscode-postgres|cko|1.4.3
vscode-eslint|dba|2.4.2
gitlens|eam|14.2.1
git-graph|mhu|1.30.0
azure-dev|ms-|0.7.0
vscode-azureappservice|ms-|0.25.0
vscode-azurecontainerapps|ms-|0.5.1
vscode-azurefunctions|ms-|1.12.4
vscode-azureresourcegroups|ms-|0.7.5
vscode-azurestaticwebapps|ms-|0.12.2
vscode-azurestorage|ms-|0.15.3
vscode-azurevirtualmachines|ms-|0.6.5
vscode-cosmosdb|ms-|0.19.4
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.40
jupyter|ms-|2023.7.1002162226
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
azure-account|ms-|0.11.5
azurecli|ms-|0.5.0
vscode-node-azure-pack|ms-|1.2.0
sqltools|mtx|0.28.0
sqltools-driver-pg|mtx|0.5.1


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
282f8724:30602487
89544117:30613380
showlangstatbar:30737416
962ge761:30823813
a2ce3375:30757347
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
asynctok:30821568
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30821571

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-30 14:45:45,question,can't able to create project,"Type: <b>Feature Request</b>

hi in my vs code it will don't show the create project option on the screen i am installing any version it will display only new file ,open files,open folder options only in the window.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19043
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-27 12:40:01,question,mysql,"
Type: <b>Feature Request</b>

i tried but find very diffculties to query mysql in vscode


VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.22000
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-24 09:56:18,question,Can you provide ways to distinguish between vscode windows.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Hi vscode developers,
The following description is based on windows.
I'm constantly working with multiple vscode windows. Many times I found it hard to find a particular window when switching from other softwares.
vscode shows filename before workspace/folder name, sometime the latter is hidden.
I tried to set title bar color for different windows but it does not show in the thumbnail windows pompts when mouse is moved to vscode icon in the taskbar.
So I'm suggesting a setting item, maybe a property to give to workspaces so they appear differently. This may help users the navigate the the window they want.
Thanks."
microsoft/vscode,2023-08-23 16:02:16,question,"The ""&"" symbol in the terminal is coming automatically after each time I change the directory using the CD command.","
Type: <b>Performance Issue</b>

The ""&"" symbol in the terminal is coming automatically after each time I change the directory using the CD command. How can I get rid from the ""&"" ? I can delete it manually each-time going backward after going the directory. Which is pretty exhausting.

Record of the issue:

https://www.loom.com/share/5f469083e1c1457ca7dd3dd95749832e


VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz (12 x 2592)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.75GB (5.47GB free)|
|Process Argv|--crash-reporter-id 1626e408-1b63-4e0a-abaa-16e76df5c15a|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   116	 22028	code main
    0	    89	  3172	window [2] (Issue Reporter)
    0	   156	 13064	window [1] (Welcome - 2.2+Native+Modules - Visual Studio Code)
    0	   190	 15408	   gpu-process
    0	    42	 17728	   utility-network-service
    0	    73	 19972	ptyHost
    0	     6	 26440	     conpty-agent
    0	    49	 26584	     C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -noexit -command ""try { . \\""c:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\out\\vs\\workbench\\contrib\\terminal\\browser\\media\\shellIntegration.ps1\\"" } catch {}""
    0	    87	 20384	shared-process
    0	   115	 20912	extensionHost [1]
    0	    92	 15420	     electron-nodejs (""C:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node --max-old-space-size=3072 ""c:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\node_modules\\typescript\\lib\\tsserver.js"" --serverMode partialSemantic --useInferredProjectPerProjectRoot --disableAutomaticTypingAcquisition --cancellationPipeName C:\\Users\\webta\\AppData\\Local\\Temp\\vscode-typescript\\4854c7face779ce21e00\\tscancellation-3176bc4ca8571adec3a9.tmp* --locale en --noGetErrOnBackgroundUpdate --validateDefaultNpmLocation --useNodeIpc)
    0	    93	 21340	     electron-nodejs (""C:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node --max-old-space-size=3072 ""c:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\node_modules\\typescript\\lib\\tsserver.js"" --useInferredProjectPerProjectRoot --enableTelemetry --cancellationPipeName C:\\Users\\webta\\AppData\\Local\\Temp\\vscode-typescript\\4854c7face779ce21e00\\tscancellation-ac8b167bdb98eaeffa73.tmp* --locale en --noGetErrOnBackgroundUpdate --validateDefaultNpmLocation --useNodeIpc)
    0	    80	 20676	       electron-nodejs (""C:\\Users\\webta\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:/Users/webta/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/node_modules/typescript/lib/typingsInstaller.js"" --globalTypingsCacheLocation C:/Users/webta/AppData/Local/Microsoft/TypeScript/5.1 --enableTelemetry --typesMapLocation ""c:/Users/webta/AppData/Local/Programs/Microsoft VS Code/resources/app/extensions/node_modules/typescript/lib/typesMap.json"" --validateDefaultNpmLocation)
    0	     6	 22856	   ""C:\\Program Files\\Google\\Drive File Stream\\79.0.2.0\\crashpad_handler.exe"" --database=C:\\Users\\webta\\AppData\\Local\\Google\\DriveFS\\Crashpad --url=https://clients2.google.com/cr/report --annotation=application=Code.exe --annotation=prod=DriveFS --annotation=ver=79.0.2.0 --initial-client-data=0x1180,0x1390,0x143c,0x1398,0x140c,0x7fff8fa3eff0,0x7fff8fa3f000,0x7fff8fa3f010
    0	    27	 24408	   crashpad-handler
    0	    76	 25656	fileWatcher [1]
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Welcome - 2.2+Native+Modules - Visual Studio Code)
|    Folder (2.2+Native+Modules): 4 files
|      File types: js(2) txt(1)
|      Conf files:;
```

</details>
<details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
copilot|Git|1.105.350
material-icon-theme|PKi|4.29.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
282f8724:30602487
89544117:30613380
a9j8j154:30646983
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
asynctok:30815620
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30812749

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-22 00:42:24,question,FONT,"Type: <b>Bug</b>

Currently I'm learning to program in HTML, I'm in the ""FONT SIZE, FONT FACE"" phase, but when I use ""FONT"" it's like it doesn't exist.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD A4-4000 APU with Radeon(tm) HD Graphics     (2 x 3232)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: disabled_off<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: unavailable_software|
|Load (avg)|undefined|
|Memory (System)|3.43GB (0.38GB free)|
|Process Argv|--crash-reporter-id 21bdeb6d-4608-40bf-9f3a-deb26c9321f8|
|Screen Reader|no|
|VM|0%|
</details>Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vscaac:30438847
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
f6dab269:30613381
vscrp:30673768
showlangstatbar:30737416
a2ce3375:30757347
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
synctok:30815622
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30812748

```

</details>

<!-- generated by issue reporter -->
![image](https://github.com/microsoft/vscode/assets/142854414/4d6d714b-71ff-4ef2-aeb7-d5b354b48bfb)
idk if i'm missing something"
microsoft/vscode,2023-08-21 18:35:46,question,About Winerror 87,"when i import eel lib on my python code it keeps shown me this error on ""eel.start('index.html')"" 
is there a problem with my code or it's a bug?
```[tasklist]
### Tasks
```
"
microsoft/vscode,2023-08-19 20:41:27,question,Make pinned tabs small,"Type: <b>Feature Request</b>

It would be nice if the pinned tabs get collapsed like in most web browsers. Please make an option in settings to collapse pinned tabs.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:18:39.991Z)
OS version: Linux x64 5.15.0-79-generic
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-19 01:55:54,question,Material-UI website not loading,"ADD ISSUE DESCRIPTION HERE

Version: 1.81.1
Commit: 6c3e3dba23e8fadc360aed75ce363ba185c49794
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->"
microsoft/vscode,2023-08-18 08:45:01,question,** On entry to DGEBAL parameter number  3 had an illegal value,"Type: <b>Bug</b>

i am trying to run a python code in the visual studio code. In the fourth loop it is showing the following error:""** On entry to DGEBAL parameter number  3 had an illegal value"". please help me fix this.

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:18:39.991Z)
OS version: Linux x64 5.15.0-78-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz (8 x 3400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off|
|Load (avg)|1, 0, 0|
|Memory (System)|31.23GB (24.85GB free)|
|Process Argv|--unity-launch --crash-reporter-id 047b8f1f-5a5d-46c5-b96d-1e3970b7a336|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
python-image-preview|076|0.1.2
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.30


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyonecf:30548226
vsccc:30803844
2e4cg342:30602488
f6dab269:30613381
2i9eh265:30646982
showlangstatbar:30737416
a2ce3375:30757347
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
asynctok:30815620
dsvsc013:30795093
dsvsc014:30804076
diffeditorv1:30812748

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-17 12:40:51,question,Something wrong with the terminal encoding,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.81.1
- OS Version: Windows 10.1 x64

When I'm trying to build C++ app, I'm getting following error that I am not able to read:
""cmd"" �� ���� ����७��� ��� ���譥�
��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.

I've already set OEMCP at regedit to 65001, but it didn't help.

Steps to Reproduce:

1. Try to build a C++ code 
2. Recieve this abracadabra as a response.
"
microsoft/vscode,2023-08-16 13:52:55,question,CODE NOT RUNNING,"Type: <b>Bug</b>

I tried running a code to extract audio features from a recordings dataset, but the only output in the terminal is different file paths. Someone else tried running it on their computer and it worked for them. Please could you help me fix this issue? Thank you.

VS Code version: Code 1.70.2 (e4503b30fc78200f846c62cf8091b76ff5547662, 2022-08-16T05:37:58.957Z)
OS version: Windows_NT ia32 6.2.9200
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5 CPU       M 560  @ 2.67GHz (4 x 2660)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: unavailable_off<br>raw_draw: disabled_off_ok<br>skia_renderer: enabled_on<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: unavailable_off|
|Load (avg)|undefined|
|Memory (System)|1.86GB (0.19GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (5)</summary>

Extension|Author (truncated)|Version
---|---|---
python|ms-|2022.16.1
vscode-pylance|ms-|2023.1.10
jupyter|ms-|2022.7.1102252217
jupyter-keymap|ms-|1.1.0
jupyter-renderers|ms-|1.0.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931:30280409
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
f6dab269:30613381
showlangstatbar:30737416
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076
diffeditorv2:30812749

```

</details>

<!-- generated by issue reporter --> "
microsoft/vscode,2023-08-16 02:41:13,question,to sets of pythan,"Type: <b>Bug</b>

I have Pythan from the extintions and one from Axs and when i type in a pythan code it shows me the same code i typed in how to i repair it for one set of code?

VS Code version: Code 1.81.1 (Universal) (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:20:33.924Z)
OS version: Darwin x64 22.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8210Y CPU @ 1.60GHz (4 x 1600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|2, 11, 34|
|Memory (System)|8.00GB (0.10GB free)|
|Process Argv|--crash-reporter-id 87b7a8a5-bf43-4a47-bc81-11da23b14e63|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (16)</summary>

Extension|Author (truncated)|Version
---|---|---
ruff|cha|2023.32.0
python-environment-manager|don|1.0.4
codewiz|fel|0.0.2
vscode-docker|ms-|1.26.0
black-formatter|ms-|2023.5.12151008
python|ms-|2023.15.12221007
vscode-pylance|ms-|2023.8.21
remote-containers|ms-|0.305.0
remote-ssh|ms-|0.102.0
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.81.0
vscode-remote-extensionpack|ms-|0.24.0
remote-explorer|ms-|0.4.1
remote-server|ms-|1.4.0
autodocstring|njp|0.6.1
even-better-toml|tam|0.19.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
282f8724:30602487
f6dab269:30613381
showlangstatbar:30737416
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-15 06:17:39,question,MacOS M1 VScode Github copilot chat icon no show,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 

Steps to Reproduce:

1. After installed ""Github copilot chat"" and reloaded vscode, no extension on side bar

<img width=""1335"" alt=""Screen Shot 2023-08-15 at 2 15 46 PM"" src=""https://github.com/microsoft/vscode/assets/132270603/8d9d9b89-bcc8-4998-9ecb-0960facd9852"">


Versions:
macoOS: 12.6.8

vscode: 

Version: 1.81.1
Commit: 6c3e3dba23e8fadc360aed75ce363ba185c49794
Date: 2023-08-09T22:40:25.698Z (5 days ago)
Electron: 22.3.18
ElectronBuildId: 22689846
Chromium: 108.0.5359.215
Node.js: 16.17.1
V8: 10.8.168.25-electron.0
OS: Darwin arm64 21.6.0

Copilot Chat: Both of below version do not work:
v0.7.2023081101 (Pre-Release)
v0.6.0
"
microsoft/vscode,2023-08-14 14:23:24,question,Inform user about workspace tsdk.,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->

### Problem

Due to security restrictions, we cannot overwrite the tsdk without user consent. It's annoying, but understandable...
The problem is that this is not really visible to the user. This can causes several issues for the team members...
For example, some plugins just stop working when using the build-in version instead of the workspace TSDK version.
The dev doesn't know about this and may only notice it much later. It's annoying and time consuming. Just avoidable.

**.vscode/settings.json** (workspace)
```json
{
  ""typescript.tsdk"": ""node_modules\\\\typescript\\\\lib"",
}
```

[Docs](https://code.visualstudio.com/docs/typescript/typescript-compiling#_using-newer-typescript-versions)

### Solution

This feature reqeust or improvement is about implementing a better way to inform the user about the workspace setting and make it easier to switch the correct version. 

We could display a bold blinking modal in the center of the screen, similar to ad banners. ""Click here to win."" But seriously... It must be prominent visible and intuitively understandable. We could show a persistent notification until the user makes a choice. ...

What's exactly the problem with the workspace setting? Potentially everyone could set any value? A path to an NPM package with vulnerabilities or other problems? We could validate the value of `typescript.tsdk` with a allow list like ""mode_modules/typescript/lib"" which is the official typescript package. But ok, this does not cover to outdated versions that may have security risks... Anyway, this should be visible to the user. Setting the tsdk in the workspace settings is done for serious reasons and should be used within the whole dev team. 

It might look like the ""Do you trust the authors of the files in this folder?"" dialog. Something like:

> This project recommends using the workspace's TypeScript TSDK. Do you trust this project and want to allow that?

And check this on every start. It seems sometimes, the version is switched back for some reasons. I cannot reproduce. But checking that on every start is not a bad idea. And remain displayed until the user makes a decision.

![demo](https://github.com/microsoft/vscode/assets/60390085/cc4e07c6-6466-443a-abfd-ce01ab36d8ba)

We could also say, just load the tsdk setting of the workspace if the user confirmed the already existing trust-dialog.

### Info

Version: 1.81.1 (user setup)
Electron: 22.3.18
Chromium: 108.0.5359.215
Node.js: 16.17.1
OS: Windows_NT x64 10.0.19045"
microsoft/vscode,2023-08-14 07:36:41,question,is it possible to setup Mqtt connection from a server to vs code extension,"
I am developing a Source control extension for vscode , and I want to establish an mqtt connection between extension and server, so that after activation of the extension if there is any event or change happened on server side, the extension will get the notification of it.

is this possible and how?

I am trying to look if, vs code allows to include such type of subscription to the server, without hampering the other processes in the extension.

I tried to execute a CLI command, which will keep listening to the server, but the executor method was just executing it and coming out of the method, it was not staying to read the sdt output, and if I tried to set an time interval, then other process were getting queued up

so now rather than executing any command, I want to create a method which will create mqtt connection with the server
"
microsoft/vscode,2023-08-12 10:06:09,question,comment supprimer waka time Api key,"
Type: <b>Feature Request</b>

je voudrais supprimer waka time Api key

VS Code version: Code 1.81.1 (6c3e3dba23e8fadc360aed75ce363ba185c49794, 2023-08-09T22:22:42.175Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-11 11:19:14,question,reload on the local host site not responding,"Type: <b>Performance Issue</b>

After startup while on my project the local host is not responding and if i type npm start on node.js command prompt it gives this message below


C:\\Users\\User>npm start
npm ERR! code ENOENT
npm ERR! syscall open
npm ERR! path C:\\Users\\User\\package.json
npm ERR! errno -4058
npm ERR! enoent Could not read package.json: Error: ENOENT: no such file or directory, open 'C:\\Users\\User\\package.json'
npm ERR! enoent This is related to npm not being able to find a file.
npm ERR! enoent

npm ERR! A complete log of this run can be found in: C:\\Users\\User\\AppData\\Local\\npm-cache\\_logs\\2023-08-11T10_21_03_535Z-debug-0.log

C:\\Users\\User>



VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz (8 x 1992)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.90GB (3.30GB free)|
|Process Argv|--crash-reporter-id bfe49888-c0e3-47a0-9708-693722c43bc6|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    1	    92	 15340	code main
    1	   136	   144	   gpu-process
    3	   161	  3728	window [1] (Message.js - reactp - Visual Studio Code)
    0	    69	  7892	ptyHost
    0	    38	 12444	     C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -noexit -command ""try { . \\""c:\\Users\\User\\Desktop\\Microsoft VS Code\\resources\\app\\out\\vs\\workbench\\contrib\\terminal\\browser\\media\\shellIntegration.ps1\\"" } catch {}""
    0	     6	 12548	     conpty-agent
    0	    26	  8544	   crashpad-handler
    3	    90	 10932	window [3] (Issue Reporter)
    0	    78	 11380	extensionHost [1]
    0	    79	 12028	     electron-nodejs (""C:\\Users\\User\\Desktop\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node --max-old-space-size=3072 ""c:\\Users\\User\\Desktop\\Microsoft VS Code\\resources\\app\\extensions\\node_modules\\typescript\\lib\\tsserver.js"" --serverMode partialSemantic --useInferredProjectPerProjectRoot --disableAutomaticTypingAcquisition --cancellationPipeName C:\\Users\\User\\AppData\\Local\\Temp\\vscode-typescript\\ebd781def187ffd9e89e\\tscancellation-666ce688eecc1201e015.tmp* --locale en --noGetErrOnBackgroundUpdate --validateDefaultNpmLocation --useNodeIpc)
    0	    82	 13104	     electron-nodejs (""C:\\Users\\User\\Desktop\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node --max-old-space-size=3072 ""c:\\Users\\User\\Desktop\\Microsoft VS Code\\resources\\app\\extensions\\node_modules\\typescript\\lib\\tsserver.js"" --useInferredProjectPerProjectRoot --enableTelemetry --cancellationPipeName C:\\Users\\User\\AppData\\Local\\Temp\\vscode-typescript\\ebd781def187ffd9e89e\\tscancellation-fde1f8ef9373787702c5.tmp* --locale en --noGetErrOnBackgroundUpdate --validateDefaultNpmLocation --useNodeIpc)
    0	    69	  6592	       electron-nodejs (""C:\\Users\\User\\Desktop\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:/Users/User/Desktop/Microsoft VS Code/resources/app/extensions/node_modules/typescript/lib/typingsInstaller.js"" --globalTypingsCacheLocation C:/Users/User/AppData/Local/Microsoft/TypeScript/5.1 --enableTelemetry --typesMapLocation ""c:/Users/User/Desktop/Microsoft VS Code/resources/app/extensions/node_modules/typescript/lib/typesMap.json"" --validateDefaultNpmLocation)
    0	    83	 13404	shared-process
    0	    66	 15600	fileWatcher [1]
    0	    42	 15956	   utility-network-service
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Message.js - reactp - Visual Studio Code)
|    Folder (reactp): 26 files
|      File types: js(7) json(3) png(2) css(2) gitignore(1) ico(1) html(1)
|                  txt(1) md(1) svg(1)
|      Conf files: package.json(1);
```

</details>
Extensions: none<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
3biah626:30602489
89544117:30613380
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-10 16:22:56,question,"I'm not able to make changes to the settings, everything I change appears this sentence ""Unable to record in user settings. Open user settings to correct errors / warnings and try again."" and does not save.","{
    ""editor.minimap.renderCharacters"": true,
    ""workbench.editorAssociations"": {
        ""*.jfif"": ""default""
    },
    ""powermode.enabled"": true,
    ""workbench.colorTheme"": ""Omni"",
    ""php.validate.executablePath"": """"
    if(isset($_POST['url']) && strlen($_POST['url']) == 0 ) {

    }
    if(isset($_POST['acao']) && $_POST['acao'] == 'enviar'){

    }

    if(isset($_FILES['arquivo']) && $_FILES['arquivo']['error'] === UPLOAD_ERR_OK) {
      $arquivo = $_FILES['arquivo'];
      $arquivonome = $_POST['arquivo'];
  } else {
      // Trate o caso de nenhum arquivo ter sido enviado
      // Por exemplo, atribuir valores padrão ou mostrar uma mensagem de erro
  }
  
      $nome         = $_POST['nome'];
      $email        = $_POST['email'];
      $assunto      = $_POST['assunto'];
      
      $arquivo = $_FILES['arquivo'];
      $arquivonome = $_POST['arquivo'];
      $mensagem     = $_POST['mensagem'];
      $data         = date('d/m/Y H:i');
        if($nome == '' || $email == '' || $assunto == '' || $mensagem == ''){
              echo '<script>alert(""Por favor, Preencha todos os campos corretamente !"");location.href=""index.php""</script>';
   
      if (!class_exists('PHPMailer')) {
      require_once(""phpmailer/class.phpmailer.php"");
      }
      
      // Inicia a classe PHPMailer
      $mail = new PHPMailer();
      $mail->CharSet = ""UTF-8"";
      // Define os dados do servidor e tipo de conexão
      //$mail->IsSMTP(); // Define que a mensagem será SMTP
      $mail->Host = ""mail.seusite.com.br""; // Endereço do servidor SMTP
      $mail->SMTPAuth = true;
      $mail->Port     = '465'; 
      $mail->Username = 'webmaster@seusite.com.br'; // Usuário do servidor SMTP
      $mail->Password = '123456'; // Senha do servidor SMTP
      
      // Define o remetente
      $mail->From     =  $_POST['email']; // Seu e-mail
      $mail->FromName =  $_POST['nome']; // Seu nome
      $mail->Sender   = 'contato@seusite.com.br';
      
      // Define os destinatário(s)
      $mail->AddAddress('contato@seusite.com.br');
      //$mail->AddCC('atendimento@seusite.com.br', 'Ciclano'); // Copia
      //$mail->AddBCC('fulano@seusite.com.br.com.br', 'Fulano da Silva'); // Cópia Oculta
      
      // Define os dados técnicos da Mensagem
      $mail->IsHTML(true); // Define que o e-mail será enviado como HTML
      //$mail->CharSet = 'iso-8859-1'; // Charset da mensagem (opcional)
      
      // Define a mensagem (Texto e Assunto)
      $local          = ""Mensagem do Artigo sobre Formulario - Seu site"";
      $mail->Subject  = $local; // Assunto da mensagem
      $mail->Body     = '
                          <div style=""border: 1px solid #f0f0f0; background:#ffffff; font-size:1em; color:#454545; margin:0px auto; padding:1em; overflow:hidden;"">
                            
                            <p style=""width:100%; float:left; margin-bottom:1px; font-size:1.2em;""> <strong style=""color:#00ABFF;"">Nome:</strong> '.$nome.' </p>
                            <p style=""width:100%; float:left; margin-bottom:1px; font-size:1.2em;""> <strong style=""color:#00ABFF;"">E-mail:</strong> '.$email.' </p>
                            <p style=""width:100%; float:left; margin-bottom:1px; font-size:1.2em;""> <strong style=""color:#00ABFF;"">Assunto:</strong> '.$assunto.' </p>
                            <p style=""width:100%; float:left; margin-bottom:0px; font-size:1.2em;""> <strong style=""color:#00ABFF;"">Mensagem:</strong> '.$mensagem.' </p>
                            <p style=""width:100%; float:left; margin-bottom:2px; color:#00ABFF; font-size:1.2em; border-top:1px #e9e9e9 solid; padding-top:5px;""> <strong>Enviado pelo site:</strong> www.seusite.com.br <br><br> <strong>Data de envio:</strong> '.$data.' </p>
                          
                          </div>
                          
                        ';
      $mail->AltBody = "" \\r\\n :)"";
      
      // Define os anexos (opcional)
      $mail->AddAttachment($arquivo['tmp_name'], $arquivo['name']);
      //$mail->AddAttachment(""c:/temp/documento.pdf"", ""novo_nome.pdf"");  // Insere um anexo
      
      // Envia o e-mail
      $enviado = $mail->Send();
      
      // Limpa os destinatários e os anexos
      $mail->ClearAllRecipients();
      $mail->ClearAttachments();
      
      // Exibe uma mensagem de resultado
      if ($enviado) {
                  echo '
                         <div style=""max-width:320px; padding:30px; border:3px solid #00ABFF; color:#00ABFF; font-family: tahoma; background:#f7f7f7; text-align:center; font-weight:800; margin:180px auto;"">
                         
                            <p style=""font-size:1.3em;"">Obrigado!</p> 
                            <p style=""font-size:1em;""> Mensagem enviada com sucesso !</p>
                         </div>
                        ';
                
                  echo    '<meta HTTP-EQUIV=""Refresh"" CONTENT=""2;URL=index.php"">';
        } else {
                
                echo ""Não foi possível enviar o e-mail."";
                echo ""<b>Informações do erro:</b> "" . $mail->ErrorInfo;
        }
      }//If Verifica Campos em Brancos
     }//Fecha Condição se alguém Clicar no botão
    }//Fecha verifica se é url vazia

?>
```[tasklist]
### Tasks
```
"
microsoft/vscode,2023-08-10 10:54:59,question,powershell,"Type: <b>Feature Request</b>

There is an issue with my terminal, that the terminal process failed to launch 

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.18363
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-10 07:47:29,question,chatview how display?,"vscode version: 1.81.0
`chatview` can not display in activity bar.  what happened?

![image](https://github.com/microsoft/vscode/assets/804446/56b03e50-376c-4be4-acd8-319bd4fb75a3)
"
microsoft/vscode,2023-08-09 17:59:16,question,MetaTrader5,"Type: <b>Bug</b>

""pip3 install metatrader5"" gives
Requirement already satisfied: MetaTrader5 in c:\\users\\eric_\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.0.45)
Requirement already satisfied: numpy>=1.7 in c:\\users\\eric_\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from MetaTrader5) (1.25.1)

""import MetaTrader5"" gives
Import ""metatrader5"" could not be resolved

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-6400 CPU @ 2.70GHz (4 x 2712)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.91GB (7.82GB free)|
|Process Argv|--crash-reporter-id 5f67829f-fbdb-4a36-84b2-18c12c20edc2|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (4)</summary>

Extension|Author (truncated)|Version
---|---|---
code-runner|for|0.12.0
vscode-docker|ms-|1.26.0
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.10


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-08 08:18:58,question,have to reopen files on startup,"Type: <b>Bug</b>

if you open a folder and create files there, then when you close and start them, they disappear, and you have to open them again, tell me how to fix this? (macos)

VS Code version: Code 1.81.0 (Universal) (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:40:02.782Z)
OS version: Darwin arm64 22.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M2 (8 x 24)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|2, 2, 2|
|Memory (System)|8.00GB (0.56GB free)|
|Process Argv|--crash-reporter-id 63c19850-02b2-4ed0-a1f1-242e41669ad3|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (7)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-language-pack-ru|MS-|1.81.2023080209
python|ms-|2023.14.0
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
icons|tal|3.7.0
cmake|twx|0.0.17

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
89544117:30613380
vscrpc:30673769
showlangstatbar:30737416
vsctsb:30748421
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-07 17:00:40,question,Errors are not showing,"
Type: <b>Bug</b>

cannot see the errors while writing code

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz (8 x 1800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.89GB (0.89GB free)|
|Process Argv|--crash-reporter-id 0e29f355-b8e6-49b3-8b7a-dc87335b869b|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (49)</summary>

Extension|Author (truncated)|Version
---|---|---
bootstrap5-vscode|Anb|0.4.2
ng-template|Ang|16.1.4
tailwind-docs|aus|2.1.0
bootstrap-5-snippets|bab|0.1.1
blackbox|Bla|1.0.61
tailwindshades|bou|0.0.5
vscode-tailwindcss|bra|0.9.11
bootstrap-5-snippets-by-coder-foundry|Cod|1.2.1
dart-code|Dar|3.70.0
flutter|Dar|3.70.0
vscode-eslint|dba|2.4.2
maven-dependency-explorer|dhr|1.0.17
javascript-ejs-support|Dig|1.3.3
es7-react-js-snippets|dsz|4.4.3
vscode-toggle-column-selection|eri|1.0.6
prettier-vscode|esb|10.1.0
file-icons|fil|1.1.0
auto-close-tag|for|0.5.14
code-runner|for|0.12.0
bootstrap5-snippets|Han|1.2.5
stylelint-plus|hex|0.56.6
csharpextensions|kre|1.7.3
csharp|ms-|2.0.328
vscode-dotnet-runtime|ms-|1.6.0
isort|ms-|2023.10.1
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.10
jupyter|ms-|2023.7.1002162226
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
color-highlight|nau|2.5.0
material-icon-theme|PKi|4.29.0
java|red|1.21.0
LiveServer|rit|5.7.9
es7-react-js-snippets|rod|1.9.3
js-jsx-snippets|sky|11.0.1
bootstrap4-vscode|the|6.1.0
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.53.0
vscode-java-dependency|vsc|0.23.1
vscode-java-pack|vsc|0.25.13
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
JavaScriptSnippets|xab|1.8.0
bootstrap-v4-snippets|Zac|1.1.3
tailwind-snippets|Zar|1.0.2

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931:30280409
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
282f8724:30602487
89544117:30613380
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
24365598:30736109
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxpt:30805730
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-06 19:13:37,question,First lesson ,"Type: <b>Bug</b>

During the first lesson, while compiling the java project: 
console.log(""------------------------"");
console.log(""Rise & Shine!"");
console.log(""Ready for a new day!!"");
console.log(""------------------------"");
selected Javascript debug terminal F5 lanched no error warning does not work
passing with the mouse on Javascript Debug terminal popsup message.
""Process ID (PID): 8492
Command line:C:\\WINDOWS\\System32\\WindowsPoweerShell\\v1.0\\powershell.exe'-noexit' '-command' 'try{. ""c:\\Program Files\\Microsoft VS Code\\resources\\app\\out\\vs\\workbench\\contrib\\terminal\\browser\\media\\shellintegration.ps1""} catch {}'
Shell integration failed to acttivate.

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Celeron(R) CPU  N2830  @ 2.16GHz (2 x 2167)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: disabled_off<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: unavailable_off<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.89GB (3.11GB free)|
|Process Argv|--crash-reporter-id c0ff269f-a2de-4c0b-97a5-b661bab3d6c1|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (17)</summary>

Extension|Author (truncated)|Version
---|---|---
python-environment-manager|don|1.0.4
python-extension-pack|don|1.7.0
vsc-python-indent|Kev|1.18.0
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.10
cpptools|ms-|1.16.3
live-server|ms-|0.4.9
autodocstring|njp|0.6.1
java|red|1.21.0
startanyshell|rem|0.3.1
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.53.0
vscode-java-pack|vsc|0.25.13
vscode-java-test|vsc|0.39.1
vscode-maven|vsc|0.42.0
jinja|who|0.0.8


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30803844
3biah626:30602489
89544117:30613380
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofa:30804715
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-05 15:02:20,question, Background color support for notebook cells,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Hi Experts,
I'm using the notebooks in VS Code for lots of Data and ML work. Yes, we always have to create lots of cells in the notebook and edit them at the same time. So I think that it is possible to set up the cells using different background colors. For example, use light green as the Python code cell and use light blue as the Markdown cell, etc.
It may help us more rapidly locate the cell which needs to edit and can be used with cell tags together."
microsoft/vscode,2023-08-04 19:13:03,question,my c programming code is not working properly,"Type: <b>Bug</b>

#include<stdio.h>

int main (){
    int rating;
    printf("" enter your rating (1-5) = \\n"");
    scanf("" %d"", rating);
    switch(rating){
        case 1:
            printf("" your rating is 1\\n"");
        break ;
        case 2:
            printf("" your rating is 2\\n"");
        break ;
        case 3:
            printf("" your rating is 3\\n"");
        break ;
        case 4:
            printf("" your rating is 4\\n"");
        break ;
        case 5:
            printf("" your rating is 5\\n"");
        break ;
        default :
            printf("" invalid rating!"");
        
    }
    
    return 0;
}

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-3470S CPU @ 2.90GHz (4 x 2893)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.90GB (3.56GB free)|
|Process Argv|C:\\\\Users\\\\pc\\\\Desktop\\\\dev.c --crash-reporter-id 8ab98f09-6d32-47e9-a55d-1dfeeb7fe37b|
|Screen Reader|no|
|VM|50%|
</details><details><summary>Extensions (8)</summary>

Extension|Author (truncated)|Version
---|---|---
prettier-vscode|esb|10.1.0
code-runner|for|0.12.0
c-cpp-runner|fra|7.1.0
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
cmake|twx|0.0.17
vscode-lldb|vad|1.9.2

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263cf:30335440
vscoreces:30445986
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
3biah626:30602489
89544117:30613380
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-04 06:52:16,question,VS code won't run anything,"Type: <b>Bug</b>

Anything i type in the VS code terminal gives me the same error: 

git : The term 'git' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, 
verify that the path is correct and try again.
At line:1 char:1
+ git clone --single-branch -b ""react-mini"" https://github.com/safak/yo ...
+ ~~~
    + CategoryInfo          : ObjectNotFound: (git:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException


git is just an example, it gives the same error in pip and everything else as well. Please help. 

I checked everywhere on the internet and the most common answer i got was check the PATH in the Environment Variables, but the PATH is correct, i've checked it multiple times.

https://stackoverflow.com/questions/76832586/vs-code-doesnt-want-to-run-anything?noredirect=1#comment135451468_76832586

VS Code version: Code 1.81.0 (6445d93c81ebe42c4cbd7a60712e0b17d9463e97, 2023-08-02T12:37:13.485Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5800H with Radeon Graphics          (16 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|13.86GB (6.78GB free)|
|Process Argv|--crash-reporter-id afa7e0e3-e800-4e6a-913b-30e6c57b357a|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (39)</summary>

Extension|Author (truncated)|Version
---|---|---
better-comments|aar|3.0.2
vscode-django|bat|1.10.0
doxdocgen|csc|1.4.0
python-environment-manager|don|1.0.4
python-extension-pack|don|1.7.0
es7-react-js-snippets|dsz|4.4.3
vsc-material-theme|Equ|33.8.0
vsc-material-theme-icons|equ|2.8.0
prettier-vscode|esb|10.1.0
mithril-emmet|Fal|0.7.7
auto-close-tag|for|0.5.14
auto-rename-tag|for|0.1.10
code-runner|for|0.12.0
better-cpp-syntax|jef|1.17.2
vsc-python-indent|Kev|1.18.0
isort|ms-|2023.10.1
python|ms-|2023.14.0
vscode-pylance|ms-|2023.8.10
jupyter|ms-|2023.7.1002162226
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.299.0
remote-ssh|ms-|0.102.0
remote-wsl|ms-|0.80.2
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
remote-explorer|ms-|0.4.1
color-highlight|nau|2.5.0
autodocstring|njp|0.6.1
vscode-css-peek|pra|4.4.1
LiveServer|rit|5.7.9
pytorch-snippets|SBS|1.0.2
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
jinja|who|0.0.8

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30792227
vscaac:30438847
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30803845
2e4cg342:30602488
89544117:30613380
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30805244
pythongtdpath:30769146
i26e3531:30792625
gsofb:30804716
pythonnosmt12:30797651
pythonidxptcf:30805731
pythonnoceb:30805159
e537b577:30795824
dsvsc013:30795093
dsvsc014:30804076

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-03 09:43:00,question,js file logo is different ,"
![Screenshot (35)](https://github.com/microsoft/vscode/assets/91602414/0b977a81-b0b7-444a-b9ce-26afe9d6b43e)
<img width=""778"" alt=""Screenshot 2023-07-21 165936"" src=""https://github.com/microsoft/vscode/assets/91602414/ab0ada40-0b3f-41f0-b079-4424fb933927"">
![Screenshot (35)](https://github.com/microsoft/vscode/assets/91602414/3799904a-704d-4e09-ad47-6b1bd818e8e1)
Type: <b>Bug</b>

When I apply Vs code icon theme through extension it works very well. Then later i try to sign in to github through vs code. Unfortunately, my JS file icon theme has been changed.

VS Code version: Code 1.80.2 (2ccd690cbff1569e4a83d7c43d45101f817401dc, 2023-07-27T20:40:28.909Z)
OS version: Windows_NT x64 10.0.22621
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-08-01 15:08:15,question,having trouble with checking my programms befor submitting them in cs50 course for python and AI,"I have wrote a program for one of the projects in my account which its name is ""home_federal_savings_bank"". The problem is I face some errors when I entre the checking command (check50 cs50/problems/2022/python/bank ) although I don't observe any error when I run it by my own in VS code. I don't know where the problem is and I appreciate if you help me through this problem. thank you.

my program:

t = input().lower().split()
tt = t[0][0]

if t[0][:5] == 'hello':
    print ('$0')

elif t[0] != 'hello' and tt == 'h':
    print ('$20')

else:
    print ('$100')
-----------------------------------
output in my own computer:
>>> hello
[output:] >>>$0
-----------------------------------
result of check50 cs50/problems/2022/python/bank command in code space:
:) bank.py exists
:( input of ""Hello"" yields output of $0
    expected prompt for input, found none
:( input of "" Hello "" yields output of $0
    expected prompt for input, found none
:( input of ""Hello, Newman"" yields output of $0
    expected prompt for input, found none
:( input of ""How you doing?"" yields output of $20
    expected prompt for input, found none
:( input of ""What's happening?"" yields output of $100
    expected prompt for input, found none
:( input of ""What's up?"" yields output of $100
    expected prompt for input, found none"
microsoft/vscode,2023-07-31 20:49:34,question,./population: line 4: syntax error near unexpected token '(',"ADD ISSUE DESCRIPTION HERE
I have this issue and I do not know how to fix it. I have done everything and am unsure if anything is wrong with my bash commands. Other files work well but this population is messing up really well and when I create a new one, I can't run any more orders.
Version: 1.80.2
Commit: 2ccd690cbff1569e4a83d7c43d45101f817401dc
User Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->"
microsoft/vscode,2023-07-29 18:29:28,question,Cannot open DB,"Type: <b>Bug</b>

I can open any DB, i get this error even on new db without any tables on it. 

[2:23:46 PM][vscode-sqlite][ERROR] Failed to open database 'c:\\Users\\steve\\Desktop\\Test_project\\project.db': Parse error near line 4: no such column: table
  aster                                 WHERE (type=""table"" OR type=""view"")     
                                      error here ---^

VS Code version: Code 1.80.2 (2ccd690cbff1569e4a83d7c43d45101f817401dc, 2023-07-27T20:40:28.909Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5700X 8-Core Processor              (16 x 3400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.92GB (3.19GB free)|
|Process Argv|--crash-reporter-id a0057069-8edb-4a13-947e-029300e7b6fe|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (23)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-sqlite|ale|0.14.1
ajax-query|Bal|0.6.0
vscode-intelephense-client|bme|1.9.5
phpserver|bra|3.0.2
xml|Dot|2.5.1
EditorConfig|Edi|0.16.4
auto-close-tag|for|0.5.14
auto-rename-tag|for|0.1.10
code-runner|for|0.12.0
node-module-intellisense|lei|1.5.0
vscode-apache|mrm|1.2.0
vscode-docker|ms-|1.26.0
data-workspace-vscode|ms-|0.5.0
mssql|ms-|1.20.0
sql-bindings-vscode|ms-|0.4.0
sql-database-projects-vscode|ms-|1.2.0
remote-containers|ms-|0.299.0
remote-wsl|ms-|0.80.2
sqltools|mtx|0.28.0
sqltools-driver-mysql|mtx|0.6.0
LiveServer|rit|5.7.9
code-spell-checker|str|2.20.5
php-debug|xde|1.33.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyonecf:30548226
2e4cg342:30602488
89544117:30613380
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
hideindicator:30785051
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonnoceb:30797650
e537b577:30795824
dsvsc013:30795093
dsvsc014cf:30797590

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-28 12:31:21,question,Error,"
Type: <b>Performance Issue</b>

Hello,
I've recently created a class that consists of a header file and an implementation file but when I try to run the code I receive an error that ""main.exe"" does not exist.
How do I fix this please?

VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-11300H @ 3.10GHz (8 x 3110)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.79GB (8.06GB free)|
|Process Argv|--crash-reporter-id c3fc9a03-f19d-43e1-841d-092e1270c5eb|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	    99	 35788	code main
    0	   214	  1792	window [1] (main.cpp - .vscode - Visual Studio Code)
    0	    78	  8472	fileWatcher [1]
    0	    87	 15820	ptyHost
    0	    63	  4516	     C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe
    0	     7	  5012	     conpty-agent
    0	     7	 30228	     conpty-agent
    0	    66	 35744	     C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe
    0	    28	 23952	   crashpad-handler
    0	   156	 25976	extensionHost [1]
    0	   133	 17984	     c:\\Users\\men3em\\.vscode\\extensions\\ms-vscode.cpptools-1.16.3-win32-x64\\bin\\cpptools.exe
    0	     4	 34824	       ""c:\\Users\\men3em\\.vscode\\extensions\\ms-vscode.cpptools-1.16.3-win32-x64\\bin\\cpptools.exe""
    0	   169	  7156	         c:\\Users\\men3em\\.vscode\\extensions\\ms-vscode.cpptools-1.16.3-win32-x64/bin/cpptools-srv.exe 17984 {BD2EDED0-D037-4DC5-BB4C-9AD3EA73A16A}
    0	    10	 33968	           C:\\WINDOWS\\system32\\conhost.exe 0x4
    0	    10	 37752	       C:\\WINDOWS\\system32\\conhost.exe 0x4
    0	    81	 31532	     electron-nodejs (""C:\\Users\\men3em\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\men3em\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\json-language-features\\server\\dist\\node\\jsonServerMain"" --node-ipc --clientProcessId=25976)
    0	    86	 27920	window [2] (Issue Reporter)
    0	    13	 28280	   C:\\Users\\men3em\\AppData\\Local\\Temp\\vscode-stable-user-x64\\CodeSetup-stable-2ccd690cbff1569e4a83d7c43d45101f817401dc.exe /verysilent /log /update=""C:\\Users\\men3em\\AppData\\Local\\Temp\\vscode-stable-user-x64\\CodeSetup-stable-2ccd690cbff1569e4a83d7c43d45101f817401dc.flag"" /nocloseapplications /mergetasks=runcode,!desktopicon,!quicklaunchicon
    0	    41	 38644	     ""C:\\Users\\men3em\\AppData\\Local\\Temp\\is-3NVHS.tmp\\CodeSetup-stable-2ccd690cbff1569e4a83d7c43d45101f817401dc.tmp"" /SL5=""$30648,92129210,828416,C:\\Users\\men3em\\AppData\\Local\\Temp\\vscode-stable-user-x64\\CodeSetup-stable-2ccd690cbff1569e4a83d7c43d45101f817401dc.exe"" /verysilent /log /update=""C:\\Users\\men3em\\AppData\\Local\\Temp\\vscode-stable-user-x64\\CodeSetup-stable-2ccd690cbff1569e4a83d7c43d45101f817401dc.flag"" /nocloseapplications /mergetasks=runcode,!desktopicon,!quicklaunchicon
    0	    92	 32112	shared-process
    0	    87	 38572	     electron-nodejs (""C:\\Users\\men3em\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\men3em\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\out\\bootstrap-fork"" ms-vscode.cppdbg ""{\\""common.vscodemachineid\\"":\\""78c7d868a7a62ab370eef109cc3cab036b011c055c040635442005ce634193bb\\"",\\""common.vscodesessionid\\"":\\""f8224bc3-d07e-4c98-b456-92040165a6911690544069520\\""}"" 0c6ae279ed8443289764825290e4f9e2-1a736e7c-1324-4338-be46-fc2a58ae4d14-7255)
    0	    43	 36396	   utility-network-service
    0	   234	 37620	   gpu-process
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (main.cpp - .vscode - Visual Studio Code)
|    Folder (.vscode): 2763 files
|      File types: py(1200) dll(389) json(293) md(168) js(84) svg(50) png(41)
|                  exe(26) map(22) vsixmanifest(19)
|      Conf files: package.json(19) launch.json(1) settings.json(1)
|                  tasks.json(1) devcontainer.json(1) dockerfile(1)
|                  makefile(1)
|      Launch Configs: cppdbg(2);
```

</details>
<details><summary>Extensions (17)</summary>

Extension|Author (truncated)|Version
---|---|---
codesnap|adp|1.3.4
vscode-intelephense-client|bme|1.9.5
c-cpp-compile-run|dan|1.0.49
composer-php-vscode|DEV|1.36.13417
phptools-vscode|DEV|1.36.13417
profiler-php-vscode|DEV|1.36.13417
prettier-vscode|esb|9.19.0
arduino-class-creator|far|1.0.2
cpp-class-creator|Fle|1.1.0
c-cpp-runner|fra|7.0.4
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
format-html-in-php|rif|1.7.0
cmake|twx|0.0.17
vscode-lldb|vad|1.9.2
php-debug|xde|1.33.0

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30792226
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
2e4cg342:30602488
f6dab269:30613381
showlangstatbar:30737416
vsctsb:30748421
03d35959:30757346
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
hideindicator:30785051
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30797651
pythonnoceb:30797650
e537b577:30795824
dsvsc013:30795093
dsvsc014:30797589

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-28 04:59:42,question,run the c code,"Type: <b>Feature Request</b>

I am unable to run the for loop in C . It shows the massage "" 'for' loop declarations are only allowed in c99 mode . please lead me to solve this issue with full details and step by step. 

VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-22 16:47:56,question,"Environment variables such as PATH only for a workspace, in settings.json","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
Is there a way to have a workspace specific path, added to settings.json for common executables. Were working on an extension where go is installed in the extension location, it would be nice to have env var support only for the workspace
"
microsoft/vscode,2023-07-20 17:03:38,question,Javascript in vscode cannot be intelligently completed!,"Type: <b>Bug</b>

Javascript in vscode cannot be intelligently completed, but it can be intelligently completed after changing the type of js file to typescript.

VS Code version: Code 1.78.2 (b3e4e68a0bc097f0ae7907b217c1119af9e03435, 2023-05-10T14:39:26.248Z)
OS version: Windows_NT x64 10.0.19045
Modes:
Sandboxed: Yes

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.80GB (1.17GB free)|
|Process Argv|--crash-reporter-id 2123c431-83b9-47c8-b7dc-2d65961922ff|
|Screen Reader|no|
|VM|33%|
</details><details><summary>Extensions (148)</summary>

Extension|Author (truncated)|Version
---|---|---
codesnap|adp|1.3.4
Bookmarks|ale|13.4.1
project-manager|ale|12.7.0
preview-pdf|ana|1.0.0
html-end-tag-labels|ant|1.0.0
browse-lite|ant|0.3.2
vite|ant|0.2.5
vue-jumper|atd|2.4.0
color-info|bie|0.7.2
Bito|Bit|1.1.2
exe-runner|bra|0.2.1
vsc-jetbrains-icons-enhanced|Bre|2.1.0
vscode-jetbrains-icon-theme|cha|2.3.0
wechat-snippet|Cha|0.4.11
npm-intellisense|chr|1.4.4
path-intellisense|chr|2.8.4
gitignore|cod|0.9.0
laravel-goto-view|cod|1.3.9
miniprogram-vscode-extension|cra|1.4.16
vscode-mysql-client2|cwe|6.6.1
vscode-office|cwe|3.1.6
xmind-viewer|cwe|1.0.0
vscode-markdownlint|Dav|0.51.0
vscode-notes|dio|1.1.0
python-preview|don|0.0.4
git-extension-pack|don|0.1.3
githistory|don|0.6.20
python-environment-manager|don|1.0.4
python-extension-pack|don|1.7.0
brackethighlighter|Dur|2.4.0
gitlens|eam|14.1.1
vscode-html-css|ecm|1.13.1
EditorConfig|Edi|0.16.4
vscode-npm-script|eg2|0.3.29
prettier-vscode|esb|9.19.0
python-code-snippets|Ext|1.1.2
git-project-manager|fel|1.8.2
auto-close-tag|for|0.5.14
auto-rename-tag|for|0.1.10
code-runner|for|0.12.0
codespaces|Git|1.14.12
remotehub|Git|0.60.0
vscode-pull-request-github|Git|0.64.0
gc-excelviewer|Gra|4.2.57
todo-tree|Gru|0.0.226
vue-snippets|hol|1.0.4
vscode-wordcount-cjk|hol|1.3.1
beautify|Hoo|1.5.0
vscode-htmlhint|HTM|1.0.5
rest-client|hum|0.25.1
output-colorizer|IBM|0.1.2
path-autocomplete|ion|1.24.1
open-file-from-path|jac|1.3.4
mysql-syntax|jak|1.3.1
dot-log|jal|0.1.6
hungry-delete|jas|1.7.0
search-node-modules|jas|1.3.0
shortcut-menu-bar|jer|3.0.4
intellij-idea-keybindings|k--|1.5.9
vsc-python-indent|Kev|1.18.0
vscode-gutter-preview|kis|0.30.0
node-module-intellisense|lei|1.5.0
vscode-settings-editor|lir|1.0.2
MagicPython|mag|1.1.0
template-string-converter|meg|0.6.1
git-graph|mhu|1.30.0
dotenv|mik|1.0.1
vscode-filesize|mkx|3.1.0
diff-merge|mos|0.7.0
easy-less|mrc|2.0.0
vscode-scss|mrm|0.10.0
vscode-docker|ms-|1.26.0
vscode-language-pack-zh-hans|MS-|1.78.2023051009
isort|ms-|2023.10.1
python|ms-|2023.8.0
jupyter|ms-|2023.4.1011241018
jupyter-keymap|ms-|1.1.0
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.299.0
remote-ssh|ms-|0.102.0
remote-ssh-edit|ms-|0.86.0
remote-wsl|ms-|0.78.9
azure-repos|ms-|0.36.0
js-debug-nightly|ms-|2023.6.2117
live-server|ms-|0.4.8
remote-explorer|ms-|0.4.0
remote-repositories|ms-|0.38.1
remote-server|ms-|1.2.1
one-dark-theme|msk|1.14.2
vscode-python-typehint|njq|1.5.1
vscode-versionlens|pfl|1.5.0
vs-browser|Phu|2.0.10
material-icon-theme|PKi|4.29.0
material-product-icons|PKi|1.5.0
vscode-css-peek|pra|4.4.1
vscode-css-navigation|puc|1.13.3
minapp-vscode|qiu|2.4.12
sqlite-viewer|qwt|0.2.5
vscode-thunder-client|ran|2.9.2
vscode-yaml|red|1.14.0
shellman|Rem|5.6.1
vscode-statusbar-json-path|ric|2.0.0
LiveServer|rit|5.7.9
any-rule|rus|0.3.18
vs-code-prettier-eslint|rve|5.1.0
partial-diff|ryu|1.4.3
multi-command|ryu|1.6.0
vue-vscode-snippets|sdr|3.1.1
markdown-preview-enhanced|shd|0.6.8
vscode-scss-formatter|sib|2.5.0
svg-preview|Sim|2.8.3
pip-manager|sli|1.1.3
css-auto-prefix|spo|0.1.7
vscode-standard|sta|2.1.3
autoimport|ste|1.5.4
code-spell-checker|str|2.20.5
sass-indented|syl|1.8.26
tabnine-vscode|Tab|3.6.77
open-in-browser|tec|2.0.0
pdf|tom|1.2.2
luna-paint|Tyr|0.16.0
sort-lines|Tyr|1.10.1
vscode-counter|uct|3.2.1
highlight-matching-tag|vin|0.11.0
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-svg-previewer|vit|0.7.0
vscode-icons|vsc|12.4.0
image-viewer|vsc|1.5.3
volar|Vue|1.8.5
vscode-typescript-vue-plugin|Vue|1.8.5
gitblame|wad|10.2.0
crs-al-language-extension|wal|1.5.28
quokka-vscode|Wal|1.0.547
fanyi|wan|1.0.6
vscode-todo-highlight|way|1.0.5
jinja|who|0.0.8
vscode-import-cost|wix|3.3.0
change-case|wma|1.0.0
JavaScriptSnippets|xab|1.8.0
vscode-preview-server|yui|1.3.0
markdown-pdf|yza|1.4.4
markdown-all-in-one|yzh|3.5.1
json|Zai|2.0.2
vscode-open-in-github|ziy|1.3.6
vue|znc|0.12.0

(9 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263cf:30792227
vscaat:30438848
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vsccc:30610678
3biah626:30602489
f6dab269:30613381
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30785052
pythongtdpath:30769146
i26e3531:30792625
pythonnosm12tcf:30779713
pythonidxpt:30784022
pythonnocebcf:30776496
h7j2d465:30786200
dsvsc013cf:30789518
dsvsc014:30791935

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-19 18:42:39,question,terminated,"Type: <b>Bug</b>

The terminal process ""C:\\Users\\venka\\.dotnet\\tools\\pwsh.exe"" terminated with exit code: 2147516547.

VS Code version: Code - Insiders 1.81.0-insider (c85bf61a82b0c39886b032d2634108782a55c637, 2023-07-19T05:34:51.441Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i3-1005G1 CPU @ 1.20GHz (4 x 1190)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.80GB (0.88GB free)|
|Process Argv|--crash-reporter-id ff0bc616-340d-4e72-a388-a233dfe53906|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
dart-code|Dar|3.69.20230719
flutter|Dar|3.69.20230703
remote-wsl|ms-|0.80.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv695:30137379
vsins829:30139715
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
pythontb:30258533
pythonptprofiler:30281269
vsdfh931cf:30280410
vshan820:30294714
vscod805cf:30301675
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30404738
py29gd2263:30784851
vsclangdf:30492506
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30618038
pylantcb52:30590116
showlangstatbar:30737417
24365598:30687740
pythonfmttext:30716741
pythoncmvfstr:30726892
fixshowwlkth:30771523
showindicator:30766888
pythongtdpath:30726887
i26e3531:30792625
gsofa:30778558
pythonnosmt12:30779711
pythonidxpt:30768918
pythondjangots:30768917
pythonnoceb:30776497
copilotsettingt:30767686
e537b577:30772214
h0f32768:30792099
synctok:30783813
dsvsc013:30777762
dsvsc014:30777825
diffeditorv2:30786206

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-19 03:07:59,question,"When the same variable exists in the open files, Intelligent completion will trigger wrong completion","Type: <b>Bug</b>

![image](https://github.com/microsoft/vscode/assets/133345898/82fe9fdb-d8b9-4eeb-9763-7c92eaa3c516)
![image](https://github.com/microsoft/vscode/assets/133345898/1f2bf595-e236-4daa-b235-f443cb8ed9af)



VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:20:23.298Z)
OS version: Darwin x64 22.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz (8 x 2000)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>metal: disabled_off<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|3, 3, 3|
|Memory (System)|32.00GB (7.37GB free)|
|Process Argv|--crash-reporter-id 25337f27-0fd6-4ad2-a343-d23e45e01d53|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (20)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-tailwindcss|bra|0.9.11
gitignore|cod|0.9.0
vscode-eslint|dba|2.4.2
gitlens|eam|14.1.1
EditorConfig|Edi|0.16.4
prettier-vscode|esb|9.19.0
auto-close-tag|for|0.5.14
code-runner|for|0.12.0
remotehub|Git|0.60.0
git-graph|mhu|1.30.0
vscode-language-pack-zh-hans|MS-|1.80.2023071209
remote-repositories|ms-|0.38.1
material-icon-theme|PKi|4.29.0
LiveServer|rit|5.7.9
even-better-toml|tam|0.19.2
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
pretty-ts-errors|yoa|0.4.1
markdown-all-in-one|yzh|3.5.1
material-theme|zhu|3.15.18


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
2e4cg342:30602488
f6dab269:30613381
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30785052
pythongtdpath:30769146
i26e3531:30792625
pythonnosmt12:30779714
pythonidxpt:30784022
pythonnoceb:30776495
h7j2d465:30786200
dsvsc013cf:30789518

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-18 09:20:42,question,Allow webview context menus triggered by primary click,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I'd like to be able to use platform native menus in extensions that use a web view. This would allow for more complex controls, such as split buttons and gear menus.

Currently it's possible for extensions to [provide a context menu](https://code.visualstudio.com/api/extension-guides/webview#context-menus), but the menu can only be triggered using a secondary click (and there's no way to hack around this).

The only way to pop up a native menu is through misusing a `<select>` element. However this causes different cross-platform interaction and styling inconsistencies. It would be much better to be able to use a real menu.

Perhaps an option to the `data-vscode-context` attribute could be added. Something like `deviceButton: primary|secondary`.

```html
<textarea data-vscode-context='{
    ""webviewSection"": ""editor"",
    ""deviceButton"": ""primary"",
    ""preventDefaultContextMenuItems"": true
}'>
</textarea>
```

It would also be useful to define multiple context menus using names.

Advantages:
- No need to implement own menus that look out of place (that can't extend outside the web view area)
- Good looking and accessible native menus on every platform
- More consistency across the IDE
- More flexibility for extension authors

cc @alexr00 @daviddossett "
microsoft/vscode,2023-07-17 17:05:21,question,Python bug,"Type: <b>Bug</b>

Hello
When i try basics like print(""Hello) it open a lot of things in the terminal but not print my hello and after a few seconde Visual Studio Code crash like i cannot click anywhere and i need to close the app by force.



VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i9-12900KF (24 x 3187)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.82GB (7.40GB free)|
|Process Argv|--crash-reporter-id f2a2bdb6-bf28-4843-aa08-0b9445264d58|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (3)</summary>

Extension|Author (truncated)|Version
---|---|---
importmagic|cod|0.2.5
python|ms-|2023.12.0
vscode-pylance|ms-|2023.7.20


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30789497
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyonecf:30548226
vscccc:30610679
2e4cg342:30602488
pyind779:30671433
f6dab269:30613381
vscrp:30673768
pythonsymbol12:30671437
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
a2ce3375:30757347
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30785052
pythongtdpath:30769146
bgfeh915:30780428
pythonnosmt12:30779714
pythonidxpt:30784022
pythonnoceb:30776495
e537b577:30786199
dsvsc013cf:30789518
dsvsc014:30791935

```

</details>

<!-- generated by issue reporter -->
![test py - Visual Studio Code 17_07_2023 19_03_35](https://github.com/microsoft/vscode/assets/139797113/6384c727-b78c-4e98-bfc0-dcb10f7db9d7)

"
microsoft/vscode,2023-07-17 01:49:49,question,[go to reference] need some jump,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

hello guys ,i dont know what i think is a feature or a bug?
what happen to me?

i use C/C++ EXTENSION for STM32 C language coding.
but when i push the button [go to reference ].it do not work ,it just show some REFERENCE at left by a new tab, do not jump ,do not report the error.
just show out at the left tab by a new page ,so i have to click the new tab page to see what i want.

is there a switch for show on reference tab or jump to reference?
i don't want my hands away from keyboard so when i just have to check out some reference.

thanks for watch ,have a nice day.
see yah.


BTW this is the png for what happen to me:
![image](https://github.com/microsoft/vscode/assets/31296263/920c7ae6-98c5-43ab-ba39-cf0d7d7cbbc2)
"
microsoft/vscode,2023-07-15 10:53:05,question,Issue in writing react native styles ,"hi, 
1. I saw an issue when  writing styles in react native. 
2. I write some inline styling like this <Text styles={styles.one}> instead  of this <Text style={styles.one}>  .

***I am not able to apply  the styles  with  this  <Text styles={styles.one}> but even I wrote like this it was  not  give any  error .
***please do something to show some error when we not apply  the styles as per the rules in react native.
[vscodeerror.zip](https://github.com/microsoft/vscode/files/12061191/vscodeerror.zip)
"
microsoft/vscode,2023-07-14 04:33:06,question,Not working and run in terminal the codes.,"
Type: <b>Feature Request</b>

vs code on windows 8/ windows server 2012 will soon  stop receiving updates. consider upgrading your windows version.

VS Code version: Code 1.80.1 (74f6148eb9ea00507ec113ec51c489d6ffb4b771, 2023-07-12T17:22:07.651Z)
OS version: Windows_NT x64 6.2.9200
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-13 15:01:38,question,install libraries ,"
Type: <b>Bug</b>

i just want to install and use some new libraries in VSCode (python)
when i use ""pip install "" in terminal
terminal gives me this errors please help me and let me know what i have to do for fix it 
""""""
ERROR: Exception:
Traceback (most recent call last):
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 438, in _error_catcher        
    yield
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 561, in read
    data = self._fp_read(amt) if not fp_closed else b""""
           ^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 527, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py"", line 90, in read
    data = self.__fp.read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\http\\client.py"", line 466, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\socket.py"", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\ssl.py"", line 1278, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\ssl.py"", line 1134, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py"", line 169, in exc_logging_wrapper 
    status = run_func(*args)
             ^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py"", line 248, in wrapper
    return func(self, options, args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\commands\\install.py"", line 377, in run
    requirement_set = resolver.resolve(
                      ^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py"", line 92, in resolve
    result = self._result = resolver.resolve(
                            ^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py"", line 546, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py"", line 397, in resolve
    self._add_to_criteria(self.state.criteria, r, parent=None)
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py"", line 173, in _add_to_criteria
    if not criterion.candidates:
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py"", line 156, in __bool__
    return bool(self._sequence)
           ^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py"", line 155, in __bool__
    return any(self)
           ^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py"", line 143, in <genexpr>
    return (c for c in iterator if id(c) not in self._incompatible_ids)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py"", line 47, in _iter_built
    candidate = func()
                ^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py"", line 206, in _make_candidate_from_link
    self._link_candidate_cache[link] = LinkCandidate(
                                       ^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py"", line 293, in __init__
    super().__init__(
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py"", line 156, in __init__
    self.dist = self._prepare()
                ^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py"", line 225, in _prepare
    dist = self._prepare_distribution()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py"", line 304, in _prepare_distribution
    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py"", line 516, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py"", line 587, in _prepare_linked_requirement
    local_file = unpack_url(
                 ^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py"", line 166, in unpack_url
    file = get_http_url(
           ^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py"", line 107, in get_http_url
    from_path, content_type = download(link, temp_dir.path)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\network\\download.py"", line 147, in __call__
    for chunk in chunks:
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py"", line 53, in _rich_progress_bar
    for chunk in iterable:
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_internal\\network\\utils.py"", line 63, in response_chunks
    for chunk in response.raw.stream(
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 622, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 560, in read
    with self._error_catcher():
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\contextlib.py"", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""E:\\SoftWare\\Programing\\Python\\Python\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py"", line 443, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.
""
thank you

VS Code version: Code 1.80.0 (660393deaaa6d1996740ff4880f1bad43768c814, 2023-07-04T15:06:02.407Z)
OS version: Windows_NT x64 10.0.19044
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 5500U with Radeon Graphics          (12 x 2096)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|9.86GB (3.11GB free)|
|Process Argv|--crash-reporter-id 4b693618-aa7b-469c-8941-3df9104aff72|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (24)</summary>

Extension|Author (truncated)|Version
---|---|---
matlab-formatter|Aff|2.10.51
matlab-interactive-terminal|apo|0.4.0
matlab-extension-pack|bat|0.1.0
matlab-code-run|bra|1.0.2
code-runner|for|0.12.0
c-cpp-runner|fra|7.0.2
matlab|Gim|3.0.2
language-matlab|Mat|1.1.1
csharp|ms-|1.26.0
python|ms-|2023.13.11941005
vscode-pylance|ms-|2023.7.20
jupyter|ms-|2023.6.1101941928
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.17
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
cmake-tools|ms-|1.14.34
cpptools|ms-|1.16.3
cpptools-extension-pack|ms-|1.3.0
ispy|Ric|0.0.4
matlab-complete|Sla|1.1.1
cmake|twx|0.0.17
vscode-lldb|vad|1.9.2
NbConverter|yig|1.5.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vswsl492cf:30256860
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931:30280409
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30784848
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
ecj1e332:30736112
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30785052
pythongtdpath:30769146
bgfeh915:30780428
pythonnosm12tcf:30779713
pythonidxpt:30784022
pythonnocebcf:30776496
e537b577:30786199

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-13 09:00:42,question,JSDoc @description tag does not respect line breaks unless two spaces are added at the end of the line,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Does this issue occur when all extensions are disabled?: Yes

- VS Code Version: 1.81.0-insider
- OS Version: macOS 13.2.1, Apple M1 Pro

I have a question about the `@description` tag in JSDoc comments in VS Code. It seems that line breaks within the `@description` tag are not respected unless two spaces are added at the end of the line. I believe this should not be the case, and line breaks should be respected without the need for two spaces at the end of the line. If VS Code is not using JSDoc, please inform me of the correct format for line breaks within the `@description` tag.

Steps to Reproduce:

1. Use the latest version of VS Code Insiders and create a new profile, only changing the theme and `editor.renderWhitespace` setting.
2. Write different function comments as shown below. Notice that the `@description` content does not respect line breaks unless two spaces are added at the end of the line.

```js
/**
 * @description
 * example1: it will no line break.
 * line 2.
 * line  3.
 */
const fn1 = () => {}

/**
* @description
* example2: 这将不会换行。
* 行2。
* 行3。
*/
const fn2 = () => {}

/**
* @description
* example1: it will line break.  
* line 2.  
* line 3.  
*/
const fn3 = () => {}

/**
* @description
* example2: 这将会换行。  
* 行2。  
* 行3。  
*/
const fn4 = () => {}
```

![Snipaste_2023-07-13_16-08-07](https://github.com/microsoft/vscode/assets/26453811/fed9874b-436f-4aaf-8d1c-4cfc1c95d5fb)

"
microsoft/vscode,2023-07-11 10:40:37,question,ports in codespace,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.80.0 (Universal)
- OS Version: Darwin x64 20.6.0,  macOS Big Sure 11.7.8

Steps to Reproduce:

1.  no ports on panel in codespace 

"
microsoft/vscode,2023-07-08 17:53:37,question,Unable to resolve resource vscode-remote://codespaces%2Bboli8845-ideal-fiesta-7r579g4xprfx57/workspaces/27709499,"ADD ISSUE DESCRIPTION HERE
Can't load my file as error message says:""Unable to resolve resource vscode-remote://codespaces%2Bboli8845-ideal-fiesta-7r579g4xprfx57/workspaces/27709499""
Version: 1.80.0
Commit: 660393deaaa6d1996740ff4880f1bad43768c814
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->"
microsoft/vscode,2023-07-08 07:02:45,question,"How do i recover deleted files? I accidentally removed my entire project file, is there any way i can recover it?","ADD ISSUE DESCRIPTION HERE

Version: 1.80.0
Commit: 660393deaaa6d1996740ff4880f1bad43768c814
User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36
Embedder: codespaces

<!-- generated by web issue reporter -->"
microsoft/vscode,2023-07-07 03:27:17,question,Repeatedly getting errors launching flutter app in Iphone [Error launching application],"
Does this issue occur when all extensions are disabled?: Yes

I just downloaded VSCODE, and when I run the application on my iphone it is extremely intermittent, it will run on 4th or 5th try.
On XCode, Android Studio it will run on first try

this is the error

```
Launching lib/main.dart on Pannam 10.5 in debug mode...
Automatically signing iOS for device deployment using specified development team in Xcode project: FTSSDSD68A
Xcode build done.                                           125.5s
Could not run build/ios/iphoneos/Runner.app on 64710b4b6289ef663692fc70dfa0912fdcc7a0ce.
Try launching Xcode and selecting ""Product > Run"" to fix the problem:
  open ios/Runner.xcworkspace

Error launching application on Pannam 10.5.
```



can assist with this. -->
- VS Code Version: 
- Version: 1.80.0 (Universal)
Commit: 660393deaaa6d1996740ff4880f1bad43768c814
Date: 2023-07-04T13:39:33.766Z
Electron: 22.3.14
ElectronBuildId: 21893604
Chromium: 108.0.5359.215
Node.js: 16.17.1
V8: 10.8.168.25-electron.0
OS: Darwin x64 21.6.0


- OS Version: 
-  System Version:	macOS 12.6.7 (21G651)
  Kernel Version:	Darwin 21.6.0
  Mac Os Monterey 


"
microsoft/vscode,2023-07-05 22:12:02,question,html incompleto,"Type: <b>Bug</b>

quando vou criar um código com html ele não ""começa"" o código pra mim, tenho que digitar tudo desde o princípio sendo que no outro computador que mexo eu digito apenas html e ele faz todo o começo do código pra mim, como o titulo, o idioma e etc. queria saber o que posso fazer para que o vscode termine o código pra mim. Por favor me ajude...


Google translate:

> ‎When I create a code with HTML it does not ""start"" the code for me, I have to type everything from the beginning being that on the other computer that I move I type only HTML and it does all the beginning of the code for me, such as the title, the language and etc. I wanted to know what I can do so that VSCode finishes the code for me. Please help me...‎




VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 2419)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.78GB (1.32GB free)|
|Process Argv|--crash-reporter-id 8c484ca8-5123-44c9-9197-034d6b067eb1|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (35)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-django|bat|1.10.0
vscode-eslint|dba|2.4.2
python-environment-manager|don|1.0.4
python-extension-pack|don|1.7.0
vscode-firefox-debug|fir|2.9.8
auto-rename-tag|for|0.1.10
copilot|Git|1.95.233
vsc-python-indent|Kev|1.18.0
vscode-docker|ms-|1.25.2
vscode-language-pack-pt-BR|MS-|1.79.2023061409
vscode-edge-devtools|ms-|2.1.2
isort|ms-|2022.8.0
python|ms-|2023.10.1
vscode-pylance|ms-|2023.6.40
jupyter|ms-|2023.5.1101742258
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
color-highlight|nau|2.5.0
autodocstring|njp|0.6.1
open-html-in-browser|pea|2.1.10
java|red|1.20.0
LiveServer|rit|5.7.9
python|tht|0.2.3
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.52.0
vscode-java-dependency|vsc|0.23.0
vscode-java-pack|vsc|0.25.12
vscode-java-test|vsc|0.39.0
vscode-maven|vsc|0.41.0
vscode-icons|vsc|12.4.0
jinja|who|0.0.8
html-css-class-completion|Zig|1.20.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscoreces:30445986
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30776702
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
a9j8j154:30646983
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
pythongtdpath:30769146
bgfeh915:30780428
pythonnosm12tcf:30779713
pythonidxpt:30772539
pythonnocebcf:30776496

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-04 09:34:59,question,js reference irrelevant file across open editors,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.79.2
- OS Version: win10

Steps to Reproduce:
![references bug](https://github.com/microsoft/vscode/assets/47163429/7a5e2154-a16d-47a4-922e-52d446d92923)
references is correct when only one file is open, but when the sercond one is open, things goes wrong.
is there a way to prevent this behavior?"
microsoft/vscode,2023-07-02 22:18:39,question,BotBuild,"
Type: <b>Performance Issue</b>

it occured when I tried to install an application

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz (12 x 2592)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.84GB (8.16GB free)|
|Process Argv|--crash-reporter-id 69e01fc4-9ccd-4bb9-80b1-d04d8a4f8fd2|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    0	   118	 12080	code main
    0	   207	  2236	window [1] (BotBuild - Visual Studio Code)
    0	    90	  6912	window [2] (Issue Reporter)
    0	    27	  8584	   crashpad-handler
    0	   209	 12156	   gpu-process
    0	   175	 18416	extensionHost [1]
    0	   154	 11888	     electron-nodejs (""C:\\Users\\jspet\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node c:\\Users\\jspet\\.vscode\\extensions\\ms-python.vscode-pylance-2023.5.40\\dist\\server.bundle.js --cancellationReceive=file:c02161c352f39d5fe13ccb45f213c121639f255104 --node-ipc --clientProcessId=18416)
    0	    79	 18196	     electron-nodejs (""C:\\Users\\jspet\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node ""c:\\Users\\jspet\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\json-language-features\\server\\dist\\node\\jsonServerMain"" --node-ipc --clientProcessId=18416)
    0	    70	 19876	     electron-nodejs (""C:\\Users\\jspet\\AppData\\Local\\Programs\\Microsoft VS Code\\Code.exe"" --ms-enable-electron-run-as-node c:\\Users\\jspet\\.vscode\\extensions\\formulahendry.auto-rename-tag-0.1.10\\packages\\server\\dist\\serverMain.js --node-ipc --clientProcessId=18416)
    0	    46	 18840	   utility-network-service
    0	    88	 19488	ptyHost
    0	     5	  4732	     C:\\WINDOWS\\System32\\cmd.exe
    0	     5	  5336	     C:\\WINDOWS\\System32\\cmd.exe
    0	     7	 15268	     console-window-host (Windows internal process)
    0	     7	 21308	     console-window-host (Windows internal process)
    0	     5	 21408	     C:\\WINDOWS\\System32\\cmd.exe
    0	     7	 21908	     console-window-host (Windows internal process)
    0	    95	 19524	shared-process
    0	    68	 19592	fileWatcher [1]
    0	     6	 20092	   ""C:\\Program Files\\Google\\Drive File Stream\\77.0.3.0\\crashpad_handler.exe"" --database=C:\\Users\\jspet\\AppData\\Local\\Google\\DriveFS\\Crashpad --url=https://clients2.google.com/cr/report --annotation=application=Code.exe --annotation=prod=DriveFS --annotation=ver=77.0.3.0 --initial-client-data=0x1694,0x17bc,0x1748,0x14d0,0x145c,0x7ffe2a1a7550,0x7ffe2a1a7560,0x7ffe2a1a7570
```

</details>
<details>
<summary>Workspace Info</summary>

```
;
```

</details>
<details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
prettier-vscode|esb|9.19.0
auto-rename-tag|for|0.1.10
fluent-icons|mig|0.0.18
gather|ms-|2022.3.2
python|ms-|2023.10.1
vscode-pylance|ms-|2023.6.40
jupyter|ms-|2023.5.1101742258
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
vsliveshare|ms-|1.0.5873
LiveServer|rit|5.7.9
vscode-icons|vsc|12.4.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30776702
vsclangdc:30486549
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30610679
3biah626:30602489
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
57b77579:30736110
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30766890
pythongtdpath:30769146
bgfeh915:30780428
pythonnosmt12:30779714
pythonidxpt:30772539
pythonnoceb:30776495

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-07-01 18:09:08,question,Only subset of references of a symbol found unless file from specific directories is opened in editor,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes _The video is recorded with extensions still enabled, however the behavior is identical after launching with no extensions._

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
Version: 1.79.2 (user setup)
Commit: 695af097c7bd098fbf017ce3ac85e09bbc5dda06
Date: 2023-06-14T08:57:04.379Z
Electron: 22.5.7
Chromium: 108.0.5359.215
Node.js: 16.17.1
V8: 10.8.168.25-electron.0
OS: Windows_NT x64 10.0.19045

**Expected behavior**: Searching for references of a symbol takes all files in a project into consideration, regardless of whether any particular file is open.
**Observed behavior**: Only if particular files are opened does it locate references in all files. It's completely unobvious which file this has to be, or why it has this weird requirement. It misses over half of usages.

**Steps to Reproduce** (I don't know which aspect of my folder structure is triggering this, possibly hard to reproduce, but you can follow these steps in the video):

1. Open a file that contains symbols with references throughout many folders
2. Control + click on a symbol to get a popup with the references
3. Note there is a small amount of references that misses most references
4. Open a file from a folder where a missing reference lives
5. Control + click again on the same symbol
6. Note that now it lists all references that were missing, even if they live in another folder than the file that was opened in step 4. In my case I think (or hope) that it now does include everything I'm looking for
7. Close the file that was opened in step 4 again
8. Now it again only includes a subset of references

https://github.com/microsoft/vscode/assets/7604138/08acd15c-3d2b-441b-9429-047cdfd079f6


The above happens for no apparent reason. I never changed any editor configuration that could reasonably be expected to affect this behavior. Nor did I find any configuration that could fix this issue.

It's definitely unrelated to long paths on Windows (mentioned in similar issues), all paths are under 100 characters, well below 256.

Likely you can clone [the repository in question](https://github.com/Inwerpsel/use-theme-editor) to reproduce the behavior.

**Why is this a problem?**

The ability to reliably locate symbol usages is part of the core, absolute minimum functionality you'd expect from an IDE. If it misses usages, this can lead to serious real world bugs. It also significantly erodes confidence and slows down work.

There is no good reason for it to exclude any file unless specifically configured to do so. And if VSCode would, for some reason, not default to looking everywhere, it should be very obvious about this and put a big warning saying that not each directory was considered for a particular search of any kind."
microsoft/vscode,2023-06-30 12:29:26,question,"chrome like behaviour when closing tabs, this is so frustrating (⚠️)","<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

if a photo worth a thousand words a video worth a million words

terrible VS code UX
[Screencast from 30-06-2023 14:24:38.webm](https://github.com/microsoft/vscode/assets/55017307/228e00fd-bc48-424f-a780-13856fb9d7ae)

great Google Chrome UX
[Screencast from 30-06-2023 14:25:22.webm](https://github.com/microsoft/vscode/assets/55017307/4e4b1456-ed2a-4302-b22f-0e82d9d9beae)

thanks a lot, love your product!

"
microsoft/vscode,2023-06-29 13:49:04,question,.length property not working?,"Type: <b>Bug</b>

let numbers = [1, 2, 99, 100]
// console.log correctly logs numbers array
console.log(numbers.length)

function findBiggestAndSmallest(numbers) {
    const biggestSmallest = {}

// TypeError: Cannot read properties of undefined (reading 'length')
// when otherwise should not be an error
    if (numbers.length === 0) {
        return biggestSmallest
    }

    let biggest = numbers[0]
    let smallest = numbers[0]

    for (let i = 1 ; i < numbers.length; i++) {
        if (numbers[i] > biggest) {
            biggest = numbers[i]
        }
        if (numbers[i] < smallest) {
            smallest = numbers[i]
        }
    }

    biggestSmallest.biggest = biggest
    biggestSmallest.smallest = smallest
    return biggestSmallest
        }


VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz (8 x 2112)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.76GB (1.10GB free)|
|Process Argv|--crash-reporter-id ee8117d4-631c-40d4-b0f9-6bc53f06d153|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (8)</summary>

Extension|Author (truncated)|Version
---|---|---
npm-intellisense|chr|1.4.4
path-intellisense|chr|2.8.4
vscode-eslint|dba|2.4.2
code-runner|for|0.12.0
vscode-versionlens|pfl|1.5.0
node-pack|Swe|0.1.16
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263cf:30335440
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263:30776702
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
pyind779:30671433
f6dab269:30613381
pythonsymbol12:30671437
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
57b77579:30736110
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
showindicator:30766890
pythongtdpath:30769146
i26e3531:30769768
e440d664:30776459
pythonidxptcf:30772540
pythondjangots:30772535
pythonnocebcf:30776496

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-28 22:44:43,question,Mouse not working at all in terminal + all terminal editor keybinding is occupied by vs code,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.79.2
- OS Version: Windows 10.

Steps to Reproduce:

1. Open Vs code
2. Open new terminal
3. SSH into a server,  for example: ssh username@192.168.2.87  (i was logged into an ubuntu server)
4. open some larger config file @ the server. for example:
                sudo nano .bashrc
                sudo micro .bashrc

### The first bug:
 **the mouse does not work at all in does editors**. Nor in nano, nor in micro.
In different terminal softwares: ""Micro editor"" >> uses the mouse with no problem.
""Nano editor"" could use the mouse with the  -m switch.  

After two hours of ai chat and google and youtube i could not use the mouse in the terminal.

### The second bug:
If in nano or micro editor i try to do anything usual (save, exit, delete a line, copy/paste), there is a huge possibility that a vs code keyboard shortcut is conflicting with the nano and or micro editor. 

- Ctrl+K in nano editor to delete a line >> no chance to use it in vs code terminal
- Ctrl+Shift+K in micro editor to delete a line >> does not work in vs code terminal
- ctrl+q to exit from micro editor in the terminal >> steps out from the terminal in vs code so you can not close the micro editor

vs code look promissing, but without the mouse usage in the terminal it has a ""DOS"" system productivity, 
and with the keybindings conflict, using vs code to ssh is basically impossible

thanks i hope you find solution, get back to me if you need help or testing.

Bence Gyulai

"
microsoft/vscode,2023-06-28 17:41:26,question,Will not run code,"Type: <b>Bug</b>

HI! 

I keep getting the following error when I try to run any code in Visual Studio Code. 
crbug/1173575, non-JS module files deprecated.


VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-7700T CPU @ 2.90GHz (8 x 2904)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|7.92GB (2.78GB free)|
|Process Argv|--crash-reporter-id 80a3cc3c-a773-4d91-8715-1b31dda2242c|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (4)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-edge-devtools|ms-|2.1.2
js-debug-nightly|ms-|2023.6.2117
open-html-in-browser|pea|2.1.10
cors-browser|Wsc|1.0.11


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931:30280409
vshan820:30294714
vstes263cf:30335440
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593:30376534
pythonvs932:30410667
py29gd2263:30776702
vscaat:30438848
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
282f8724:30602487
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
pythongtdpath:30769146
bgfeh915:30769767
dh2dc718:30776458
pythonidxptcf:30772540
pythondjangots:30772535
pythonnocebcf:30776496

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-26 22:05:34,question,Code actions on save can also be used in a runCommands keybinding,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
I want to be able to run the following commands with a keyboard shortcut _without saving the file_:
- ""source.addMissingImports""
- ""source.fixAll""
- ""source.organizeImports""

I tried to do this with a custom runCommands keybinding, but I get error ""command 'source.addMissingImports' not found"".
```
{
    ""command"": ""runCommands"",
    ""key"": ""alt+s"",
    ""args"": {
      ""commands"": [
        ""source.addMissingImports"",
        ""source.fixAll"",
        ""source.organizeImports""
      ]
    },
    ""when"": ""editorTextFocus""
  }
```

Appreciate y'all!"
microsoft/vscode,2023-06-26 15:45:23,question,terminal,"Type: <b>Bug</b>

running a c program on using ""gets (str)"" but the terminal says its running code and no results displayed 2
The expected results is that the user should be allowed to enter their complete na7mes and the program runs as designed

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.19045
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Celeron(R) N4000 CPU @ 1.10GHz (2 x 1094)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: disabled_off<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|3.83GB (0.94GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (11)</summary>

Extension|Author (truncated)|Version
---|---|---
code-runner|for|0.12.0
python|ms-|2023.10.1
vscode-pylance|ms-|2023.6.30
jupyter|ms-|2023.5.1101742258
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
cmake-tools|ms-|1.14.33
cpptools-extension-pack|ms-|1.3.0
cmake|twx|0.0.17

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vsdfh931cf:30280410
vshan820:30294714
vstes263:30335439
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30773604
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
2e4cg342:30602488
pyind779:30671433
f6dab269:30613381
pythonsymbol12:30671437
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
showindicator:30766890
pythongtdpath:30769146
i26e3531:30769768
gsof1:30774496
e440d664:30776459
pythonidxptcf:30772540
pythondjangotscf:30772537
h7j2d465:30772216

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-24 02:26:58,question,Bug in update.,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version:  
- OS Version: 
![image](https://github.com/microsoft/vscode/assets/60914359/6c426987-e25b-407e-901f-7c912e6d4157)

Steps to Reproduce:

1. Click download update in vscode and get the zip.
2. Upzip the zip in the folder and overlap original files.
3. It updated successfully and the version became 1.79.2.
4. When I rebooted computer, the version reverted to the original version.
"
microsoft/vscode,2023-06-23 14:26:39,question,cant use my import,"
Type: <b>Bug</b>

like i have use and install openai but is not importing and showing missing import

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.22000
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx   (8 x 2096)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|5.95GB (0.57GB free)|
|Process Argv|--crash-reporter-id a751a3b7-cf5a-4f61-ab08-04c382dc8d34|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (66)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-openapi|42C|4.18.2
android-dev-ext|ade|1.3.2
vscode-django|bat|1.10.0
simple-react-snippets|bur|1.2.7
npm-intellisense|chr|1.4.4
doxdocgen|csc|1.4.0
dart-code|Dar|3.66.0
flutter|Dar|3.66.0
vscode-eslint|dba|2.4.2
emulate|Die|1.6.0
python-environment-manager|don|1.0.4
python-extension-pack|don|1.7.0
gitlens|eam|14.0.1
vscode-html-css|ecm|1.13.1
vscode-npm-script|eg2|0.3.29
react-native-react-redux|EQu|2.0.6
prettier-vscode|esb|9.16.0
vscode-firefox-debug|fir|2.9.8
code-runner|for|0.12.0
copilot|Git|1.92.177
pyformat|giy|1.0.2
open-url|Gru|0.0.4
better-cpp-syntax|jef|1.17.2
vscode-insertdatestring|jsy|2.3.1
solidity|Jua|0.0.165
vsc-python-indent|Kev|1.18.0
csharpextensions|kre|1.7.3
bash-ide-vscode|mad|1.37.0
fluent-icons|mig|0.0.18
vscode-docker|ms-|1.25.1
csharp|ms-|1.26.0
vscode-kubernetes-tools|ms-|1.3.13
playwright|ms-|1.0.13
isort|ms-|2022.8.0
python|ms-|2023.10.1
vscode-pylance|ms-|2023.6.30
jupyter|ms-|2023.5.1101742258
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.295.0
cmake-tools|ms-|1.14.33
cpptools|ms-|1.16.2
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2023.6.2117
powershell|ms-|2023.6.0
vscode-react-native|msj|1.11.0
awesome-flutter-snippets|Nas|3.0.3
autodocstring|njp|0.6.1
material-icon-theme|PKi|4.28.0
vscode-rapidapi-client|Rap|1.10.2
vscode-services|rap|1.0.2
vscode-yaml|red|1.13.0
LiveServer|rit|5.7.9
tabnine-vscode|Tab|3.6.60
python|tht|0.2.3
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.51.0
vscode-java-dependency|vsc|0.23.0
vscode-maven|vsc|0.41.0
vscode-icons|vsc|12.4.0
jinja|who|0.0.8
commandlist|yam|1.1.0

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242cf:30382550
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805:30301674
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263:30773603
vsclangdf:30486550
c4g48928:30535728
dsvsc012:30540252
pynewext54:30695312
azure-dev_surveyone:30548225
3biah626:30602489
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixshowwlkth:30771522
hideindicator:30766889
pythongtdpath:30769146
gsof2:30774497
dh2dc718:30770000
pythonidxptcf:30772540
pythondjangots:30772535
e537b577:30772215

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-22 16:07:36,question,Environment variables in %PATH% are not expanded,"Type: <b>Bug</b>
> This issue seems to be a duplicate of #26048, but since that was closed and I had some more info I couldn't add, I created this one.

### Summary
I have some environment variables set to some directories, and they're used in my %PATH% variable (System space, not user) and they're not getting expanded in the integrated terminal. If I type `echo %PATH%` I see the variables unexpanded, instead of the directories they point to (Here's the output) :
```
C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS\\;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS\\;%SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem;%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0\\;%SYSTEMROOT%\\System32\\OpenSSH\\;C:\\Program Files\\dotnet\\;C:\\Program Files (x86)\\Bitvise SSH Client;C:\\Program Files\\Git\\cmd;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;%PATH_PROGS%;C:\\Program Files\\PuTTY\\;%ARINA_CLI_DIR%;A:/Prog/Dev/Rust/Win/CargoHome\\bin;C:\\Users\\Dante\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Dante\\.dotnet\\tools;
```
But variables in User space (Everything after `A:/Prog/Dev/Rust/Win/CargoHome\\bin` above) are expanded, Here is their unexpanded version from system settings :
```
%USERPROFILE%\\AppData\\Local\\Microsoft\\WindowsApps;%USERPROFILE%\\.dotnet\\tools;
```
### Workaround
Seeing this behavior I tried running VSCode as Administrator, and this time the variables are correctly expanded.

### Info
VS Code version: Code 1.79.0 (b380da4ef1ee00e224a15c1d4d9793e27c2b6302, 2023-06-07T14:26:35.552Z) (Portable)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz (8 x 1992)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|31.88GB (25.11GB free)|
|Process Argv|--crash-reporter-id ea812d11-4c3b-413a-b4b7-c326d9aa68d1|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (7)</summary>

Extension|Author (truncated)|Version
---|---|---
git-graph|mhu|1.30.0
csharp|ms-|1.25.9
remote-ssh|ms-|0.102.0
remote-ssh-edit|ms-|0.86.0
hexeditor|ms-|1.9.11
remote-explorer|ms-|0.4.0
vscode-open|san|0.1.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vsreu685:30147344
python383cf:30185419
vspor879:30202332
vspor708:30202333
vspor363:30204092
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscorecescf:30445987
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
py29gd2263cf:30773604
vsclangdc:30486549
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyone:30548225
vscccc:30610679
3biah626:30602489
pyind779:30671433
f6dab269:30613381
pythonsymbol12:30671437
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
03d35959:30757346
pythonfmttext:30731395
pythoncmv:30756943
fixshowwlkth:30771522
hideindicator:30766889
pythongtdpath:30769146
e440d664:30770001
pythonidxptcf:30772540
pythondjangotscf:30772537
pythonnoceb:30773526
e537b577:30772215

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-21 14:12:05,question,How to Automatically Guide Users to Upgrade from One VSCode Extension to Another?,"Hi,

I'm seeking advice on an issue related to VSCode themes. Previously, I published a theme called luke-dark-theme, and recently, I released a new theme called Photonica. Photonica is intended to be a direct upgrade from luke-dark-theme.

I am wondering if there is a way to facilitate users who have installed luke-dark-theme to easily or automatically upgrade to the new Photonica theme. How can I do this?

Thank you in advance!
"
microsoft/vscode,2023-06-19 15:16:32,question,Active tab in Vs Code is not very clear,"Type: <b>Bug</b>

Vs Code is great, but there is one bad thing about it annoys me too much. ""Active tab"" in VS Code in not very clear to detect. Either I code or when I see video course, it is very hard for me to detect active tab in first glance! 
In fact,  active tab is lost between different colors of tabs ( because of git property for changed files, new files, etc).

I suggest that,  show active tab  in VS Code with ""bordered thick blue color"".

Please, fix this. 

VS Code version: Code 1.78.2 (b3e4e68a0bc097f0ae7907b217c1119af9e03435, 2023-05-10T14:39:26.248Z)
OS version: Windows_NT x64 10.0.19045
Modes:
Sandboxed: Yes


<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-18 12:24:30,question,The facility for exposing default settings launch.json,"<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->
The official documentation for VS Code's built-in debugger doesn't provide comprehensive detail on the attributes exposed by Intellisense for launch.json. For example, showAsyncStacks is a boolean, for which no default is indicated. Neither symbols nor settings expose its value. 

For example, Intellisense does not expose an `address` attribute, contrary to the 'official' documentation, which asserts that remote debugging is baked into VS Code, and is specified with the `address` attribute. Without a facility to expose internals, it's difficult to know if this was simply an oversight in the preparation of the Intellisence database, or if VS Code doesn't recognize an `address` attribute, if included in launch.json. Intellisense exposes the following 48 attributes: 

args, attachSimplePort, autoAttachChildProcesses, cascadeTerminateToConfigurations, console, customDescriptionGenerator, customPropertiesGenerator, debugServer, enableContentValidation, enableTurboSourcemaps, env, envFile, internalConsoleOptions, killBehavior, linus, localRoot, name, nodeVersionHint, osx, outFiles, outputCapture, pauseForSourceMap, postDegugTask, preLaunchTask, presentation, profileStartup, program, remotesRoot, request, resolveSourceMapLocations, restart, runtimeArgs, runtimeExecutable, runtimeSourcemapPausePatterns, runtimeVersion, serverReadyAction, showAsyncStacks, skipFiles, smartStep, sourceMapPathOverrides, sourceMapRenames, sourceMaps, supressMultipleSessionWarning, timeout, timeouts, trace, type, windows

BTW, the attribute, runtimeArgs, appears to be misnamed for its intended purpose: to include command-line arguments. runtimeArgs has the same symantics as Args (args are passed at runtime). `commandOptions` would be more appropriate.


-MDA
michael@speedroi.com"
microsoft/vscode,2023-06-17 00:56:01,question,Background theme,"
Type: <b>Bug</b>

My usual theme would be the ""shade of purple"" and this allow the background of the vscode theme to turn purple. the problem is when i want to  change the theme, only code color that change but not the background. this make my report progression slow as i need to screenshot and print the report and i dont like the pruple background and want to change it to another theme. could u help to solve this issues
Thanks
sincerely,
Mrkiko

VS Code version: Code 1.79.2 (695af097c7bd098fbf017ce3ac85e09bbc5dda06, 2023-06-14T08:57:04.379Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz (8 x 1800)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|11.85GB (4.14GB free)|
|Process Argv|--crash-reporter-id acb7f439-2957-4d71-8f63-6803f8ec56e2|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (68)</summary>

Extension|Author (truncated)|Version
---|---|---
html-snippets|abu|0.2.1
codesnap|adp|1.3.4
vscode-sqlite|ale|0.14.1
flutter-snippets|ale|3.0.0
jar-builder|asl|1.1.1
code-gnu-global|aus|0.2.2
phpserver|bra|3.0.2
java-run|cao|1.1.4
coddx-alpha|cod|0.3.1
doxdocgen|csc|1.4.0
c-cpp-compile-run|dan|1.0.45
dart-code|Dar|3.66.0
flutter|Dar|3.66.0
composer-php-vscode|DEV|1.34.13297
phptools-vscode|DEV|1.34.13297
githistory|don|0.6.20
javadebugger|don|0.1.5
vscode-html-css|ecm|1.13.1
auto-close-tag|for|0.5.14
auto-complete-tag|for|0.1.0
auto-rename-tag|for|0.1.10
html-preview-vscode|geo|0.2.5
vscode-javac|geo|0.2.45
html-snippets|gey|0.2.3
remotehub|Git|0.58.0
vscode-pull-request-github|Git|0.66.1
jason-vscode-pack|jas|0.0.1
elixir-ls|Jas|0.2.26
better-cpp-syntax|jef|1.17.2
subway-surfers|jir|2.0.0
flutter-tree|mar|1.0.0
vscode-html-format|moh|0.1.2
vscode-dotnet-runtime|ms-|1.6.0
jupyter|ms-|2023.5.1001582324
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.295.0
remote-wsl|ms-|0.79.5
azure-repos|ms-|0.34.0
cmake-tools|ms-|1.14.33
cpptools|ms-|1.15.4
cpptools-extension-pack|ms-|1.3.0
remote-repositories|ms-|0.36.0
vsliveshare|ms-|1.0.5873
sqltools|mtx|0.27.1
awesome-flutter-snippets|Nas|4.0.1
reload|nat|0.0.6
material-icon-theme|PKi|4.28.0
java|red|1.19.0
format-html-in-php|rif|1.7.0
LiveServer|rit|5.7.9
html5-boilerplate|sid|1.1.1
cmake|twx|0.0.17
lorem-ipsum|Tyr|1.3.1
intellicode-api-usage-examples|Vis|0.2.7
vscodeintellicode|Vis|1.2.30
vscode-java-debug|vsc|0.51.0
vscode-java-dependency|vsc|0.23.0
vscode-java-pack|vsc|0.25.11
vscode-java-test|vsc|0.39.0
vscode-maven|vsc|0.41.0
codetour|vsl|0.0.59
JavaScriptSnippets|xab|1.8.0
php-debug|xde|1.32.1
php-pack|xde|1.0.3
php-intellisense|zob|1.1.3

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vsreu685:30147344
python383:30185418
vspor879:30202332
vspor708:30202333
vspor363:30204092
vstes627:30244334
vslsvsres303:30308271
vserr242:30382549
pythontb:30283811
vsjup518:30340749
pythonptprofiler:30281270
vshan820:30294714
vstes263:30335439
vscod805cf:30301675
binariesv615:30325510
bridge0708:30335490
bridge0723:30353136
vsaa593cf:30376535
pythonvs932:30410667
vsclangdf:30486550
c4g48928:30535728
dsvsc012cf:30540253
pynewext54:30695312
azure-dev_surveyonecf:30548226
3biah626:30602489
pyind779:30671433
89544117:30613380
pythonsymbol12:30671437
2i9eh265:30646982
showlangstatbar:30737416
vsctsb:30748421
pythonms35:30701012
a2ce3375:30757347
24365598:30736109
pythonfmttext:30731395
pythoncmvfstrcf:30756944
fixhidewlkth:30730051
showindicator:30766890
pythongtdpath:30769146
dh2dc718:30770000
pythondjangots:30768919

```

</details>

<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-15 23:05:33,question,Issue with ESC key mapping and Copilot,"<!-- ⚠️⚠️ Do Not Delete This! bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- 🕮 Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- 🔎 Search existing issues to avoid creating duplicates. -->
<!-- 🧪 Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- 💡 Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- 🔧 Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No, copilot issue

<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: Version: 1.79.1 - Commit: 4cb974a7aed77a74c7813bdccd99ee0d04901215
- OS Version: macOS Ventura 13.4

Steps to Reproduce:

1. Create two cursors
2. Start typing until Copilot suggests something.
3. Press ESC.

Desired behavior:

Suggestion goes away *and* my cursors reset to one cursor.

Actual behavior:

Suggestion goes away I still have multiple cursors.


Details:

While the multi-cursor example will show you the behavior without any extensions besides Copilot, the real issue here is when using `escape` as a keybind for anything. If the Copilot suggestion is shown, the `escape` keybind isn't populated down to the rest of the features that want it. This is a huge issue for VSCodeVim.

I tried looking for a way to rebind the ""cancel copilot suggestion"" action, but couldn't find a relevant action that worked. I was certain it was going to be `editor.action.inlineSuggest.hide`, setting the following in my keybinds did not unbind `escape` from hiding Copilot suggestions:

```
    {
        ""key"": ""escape"",
        ""command"": ""-editor.action.inlineSuggest.hide"",
        ""when"": ""inlineSuggestionVisible""
    },
```

I have no data to back this up, but I'm fairly certain this is new-ish behavior because my vim muscle memory for hitting the ESC key has me stumbling all over the place recently as I've been having to hit ESC ESC to go back to Normal mode.

"
microsoft/vscode,2023-06-15 11:55:58,question,"Can't launch VS Code from Linux terminal: symbol lookup error: ...,undefined symbol: gbm_bo_map","
Type: <b>Bug</b>

1.  ~$ : code
2.   symbol lookup error: /usr/share/code-insiders/bin/../code-insiders: undefined symbol: gbm_bo_map

VS Code version: Code - Insiders 1.80.0-insider (6545c4c2671127c323182225963fcd732e1fbcc5, 2023-06-15T05:33:30.720Z)
OS version: Windows_NT x64 10.0.19045
Modes:


<!-- generated by issue reporter -->"
microsoft/vscode,2023-06-14 14:15:37,question,Opening a file replaces another open file,"Type: <b>Bug</b>

Open a text file (source code, .txt, .sh), then open another.  I expect to have two files open in different tabs.  Sometimes, only the second is open.  It has replaced the first. I have not found a pattern for when this happens, except that the disappearing file is never dirty.


VS Code version: Code 1.79.1 (4cb974a7aed77a74c7813bdccd99ee0d04901215, 2023-06-12T16:14:05.102Z)
OS version: Windows_NT x64 10.0.22621
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i5-1235U (12 x 2496)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled|
|Load (avg)|undefined|
|Memory (System)|15.69GB (6.40GB free)|
|Process Argv|--crash-reporter-id 1858a141-bb01-42e0-bcd1-70cad9e543e3|
|Screen Reader|no|
|VM|50%|
</details><details><summary>Extensions (12)</summary>

Extension|Author (truncated)|Version
---|---|---
git-graph|mhu|1.30.0
python|ms-|2023.10.1
vscode-pylance|ms-|2023.6.10
jupyter|ms-|2023.5.1001582324
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.15
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
cmake-tools|ms-|1.14.33
cpptools|ms-|1.15.4
cpptools-extension-pack|ms-|1.3.0
cmake|twx|0.0.17

(1 theme extensions excluded)

</details>
<!-- generated by issue reporter -->"
bitcoin/bitcoin,2023-09-10 04:53:51,feature,#F,#
bitcoin/bitcoin,2023-09-01 22:33:04,feature,Additional Bitcoin-Qt capability to mine regtest blocks,"### Please describe the feature you'd like to see added.

Able to mine blocks through the Bitcoin-Qt while in Regtest mode.

### Is your feature related to a problem, if so please describe it.

Feature addresses none ideal uX with Bitcoin-Qt and mining regtest blocks. Currently if you want to use the console of Bitcoin-Qt you would need to close this out and then use `bitcoin-cli` separately to mine blocks.

When running Bitcoin-Qt in regtest mode it's clear by seeing a blue Bitcoin logo instead of the orange one.
<img width=""75"" alt=""Screenshot 2023-09-01 at 6 23 34 PM"" src=""https://github.com/bitcoin/bitcoin/assets/7761473/d792ef8c-9078-4132-ab19-71af9ac6bf18"">
However, there is no way to thoroughly test some capabilities without having the ability to mine blocks. 

### Describe the solution you'd like

Bitcoin-Qt is able to use `generate` or `-generate` flag similar to what `bitcoin-cli` can do.

Currently if you run `generate` the response is:
```
has been replaced by the -generate cli option. Refer to -help for more information.
 (code -1)
```

### Describe any alternatives you've considered

Having bitcoin-cli shipped with all binaries on bitcoincore.org.

### Please leave any additional context

low priority"
bitcoin/bitcoin,2023-08-11 01:45:11,feature,easy loading custom blockchains,"### Please describe the feature you'd like to see added.

running custom coins(genesis)/configs could be easier to maintain if the source code was built for it rather than forking from it with no idea how to update the forks.

### Is your feature related to a problem, if so please describe it.

once forked and modified its hard to maintain (stay uptodate) to existing bitcoin source tree automatically

### Describe the solution you'd like

put custom coin genesis loading/generation into the main branch so its easier to maintain coins.

### Describe any alternatives you've considered

_No response_

### Please leave any additional context

_No response_"
bitcoin/bitcoin,2023-06-27 11:43:19,feature,An option for a shell command that runs just before bitcoind completes shutting down.,"### Please describe the feature you'd like to see added.

It basically works similar to the blocknotify config option which runs a shell script when a block is mined, but the purpose of this new option is different.

It can be called something like ""shutdowncomplete"" and a shell command is passed to it as an argument, which then runs just before the following message is printed in debug.log:

```
2023-06-26T14:00:07Z Shutdown: done
```

It can be ran utilizing a function such as `runCommand`.

The reason such an option will be useful is to support automatic updates of Bitcoin Core to newer version. Some 3rd-party program used for detecting new Bitcoin Core releases on bitcoincore.org, downloading/verifying them, installing them to /usr/local or wherever the user-specified path is or alternatively running an installer, needs to be able to stop Bitcoin Core first and know when the process is about to quit so that it can proceed with installing (following which it simply runs the new version of Bitcoin daemon and exits).

Currently this can be done by detecting the bitcoind process, but it's not portable across multiple operating systems, which all have wildly different Process APIs.

### Is your feature related to a problem, if so please describe it.

The problem is: How can Bitcoin Core make it easier to create external programs for automatically updating installations to the newest version?

### Describe the solution you'd like

As specified above, a command-line & config option for running a shell script right before shutdown is the solution for facilitating automatic updates of bitcoin core (similar to stuff like Livepatch and k-splice for the linux kernel).

### Describe any alternatives you've considered

As noted, the bitcoind process can be detected in the running process lists, but this can cause a deadlock inside applications if Bitcoin Core takes a long time to shut down (eg. a network thread takes too long to exit).

### Please leave any additional context

None."
bitcoin/bitcoin,2023-06-23 08:52:50,feature,Optimization: utilize 100% resources of a computer in long operations like rescan/initial block download,"### Please describe the feature you'd like to see added.

As a user, I want Bitcoin Core to utilize maximum resources (CPU, network bandwith, memory) so that intensive operations are perfomed faster (take shorter time), e.g. rescan after importing a key to a wallet opened in a pruned node.

### Is your feature related to a problem, if so please describe it.

There is the problem that intensive operations take long while computer resources are not utilized in full:
- CPU utilization never exceeds 30%,
- bandwith utilization rarely exceed a few MB (approx. 10-20% of capacity, wired),
- RAM < 50%,
- disk (SSD) rarely exceeds a few %.

The computer specification:
[CPU]
11th Gen Intel(R) Core(TM) i5-11400H @ 2.70GHz
Base speed:	2.69 GHz
Sockets:	1
Cores:	6
Logical processors:	12
Virtualization:	Enabled
L1 cache:	480 KB
L2 cache:	7.5 MB
L3 cache:	12.0 MB
[RAM]
16.0 GB
Speed:	3200 MHz
Slots used:	2 of 2
Form factor:	SODIMM
Hardware reserved:	280 MB
[SSD]
NVMe Micron 2300 NVMe 512GB
Capacity:	477 GB
Formatted:	477 GB
System disk:	Yes
Page file:	Yes
Type:	SSD
[Connectivity]
1Gb Ethernet, 600 Mbit Internet bandwith.
[Software]
Windows 11, offical build of Bitcoin Core 25.0 (altough the same concerns the older versions), default configuration (e.g. block storage pruned to 2GB).

### Describe the solution you'd like

_No response_

### Describe any alternatives you've considered

_No response_

### Please leave any additional context

There is 100% CPU utilization when Prime95 stress test is run on the machine."
bitcoin/bitcoin,2023-06-09 23:18:21,feature,Bitcoin's relay fee refactoring,"### Please describe the feature you'd like to see added.

It would be great to refactor the Bitcoin relay fee in a way that would benefit full nodes operators

### Is your feature related to a problem, if so please describe it.

Since BTC is getting bigger and bigger, the incentive to run a full bitcoin node is just ridiculous.
Data storage cost a lot, and laptop manufacturers doesn't give a damn about it.

### Describe the solution you'd like

It would be nice to reward full nodes with a few satoshis, 5k sat would be a good start.

### Describe any alternatives you've considered

A good alternative would be to take parts of the miner's fees and spread them across the nodes who relayed transactions.

### Please leave any additional context

I think there is a way to make it fair for everyone, and i know this was discussed for a while, but it would be good to have an update about this anyways.

Maybe there is something to be done through lightning ???

I would love to run a full BTC node with txindex and all, but god damn it it is too expencive !"
bitcoin/bitcoin,2023-06-02 18:48:23,feature,depriortisetransaction,"### Please describe the feature you'd like to see added.

Let me know if this is a wanted change and I can work on it but,

If a user had already prioritized a transaction to be mined there would be no way to get it deprioritized until after this PR
https://github.com/bitcoin/bitcoin/pull/27501 which adds a new RPC call to getpriotisationmap. There should be a way to remove the prioritization entirely and set it to zero without calling two separate RPCs.

### Is your feature related to a problem, if so please describe it.

If a miner wants to deprioritize a transaction then they need to call two RPC methods instead of just one which would simplify things

### Describe the solution you'd like

Add new RPC deprioritisetransaction(txid) this will set the delta to 0 and remove the txid from the delta map


### Describe any alternatives you've considered

Change prioritisetransaction(txid, 0, fee_delta=0) (may cause breakage)
where if fee_delta=0 then we set the delta to zero instead of modifying it 0

or

Add new param prioritisetransaction(txid, 0, fee_delta(optional), hardset_delta(optional))
Where now fee_delta and hardset_delta are both optional but at least one is needed

### Please leave any additional context

This idea mentioned here towards the end in PR review club
https://bitcoincore.reviews/27501

Related PR
https://github.com/bitcoin/bitcoin/pull/27501"
bitcoin/bitcoin,2023-05-12 11:47:38,feature,Script verification being run when rebuilding UTXO database.,"It seems that script verification is being run during the rebuilding of the UTXO database, which, if the intention is simply to rebuild the UTXO (due to a disk corruption) then this ought to be unnecessary given the verification has already occurred.

GPT-4 suggested a simple code-change, although it's a little more complex than this as it needs to ensure that the script verification is only skipped on blocks we can be sure have previously been checked.

GPT-4's suggestion nevertheless: https://shareg.pt/qDkfAkx (I love how it's familiar with the project's code though!)"
bitcoin/bitcoin,2023-05-07 20:04:05,feature,provide optional RPC parameter to not scramble sendmany transactions,"### Please describe the feature you'd like to see added.

passed bitcoin core 16.X  the order of sendmany transactions is scrambled via RPC command forcing projects that utilizing the order of transactions to use older wallets.  

### Is your feature related to a problem, if so please describe it.

_No response_

### Describe the solution you'd like

add an optional parameter to the sendmany RPC command that allows for the sendmany transaction to work as it did before.   giving the option to scramble or not to scramble the order of a sendmany transaction back to bitcoin users.

### Describe any alternatives you've considered

_No response_

### Please leave any additional context

_No response_"
bitcoin/bitcoin,2023-04-09 12:08:01,feature,Add parallelism for downloading the blockchain,"Please add parallelism (at least, multithreading; multithreading + some kind of clustering are better) for downloading blockchain by client/wallet (required by `getblocktemplate`).
**The speed of downloading the blockchain of Bitcoin is very slow, so much time is required, it's absolutely awful. And the further, the worse.**
As I can understand, it happens because of nature of blockchain processing: first, client must check block N contents (transactions), then it can proceed to check of block N+1.
But we can behave like modern CPUs branch predictors. Why not? Just pretend block N is correct for thread M and let thread M+1 check block N+1. Then let thread M+1 pass necessary information to thread M+2 for checking block N+2. And so on. So all threads can check it's blocks, doing CPUs utilization. Threads can be reused as they become free. Just break processing at some points and do rollback, if some previous blocks (N, N+1, ...) gets corrupted.
As far I can see by CPUs usage, Bitcoin Core/Wallet does not behave like that. My Internet connection is not such slow and unstable; I once experimented with connection limit options, it didn't work for me.
The behavior I suggest has some performance risk, so it can be explicitly controlled by the user with launch option[s] (command line, ...).
<details>
<summary>A little spoiler ;)</summary>
It wouldn't be a problem at all, if the user could just download the blockchain (full or pruned) from some popular (so popular to be found) websites he trust.

(I aware about the security considerations, but this is not a issue if the user just wants to try mining or something.)
</details>"
bitcoin/bitcoin,2023-03-05 18:07:28,feature,Full Support for Spending Untrusted Unconfirmed Outputs,"Spending untrusted unconfirmed outputs should be supported and robust.

Key feature:

1. Calculation of the appropriate fee based upon the transaction package.
2. When a dependent transaction is modified, (for example RBF increase), the dependent transaction should be updated appropriately. Users can optionally automate this process, and keep the appropriate signing keys online until the transaction package confirms (as this involves decreasing the transaction fee as the dependent transaction has been replaced with a larger fee version).
3. When a conflicting unconfirmed-transaction package gets a higher fee, the user should be able to compare the transactions, and provided an option to increase the fee, change the funding inputs, or abandon the transaction all-together.
4. When the unconfirmed input goes out-of-scope (a conflicting transaction is confirmed in it's place), the dependent transactions should become ""unfunded"" and the user will need to select new inputs to fund the transaction, or abandon it.

New concepts:
Unfunded, Partiality, and Fully Funded Transactions, and supporting the lifecycle between these states.

Related Issues:
[policy: allow RBF descendant carveout whenever conflicts exist, #16819 ](https://github.com/bitcoin/bitcoin/issues/16819)
[Enable CPFP via GUI #242 ](https://github.com/bitcoin-core/gui/issues/242)
[Coin Controll for Unconfirmed Outputs #27190](https://github.com/bitcoin/bitcoin/issues/27190)"
bitcoin/bitcoin,2023-02-27 06:57:14,feature,Bitcoin core full node with S3 bucket,"**Is your feature request related to a problem? Please describe.**
I am trying to run a bitcoin full node on a aws ec2 instance. The problem is when allocating ssd, the cost is too much. The cost for S3 storage is really low. Since bitcoin total block chain size is increasing every day it is better if we can use these kind of services. As I understand frequency for a full node to get the old data is relatively low. 

**Describe the solution you'd like**
I suggest, we should be able to configure how we store the blocks. One possible way is storing the data and fetching the data using REST calls. In this approach bitcoin core will not worry about how blocks are stored and where. Whenever it gets a new block it will invoke a POST http call to the confgured url and save it. Whenever it needs a old block data it will do a GET call to the server using the configured url. This way individuals can develop their own version of servers that will use different type of storage services. The REST approach is just a suggestion we can implement anything that let us configure to use different type of storage mechanisms. 

One of the main argument against bitcoin is: Because of the growing blockchain size it will become impossible to run a full node by an average individual. With this kind of feature we can use very cost effective solutions for data storage. 

"
bitcoin/bitcoin,2023-02-07 11:55:29,feature,Add configuration option that will allow for setting the upper bound on transaction size for relaying,"The recent events, where people store big data on the blockchain in the witness (like images) showed that the dispute about this topic was not settled during OP_RETURN discussion. Some people like the idea that every payed transaction is not a spam, some don't.   

I suggest that users could set the transaction size limit in the configuration file that will make node drop transactions from the memory pool that are larger. This way node operators would decide what kind of transactions gets relayed via their node.
 
Alternatively this limit could be on the consensus layer, but I don't think it is a good idea.

If I would get some help, I can try to implement this feature (no guarantee of success, because i don't know the Bitcoin Core codebase yet, however I have some experience in programming)."
bitcoin/bitcoin,2023-02-05 18:11:47,feature,Summary of Bitcoin Core Improvements,"Scalability: Bitcoin's current architecture has scalability limitations, and efforts should be made to improve its ability to process a large number of transactions per second.

Privacy: While bitcoin is pseudonymic, it can still be traceable. Improving privacy could involve implementing privacy-enhancing technologies like Coinjoin or zero-knowledge proofs.

Interoperability: Making it easier for different bitcoin-based systems to interact with each other could help to foster innovation and adoption.

Security: The code should be continuously audited and improved to prevent potential security vulnerabilities.

Usability: Improving the user experience could involve making the software easier to use, speeding up confirmation times, or simplifying the process of setting up a full node.
"
bitcoin/bitcoin,2023-01-19 10:23:40,feature,ci: Use containerfiles?,"containerfiles have the advantage that they can be cached locally, so for example an `apt` operation in an image layer is faster to retrieve from the (local) cache than to run vanilla.

Converting the ci system to those is non-trivial, because it uses a lot of env vars, different configs, and numerous per-config hacks.

My understanding of docker is limited, but to pass env vars into a containerfile would require using `ARG` in the file, as well as code to pass in the ARG at runtime via the command line. For hundreds of args, this should be possible, but verbose, which is why I haven't looked at it yet."
bitcoin/bitcoin,2023-01-11 17:55:14,feature,.,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->"
bitcoin/bitcoin,2022-12-08 20:07:25,feature,Bring back the zap (zaptransaction RPC),"It can be useful to completely remove a transaction from a wallet rather than just abandon it. See e.g. #26667.

The original `-zapwallettxes` startup option was removed in #19671. It was perhaps overkill, but a simple `zaptransaction` RPC would be nice."
bitcoin/bitcoin,2022-11-21 17:22:47,feature,Keep seed phrase/mnemonic on cold encrypted,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
It is not safe to keep the seed phrase or private keys on the computer or smartphones. 

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
Need to have an option to user save the seed phrase or private keys encripted on NFC Card or USB device, like a cold wallet integrated with Bitcoin core.

So every time user wants to withdraw Bitcoin, need to have the USB ""pendrive"" connected on PC or tap the NFC Card.

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
I have checked and there is another wallets using this solution.

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
I already have this working without integration with Bitcoin Core."
bitcoin/bitcoin,2022-10-07 12:59:12,feature,Creat code example with python to send bitcoin (bc1 - P2WPKH / Bech32),"Hi! I'm looking for someone who could make me a python code example with which I can send bitcoin with python from a ""bc1"" address to another ""bc1"" address. (bc1 = P2WPKH / Bech32 e.g. address example (last 3 characters removed for safety): bc1qm9lr3s4w7tv67v9z92af46qkzpw9sayler7***)"
bitcoin/bitcoin,2022-09-17 02:29:39,feature,no surprise rescan for importprivkey newbie,"**Is your feature request related to a problem? Please describe.**
Calling importprivkey(1) triggers rescan. Rescan of long chains using bad and old equipment takes many days.

**Describe the solution you'd like**
Reject importprivkey(1) completely. Deprecate in release i, disable in release i+1 (prompting user to use importprivkey(2) or importprivkey(3)), remove in release i+2. 

**Describe alternatives you've considered**
Make the program so it knows that equipment is old and chain is long, then estimate rescan is large, if so rejecting importprivkey(1) unless confirmed YES when prompted.

**Additional context**
TBA.
"
bitcoin/bitcoin,2022-08-21 01:30:20,feature,It is possible Bitcoin core be rewarded?,"Theoretically, a reward for running a full node can be useful to the network.
Possibly more people will join efforts and the the network will become stronger.
Run a full node 24 hours and 7 days per week, can be expensive (price of electricity, internet and computer available).
Why not reward the fullnoders whit Sats?"
bitcoin/bitcoin,2022-07-26 20:00:18,feature,[brainstorm] policies for default `minrelaytxfee` as experiment for a release ,"**Is your feature request related to a problem? Please describe.**

Some users have been curious about a lower minimum fee rate. A recent thread on mailing list:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-July/020784.html

**Describe the solution you'd like**

A new config option that changes the default minrelaytxfee to one of these for a minor release after v24.0:

1. Odd/Even: 0.001 sat/vB on odd dates and 1 sat/vB on even dates.
2. Market: 0.001 sat/vB during US market open hours else 1 sat/vB.
3. Interval: Starts with 0.001 sat/vB and keep changing after defined time interval

**Describe alternatives you've considered**

If enough users and miners agree to experiment this could be done without making changes in Bitcoin Core. Since most of the nodes use default it's difficult to experiment and know if one of these policies increase revenue for miners, provides better fee rates etc.


---

A related tweet thread: https://twitter.com/murchandamus/status/1480553169239355393"
bitcoin/bitcoin,2022-07-13 15:02:11,feature,Miniscript support for decodescript,"Current behavior:

```
src/bitcoin-cli -signet decodescript 2103d3b9c8cc852f3b4a06bad1711fd1a1761a592bf1475edc0a5f97b31759eba330ac736476a914b6e87320e876a740fc1365aeb1f4b6ab2fe4210b88ad53b268
```
```json
{
  ""asm"": ""03d3b9c8cc852f3b4a06bad1711fd1a1761a592bf1475edc0a5f97b31759eba330 OP_CHECKSIG OP_IFDUP OP_NOTIF OP_DUP OP_HASH160 b6e87320e876a740fc1365aeb1f4b6ab2fe4210b OP_EQUALVERIFY OP_CHECKSIGVERIFY 3 OP_CHECKSEQUENCEVERIFY OP_ENDIF"",
  ""desc"": ""raw(2103d3b9c8cc852f3b4a06bad1711fd1a1761a592bf1475edc0a5f97b31759eba330ac736476a914b6e87320e876a740fc1365aeb1f4b6ab2fe4210b88ad53b268)#uul8nd2t"",
  ""type"": ""nonstandard"",
  ""p2sh"": ""2N62ryy2gBC2QBSdsm832Tv6qCW5py53wJn"",
  ""segwit"": {
    ""asm"": ""0 43c1d0387e91045591f19ff6d60af2bc0d91473b79dc3238aa9d0430f89741bc"",
    ""desc"": ""addr(tb1qg0qaqwr7jyz9ty03nlmdvzhjhsxez3em08wryw92n5zrp7yhgx7qj56vra)#ugngfnm5"",
    ""hex"": ""002043c1d0387e91045591f19ff6d60af2bc0d91473b79dc3238aa9d0430f89741bc"",
    ""address"": ""tb1qg0qaqwr7jyz9ty03nlmdvzhjhsxez3em08wryw92n5zrp7yhgx7qj56vra"",
    ""type"": ""witness_v0_scripthash"",
    ""p2sh-segwit"": ""2N9diLEHbASsz1oAeDQJPg5mb4LaAju9175""
  }
}
```

It would be nice if this (also) returned miniscript.

Perhaps `getaddressinfo` could do this as well (after #24148).

cc @sipa, @darosior "
bitcoin/bitcoin,2022-06-15 08:54:03,feature,"benchmarks: ""skip slow benchmarks"" option?","Often I'd like to do some quick benchmarks on a slower platform (e.g. RISC-V, ARM32). Various `Wallet*` benchmarks are extremely slow even on state of the art hardware. I think an option to skip slow benchmarks would be useful.

It might be possible with a `-filter=` expression, I'm not sure."
bitcoin/bitcoin,2022-05-31 21:08:24,feature,Parallel compact block download,"There have been a couple of old attempts to enable parallel compact block downloads, see #9447 and #10984. There are additional notes in #16172.

The idea is that we currently might request missing transactions for a new block from the first peer to announce the block to us; but if that peer is a miner (or close to a miner), that peer might be busy (eg it might be in ConnectTip), and not able to reply quickly, while some other node has meanwhile obtained the block and is able to quickly give you the few transactions you're missing.

Therefore, at some point we should pick up the patches in those earlier PRs, or otherwise solve this :)"
bitcoin/bitcoin,2022-04-25 15:04:18,feature,getrpcinfo should provide version number,"**Is your feature request related to a problem? Please describe.**

RPC details change, like fields in responses & I want to write robust code that can handle that. 

**Describe the solution you'd like**
`getrpcinfo` or a new RPC call should provide the version of the client as a string such as ""22.0"" or similar.

**Describe alternatives you've considered**
Would need to fingerprint a few responses and apply heuristics to determine what we are talking to otherwise.

"
bitcoin/bitcoin,2022-04-07 17:22:10,feature,Add an option to create taproot descriptor for old descriptor wallets,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
New wallets created with Bitcoin Core  v 23.0 RC3 have by default descriptors=true and taproot descriptor is generated.

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
Using a wallet created with descriptors=true with the precedent version 22.0 does not allow me to create receiving Bech32taproot address (which is normal, since by that time ""desc"": ""tr was not present).

I've tried to upgrade wallet with the command ""upgradewallet"" but it is already at latest version:
{
  ""wallet_name"": ""wallet_22.0"",
  ""previous_version"": 169900,
  ""current_version"": 169900,
  ""result"": ""Already at latest version. Wallet version unchanged.""
}

 I think it would be great a way to upgrade/add taproot descriptors to the wallet instead of having to create a new one."
bitcoin/bitcoin,2022-04-04 13:30:08,feature,Python tests: use black and isort for formatting,"There doesn't seem to be an agreed upon code formatter for the files in ```tests/```. 

Some widely used formatters are [```black```](https://github.com/psf/black), which does general code formatting, and [```isort```](https://github.com/PyCQA/isort), which sorts the imports. Example usage:

```
black .
isort . --profile black
```

This could reduce time spent reviewing and discussing code style."
bitcoin/bitcoin,2022-03-24 02:45:10,feature,util/check.h Assert/Assume: namespacing issues,"The `Assert`/`Assume` macros are implemented via lambda functions to allow the result of the assertion to both be evaluated (and trigger an abort) and be returned without having the expression be evaluated twice.

This causes some ugly namespacing issues though. Because there's a function call, clang's thread safety annotations don't get passed through (as the lambda is unannotated), possibly causing an unnecessary compiler error because the compiler loses track that a mutex is held when a guarded variable is accessed.

It also seems that gcc (but not clang) gets confused about member functions (but not member variables), eg:

```c++
class TestAssert
{
public:
    int variable = 3;
    int test_1(void) { return variable; }
    int test_2(void) {
        auto x = [&]() {
            Assert(test_1() == 3);
        };
        x();
        return ++variable;
    }
};
```

results in:

```
test/util_tests.cpp:91:26: error: cannot call member function ‘int util_tests::TestAssert::test_1()’ without object
   91 |             Assert(test_1() == 3);
```

requiring you to write `Assert(this->test_1() == 3)` instead."
bitcoin/bitcoin,2022-02-22 19:30:45,feature,[brainstorm] Improving `makeseeds.py`,"
A. Filtering hosts with multiple ports can be removed IMO:

https://github.com/bitcoin/bitcoin/blob/c44e734dca64a15fae92255a5d848c04adaad2fa/contrib/seeds/makeseeds.py#L215


B. Tor v3 can also be included in the results.

C. Recent observation which can be confirmed with:

```
wget https://gitlab.com/api/v4/projects/33695681/packages/generic/nrich/0.1.1/nrich_0.1.1_amd64.deb
sudo dpkg -i nrich_0.1.1_amd64.deb
host -t a seed.bitcoin.sipa.be | sed -e 's/seed.bitcoin.sipa.be has address //g' | nrich -
```

Possible reasons for vulnerable machines used for bitcoin nodes:

1. False positives
2. Users not aware or don't care
3. Attackers prefer using these for better results
4. Honeypots
5. Other reasons

Leaving 1 which won't be true for all the results, filtering such nodes in `makeseeds.py` should make sense. Below is an example for one IP copied from [`suspicious_hosts.txt`](https://github.com/bitcoin/bitcoin/blob/master/contrib/seeds/suspicious_hosts.txt)


```python
ip = '88.198.17.7'

url = 'https://internetdb.shodan.io/' + ip
response = requests.get(url)

if response.text.find('CVE') != -1:
    print('vulnerable')
```"
bitcoin/bitcoin,2022-02-15 02:04:10,feature,guix-attest should support custom GPG executable names,"**Is your feature request related to a problem? Please describe.**

In Qubes OS, the ""Split-GPG"" feature allows keeping the private key in a separate VM from the application (in this case guix-attest).  This prevents a compromised VM (in which Bitcoin Core was built) from stealing the private signing key.  Qubes provides a `qubes-gpg-client-wrapper` executable that has the same API as `gpg`.  Unfortunately, there is currently no way to make `guix-attest` use that executable instead of plain `gpg`.

**Describe the solution you'd like**

Support an optional environment variable in `guix-attest`, which allows specifying an arbitrary command name that replaces `gpg`.

**Describe alternatives you've considered**

I considered a command-line parameter, but it seems that environment variables are the convention in `guix-attest`.

**Additional context**

I believe OpenTimestamps provides a wrapper with `gpg`'s API as well, so maybe this would also be helpful for facilitating OpenTimestamps with Guix.
"
bitcoin/bitcoin,2022-02-07 15:00:03,feature,pruneblockchain should be able to increase the size of pruned blockchain,"Is it possible to update `pruneblockchain` command so it can also increase the size of pruned blockchain?

It seems to me that now it is only able to decrease it:

```
bitcoin@server:~$ bitcoin-cli pruneblockchain 650000
error code: -8
error message:
Blockchain is shorter than the attempted prune height.
```


"
bitcoin/bitcoin,2022-02-01 15:39:49,feature,Sync pruned blockchain,"**Is your feature request related to a problem? Please describe.**
Syncing Bitcoin can take tens or hundreds of hours. This is annoying especially if you are a developer and want to use BitcoinCore's RPC API.

**Describe the solution you'd like**
Adding a ""sync pruned blocks"" feature would be cool. It would for example allow you to only download the part of the blockchain you actually need. The cryptocurrency Monero has already got this feature.
"
bitcoin/bitcoin,2022-01-27 04:13:26,feature,Moderation required for BitcoinCore in the Microsoft winget package repository,"BitcoinCore is showing up in the [winget package repo](
https://github.com/microsoft/winget-pkgs/tree/master/manifests/b/BitcoinCoreProject
).  It's great to see the packages there, but you might want to take over the PR submission for your ORG.  The packages there seem legit, but technically, anyone could contribute any package there and call it ""BitcoinCore"".  Your dev team may want to take over these submissions, or request you appear on the PR approval list for your ORG.

* [BitcoinCore Products on Microsoft Repo](https://github.com/microsoft/winget-pkgs/search?q=bitcoincoreproject&type=issues)
* [Package Contributing Guidelines](https://github.com/microsoft/winget-pkgs#contributing)
* [Open Discussion on Microsoft Moderation Practices](https://github.com/microsoft/winget-pkgs/discussions/15607)"
bitcoin/bitcoin,2022-01-14 14:41:37,feature,doc: add a brief description to each namespace,"**Is your feature request related to a problem? Please describe.**

Many namespaces are as yet un-annotated with a brief description, as used by Doxygen to generate developer documentation, locally or at https://doxygen.bitcoincore.org/namespaces.html.

<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

Incomplete developer documentation may affect developer onboarding time, and this seems like low-hanging fruit.

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

To add annotations to each of these namespaces. There may be existing descriptions elsewhere, if not I or someone else could attempt to describe each namespace, and reviewers could suggest improvements or alternatives.

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

Ideally each description would be committed by its author, so perhaps handling this with one PR that is then squashed isn't the best solution.

On the other hand, if this would ease developer onboarding perhaps that's more important.

**Additional context**

Documentation at time of opening 14/12/2022:

![doxygen-namespaces-20211214](https://user-images.githubusercontent.com/97605837/149532363-fc8e7c16-a253-417b-92bd-5a3b8419a2af.png)

"
bitcoin/bitcoin,2021-12-27 14:43:43,feature,allow duplicate output addresses `createrawtransaction`,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
`Invalid parameter, duplicated address: tb1qcum5rgwvzmtfvtwk80y4zp7mv5mgg384ygfsn3`

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
remove the duplicate address check or add an optional argument to remove the check

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
hex editing

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
[40ac7fcef6a918e0c073f74cf79bb97b043f311e9f3b6e3d5067b0e6f2b6be5a](https://mempool.space/testnet/tx/40ac7fcef6a918e0c073f74cf79bb97b043f311e9f3b6e3d5067b0e6f2b6be5a)

"
bitcoin/bitcoin,2021-12-21 13:02:35,feature,[meta] Reworking the merge commit format,"Currently merge commits include the list of commits, the description and a list of ACKs. (exmaple merge: 887796a5ffcbafcd281b920f8d55fcb6e8347584)

I think it could make sense to also include:

* a list of stale ACKs and potentially non-code-review ACKs or NACKs. Obviously this has the issue that any of them might be stale, but it might make it easier to get a general idea of the support/opposition on a pull request.
* signatures for ACKs. The benefit will be that signatures are less likely to be ""made up"" (for example by GitHub or by someone who compromised a GitHub account). The problem is that practically no one signs ACKs and that signature formats are too verbose to include verbatim in the merge commit message. If there was a better way to include signatures, we could move toward a scheme where several reviewers contribute toward the merge commit similar to how the guix attestations are produced. "
bitcoin/bitcoin,2021-12-15 17:11:34,feature,wallet: Sweep function and deprecating subtractFeeFromOutputs,"The subtractFeeFromOutputs function has long been the cause of many issues in coin selection. It is unclear whether it is being used for things other than sweeping the entire wallet balance, and sweeping specific inputs (i.e. spending inputs without creating change and without figuring out the feerate manually). Many issues opened about subtractFeeFromOutputs indicate that users are using it for sweeping, and the linked issue in the PR that adds it also discusses sweeping as the intended feature. Instead of using subtractFeeFromOutputs in order to implement this feature, it makes more sense to me to add specific functionality for sweeping. This would allow us to de-complicate our coin selection code by removing subtractFeeFromOutputs.

***

There are a few specific things that I would want to see in a sweep function, besides the obvious spending all UTXOs with one destination.

It is possible that users will want to sweep to multiple destinations, so the sweep function should optionally allow users to specify amounts with addresses. If no amount is provided for an address, then the remaining value being swept will be sent to that address. Furthermore, multiple addresses may have no amount specified, in which case the remainder is split equally among the addresses, much in the same way that subtractFeeFromOutputs currently distributes the fee among the outputs.

Additionally, sweeping may be used to sweep only specific inputs. So this sweep function should be able to allow users to specify which inputs to use. All specified inputs should be spent. If no inputs are provided, then the entire wallet will be spent.

***

There are a few ways this could be implemented. The obvious is a new RPC with an API similar to `send` so that all of the relevant options can be provided. This would allow for a psbt to be returned in the case that the wallet does not have private keys, or if the user requests it. Another way would be to add it as an option to `send`, however I don't think that the API for using it would be easy to understand. For the GUI, there should be a dedicated button.

Internally, it should be completely separate from existing coin selection (i.e. not in `CreateTransaction`, `SelectCoins`, or `AttemptSelection`).

Lastly, when sweep is implemented, subtractFeeFromOutputs should be added to `-deprecatedrpc` and the option removed from the GUI. Users should be informed that they can use the sweep function if that is what they are doing. Otherwise they can open an issue to discuss their use case. After one (or two) major release with `-deprecatedrpc` and no users complaining about a use case for subtractFeeFromOutputs that was not sweeping, then the subtractFeeFromOutputs option can be removed and the code for handling it removed from coin selection."
bitcoin/bitcoin,2021-12-15 07:24:39,feature,Wallet rescan multiple wallets,Why does rescanblockchain not scan many wallets the same time? It can only scan one (inefficient when requiring to scan multiple wallets).
bitcoin/bitcoin,2021-11-28 21:36:36,feature,"Password for encrypted wallets on all pw protected functions (such as sendtoaddress, dumpprivkey, etc)","**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
I want this to be VERY clear,  this is not from an experience of being hacked (well, yet anyway,  knock on wood).  This is completely theoretical based on my experience creating a webapp that interacts with bitcoind via RPC calls and thus it got me to thinking about how the hotwallet could be hacked,  how to prevent it,  etc.

Unlocking the wallet for X amount of time is a MASSIVE security flaw imo.  Let's say you have 10 'withdraw' processes running and they all have to 'unlock' the wallet to processes their sends.  Thats fine if they finish quickly (this isnt always the case btw).  But what if you have 100s or 1000s or something just causes it to go slowly and you have to increase the unlock period for some reason?  Then what you've got is a wallet that is essentially in an ""unlocked"" stated almost constantly.  This is a really bad idea.  It would be better to ask for the pw on specific functions instead of a global scope.

Use case:
Server 1:  Apache webserver, maybe has other services on it like mysql, ssh, etc,  something that's vulnerable to be hacked
Server 2: Hotwallet server, has absolutely nothing on it other than bitcoind in RPC mode,  it's buttoned down as much as possible short of actually unplugging it from the router.

Server 1 has RPC creds on it so that some application (like a web app) can interact with a BTC wallet.  It does NOT have the BTC wallet encryption pw on it however.  So it's only means of sending out BTC or creating transactions is when Server 2 checks a queue that resides on server 1......... Months go by and one day the machine is HACKED.  The hacker then monitors the server and notices that they can make RPC calls to the wallet (oh crap).  But the wallet is encrypted so we're good right?!  HA.  WRONG.  They have access to the queue which seems to take a long time to empty.  Then they notice that the wallet is being unlocked consistently to empty the queue...... It this point it's game over for mr wallet.

Server 2 
**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
I'd like to see at minimum sendtoaddress, sendmany, sendrawtransaction, send, and createrawtransaction refuse to function if a pw on an encrypted wallet is detected.  Essentially anything that isnt a Read (except for something like dumping keys, obviously that should be as well).

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
There are alternatives but I'm not interested in ANY of them as they are overly convoluted, difficult to setup, and just not a good user experience as a developer at all,  all the while solving no security problems at all,  at some point on some device (whether connected to other devices or not) that unlock period is going to come into play and when it does, this vulnerability rears it's head.  I realize I can setup a 'read only' wallet (and I have),  but had bitcoind not had this issue,  I would not have had to do this for my purposes.  I can see other use cases involving cell phones as well.

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
"
bitcoin/bitcoin,2021-11-01 17:13:02,feature,Use notifications.dat for -*notify,"**Is your feature request related to a problem? Please describe.**

`-startupnotify` and `-shutdownnotify` configuration parameter accept shell commands to be executed after Bitcoin Core starts/shutdown

One is still WIP and not merged: https://github.com/bitcoin/bitcoin/pull/23395

This provides options for attackers to target new users of Bitcoin Core and one example which involves some social engineering is explained in https://github.com/bitcoin/bitcoin/pull/23395#issuecomment-956353035

**Describe the solution you'd like**

I am not sure how this is used currently by different users and projects however if reading notifications about start/shutdown is the goal maybe a file with name `notifications.dat` with below format can help:

```
lastshutdown=1635786273
lastrestart=SOMEUNIXTIME
currentstate=shutdown
```

Initially suggested this solution in https://github.com/bitcoin/bitcoin/pull/23395#issuecomment-955775728

**Describe alternatives you've considered**

Educate users about misuse of *notify options
"
bitcoin/bitcoin,2021-10-17 14:23:23,feature,tracing: tests for USDT tracepoints ,"#22006 added the first three USDT based tracepoints to Bitcoin Core. To provide a [semi-stable](https://github.com/bitcoin/bitcoin/blob/master/doc/tracing.md#semi-stable-api) tracepoint API the tracepoints need test coverage. The tracepoints can be tested in the functional tests using the Python wrapper of [BCC](https://github.com/iovisor/bcc). 

Before adding more tracepoints, the existing three tracepoints from #22006 should be tested.


Notes:
1. We currently only support the tracepoints on Linux. The tests should be skipped on other operating systems.
2. Hooking into the tracepoints via the Linux kernel requires special privileges. Since kernel version 5.8. (Aug. 2020) the `CAP_BPF` can be used. On older kernel version the overloaded catch-all capability `CAP_SYS_ADMIN` is required. Functional tests shouldn't require `CAP_SYS_ADMIN` as that essentially means running the test suite with `root` privileges.  
3. The tests require the BCC Python library. This should be an optional dependency. Tests should be skipped if the dependency isn't present. 

The `connect_block` tracepoint can be tested by mining blocks with transactions and checking that the tracepoint passes the correct data. The `net` `inbound_message` and `outbound_message` tracepoints can be tested by checking the traffic between two nodes. 
"
bitcoin/bitcoin,2021-10-11 20:37:25,feature,bitcoin-cli getaddressofwallet ,"sorry to ask for this feature 

i have created a regtest  network on ubuntu but  when i create a bitcoin wallet i can not get the address of that with bitcoin-cli 
please provide a method to get the current wallet address 


"
bitcoin/bitcoin,2021-10-05 13:39:37,feature,RPC: Allow user to supply weight of external input,"This would allow wallet operations like funding transactions with smart contract-based inputs such as LN, or any input the Core wallet doesn't know how to sign for.

https://github.com/bitcoin/bitcoin/pull/17211#issuecomment-933674310"
bitcoin/bitcoin,2021-10-03 02:00:38,feature,Coin selection algorithm proposal,"I’ve been working on a coin selection algorithm using Evolutionary Algorithm. I am creating this issue to discuss the possibilities related to it. If here is not the best place to discuss it, feel free to close this issue. 

If you don’t know what an Evolutionary Algorithm is, here is a good definition: ""EA is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators"". Anyway, I recommend you to study more about EA before trying to understand this proposal. 

In Evolutionary Algorithms we have genes, chromosomes, and population, for example:
![image](https://user-images.githubusercontent.com/19480819/135721326-4b8213c5-e9ad-4656-b6ef-f68dbbb5ddac.png)

So, to begin our algorithm, we must first create an initial population. The population will contain an arbitrary number of possible solutions to the problem, oftentimes called members. In this case, a gene is a UTXO, so, every chromosome is a set of UTXOs. 

```
member1 = [utxo1, utxo2, utxo3]
member2 = [uxto2, utxo5, utxo6, utxo7, utxo8]
...
```

To create the initial population, we can use the following approach (considering 5 members per population):

- 1 member composed of all UTXOs
- 1 member that selects randomly from the shuffled UTXOs until the target is exceeded
- 3 random members

Obs.: Most evolutionary algorithms set a length for the chromosome. However, for this approach, we don’t do it because we don't know how many UTXOs our final solution will use. 

Ok, having our initial population, it is time to evaluate each solution (fitness). To do it, we can use the waste metric, introduced in Bitcoin Core recently. So, our metric to evaluate the members is the cost of creating change, the excess selection amount, and the cost of spending inputs now as opposed to sometime in the future (when we expect to be able to consolidate inputs). See more: https://bitcoincore.reviews/22009

After evaluating each member, we can define what is the best one among them and build our next population (new generation).

Our new population will have (considering 5 members): 

- 3 members from mutation
- 1 member keeping the same chromosome of the best solution from the previous generation
- 1 new random member

Considering 50% mutation rate, we create 3 members copying the same chromosome of the best solution from the previous generation and applying mutation, like:

```
for (gene in chromosome) {
  const random_value = getRandomValueBetween0and1()
  if (random_value == 1) {
    gene = getRandomUtxo()
  }
}
```

Ok, now we have a new population, and then, we can repeat all the processes (fitness and mutation) N times (being N the number of generations), the best member of the last generation will be our final solution."
bitcoin/bitcoin,2021-09-23 15:18:00,feature,Full CJDNS support,"CJDNS overview
=====

CJDNS is like a distributed, shared VPN with multiple entry points where every participant can reach any other participant. All participants use addresses from the `fc00::/8` network (reserved IPv6 range). Installation and configuration is done outside of applications, similarly to VPN (either in the host/OS or on the network router).

Motivation
=====

Even without this PR it is possible to connect two Bitcoin Core nodes through CJDNS manually by using e.g. `-addnode` in environments where CJDNS is set up. However, this PR is necessary for address relay to work properly and automatic connections to be made to CJDNS peers. I.e. to make CJDNS a first class citizen network like IPv4, IPv6, Tor and I2P.

Considerations
=====

An address from the `fc00::/8` network, could mean two things:
1. Part of a local network, as defined in RFC 4193. Like `10.0.0.0/8`. Bitcoin Core could be running on a machine with such address and have peers with those (e.g. in a local network), but those addresses are not relayed to other peers because they are not globally routable on the internet.
2. Part of the CJDNS network. This is like Tor or I2P - if we have connectivity to that network then we could reach such peers and we do relay them to other peers.

So, Bitcoin Core needs to be able to tell which one is it when it encounters a bare `fc00::/8` address, e.g. from `-externalip=` or by looking up the machine's own addresses. Thus a new config option is introduced `-cjdnsreachable`:
* `-cjdnsreachable=0`: it is assumed a `fc00::/8` address is a private IPv6 (1.)
* `-cjdnsreachable=1`: it is assumed a `fc00::/8` address is a CJDNS one (2.)

After setting up CJDNS outside of Bitcoin Core, a node operator only needs to enable this option.
Addresses from P2P relay/gossip don't need that because they are properly tagged as IPv6 or as CJDNS.

For testing
=====
```
[fc32:17ea:e415:c3bf:9808:149d:b5a2:c9aa]:8333
[fc68:7026:cb27:b014:5910:e609:dcdb:22a2]:8333
[fcb3:dc50:e1ae:7998:7dc0:7fa6:4582:8e46]:8333
[fcc7:be49:ccd1:dc91:3125:f0da:457d:8ce]:8333
[fcf2:d9e:3a25:4eef:8f84:251b:1b4d:c596]:8333
```"
bitcoin/bitcoin,2021-09-22 12:26:41,feature,Allow UTXO locks to be written to wallet DB,"Addresses and closes #22368

As per that issue (and its predecessor #14907), there seems to be some interest in allowing unspent outputs to be locked persistently. This PR does so by adding a flag to lockunspent to store the change in the wallet database. Defaults to false, so there is no change in default behaviour.

Edit: GUI commit changes default behaviour. UTXOs locked/unlocked via the GUI are now persistent."
bitcoin/bitcoin,2021-09-10 22:41:05,feature,.,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
ORO COIN "
bitcoin/bitcoin,2021-09-02 21:15:48,feature,Add CBOR RPC interface,"This issue is a proposal to add a CBOR RPC interface to bitcoin-core. The interface would be in addition to the current JSON-RPC, not a replacement.

The main goal of the proposal is to offer an RPC interface that is more efficient than the current JSON-RPC. Remote clients with constraints on data usage will see the biggest advantage.

CBOR seems like the best choice for a minimal RPC implementation, since it has wide support in industry, and is the de-facto standard for IoT communication protocols like [CoAP](https://en.wikipedia.org/wiki/CoAP
). Meaning, there is likely to be a large community outside Bitcoin to get support/developers.

## Advantages

- RFC standard specification: [RFC 8949](https://datatracker.ietf.org/doc/html/rfc8949)
- CBOR uses binary representation
  - no need for Base64 or other encoding
  - well-defined data types with minimal encoding overhead
- Small amount of types
- Several existing free open-source libraries to fork
- Many existing implementations are small (~400-750 LoC)
- Decreased data transmission for remote clients

## Disadvantages

- Added code to the core implementation
  - larger attack surface
  - maintenance costs
- Added complexity
- Unclear data-size savings versus compressed JSON/Base64

## Alternative approaches

### External CBOR proxy

One possible alternative, suggested by @cfields, is to write a proxy translating CBOR to JSON.

The proxy could be stand-alone from bitcoin-core, thus removing the disadvantage of added code.

Also, the proxy could be written in a memory-safe language like Rust, decreasing the attack surface.

There would also be disadvantages to a proxy implementation:

- small overhead of translating CBOR to JSON, and passing to the original JSON-RPC
- installation of an additional piece of software

### Internal CBOR proxy

Another alternative is to add CBOR support to Univalue, and implement the proxy in bitcoin-core.

This would have the advantage of direct access to RPC internals, and potentially reduce overall code size.

The proxy would listen on a separate port, translate the incoming CBOR to JSON, and pass the JSON to existing RPC interfaces.

## Free open-source implementations

Here is a list of some of the better candidates for a bitcoin-core CBOR fork:

- cb0r: https://github.com/quartzjer/cb0r
  - zero-allocation C implementation
- cppbor: https://github.com/rantydave/cppbor
  - C++17 implementation based on `std::variant`
- cbor11: https://github.com/jakobvarmose/cbor11
  - C++11 implementation
- tinycbor: https://github.com/intel/tinycbor
  - Intel implementation, includes CBOR-JSON translation
  - ironically, largest LoC count of the candidates
- cbor-lite: https://bitbucket.org/isode/cbor-lite
  - C++14 implementation, header-only
  - used in [bc-ur](https://github.com/BlockchainCommons/bc-ur/blob/master/src/cbor-lite.hpp) 

## Comparing to JSON-RPC

@laanwj raised the point that compressed JSON may provide similar savings to CBOR. To test whether the savings from CBOR provides significant size reduction, I will implement a small number of RPC calls in CBOR,
and compare the uncompressed and compressed sizes against the current JSON encoding.
"
bitcoin/bitcoin,2021-08-16 10:29:42,feature,Protect a number of outbound tor connections from eviction,"**Is your feature request related to a problem? Please describe.**

Outbound tor connections lose against outbound clearnet connections after the eviction logic starts having an effect. Although I ""manually"" add outbound tor connections (via `bitcoin-cli`), after some time typically only 0 or 1 outbound tor connection survives. My node currently has 12 outbound connections, of which only 1 is a tor connection (Bitcoin Core version 0.21.1). The issue does not depend on which tor version is being used. The (good) outbound connections that are manually added from the start are the 3 Tor v3 onion addresses recommended [here](https://github.com/bitcoin-core/bitcoin-devwiki/wiki/22.0-Release-Candidate-Testing-Guide).

**A clear and concise description of what you want to happen.**

Inbound tor connections are well protected from eviction since [19670](https://github.com/bitcoin/bitcoin/pull/19670) was merged. This suggests it would be good idea to apply a similar method to protect a number of outbound tor connections from eviction (3 for default parameters, for example).

`bitcoin-cli -netinfo` on my node (default parameters) shows:

             ipv4    ipv6   onion   total  block-relay
    in        92       0      22     114      17
    out       11       0       1      12       2
    total    103       0      23     126      19

"
bitcoin/bitcoin,2021-08-10 18:05:22,feature,docs: add more examples and clarifications to external-signer.md,"proposing updates to `docs/external-signer.md`, specifically an explanation of how accounts are being used and examples of using the same device to create multiple wallets (motivation and discussion: #22635)

"
bitcoin/bitcoin,2021-07-23 07:48:14,feature,"Please make GUI responsive while syncing with network, especially to stop sync ","It is not possible to enter a transaction while the gui is syncing, because its lagging. 
sometimes it is important to enter a transaction and go away and the transaction will only be fullfilled after the blockchain sync is finished. for that there needs to be a responsive button to on/off the sync. i know the network symbol is there, but it does work that laggy..thanks
as a workaround i tried -connect=127.0.0.1

a simple fix would be a -startwithnosync option

same for ""processing blocks on disk"". everything is stuck."
bitcoin/bitcoin,2021-07-12 01:25:09,feature,"Additional arg: ""-groupcookie"" ","**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
The RPC .cookie is generated with 0600 permissions (i.e. only the user running the daemon can use the cookie).  When running bitcoind under a dedicated user and group (e.g. user = ""bitcoin"" and group = ""bitcoin""), distinct users who are also in the ""bitcoin"" group cannot use the cookie for auth.  This forces your hand when running additional RPC client software like electrs to have it also run under the ""bitcoin"" user if you want to use cookie based authentication.  Currently the only way to alter this is to pass ""-sysperms"" but this uses the system umask for all files/dirs, not just the cookie.  The cookie permissions should be individually tweak-able.  

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
A ""-groupcookie"" argument could be added which generated the cookie with 0640 permissions.  This would allow distinct users added to the ""bitcoin"" group the ability to authenticate using the RPC cookie.  In turn affording more flexible, and potentially more secure, installations.

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
Continue to run additional RPC client software under the same user as the daemon or settle for username/password authentication."
bitcoin/bitcoin,2021-06-29 09:50:13,feature,wallet: use database for locked coins,"**Is your feature request related to a problem?**
UTXO locks are stored in memory only. Nodes start with zero locked outputs, and the locked output list is always cleared (by virtue of process exit) when a node stops or fails. It was added as memory-only filter in https://github.com/bitcoin/bitcoin/pull/1861/

Few users wanted persistency for locking unspent in issue: https://github.com/bitcoin/bitcoin/issues/14907 but it was closed in May 2020 because of lack of interest.

**Describe the solution you'd like:**
Use wallet db for locked UTXOs

**Describe alternatives you've considered:**
Use other wallets

**Additional context:**
I tried fixing this, initially had issues related to [wrong use of vector in C++](https://bitcoin.stackexchange.com/questions/106701/use-wallet-db-for-locked-utxos-in-bitcoin-core) and could never resolve it after trying few things.

I think it will be easy for devs who work on wallet related issues in Bitcoin Core or anyone else good with C++. Would appreciate if someone could fix this and improve privacy in Core wallet.
"
bitcoin/bitcoin,2021-05-21 10:21:36,feature,add a way to decrypt wallets without password,i dont know maybe you add a semi complex puzzle to do it so it can be a little hard to decrypt not too hard though. IDK
bitcoin/bitcoin,2021-05-11 22:47:51,feature,listransactions : documentation missing : erasure of txs with negative confs,"I figured out that conflicted transactions are erased from the list

How much many time is needed or in which conditions they are erased?"
bitcoin/bitcoin,2021-05-09 09:00:40,feature,Sanitize fee rates  (user input),"It would be nice to sanitize fee rates from user input.

For example the block min fee rate is simply parsed as int64_t value. As fee rates are multiplied by the package size, this can easily lead to overflow.

```
    if (gArgs.IsArgSet(""-blockmintxfee"") && ParseMoney(gArgs.GetArg(""-blockmintxfee"", """"), n)) {
```

Assuming a maximum transaction size of at most 4MvB, this would give an upper bound for the fee rate of ~46116 BTC/kvB. Though, any fee rate larger than 1 BTC/kvB is probably nonsense and should be rejected early on startup."
bitcoin/bitcoin,2021-04-21 02:23:41,feature,Error messages for invalid address,"Can we add more information in error messages for invalid address especially the errors mentioned in PR: https://github.com/bitcoin/bitcoin/pull/20832/

Example: 

Change https://github.com/bitcoin/bitcoin/blob/f385ad765174afb02e60900581612a19c143cf83/src/key_io.cpp#L103

To

```C++
error_str = ""Invalid prefix for Bech32 address. Valid Bech32 address starts with `bc1` (mainnet) or `tb1` (testnet)"";
```

Context: https://github.com/bitcoin-core/gui/pull/280#issuecomment-820957002"
bitcoin/bitcoin,2021-04-15 12:16:38,feature,TX fee estimation does not take into account time of day or day of week,"There seem to be patterns regarding fee required based on time of day and day of week, which is not currently (last I checked) being factored into the fee estimation code. For example, rather than giving a duration, it perhaps ought to give a estimated day of the week or time of the day of anticipated inclusion into a block with a disclaimer that the estimate is based on patterns of the past (which do not predict the future).

In terms of specific development of a GUI wallet, it could be useful to have some visual representation of the minfees that were successful over the last 2 weeks to help the user decide on a suitable fee."
bitcoin/bitcoin,2021-04-09 19:52:28,feature,having dataworkdir and dataarchivedir to increase velocity,"**Process is very slow.**
It makes many days to import full blocks.
In my case, I had an index issue during download and resetindexes was very long process (too long in fact)

**SSD drives seems to increase process speed**
Looking at forum and my personal tests, using SSD drive increase process speed. unfortunatly, big SSD drives are very expensive.

**Speed up is only needed during current traitment**
One solution should be to have 2 data emplacements instead of 1:

- dataworkdir: path inside SSD drive to store curently files in process (block, index, chainstate)

- archivedir: path where to store ended files (those files are not modifed often)

Like this, no need of a large and expensive SSD drive to run Bitcoin Core server
"
bitcoin/bitcoin,2021-04-01 13:16:08,feature,[wallet] dumprivkey descriptor support,"I'd like to able to access the `xpriv` corresponding to the `xpub` returned by `getaddressinfo`.

We could have `dumprivkey` return the private key at the `parent_desc` level, either by default or by adding an boolean argument `parent` to the method."
bitcoin/bitcoin,2021-03-29 14:08:51,feature,Simple and intuitive way of watching/monitoring xpub keys?,"**Is your feature request related to a problem? Please describe.**
Trying to keep track of xpub addresses using core rpc and did not figure out how to do so.

**Describe the solution you'd like**
Any clear way like importaddress or importmulti to keep track of xpub addresses?

Is there any way to ""monitor"" all addresses of xpub-key without having to resolve every path/address adding them each by each manually?"
bitcoin/bitcoin,2021-02-25 14:55:56,feature,Add `include_unsafe` option to `fundrawtransaction`,"Would it be completely unreasonable to allow `fundrawtransaction` to use unsafe outputs?

I understand this means the resulting transaction may be invalidated whenever the unsafe output I'm relying one disappears (e.g. because the transaction that produced it was RBF-ed) but in my case that's ok, I'll react to that and re-publish the tree of child transactions accordingly.

But let me know if you think that's too much of a footgun for users, in that case I'll architect my solution differently."
bitcoin/bitcoin,2021-02-23 17:29:17,feature,.,.
bitcoin/bitcoin,2021-02-19 09:39:16,feature,.,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
"
bitcoin/bitcoin,2021-02-17 06:44:10,feature,iOS build support in macos bigsur 2020,"I'm trying to build the bitcoin core for ios but it seems that the library only support for mac linux ubuntu and android so far. 

I have look at this repository https://github.com/Sjors/iOS-bitcoin-full-node. 

But it seems out of maintain for a long time. And when I call **make**, it still show out a lib for macos instead of iOS.

Can anyone help me for some instructions? 

I'm using Xcode12 and mac bigsur 11.2.1.


openssl for ios
https://github.com/x2on/OpenSSL-for-iPhone

(we can refer to https://github.com/leetal/ios-cmake for c++ xcode example for ios)


"
bitcoin/bitcoin,2021-02-16 11:25:56,feature,guix: Make it arch agnostic,"It would be nice if guix was arch agnostic, so different archs can be used to get the same binary.

I tried running guix on amd64 and arm64, for the target `bitcoin-62cc2180afc1-powerpc64le-linux-gnu.tar.gz` and only got a few bits difference:

```diff
--- ./bitcoin-arm64/bin/bitcoin-cli
+++ ./bitcoin-amd64/bin/bitcoin-cli
├── readelf --wide --decompress --hex-dump=.gnu_debuglink {}
│ @@ -1,5 +1,5 @@
│  
│  Hex dump of section '.gnu_debuglink':
│    0x00000000 62697463 6f696e2d 636c692e 64626700 bitcoin-cli.dbg.
│ -  0x00000010 13a837f9                            ..7.
│ +  0x00000010 08b973c6                            ..s.
"
bitcoin/bitcoin,2021-02-08 10:49:31,feature,New Feature: Faster --reindex speed optimisation,"Hi,

If a user uses the --reindex command to download the block chain this works fine.
However if you stop it midway for what ever reason for example:

- Computer Crash
- Not leaving computer on for days
- Running out of disk space

Then in this case you need to use the ```--reindex``` command again.
However it takes forever to recheck each already downloaded ```blk000*.dat``` file before continuing to reindex.

Currently on my machine my blocks are reindexing between 5-26 seconds per block. 
(My Machine: Mac Mini with External HDD)

Do you think it would be a good idea to use file hashing checks to speed up this process for example on ubuntu you can run this command:

```sha256sum blk00000.dat```

which returns this hash

```be88bbfc0c09b3527e71f38fe14ba8693d35271d15bfdca57567429e06671003```

Would it be possible to create another .dat file containing all blk file hashes.
Then this file can be cross referenced against your file system to ensure it matches.
As long as each node contains a copy of the same file, then this ```block-hashes.dat``` type file 
can be redownloaded from a peer or a hash check to compare it with your current ```block-hashes.dat``` file.

Then your computer can cross reference each file sha256sum with the ```block-hashes.dat``` file to speed up the process.

Then i think this would speed up the duplicate --reindex by up to 8-10x faster depending on your machine

Currently my blocks are reindexing between 5-26 seconds per block.
If this was using the ```sha256sum``` it would take about 3 seconds per block on a standard HDD
Im am sure SSD would probably be even faster.

I believe the change would need to be made in this file
https://github.com/bitcoin/bitcoin/blob/f1239b70d116ea28b65e60993a6e4ac82cc6c2b1/src/validation.cpp

near line 4611

I would do this myself yet i am not a C programmer.

- Does anybody think this would be an improvement.
- would anybody like to implement this update?
- I think this would take about 15 minutes to add this feature for a fluent c programmer.


"
bitcoin/bitcoin,2021-02-04 11:42:21,feature,Follow-ups to PR 19509 - Message Capture,"This issue tracks follow-up actions from #19509

- [ ] Use 1 file = 1 session https://github.com/bitcoin/bitcoin/pull/19509#issuecomment-765625996
- [x] Update message type checking in p2p_message_capture.py: https://github.com/bitcoin/bitcoin/pull/19509#discussion_r568553627
- [x] Remove MakeUCharSpan https://github.com/bitcoin/bitcoin/pull/19509#discussion_r562515326
- [ ] Add RPC to enable/disable message capture per peer
- [x] Vulture warning about discarded value https://github.com/bitcoin/bitcoin/pull/19509/files#r571454691"
bitcoin/bitcoin,2021-02-03 04:38:01,feature,"~~$1,500 Bounty for Offline Multisignature through the GUI~~ View #24861","There has been incredible work implementing descriptor wallets, PSBTs, offline signing, HWWs; and GUI support for it all into Bitcoin Core.  It has been incredibly exciting to watch!

My goal is to financially support developers to implement offline multisignature wallets into Bitcoin Core.  I believe we are almost there.

I am here to humbly try to attempt to describe what I see, as a user and through talks with other developers, as the work that is needed to implement secure, offline multisignature wallets in Bitcoin Core.

1) A standard for Coordinating Multisignature Wallets, with an authentication scheme.  This is being discussed [here](https://github.com/bitcoin/bitcoin/issues/18142).

BlockchainCommons has done a lot of work around this:

Online Node Coordinator sends a 'Policy Request' to Offline Co-Signers, if acceptable, co-signers send a 'Keyset', and then get an 'Account Map' back.

Policy Request = empty descriptor

Keyset = BIP48 path

Account Map = descriptor w/ no xprvs in it

AND/OR

Co-Signers send array of descriptors for all script types ([BCR-2020-015](https://github.com/BlockchainCommons/Research/blob/master/papers/bcr-2020-015-account.md)) to Coordinator, Coordinator selects necessary descriptor and creates wallet, then sends 'Account Map' back.

Concerns:  lack of complete standardization.  use of BIP48.  Names still up in the air.

@Sjors also discussed the possibility of a 'Wallet Composer File' that could compose a wallet interactively, use future-proof syntax (can the signer decompile miniscript?), and support other capabilities.  Not sure how much work has been done on this.

1 A)  The authentication:

Just trust on first use, with warnings if data changes?  @benma has done great research and reporting on this:

To receive securely, the offline signers need to be able to verify the following:

> The receive address, which has to encode the hash of an ordinary multisig redeem script with no other spending conditions
> The key of the hardware wallet, which has to be one of the public keys in the redeem script
> The keypath of the displayed address in order to avoid ransom attacks if no restrictions are enforced by the hardware wallet
> The number of cosigners in order to prevent an attacker from adding more
> The threshold of required signatures to not be higher or lower than intended
> The xpubs of the cosigners in order to prevent an attacker from swapping them

The above is written for hardware wallets, but I believe any BIP written and code implemented should be for both HWWs and offline Core wallets.  With descriptor wallets, I believe a lot of the above authentication can be automated by the info in the Account Map that all offline co-signers have.

To send securely, the offline signers need to be able to verify the following:

> The recipient’s address, displayed and confirmed by the user like with singlesig
> The change address, having the same cosigners and threshold in an ordinary multisig script with no other spending conditions
> The change goes to an address at a keypath recoverable by the user

I think all the above is very important, since there have been many instances of devices not properly implementing the above security measures, so having a BIP for devices/software to comply with would be important.


@Sjors and @achow101 have been doing a ton of work integrating HWWs.  This looks like it will be merged soon: (https://github.com/bitcoin/bitcoin/pull/16546).  After that, the UI will be worked on (https://github.com/bitcoin-core/gui/pull/4), and there is a currently-closed PR for HWW multisig functionality on top (https://github.com/bitcoin/bitcoin/pull/16895).  Perhaps some of this code can be re-used for multisig with Core on offline computers, but I am not sure.

I believe having this ability would be an extensive security gain (being able to create a multisig wallet with Core on offline computers through the GUI).


2)  Represent multiple derivation paths with one descriptor (https://github.com/bitcoin/bitcoin/issues/17190)

Single descriptors make creating, backing up, and restoring multisignature wallets much simpler.  For example:

To create a multisignature wallet in Bitcoin Core, you can currently:

1. Create wallet
2. Dump `hdseed`
3. Create address, dump xpriv
4. Repeat Steps 1-3 N times
5. Create multisig wallet using descriptor containing all N xprvs (M of N multisig)
6. Use the N `hdseed`s along with Account Map as backup

However, the above wallet can not create change addresses.  You need to create (backup and restore) a second descriptor for change addresses, even though most of the descriptor is the same, except for the derivation path at the end.  This can be very time consuming/confusing for end users, especially when backing up by paper/hand.

For a comprehensive offline multisignature wallet, I believe this change needs to be implemented.  Currently in Yeti, we bypass this by recommending coin control and only sending entire UTXO's (which is a pretty bad user experience and short term solution we have chosen).

Some users like setting custom change descriptors (for example, for sending to an application for mixing change) so I believe this should be able to be done behind expert options, and perhaps this makes a single descriptor less controversial.


3)  QR code scanner (https://github.com/bitcoin/bitcoin/issues/9913)

QR codes are the most secure way to pass data between airgapped signers.  I currently have a $300 bounty on implementing this.  BlockchainCommons has a standard ([BCR-2020-005](https://github.com/BlockchainCommons/Research/blob/master/papers/bcr-2020-005-ur.md)) that is compatible with PSBTs (multiple QR codes checksummed; or a single animated QR code), and has a C++ reference implementation (https://github.com/BlockchainCommons/bc-ur).


4)  After the above (1-3) is finished, I believe implementing offline multisignature wallets into Bitcoin Core will be possible.

I am not trying to influence consensus (this should just be wallet code), and ultimately it is of course up to the developers and the community if these are changes that are good for Bitcoin, and if the above is how it should be implemented.  I am just trying to put all the thoughts/discussions down in one place.


Developers who might be interested:

@Sjors @gwillen @instagibbs @achow101 

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

I, Robert Spigler, am offering a $1,500 bounty for implementing 1, 2, 3, and 4 (standard for coordination of co-signers, single descriptors, QR codes, and offline multisignature wallets) in Bitcoin Core.

This does not include the already existing $300 bounty for #3 (QR codes).

The bounty will be split between developers as I see most fair (author and substantial reviews/discussions).

The bounty has no expiration and will be paid in bitcoin.

The date is 2021-02-02.
-----BEGIN PGP SIGNATURE-----

iQIzBAEBCgAdFiEEf4WKHNGE9pWzsby0UsewL8eQ8/AFAmAaJz0ACgkQUsewL8eQ
8/CVqw//ZOQkDabWlYxqcf669FqvsWRK/q/mNidaBor9gVwFiXZsBP1j7knGgpvZ
WCpek6JpMU4b1ZA2jobbSAKcaEarUiBRMVoXWnHmeg0igJmNhZyhL9D4lvy1kUBd
3F8KhqZ0yIlAzGRWu6+CKOVlTMQFwS/giAgoWeQCa8a4Nx0WjbhtyMWvXQmXlbnJ
7zjN3nfJDi7CYxMGGyfnLLJSCdUGt205PIwphwI9OR2TJVwfl6rRLR1rzakCHOOa
zvd6Ud0mGCHiJMvlWv5Qyk6KGm4O9XTVPYWU9DkWq1Jahv4MtZvb+n1DsgT6ZQxF
gNZVOp+iuigEmgWe1Qw8wsiWjubJrVBhWcg3cFfchmRYefMo6+53RCYM8+VnbYEm
VFts1rmMtLex8UTfrXX4FAla1WdPhlNFCNIlNnabBMTrck7WUTQdO07eNc6kI/wz
QXBAbuLqIp3FEhLcsv9oZQIkQmtAwN2235aN+B0J7bFLaegBg2E+Ur3lIlDPKZvo
U1r+NgDfNf9ZkA8rKJxRENCH5o46SbxI6gPYVTVY8NAy8pj2RFcGMYg1a0DEW1G2
zy/3/4Umud5Mr92ggkJWbKKe7HWqypYEXWhc+vo5fOoCY5XTJza0hOyiW+5vHKMM
YGS81OAQfIV/MJFffSJ581uU6th1s+7g9nWjeT9vGqTSJbNd1P4=
=Xm2a
-----END PGP SIGNATURE-----
"
bitcoin/bitcoin,2021-02-01 18:39:46,feature,Setting a label on bitcoin-qt changes the label of all unlabeled transactions,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

When selecting one transaction, and choosing ""Edit Label"", it changes the label of every single transaction which didn't have a label previously. 

**Actual behavior**

What happens is the aforementioned effect - all transactions get such label

<img width=""325"" alt=""Screen Shot 2021-02-01 at 19 33 19"" src=""https://user-images.githubusercontent.com/878399/106502247-6cffc080-64c4-11eb-9b9e-df5ceb2e128e.png"">

<img width=""246"" alt=""Screen Shot 2021-02-01 at 19 33 36"" src=""https://user-images.githubusercontent.com/878399/106502415-9ddff580-64c4-11eb-890f-3c8ed68570b7.png"">

** Expected behavior **

As one clicks one transaction and it says ""edit label"", I would expect that the label is transaction-related, not per receiving address. Either the GUI is misleading in the description or it's not behaving as it should.

**To reproduce**

Choose one transaction. Choose edit label with mouse right-button. All transactions to that receiving address will change to such label.

**System information**

Bitcoin-QT 0.21.0 from bitcoin.org

Mac OS Big Sur, whatever version came out last week

"
bitcoin/bitcoin,2021-01-22 22:09:12,feature,RPC: Derive wallet addresses,"**Is your feature request related to a problem? Please describe.**
It is not possible to simply derive the wallet's addresses. I have a need to find the address inside the HD wallet at a given HD path.

**Describe the solution you'd like**
- Derive address using the node i.e. `derivewalletaddress(path)` that returns the same as `getaddressinfo`

**Describe alternatives you've considered**
- Export xpub - not possible
- Export the seed - `dumpwallet` only exports to file - cannot be used over network
- `deriveaddresses` - requires xpub/xprv
- `listaddressgroupings` + `getaddressinfo` for every address and find by `hdkeypath` - This is way too intensive

**Additional context**
Some context I found about xpub exporting explains that this is not possible: https://bitcoin.stackexchange.com/a/90149/106210
"
bitcoin/bitcoin,2021-01-16 04:13:25,feature,dbcache config option is missing from example bitcoin.conf,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

No, just requesting an update to the example bitcoin.conf

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

`dbcache` config option to be added to [example config](https://github.com/bitcoin/bitcoin/blob/master/share/examples/bitcoin.conf)

ref: https://github.com/bitcoin/bitcoin/issues/20645#issuecomment-744078075

"
bitcoin/bitcoin,2020-12-29 04:14:52,feature,Can I make readme.md and other introduction files in another language?,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
I think I can add the another version of readme.md and other introduction to Bitcoin files in another language. I think this can improve the accessibility of this project.
<br>
**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
I'll translate the documents, then other people can help

**etc.**
I'm Korean, so I'll translate this documents in Korean."
bitcoin/bitcoin,2020-12-17 12:49:55,feature,Add I2P support using I2P SAM,"Add I2P support by using the [I2P SAM](https://geti2p.net/en/docs/api/samv3) protocol. Unlike Tor, for incoming connections we get the I2P address of the peer (and they also receive ours when we are the connection initiator).

Two new options are added:

```
  -i2psam=<ip:port>
       I2P SAM proxy to reach I2P peers and accept I2P connections (default:
       none)

  -i2pacceptincoming
       If set and -i2psam is also set then incoming I2P connections are
       accepted via the SAM proxy. If this is not set but -i2psam is set
       then only outgoing connections will be made to the I2P network.
       Ignored if -i2psam is not set. Notice that listening for incoming
       I2P connections is done through the SAM proxy, not by binding to
       a local address and port (default: true)
```

# Overview of the changes

## Make `ReadBinary()` and `WriteBinary()` reusable

We would need to dump the I2P private key to a file and read it back later. Move those two functions out of `torcontrol.cpp`.

```
util: extract {Read,Write}BinaryFile() to its own files
util: fix ReadBinaryFile() returning partial contents
util: fix WriteBinaryFile() claiming success even if error occurred
```

## Split `CConnman::AcceptConnection()`

Most of `CConnman::AcceptConnection()` is agnostic of how the socket was accepted. The other part of it deals with the details of the `accept(2)` system call. Split those so that the protocol-agnostic part can be reused if we accept a socket by other means.

```
net: check for invalid socket earlier in CConnman::AcceptConnection()
net: get the bind address earlier in CConnman::AcceptConnection()
net: isolate the protocol-agnostic part of CConnman::AcceptConnection()
net: avoid unnecessary GetBindAddress() call
```

## Implement the I2P [SAM](https://geti2p.net/en/docs/api/samv3) protocol (not all of it)

Just the parts that would enable us to make outgoing and accept incoming I2P connections.

```
net: extend CNetAddr::SetSpecial() to support I2P
net: move the constant maxWait out of InterruptibleRecv()
net: dedup MSG_NOSIGNAL and MSG_DONTWAIT definitions
net: extend Sock::Wait() to report a timeout
net: extend Sock with methods for robust send & read until terminator
net: extend Sock with a method to check whether connected
net: implement the necessary parts of the I2P SAM protocol
```

## Use I2P SAM to connect to and accept connections from I2P peers

Profit from all of the preceding commits.

```
init: introduce I2P connectivity options
net: add I2P to the reachability map
net: make outgoing I2P connections from CConnman
net: accept incoming I2P connections from CConnman
net: recognize I2P from ParseNetwork() so that -onlynet=i2p works
net: Do not skip the I2P network from GetNetworkNames()
```"
bitcoin/bitcoin,2020-12-05 11:12:46,feature,RFC on logging improvements,"Perhaps we can consider creating different levels of net logging.

For instance, we could separate lower-frequency, important peer-level events (`netpeers`) from very high-frequency message-level passing (`netmessages`).

Categories and naming suggestions welcome.

One further suggestion by @jnewbery was:
> No objections. I'd take it ever further though, and add an (optional) logging severity (DEBUG/INFO/WARNING/ERROR or similar) that can be added to all log messages. The user can then either choose what severity logs they want for each category (eg = -debug=net:warning,tor:debug etc), or have a logging post-processor that can filter by severity/category.

I like the idea of optional logging levels (debug/info/warning/error) for each category, including the default debug log, but agreement on which events go into which level may be difficult to achieve.

To begin with, I propose separating the net logging into at least two categories. Thoughts? Implementation suggestions?"
bitcoin/bitcoin,2020-11-30 14:20:34,feature,PSBT is not handling PSBT_GLOBAL_XPUB,"The PSBT_GLOBAL_XPUB should be populated when creating PSBTs when using multisig, as offline signers such as the BitBox02 hardware wallet require it to register and retrieve a multisig account.

https://github.com/bitcoin/bitcoin/blob/817aeca57a4cc3c8b4b321dbc653bfc4a8d61b0c/src/psbt.h#L20 

Would there be any obstacles in implementing this?"
bitcoin/bitcoin,2020-11-26 02:36:53,feature,use try_emplace on std::map with c++17,"Let's have a discussion and see if a structure change is warranted to many places std::maps/unsorted_maps are used in the code.

Now that we support c++17 as a minimum as of #[20413](https://github.com/bitcoin/bitcoin/blob/2f71a1ea35667b3873197201531e7ae198ec5bf4/src/coins.cpp#L42) we should consider using try_emplace to gain efficiency when objects are not added, it also leads to more expressive and safer code(prevents stealing from arguments during failed insertion). For example, it seems FetchCoin uses emplace but does a piecewise_construct however it can be replaced with try_emplace:

`CCoinsMap::iterator ret = cacheCoins.emplace(std::piecewise_construct, std::forward_as_tuple(outpoint), std::forward_as_tuple(std::move(tmp))).first;`

becomes
`
CCoinsMap::iterator ret = cacheCoins.try_emplace(outpoint, std::move(tmp)).first;`

I also noticed emplace was used on dir_locks which is a map to unique_ptrs which is generally discouraged. It is used [here](https://github.com/bitcoin/bitcoin/blob/5a6f3c5a01eaf904c42bd77dbed931b49a8fec74/src/util/system.cpp#L107) and should likely just look like this:

`dir_locks[pathLockFile.string()] = std::move(lock));`"
bitcoin/bitcoin,2020-11-20 10:09:29,feature,Bitcoin address format.,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
Dear everyone,
I'm working on cryptocurrency exchange platform newly.
I downloaded bitcoin-core and synchronized  full node.
I'm using JSON-RPC request to connect to bitcoind.
I created new wallet and it always returns  segwit address.
**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
There are 3 formats for bitcoin address.
P2PKH or Legacy Address Format (addresses start with “1”)
P2SH or Compatibility Address Format (addresses start with “3”)
Bech32 or Segwit Address Format (addresses start with “bc1”)
**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
But Bech32 address is not recognized in several exchanges and I need get P2PKH or P2SH address.
**Additional context**
How can I get this type address by using JSON-RPC ?
Please help me !!!!!
<!-- Add any other context or screenshots about the feature request here. -->
"
bitcoin/bitcoin,2020-11-10 19:05:12,feature,test_bitcoin-qt shouldn't bind to regtest ports,`test_bitcoin-qt` binds to the regtest ports which causes it to hang if a regtest bitcoind or bitcoin-qt is already running. This causes `make check` to hang and be annoying to kill. So `test_bitcoin-qt` shouldn't bind to these ports to avoid conflicting with an active regtest instance. Or it should fail gracefully instead of just hanging and then timing out.
bitcoin/bitcoin,2020-10-26 14:06:03,feature,Document JSON-RPC wallet endpoints,It appears we don't have any documentation of the /wallet/&lt;walletname> JSON-RPC endpoint. Maybe JSON-RPC-interface.md is a good place to put it.
bitcoin/bitcoin,2020-10-23 08:41:04,feature,Dependency on GitHub,"After reading [this post](https://unixsheikh.com/articles/important-open-source-projects-should-not-use-github.html) with the title «Important Open Source projects should not use GitHub» I thought instantly about Bitcoin and it's source code. It is the most important open source project and it is hosted on GitHub.

Was this topic ever discussed? I would think so after Microsoft aquired GitHub, but I couldn't find it. What happens if GitHub requires a Microsoft account to login to GitHub, are all developers willing to create a Microsoft account?

I know, I don't offer a solution at this point. I heard about [Sourcehut](https://sourcehut.org/) as an Alternative to GitHub. The problem is, the source code of Bitcoin will (with the current available git hosting providers) always be centralized. Is that the reason why Bitcoin didn't move away from GitHub?

This is more like a discussion, if this is the wrong place for that or there is already an open issue. Please close this one and point me in the right direction. Thanks!
"
bitcoin/bitcoin,2020-10-17 20:02:57,feature,Allow spending from segwit addresses created from uncompressed private keys (P2SH-P2WPKH),"Trying to create a transaction that is sent from a segwit addresses created from an uncompressed private key raises this error: 
_non-mandatory-script-verify-flag (Using non-compressed keys in segwit) (code 64)_

This makes the transaction non-standard, which has resulted in stuck user funds. A small change is needed in the source code to allow for such segwit addresses in order to have their transactions not marked as non-standard.

**Please enable relaying and mining of non-standard P2SH-P2WPKH transactions that use uncompressed public keys.**


**Additional context**
One specific user has over 5.7 BTC that is unspendable due to all outgoing transactions being marked as non-standard.
Address with funds: 34dqaqvQNWMgbMJmmxVa8LeGz7St6ATT97
Please see: https://bitcointalk.org/index.php?topic=5278860.0"
bitcoin/bitcoin,2020-10-15 12:56:04,feature,rpc: fetch block from peer,"The node is very efficient at not downloading blocks it doesn't care about. E.g. if a peer has headers for a lower proof-of-work branch, we won't fetch the full block. It'll appear as `headers-only` or `valid-headers` in `getchaintips`.

This is fine, but if a user is curious about a block, there's no easy way to obtain it.

1. you can call `invalidateblock` on the current tip and wait for the node to jump to the other branch
2. you can use some other software to obtain the block via p2p and then feed it to the node with `submitblock`

It would be nice if you can just fetch it from a peer, if you know they have it. In the most simple implementation, the user has to specify which node to ask. A more fancy version could automatically try all nodes, and perhaps even randomly connect to new peers until it finds the block.

Usage:
```
bitcoin-cli getblockfrompeer HASH peer_n
```"
bitcoin/bitcoin,2020-10-14 20:26:19,feature,Built in restore wallet,We have a `backupwallet` RPC and a way to backup a wallet from the GUI. But there's no way to restore a wallet other than copying the backup file into the walletdir. We should add a restore.
bitcoin/bitcoin,2020-10-09 11:44:23,feature,Improve block file pre-allocation speed on Linux,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->

The block file pre-allocation takes too long as currently done by `fallocate_posix` on Linux (ext4).

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

There is a way to make it uncomparably faster. I am not yet sure if it is switching to another function for Linux, or just setting some options for the `fallocate_posix`.

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

I know that file allocation in [TransmissionBT](https://github.com/transmission/transmission) torrent client is very fast on Linux. These commits I have identified from searching the logs as related, ordered from newest to oldest: transmission/transmission@a2d56b832, transmission/transmission@99d53d7ef, transmission/transmission@de2d0154e, transmission/transmission@4723b4a6e.

```
$ man 2 fallocate
```

https://www.man7.org/linux/man-pages/man2/fallocate.2.html

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->

Because I can not work more on it right now, I am dumping what I have found so far into this issue. Feel free to comment or even make a PR. Thank you!"
bitcoin/bitcoin,2020-10-05 17:30:37,feature,fuzz: how to scale fuzzing with the number of fuzz targets,"Having different fuzz targets is useful to give the fuzzer a specific and well defined task to work on. This makes it also easier for developers to see what an individual fuzz test/target is doing. Moreover, the fuzzer might be more performant in finding new inputs because the input directory as well as the search space is smaller.

However, there are also several downsides:

* Limiting the overall search space the fuzzer can explore will make it impossible to reach coverage for the code paths that have been excluded.
* Building numerous small fuzz targets, instrumenting them and linking them with debug symbols is costly in CPU time and disk space. A quick build is not only important for devs, but also for CI.

Similar to how the unit tests are compiled and linked into one binary, we could look into linking the fuzz targets into one binary. Individual targets could be selected with some kind of runtime argument."
bitcoin/bitcoin,2020-09-25 20:49:44,feature,[Idea] Multiple ports for faster overall performance?,"I'm coming at this from the angle of overall throughput of the network. So it looks like the Bitcoin daemon listens on 8333 and 18333 for connections.

What if, Bitcoin used... say 50 ports, or maybe even just 20 on both sides.  I think you'd be able to receive multiple at the same time given they were operating in an asynchronous manner. I'm not sure of the overall code structure, but wouldn't it allow for more throughput overall if both sides were communicating and routing based on open sockets? 

I don't know the deep technicalities of ports and throughput and I know it's not your job to teach me, but I am curious if there's any possible gain here."
bitcoin/bitcoin,2020-09-22 12:09:08,feature,Rescanning use only 1 core,"When i run **bitcoin-qt.exe** with _-zapwallettxes_ parameter, it rescan my wallet, but using only 1 of 20 core ~6.6% of maximum, and only 60-90 MB/sec reading on M2SSD drive. How can i add more cores for running QT ?

[https://i.imgur.com/zNQMgJ3.png](TASKMGR_SCREEN)
bitcoin.conf = `dbcache = 20000
par = 16
txindex = 1
addresstype = legacy
proxy=127.0.0.1:9050
listen=1
bind=127.0.0.1
onlynet=onion
datadir=E:\\Roaming\\Bitcoin
blocksdir=E:\\Roaming\\Bitcoin`"
bitcoin/bitcoin,2020-09-10 10:15:17,feature,RFC: SVG images/icons support,"There are some concerns about possible security issues.

[**luke-jr**](https://github.com/bitcoin/bitcoin/pull/19780#issuecomment-678697635):
> ... the concern is importing something complex like qtsvg into the overall codebase/runtime process... Any vulnerability compromises the entire program.

OTOH, [**NicolasDorier**](https://github.com/bitcoin/bitcoin/pull/19780#issuecomment-680750363):
> Given SVG files are auditable (it's only text), and we don't accept SVG from any untrusted source, I don't think this is a good concern **luke-jr**

And one more concern:
[**MarcoFalke**](https://github.com/bitcoin/bitcoin/pull/19780#issuecomment-680752530):
> > Concept ACK if this worked out of the box without the build system changes.
>
> The build system changes are required for static builds.

The questions are:
1. If the project really sticks to the PNG only image/icon format, why `src/qt/res/src/*.svg` are still present in the repo?

2. If concerns about SVG are not good, why not switch all icons to SVG (now we have designers to help with that), and get rid of `src/qt/res/icons/*.png`?"
bitcoin/bitcoin,2020-09-09 09:26:16,feature,Improve Tor support documentation,"**Is your feature request related to a problem? Please describe.**

I think we can add few things in https://github.com/bitcoin/bitcoin/blob/master/doc/tor.md to make it easier and better for users to use Bitcoin over Tor

**Describe the solution you'd like**
1. Add few examples to run Bitcoin as onion service: https://bitcoin.stackexchange.com/questions/98913/how-to-run-bitcoin-core-as-onion-service-on-windows-ubuntu-and-android

2. Add information about use of tor bridges and safe ways to connect over tor: https://bitcoin.stackexchange.com/questions/98772/what-are-the-safe-ways-to-connect-to-bitcoin-network-using-tor

3. If we can't add everything in [tor.md](https://github.com/bitcoin/bitcoin/blob/master/doc/tor.md), maybe add more docs for separate platforms and issues. Example: `tor-win`, `tor-linux`, `tor-privacy`

**Describe alternatives you've considered**
I have tried to experiment, research and share things on https://bitcoin.stackexchange.com to discuss about tor related setups involved in using Bitcoin

**Additional context**

I am only suggesting few ideas based on my research and observation. I am sure people who use or build things related to Tor and more experienced than me are involved in Bitcoin so maybe we can improve the docs related to 'Tor Support'.
"
bitcoin/bitcoin,2020-09-04 19:58:04,feature,Script interpreter cleanups,"Extracting this topic from #17977 as during its review general cleanups of the script interpreter clean have been advocated.

Given that taproot implementation review is already far advanced, I think no one is proposing to address them now ? This issue is more to track interesting points raised.

I guess there is at least two different axis which have been under discussion (but they may overlap a bit):
* @theuni has a branch splitting consensus from policy : https://github.com/theuni/bitcoin/commits/policy-split2. As of today, it sounds like the distribution of consensus and policy checks is fairly arbitrary and may lead to confusion on expected behaviors (see https://github.com/bitcoin/bitcoin/pull/17977#discussion_r483335562)
* @JeremyRubin was proposing to split further interpreters (see https://github.com/bitcoin/bitcoin/pull/17977#issuecomment-667475959). Taproot is complexifying the script rules matrix thus making it hard to reason on in the prevision of future script softforks

If you have more comments worthy to be pinned please add them."
bitcoin/bitcoin,2020-08-27 15:11:25,feature,Transactions propagation design goals,"During the last P2P meeting, @sdaftuar pointed the lack of a transactions propagation framework thus hindering progress in the tx-relay network area, devoid of clear goals to achieve.

This is a blocking issue in a spawn of subjects, like countering cheap RBF replacements, increasing mempool feerate of resources constraints nodes, better support of Bitcoin applications with more demanding fee/tx-relay requirements, scope of tx-rejection filter, etc

## Core mechanisms of Tx-Relay

I think one of the starter goal of the tx-relay network is obviously to let miners discover the best-feerate transactions candidate for inclusion in block.  As Bitcoin censorship-resistance lays on distribution of mining, bias in tx-relay topology privileging a subset of miners with an advantage in fee discovery may provoke disequilibrium beyond their hashrate contributions. Thus an unstructured p2p network, vetted of uniform tx-relay peers selection is likely the best option. A distributed network is also more-robust against infrastructure disruption or targeted transaction censorship.

An additional goal is preserving the pseudonymity of transactions original broadcaster which can be enforced with high guarantees only if tx-relay topology is non-observable by a protocol participants or a coalition of them. Introduction of randomized timers at propagation (e.g https://github.com/bitcoin/bitcoin/pull/14897) is an improvement towards this direction, even if [research](https://arxiv.org/pdf/1812.00942.pdf) has hinted that the number of tx-relay mechanisms potentially exploitable to learn about topology is likely wide. Future improvements like mempool rebroadcasts (https://github.com/bitcoin/bitcoin/pull/16698) or initial-broadcast-over-Tor-only may improve the situation.

Lastly, an other important goal to consider is bandwidth-savings, as underscored in the [Erlay](https://arxiv.org/pdf/1905.10518.pdf) paper, a too prohibitive tx-relay cost disincentives potential node operators, especially running full-relay public ones contributing the most to good health of the network. This goal might be in trade-offs with aforementioned ones, where tx-relay peering redundancy increases robustness and higher mempool rebroadcast frequencies add noise to the propagation graph at the price of higher bandwidth costs.

## Tx-Relay & Node Policies

If the set of messages as inherited from first protocol versions or specified in peer services BIPs defines what bitcoin data structures are accepted by compliant peers, AFAIU it doesn't mandate a relay behavior, neither outlaw a superset of constraints on p2p messages, at the discretion of local node. This superset of constraints is I guess what people commonly called P2P network transaction policy, even in fact such _policies_ vary by full-node implementation, versions and local node settings.

On the Core side, such policies encompass some DoS/vulnerabilities counter-measures like `SCRIPT_VERIFY_MINIMALIF` at the script interpreter level or `MIN_STANDARD_TX_NONWITNESS_SIZE` at the mempool level, invariants which can't be disabled by local settings, so we can qualify them as implementation/version policy . Another range of policies checks are vetted with hopefully anti-DoS default values like `incrementalRelayFee` or `m_limit_descendant` but configurable by node operator to express a different pricing of resources offered to the network or increasing the chance to learn good feerate chain of transactions by accepting higher DoS risks. 

Due to this divergence of policies across the network and adding to the absence of events order in a distributed system, mempools convergence has never been considered as a goal. However, there is a p2p mechanism for peers to discover and adapt its peers policies, namely [`feefilter`](https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki). This mechanism prevent bandwidth-waste by will-be-rejected low-feerate transactions. Further p2p mechanisms could be devised to improve peer policy negotiation, like `txfilter` announcing the local standardness applied or `utxofeerate` announcing the feerate of a package for a known utxo spend candidate. That said, communicating more information to opportunistically save more bandwidth is likely quickly bounded.

## Fee/Relay Assumptions for Bitcoin Applications

Bitcoin applications aiming to confirm their transactions should ensure first to be connected to a high number of tx-relay peers, then ensure these transactions are formatted to pass at least `testmempoolaccept` of their local nodes. If a propagation failure is detected or suspected, a) a rotation should be triggered to probabilistically find a peer with an identical policy, at least for the subset of policy rules you share  or b) a warning can be triggered back to the user hinting to do something about likely a badly-formatted transaction.

Such model doesn't work great with regards to multi-party Bitcoin applications (e.g LN or vaults), where signing interactivity is costly or impossible. Further, if a transaction is time-sensitive with regards to advancing state of protocol forward, propagation failures are direct risks for fund safety. Even further such potential propagation failures could be exploited by a malicious counterparty, if applications participants are assumed to be distrusted. Such application can never be sure that transaction will confirm but should be able to express its best-feerate bid for this time-sensitive transaction, even if bid isn't won due to a better blockspace demand.

Any such application/protocol developper will be confronted with the following questions:
- how to decide the format of my transactions (size, scripts, witness, minimal fee, ...) ?
- what should be the size/weight of chain of transactions ?
- what should be my tx-relay strategy and should rebroadcast/peer rotation be triggered ?
- what fee-bumping strategy are offered between RBF, CPFP, Parent-Pay-For-Child, etc ? how network mempools will evaluate each one ? how a malicious counterparty could leverage [them](https://github.com/t-bast/lightning-docs/blob/master/pinning-attacks.md) ?
- what dev process will follow ecosystem/implementations in case of rules tightening/changes potentially hindering application/protocol security, e.g what are Core guarantees wrt to [carve-out](https://github.com/bitcoin/bitcoin/pulls?q=is%3Apr+author%3ATheBlueMatt+is%3Aclosed) backward-compatibility ?

Historically, we have an example of BIP 125, specifying out the mempool RBF policy, ready to be consumed as an interface by Bitcoin applications.  But otherwise, due to fear of silently breaking an obscure policy rule, some application are literally leaking them in their stack (see https://github.com/ElementsProject/lightning/blob/4302afd9a58f0c455bb812b63e9cdf377ebb74d4/bitcoin/signature.c#L214)


Of course to avoid getting stuck in endless debates, here few questions we may evaluate as a guideline:
- what have been historically propagation assumptions of Bitcoin applications ? how newer class of Bitcoin applications should conceive their operational and security models with regards to propagation ?
- should we support requirements of newer classes of Bitcoin applications ? If so, what should be the scope and expressivity of an fee/tx-relay API rules ?
- what set of constraints (DoS, bandwidth, privacy, deployment, ...) do we have to bind to ?
"
bitcoin/bitcoin,2020-08-26 01:38:38,feature,Signing Transactions on Offline Computer without Blockchain,"**Is your feature request related to a problem? Please describe.**
I am working with a team on Yeti (https://github.com/JWWeatherman/yeticold), which is a script for a UI for setting up offline, HD multisig with only minimal software beyond Bitcoin Core.  Only QR codes are used for transferring private keys.  As Core continues to merge PRs on offline, multisig, and UI work, Yeti will remove its own written code for the more peer reviewed Core releases.  The goal is that Yeti is eventually not needed at all.

Currently, Core needs a blockchain to sign transactions, even if the wallet is offline (and doesn't truly need a blockchain).  This means that although the wallet is getting the data it needs to sign from the QR code, the process now must look like this:

The computer needs to be online for days first in order to sync the blockchain (even though it'll never be used), then network access can be disabled, and descriptors generated.  This could be made much simpler without having to sync.

**Describe the solution you'd like**
I know there is a lot of work on making Core more modular (https://github.com/bitcoin/bitcoin/projects/10).  Once that project is finished, is it part of the plan to be able to sign transactions without a blockchain present?
"
bitcoin/bitcoin,2020-08-17 14:21:43,feature,Deprecate banlist.dat,"`banlist.dat` was introduced in https://github.com/bitcoin/bitcoin/pull/6310, storing nodes that were automatically banned (due to exceeding the misbehavior threshold) and manually banned (through the `setban` RPC).

Automatic bans were removed in https://github.com/bitcoin/bitcoin/pull/19219 and replaced with a discouragement filter. That filter is not saved to disk/persisted over shutdown/startup. The `banlist.dat` therefore now only contains addresses that have been banned through the `setban` rpc.

Since https://github.com/bitcoin/bitcoin/pull/15935, we have a specific file for configuration that is updated through the RPC and persisted over shutdown/startup, namely `settings.json`. We should therefore move the manual ban configuration from `banlist.dat` to `settings.json`. Doing so has a couple of benefits:

- `settings.json` is human readable and easily analyzable by any json parser.
- we wouldn't need to maintain custom [de]serialization code for the `banlist.dat` file.

`banlist.dat` is expected to be fairly small and infrequently updated, so disk space/performance are not huge concerns."
bitcoin/bitcoin,2020-08-06 22:26:26,feature,Expose compact blocks high-bandwidth mode state through getpeerinfo,"For every peer expose through `getpeerinfo` RPC whether or not we selected them as HB peers, and whether or not they selected us as HB peers.

Suggestion by @gmaxwell."
bitcoin/bitcoin,2020-08-04 18:11:32,feature,Add explicit feeRate option for sendmany RPC method,"**Is your feature request related to a problem? Please describe.**
It's always hard to send a bunch of transactions with sendmany method using conf_target option. Cause build-in fee estimator not suggesting required fee rate for our purposes. How could I specify transaction fee in BTC/kB for sendmany without creating raw transaction (building outputs and inputs manually)?

**Describe the solution you'd like**
Add fee_rate option to sendmany RPC method.

**Describe alternatives you've considered**
Currently I'm trying to estimate required conf_target based on mempool size and making weird algorithms that limiting that value someway.

**Additional context**
That is not so convenient for us. So the better solution would be to provide fee_rate option that is much easier to manipulate. Thanks in advance!
"
bitcoin/bitcoin,2020-07-27 23:59:30,feature,"Show the ""<n> of the last 100 blocks have unexpected version"" warning only when running -debug=validation?","[Google search results](https://www.google.com/search?q=of+the+last+100+blocks+have+unexpected+version) suggest that our users are more confused than helped by the `<n> of the last 100 blocks
have unexpected version` warning we're printing.

As developers we know the unfortunate reason behind this warning and how ""unexpected"" should be interpreted in this context, and thus why it is safe to disregard this warning. However, I don't think it reasonable to expect our users to know the historical context here.

In order to not desensitize our users to potentially critical ""real"" warnings, would it make sense to move this message to the `-debug=validation` log category?"
bitcoin/bitcoin,2020-07-24 13:52:35,feature,Solve year 2106 problem by taking timestamps mod 2^32,"**Is your feature request related to a problem? Please describe.**
With the current block validation rules, Bitcoin will ""die"" on 2106-02-07, when the unsigned 32-bit timestamp rolls over.

**Describe the solution you'd like**
Currently, Bitcoin's timestamp protection rules work as follows:
1. The new block timestamp may not be lower than the median of the last 11 blocks'
2. The new block timestamp may not be greater than the current time plus two hours

If they would be changed to the following, the problem would be solved:
1. The new block timestamp plus *k*\\*2^32, where *k* is an integer, may not be lower than the median of the last 11 blocks'
2. The new block timestamp plus *k*\\*2^32, where *k* is an integer, may not be greater than the current time plus two hours
3. The values of *k* in 1 and 2 must be the same

This would cause a hardfork in 2106, which is 85.5 years from now, by which time 95% of nodes would hopefully have updated.

**Describe alternatives you've considered**
64-bit timestamps have been proposed. They would break compatibility with a lot of other software, and cause a hardfork before the date of timestamp overflow."
bitcoin/bitcoin,2020-07-17 15:17:15,feature,"Normalize fee units for RPC (""BTC/kB"" and ""sat/B)","This needs to happen before the next major release. Otherwise, it will be a breaking change.

Some more context: https://github.com/bitcoin/bitcoin/pull/11413#issuecomment-649730975"
bitcoin/bitcoin,2023-09-08 22:43:23,question,Sign and Verify Message not working!,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

it is example address generated to purpose to test sign and verify a message.

Address : bc1qlhkfryasnna2fy0jrz794gmqkqnddk6pcpugg2
Private key: L5jS9y2yF17KKQPsgaWjh61uHm168p9VJVhje15tvcSbL18kgFDu

the message for verification : ""message""
signature: H7uksk3skjn4jJxYVvYpavUSNwBh1a1zJUo8G7IUHzQ5dRqnZYvvGzp/v+lk+91uu4dLnCos6RIfgVoqUS3wd/Q=
the signature only successful using private key commands.
using the online tool verifying the signature and message is successful : https://www.verifybitcoinmessage.com/
Note:  wallet passphrase  used :  deneme1 

But the bitcoin QT wallet version 25.0 and also previous versions error!
![qtError](https://github.com/bitcoin/bitcoin/assets/121049274/6410ec9a-52f6-4ea0-9df2-407f0793f38d)

### Expected behaviour

sign and verify 

### Steps to reproduce

trying to sign by rpc,
trying to sign using console,
trying to veryfy both.

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

v24.1 and v25.0

### Operating system and version

windows

### Machine specifications

I9 and 128gb ram, 2 tb ssd"
bitcoin/bitcoin,2023-08-28 09:07:53,question,"I didn't modify anything before I run make check,but it can't work","### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

when I run make check on my ubuntu,it said

In file included from ./primitives/transaction.h:11:0,
                 from ./primitives/block.h:9,
                 from ./consensus/merkle.h:10,
                 from consensus/merkle.cpp:5:
./script/script.h: In function ‘CScript BuildScript(Ts&& ...)’:
./script/script.h:601:30: error: parameter packs not expanded with ‘...’:
     ([&ret, &cnt] (Ts&& input) {
                              ^
./script/script.h:601:30: note:         ‘Ts’
./script/script.h: In lambda function:
./script/script.h:602:23: error: parameter packs not expanded with ‘...’:
         if constexpr (std::is_same_v<std::remove_cv_t<std::remove_reference_t<Ts>>, CScript>) {
                       ^~~
./script/script.h:602:23: note:         ‘Ts’
./script/script.h:605:21: error: parameter packs not expanded with ‘...’:
                 ret = std::forward<Ts>(input);
                 ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
./script/script.h:605:21: note:         ‘Ts’
Makefile:13994: recipe for target 'consensus/libbitcoinconsensus_la-merkle.lo' failed
make[2]: *** [consensus/libbitcoinconsensus_la-merkle.lo] Error 1
.....
Makefile:811: recipe for target 'check-recursive' failed
make: *** [check-recursive] Error 1

what should I do now?I just want to run the code

### Expected behaviour

Normal compilation

### Steps to reproduce

./autogen.sh
./configure

nothing wrong

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

lastest

### Operating system and version

ubuntu 18

### Machine specifications

_No response_"
bitcoin/bitcoin,2023-08-14 05:06:04,question,"Make it very obvious to the new people that the Bitcoin Core program first needs to be installed and run on the ""C"" drive. ","### Issues, reports or feature requests related to the GUI should be opened directly on the GUI repo

- [X] I still think this issue should be opened here

### Report

Hello developers,

I know that you probably think that this is so obvious and basic but it needs to be said. 

When a new person wants to run a full BTC node on their computer for the first time can you make it very obvious to that person that the Bitcoin Core program needs to be downloaded and installed onto their ""C"" drive of their computer first.  

Then that person will have the ability to sync up and store the BTC blockchain to what every drive they intend, whether that be the ""D"" drive of their computer or an external HDD or SSD so that they can carry around the node with them. 

The problem i had was that i had downloaded and installed the Bitcoin Core program onto an external HDD, then i plugged that external HDD into my laptop thinking that it would just carry on syncing up, it did not because the Bitcoin Core program needs to be already installed on to the C drive of the laptop first, then i would need to point the program as to where it needs to sync up the data to (in this case my external HDD or that I'm not using up all of the space on the laptop). 

Thanks ! "
bitcoin/bitcoin,2023-08-07 10:51:47,question,error C3203: 'UniqueLock',"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

When compiling the libbitcoin_node library in Microsoft Visual Studio Community 2019, Version 16.11.28
got error C3203: 'UniqueLock': unspecialized class template can't be used as a template argument for template parameter '_Ty', expected a real type
in the **validationinterface.cpp** file,
code section:

     template<typename F> void Iterate(F&& f) EXCLUSIVE_LOCKS_REQUIRED(!m_mutex)
     {
         WAIT_LOCK(m_mutex, lock);
         for (auto it = m_list.begin(); it != m_list.end();) {
             ++it->count;
             {
                 REVERSE_LOCK(lock);
                 f(*it->callbacks);
             }
             it = --it->count ? std::next(it) : m_list.erase(it);
         }
     }

line:
`REVERSE_LOCK(lock);`

REVERSE_LOCK macro in **sync.h** file:
`#define REVERSE_LOCK(g) typename std::decay<decltype(g)>::type::reverse_lock UNIQUE_NAME(revlock)(g, #g, __FILE__, __LINE__)`


### Expected behaviour

The compilation problem was solved by adding a macro:
`#define REVERSE_LOCK_(g) typename UniqueLock<Mutex>::reverse_lock UNIQUE_NAME(revlock)(g, #g, __FILE__, __LINE__)`

with explicit `typename UniqueLock<Mutex>::reverse_lock` instead of `typename std::decay<decltype(g)>::type::reverse_lock`

after which the `REVERSE_LOCK(lock)` call is changed to `REVERSE_LOCK_(lock)`

After compilation, run the test:
`test_bitcoin --run_test=""validationinterface_tests"" -- -checkaddrman=1 -printtoconsole=1`

Result:
`***No errors detected`


### Steps to reproduce

Before compiling in VS2019,
in the project configuration file **common.init.vcxproj**
`<PlatformToolset>` tag change to `v142`

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

v25.0

### Operating system and version

Windows 10

### Machine specifications

_No response_"
bitcoin/bitcoin,2023-08-03 08:57:28,question,Master doesn't compile on MacOS X 10.13 High Sierra,"Hello guys, I'm trying to compile the current Master branch on a quite old Macbook Pro (early 2011) for which the latest supported os is Mac OS X 10.13 high sierra, and I'm getting the following error:

```
Alessios-MacBook-Pro:src feeder$ make V=1
g++ -std=c++17 -DHAVE_CONFIG_H -I. -I../src/config   -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=3 -DHAVE_BUILD_INFO -Xclang -internal-isystem/usr/local/include -DMAC_OSX -DOBJC_OLD_DISPATCH_PROTOTYPES=0 -DPROVIDE_FUZZ_MAIN_FUNCTION -I. -I./minisketch/include -I./secp256k1/include -I./univalue/include -I./leveldb/include   -Wstack-protector -fstack-protector-all -Wall -Wextra -Wgnu -Wformat -Wformat-security -Wvla -Wshadow-field -Wthread-safety -Wloop-analysis -Wredundant-decls -Wunused-member-function -Wdate-time -Wconditional-uninitialized -Woverloaded-virtual -Wunreachable-code-loop-increment -Wimplicit-fallthrough -Wdocumentation -Wno-unused-parameter -Wno-self-assign       -g -O2 -MT bitcoind-bitcoind.o -MD -MP -MF .deps/bitcoind-bitcoind.Tpo -c -o bitcoind-bitcoind.o `test -f 'bitcoind.cpp' || echo './'`bitcoind.cpp
In file included from bitcoind.cpp:10:
In file included from ./chainparams.h:9:
In file included from ./kernel/chainparams.h:10:
In file included from ./netaddress.h:18:
./util/strencodings.h:15:10: fatal error: 'charconv' file not found
#include <charconv>
         ^~~~~~~~~~
1 error generated.
make[1]: *** [bitcoind-bitcoind.o] Error 1
make: *** [all-recursive] Error 1
```
I think this is a known issue, and seems to be related to an old compiler, that does not fully support C++17 (and charconv, in particular) 

My current compiler is an Apple LLVM 10, that I would believe is not that old:

```
Alessios-MacBook-Pro:src feeder$ g++ --version
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.10.44.4)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```

Does anybody know if MacOS 10.13 is not supported anymore and it's necessary to upgrade to a different release? 

Is it known if  MacOS 10.15 Catalina is supported?

PS: I also tried to install a newer version of GCC through BREW, but this release of MacOS is not supported anymore, so I'm just considering to change environment. "
bitcoin/bitcoin,2023-06-27 06:46:50,question,Wallet not loaded,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

Suddenly wallet not loaded. I was resyncing blockchain (now synced). The wallet is old and has balance. Yesterday there was no problem. 
Recently updated for V24 to V25. All was working ok until today. Node is not pruned.

Disk that blocks are stored is external Toshiba with USB connection, i have just a soft link between .bitcoin/blocks to blocks at Toshiba. I have repaired this disk a couple of times using windows disk tools. I did a repair because bitcoincore wanted to reindex blockchain. After the repair bitcoinore was ok (not asking for reindex) and continue syncing.
Tried this command: bitcoin-cli listaccounts and get error code: -32601/ Method not found error message

### Expected behaviour

load my wallet

### Steps to reproduce

run bitcoin-qt or bitcoind, all working ok except wallet loading

### Relevant log output

""Wallet disabled!"" from bitcoind running messages

### How did you obtain Bitcoin Core

Pre-built binaries

### What version of Bitcoin Core are you using?

v25.0

### Operating system and version

Fedora Linux 38

### Machine specifications

intel 8265U i5 cpu/16g ram/samsung evo 850 ssd drive/ external usb  disk for blocks storage"
bitcoin/bitcoin,2023-06-07 11:43:15,question,Alternative read only paths for -blocksdir,"### Please describe the feature you'd like to see added.

The Blockchain is growing in size over the years. It would be nice to be able to store it accross multiple hard drives and specify additional blocksdirs that are meant for validation in full nodes. When the client writes a file, it is still written to the -blocksdir, always. When reading a file, it is first looked up in the -blocksdir. When it's not present there, it is looked up in the additional blocksdirs, in order as they are specified by the command line arguments. 

### Is your feature related to a problem, if so please describe it.

_No response_

### Describe the solution you'd like

_No response_

### Describe any alternatives you've considered

Combining multiple hard drives is possible using a RAID, but these are prone to failure and would force the user to format their drives, which is possibly something that the user doesn't want to do.

### Please leave any additional context

I think I would be able to implement this myself. However, I want to know if this feature is wanted or if I might have overlooked some existing possibility to achieve this and generally want to collect some feedback on this."
bitcoin/bitcoin,2023-05-31 11:03:32,question,"Indicate RBF replaceability, also after transactions have been confirmed ","### Please describe the feature you'd like to see added.

Wallet transactions retrieved with RPC should indicate RBF replaceability, also after they are confirmed. 

### Is your feature related to a problem, if so please describe it.

Currently, RBF replaceability is set to a hard 'no' if the transaction has any confirmations:

https://github.com/bitcoin/bitcoin/blob/30d6c7d8c0441956fac37252921c795569002d07/src/wallet/rpc/transactions.cpp#L46-L55

This makes it hard to figure out if this transaction has the RBF flag set. To determine this currently, the user would need to parse the raw transaction data and iterate over the transaction inputs, and inspect their sequence number. This is not something I think it's reasonable to expect people to do. 

### Describe the solution you'd like

I'd like the `bip125-replaceable` field to indicate the RBF flag of the transaction, also after it has received confirmations. 

### Describe any alternatives you've considered

_No response_

### Please leave any additional context

_No response_"
bitcoin/bitcoin,2023-05-20 11:38:09,question,Compute 'short id' when transaction joins mempool,"When a node receives a `cmpctblock` it has to verify to its check mempool in order to know whether it has all the required transactions to construct that block. If it doesn't, it will send `getblocktxn` to fetch the missing tx{s}. 

`PartiallyDownloadedBlock::InitData` shows we have to iterate the whole mempool in order to get the short id and do the verifications, see:
```cpp
for (size_t i = 0; i < pool->vTxHashes.size(); i++) {
    uint64_t shortid = cmpctblock.GetShortID(pool->vTxHashes[i].first);
    std::unordered_map<uint64_t, uint16_t>::iterator idit = shorttxids.find(shortid);
    if (idit != shorttxids.end()) {
        if (!have_txn[idit->second]) {
            txn_available[idit->second] = pool->vTxHashes[i].second->GetSharedTx();
            have_txn[idit->second]  = true;
            mempool_count++;
        } else {
            // If we find two mempool txn that match the short id, just request it.
            // This should be rare enough that the extra bandwidth doesn't matter,
            // but eating a round-trip due to FillBlock failure would be annoying
            if (txn_available[idit->second]) {
                txn_available[idit->second].reset();
                mempool_count--;
            }
        }
    }
    // Though ideally we'd continue scanning for the two-txn-match-shortid case,
    // the performance win of an early exit here is too good to pass up and worth
    // the extra risk.
    if (mempool_count == shorttxids.size())
        break;
}
```

This means that every time we receive a compact block we have to iterate the whole mempool and calculate the ""short id""s all over again. Couldn't`CTxMemPool` have a hashmap where we could store the transactions' short id right after joining the mempool and remove it once the tx gets confirmed/out of mempool?"
bitcoin/bitcoin,2023-05-16 21:33:42,question,Can't compile v24.0.1,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

I'm trying to install bitcoin core headless with wallet on Raspi4 with Raspi OS installed on a bootable SSD.
 
Berkeley DB 4.8 is installed as per this tutorial https://raspnode.com/diyBitcoin.html#swap

When I try to compile the 24.0.1 branch, i only get as far as the configure command here: 
When I enter (in the cloned bitcoin folder):

`./configure CPPFLAGS=""-I/usr/local/BerkeleyDB.4.8/include -O2"" LDFLAGS=""-L/usr/local/BerkeleyDB.4.8/lib""`

I get the following response:
```
checking for pkg-config... /usr/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
configure: error: cannot run /bin/bash build-aux/config.sub
```
Not sure why this is happening. When I look at the config file in the ""build-aux"" folder, it looks like the config file i updated previously when trying to solve a previous error with installing the BerkeleyDB.

I'm a bit out of my depth and would appreciate any help!

### Expected behaviour

Bitcoin core compiles correctly

### Steps to reproduce

Setup Raspi4 with Raspi OS installed on a bootable SSD
Install BDB 4.8
Clone bitcoin repo 24.0.1
configure 

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

v24.0.1

### Operating system and version

Raspberry OS (Raspnode)

### Machine specifications

Raspi 4B 8GB , 1TB SSD"
bitcoin/bitcoin,2023-05-02 09:29:59,question,Build broken when enabling fuzzing on Apple M1 hw using homebrew llvm.,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

I got same error in https://github.com/bitcoin/bitcoin/issues/24386


### Expected behaviour

shoule be able to compolice fuzzer

### Steps to reproduce


```bash
./configure --enable-fuzz --with-sanitizers=fuzzer,address,undefined --disable-asm CC=/opt/homebrew/opt/llvm/bin/clang CXX=/opt/homebrew/opt/llvm/bin/clang++

make
Making all in src
  CXXLD    test/fuzz/fuzz
Undefined symbols for architecture arm64:
  ""crc32c::ExtendArm64(unsigned int, unsigned char const*, unsigned long)"", referenced from:
      crc32c::Extend(unsigned int, unsigned char const*, unsigned long) in libcrc32c.a(libcrc32c_a-crc32c.o)
ld: symbol(s) not found for architecture arm64
clang-16: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [test/fuzz/fuzz] Error 1
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1
```

```
/opt/homebrew/opt/llvm/bin/clang --version
Homebrew clang version 16.0.2
Target: arm64-apple-darwin22.4.0
Thread model: posix
InstalledDir: /opt/homebrew/opt/llvm/bin
```



### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

v24.0.1

### Operating system and version

MacOS: 13.3.1 (22E261)

### Machine specifications

Chip Apple M1 Pro, 16GB memory"
bitcoin/bitcoin,2023-04-04 00:50:19,question,-fallbackfee should apply to estimatesmartfee,"### Please describe the feature you'd like to see added.

`-fallback` fee is used by `sendtoaddress` to provide a fallback fee when sending transactions. However, this does not apply to `estimatesmartfee`, which still returns the error.

### Is your feature related to a problem, if so please describe it.

The reason for this is because it's difficult to test client applications on regtest, that would use `estimatesmartfee` on mainnet, but because it requires a lot of transactions before `estimatesmartfee` to work, it's difficult. This related stack exchange question has had a bit of interest:
https://bitcoin.stackexchange.com/questions/89607/how-can-i-force-estimatesmartfee-to-return-estimates-on-regtest

### Describe the solution you'd like

See above.

### Describe any alternatives you've considered

- A separate flag would also work, like `fallbackestimatefee`

### Please leave any additional context

_No response_"
bitcoin/bitcoin,2023-03-28 14:06:07,question,index: ThreadSanitizer: data race on vptr ,"Seen with master @ 220008604f15d5078092dea28be3e3f7f11b6c8f on aarch64 (with `NO_BDB=1`). Follow up from #27298. See also #26188.

```bash
2023-03-28T13:52:47.882295Z (mocktime: 2020-08-31T15:34:12Z) [test] [dbwrapper.cpp:158] [CDBWrapper] Opened LevelDB successfully
2023-03-28T13:52:47.882390Z (mocktime: 2020-08-31T15:34:12Z) [test] [dbwrapper.cpp:183] [CDBWrapper] Using obfuscation key for /tmp/test_common_Bitcoin Core/0959fba80d599fe246b0b5ced04b4d1ecd504db9812edbff4ada2d88c6b218ae/regtest/indexes/txindex: 0000000000000000
2023-03-28T13:52:47.884002Z (mocktime: 2020-08-31T15:34:12Z) [txindex] [util/thread.cpp:20] [TraceThread] txindex thread start
test/txindex_tests.cpp(38): fatal error: in ""txindex_tests/txindex_initial_sync"": critical check time_start + timeout_ms > GetTimeMillis() has failed
LLVMSymbolizer: error reading file: No such file or directory
make[3]: *** [Makefile:21823: test/txindex_tests.cpp.test] Error 1
make[3]: Leaving directory '/home/fedora/bitcoin/ci/scratch/build/bitcoin-aarch64-unknown-linux-gnu/src'
make[2]: *** [Makefile:19828: check-am] Error 2
make[2]: Leaving directory '/home/fedora/bitcoin/ci/scratch/build/bitcoin-aarch64-unknown-linux-gnu/src'
make[1]: *** [Makefile:19493: check-recursive] Error 1
make[1]: Leaving directory '/home/fedora/bitcoin/ci/scratch/build/bitcoin-aarch64-unknown-linux-gnu/src'
make: *** [Makefile:816: check-recursive] Error 1
==================
WARNING: ThreadSanitizer: data race on vptr (ctor/dtor vs virtual call) (pid=24142)
  Write of size 8 at 0xfffffee67968 by main thread:
    #0 BaseIndex::~BaseIndex() src/index/base.cpp:80:1 (test_bitcoin+0xc54cc8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #1 TxIndex::~TxIndex() src/index/txindex.cpp:56:19 (test_bitcoin+0xc6daec) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #2 txindex_tests::txindex_initial_sync::test_method() src/test/txindex_tests.cpp:82:1 (test_bitcoin+0x7c281c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #3 txindex_tests::txindex_initial_sync_invoker() src/test/txindex_tests.cpp:17:1 (test_bitcoin+0x7c0f78) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #4 boost::detail::function::void_function_invoker0<void (*)(), void>::invoke(boost::detail::function::function_buffer&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:117:11 (test_bitcoin+0x2b78e8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #5 boost::function0<void>::operator()() const /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:763:14 (test_bitcoin+0x24f35c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #6 boost::detail::forward::operator()() /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1388:32 (test_bitcoin+0x24f35c)
    #7 boost::detail::function::function_obj_invoker0<boost::detail::forward, int>::invoke(boost::detail::function::function_buffer&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:137:18 (test_bitcoin+0x24f35c)
    #8 boost::function0<int>::operator()() const /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:763:14 (test_bitcoin+0x1e3384) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #9 int boost::detail::do_invoke<boost::shared_ptr<boost::detail::translator_holder_base>, boost::function<int ()>>(boost::shared_ptr<boost::detail::translator_holder_base> const&, boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:301:30 (test_bitcoin+0x1e3384)
    #10 boost::execution_monitor::catch_signals(boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:903:16 (test_bitcoin+0x1e3384)
    #11 boost::execution_monitor::execute(boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1301:16 (test_bitcoin+0x1e367c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #12 boost::execution_monitor::vexecute(boost::function<void ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1397:5 (test_bitcoin+0x1dcec0) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #13 boost::unit_test::unit_test_monitor_t::execute_and_translate(boost::function<void ()> const&, unsigned long) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_monitor.ipp:49:9 (test_bitcoin+0x1dcec0)
    #14 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:815:44 (test_bitcoin+0x20cdfc) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #15 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:784:58 (test_bitcoin+0x20d128) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #16 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:784:58 (test_bitcoin+0x20d128) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #17 boost::unit_test::framework::run(unsigned long, bool) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:1722:29 (test_bitcoin+0x1dbe64) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #18 boost::unit_test::unit_test_main(boost::unit_test::test_suite* (*)(int, char**), int, char**) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_main.ipp:250:9 (test_bitcoin+0x1f57c8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #19 main /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_main.ipp:306:12 (test_bitcoin+0x1f5d74) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)

  Previous read of size 8 at 0xfffffee67968 by thread T4:
    #0 BaseIndex::ThreadSync() src/index/base.cpp:217:18 (test_bitcoin+0xc55ba0) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #1 BaseIndex::Start()::$_0::operator()() const src/index/base.cpp:404:73 (test_bitcoin+0xc59324) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #2 decltype(std::declval<BaseIndex::Start()::$_0&>()()) std::__1::__invoke[abi:v160000]<BaseIndex::Start()::$_0&>(BaseIndex::Start()::$_0&) /usr/lib/llvm-16/bin/../include/c++/v1/__functional/invoke.h:394:23 (test_bitcoin+0xc59324)
    #3 void std::__1::__invoke_void_return_wrapper<void, true>::__call<BaseIndex::Start()::$_0&>(BaseIndex::Start()::$_0&) /usr/lib/llvm-16/bin/../include/c++/v1/__functional/invoke.h:487:9 (test_bitcoin+0xc59324)
    #4 std::__1::__function::__alloc_func<BaseIndex::Start()::$_0, std::__1::allocator<BaseIndex::Start()::$_0>, void ()>::operator()[abi:v160000]() /usr/lib/llvm-16/bin/../include/c++/v1/__functional/function.h:185:16 (test_bitcoin+0xc59324)
    #5 std::__1::__function::__func<BaseIndex::Start()::$_0, std::__1::allocator<BaseIndex::Start()::$_0>, void ()>::operator()() /usr/lib/llvm-16/bin/../include/c++/v1/__functional/function.h:356:12 (test_bitcoin+0xc59324)
    #6 std::__1::__function::__value_func<void ()>::operator()[abi:v160000]() const /usr/lib/llvm-16/bin/../include/c++/v1/__functional/function.h:510:16 (test_bitcoin+0x10cbd30) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #7 std::__1::function<void ()>::operator()() const /usr/lib/llvm-16/bin/../include/c++/v1/__functional/function.h:1156:12 (test_bitcoin+0x10cbd30)
    #8 util::TraceThread(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>) src/util/thread.cpp:21:9 (test_bitcoin+0x10cbd30)
    #9 decltype(std::declval<void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>)>()(std::declval<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>(), std::declval<BaseIndex::Start()::$_0>())) std::__1::__invoke[abi:v160000]<void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, BaseIndex::Start()::$_0>(void (*&&)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&&, BaseIndex::Start()::$_0&&) /usr/lib/llvm-16/bin/../include/c++/v1/__functional/invoke.h:394:23 (test_bitcoin+0xc58e70) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #10 void std::__1::__thread_execute[abi:v160000]<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, BaseIndex::Start()::$_0, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, BaseIndex::Start()::$_0>&, std::__1::__tuple_indices<2ul, 3ul>) /usr/lib/llvm-16/bin/../include/c++/v1/thread:282:5 (test_bitcoin+0xc58e70)
    #11 void* std::__1::__thread_proxy[abi:v160000]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, BaseIndex::Start()::$_0>>(void*) /usr/lib/llvm-16/bin/../include/c++/v1/thread:293:5 (test_bitcoin+0xc58e70)

  Location is stack of main thread.

  Location is global '??' at 0xfffffee4a000 ([stack]+0x1d968)

  Thread T4 'b-txindex' (tid=24147, running) created by main thread at:
    #0 pthread_create <null> (test_bitcoin+0x13776c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #1 std::__1::__libcpp_thread_create[abi:v160000](unsigned long*, void* (*)(void*), void*) /usr/lib/llvm-16/bin/../include/c++/v1/__threading_support:378:10 (test_bitcoin+0xc58b00) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #2 std::__1::thread::thread<void (*)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, BaseIndex::Start()::$_0, void>(void (*&&)(std::__1::basic_string_view<char, std::__1::char_traits<char>>, std::__1::function<void ()>), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&, BaseIndex::Start()::$_0&&) /usr/lib/llvm-16/bin/../include/c++/v1/thread:309:16 (test_bitcoin+0xc58b00)
    #3 BaseIndex::Start() src/index/base.cpp:404:21 (test_bitcoin+0xc58b00)
    #4 txindex_tests::txindex_initial_sync::test_method() src/test/txindex_tests.cpp:32:5 (test_bitcoin+0x7c18d8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #5 txindex_tests::txindex_initial_sync_invoker() src/test/txindex_tests.cpp:17:1 (test_bitcoin+0x7c0f78) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #6 boost::detail::function::void_function_invoker0<void (*)(), void>::invoke(boost::detail::function::function_buffer&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:117:11 (test_bitcoin+0x2b78e8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #7 boost::function0<void>::operator()() const /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:763:14 (test_bitcoin+0x24f35c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #8 boost::detail::forward::operator()() /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1388:32 (test_bitcoin+0x24f35c)
    #9 boost::detail::function::function_obj_invoker0<boost::detail::forward, int>::invoke(boost::detail::function::function_buffer&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:137:18 (test_bitcoin+0x24f35c)
    #10 boost::function0<int>::operator()() const /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/function/function_template.hpp:763:14 (test_bitcoin+0x1e3384) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #11 int boost::detail::do_invoke<boost::shared_ptr<boost::detail::translator_holder_base>, boost::function<int ()>>(boost::shared_ptr<boost::detail::translator_holder_base> const&, boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:301:30 (test_bitcoin+0x1e3384)
    #12 boost::execution_monitor::catch_signals(boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:903:16 (test_bitcoin+0x1e3384)
    #13 boost::execution_monitor::execute(boost::function<int ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1301:16 (test_bitcoin+0x1e367c) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #14 boost::execution_monitor::vexecute(boost::function<void ()> const&) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/execution_monitor.ipp:1397:5 (test_bitcoin+0x1dcec0) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #15 boost::unit_test::unit_test_monitor_t::execute_and_translate(boost::function<void ()> const&, unsigned long) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_monitor.ipp:49:9 (test_bitcoin+0x1dcec0)
    #16 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:815:44 (test_bitcoin+0x20cdfc) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #17 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:784:58 (test_bitcoin+0x20d128) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #18 boost::unit_test::framework::state::execute_test_tree(unsigned long, unsigned long, boost::unit_test::framework::state::random_generator_helper const*) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:784:58 (test_bitcoin+0x20d128) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #19 boost::unit_test::framework::run(unsigned long, bool) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/framework.ipp:1722:29 (test_bitcoin+0x1dbe64) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #20 boost::unit_test::unit_test_main(boost::unit_test::test_suite* (*)(int, char**), int, char**) /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_main.ipp:250:9 (test_bitcoin+0x1f57c8) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)
    #21 main /home/fedora/bitcoin/depends/aarch64-unknown-linux-gnu/include/boost/test/impl/unit_test_main.ipp:306:12 (test_bitcoin+0x1f5d74) (BuildId: 4e139378821749f690b2477a97607af03b8272e9)

SUMMARY: ThreadSanitizer: data race on vptr (ctor/dtor vs virtual call) src/index/base.cpp:80:1 in BaseIndex::~BaseIndex()
==================

real	30m28.818s
user	0m1.731s
sys	0m1.874s
```"
bitcoin/bitcoin,2023-02-24 15:35:28,question,"Bitcoind crashed, how to debug","OS: Debian GNU/Linux 11 (bullseye)
Available RAM: ~3GB

I run `bitcoind` on the machine by `~/bin/bitcoin-22.0/bin/bitcoind -daemon` as `root` (as this is a VM dedicated to `bitcoind`, no other ordinary users are added).
I make local RPC calls relatively frequently (up to 10k calls per sec) but only sequentially (i.e., the 2nd RPC call is only made if the first RPC call returned).

It usually works fine for a few days/weeks, then the process just disappears. The log is like the following (the 2nd part of the log is written by the new `bitcoind` process):

```
2023-02-23T07:21:49Z UpdateTip: new best=00000000000000000005260d63755c13802609649b30886a021edc108f724294 height=777918 version=0x2c8d2000 log2_work=94.020512 tx=807945114 date='2023-02-23T07:21:10Z' progress=1.000000 cache=125.0MiB(896432txo)
2023-02-23T07:24:36Z New outbound peer connected: version: 70016, blocks=777918, peer=386 (block-relay-only)
2023-02-23T07:26:04Z UpdateTip: new best=0000000000000000000194297149fa2e2a5a8003335ec1f05bd5826e49286203 height=777919 version=0x2ba92000 log2_work=94.020524 tx=807946053 date='2023-02-23T07:25:35Z' progress=1.000000 cache=125.2MiB(898284txo)
2023-02-23T07:26:26Z UpdateTip: new best=00000000000000000002bd51f8197b30b21799f42ac7bd305a5114c486bc6699 height=777920 version=0x2aaa0000 log2_work=94.020536 tx=807946317 date='2023-02-23T07:26:04Z' progress=1.000000 cache=125.3MiB(898554txo)
2023-02-23T07:27:32Z UpdateTip: new best=00000000000000000004d9729560ddab94371c2d357d9b6e9eb9bdcccd7ef6ba height=777921 version=0x20400000 log2_work=94.020548 tx=807946689 date='2023-02-23T07:27:06Z' progress=1.000000 cache=125.3MiB(898977txo)
2023-02-23T07:28:22Z New outbound peer connected: version: 70016, blocks=777921, peer=387 (block-relay-only)
2023-02-23T07:28:53Z UpdateTip: new best=000000000000000000043a08e446a1493a97472fe2f110d8479b7107584b8ec5 height=777922 version=0x2456e000 log2_work=94.020560 tx=807947060 date='2023-02-23T07:28:33Z' progress=1.000000 cache=125.4MiB(899783txo)
2023-02-23T07:33:13Z UpdateTip: new best=0000000000000000000559b6d6e4f6e0b023f989b9049762f40a3efe622832e1 height=777923 version=0x20084000 log2_work=94.020572 tx=807947934 date='2023-02-23T07:32:56Z' progress=1.000000 cache=126.0MiB(904538txo)
2023-02-23T07:45:29Z UpdateTip: new best=000000000000000000033d0f15e488c5205370c9ded96f3fdb40ac65238db6ad height=777924 version=0x2001c000 log2_work=94.020585 tx=807949972 date='2023-02-23T07:45:25Z' progress=1.000000 cache=126.8MiB(911094txo)
2023-02-23T07:46:02Z UpdateTip: new best=00000000000000000005cbd091c3a153f2bcd918f9d08306a76c0ca93c105f8b height=777925 version=0x20000000 log2_work=94.020597 tx=807950138 date='2023-02-23T07:45:46Z' progress=1.000000 cache=126.8MiB(910923txo)
2023-02-23T07:48:33Z UpdateTip: new best=0000000000000000000436a430222ee553d9898c8f7f113f337372f6df2d0bb2 height=777926 version=0x20e00000 log2_work=94.020609 tx=807950852 date='2023-02-23T07:48:18Z' progress=1.000000 cache=127.0MiB(912707txo)
2023-02-23T07:53:04Z New outbound peer connected: version: 70016, blocks=777926, peer=388 (block-relay-only)










2023-02-24T15:29:56Z Bitcoin Core version v22.0.0 (release build)
2023-02-24T15:29:56Z Assuming ancestors of block 00000000000000000008a89e854d57e5667df88f1cdef6fde2fbca1de5b639ad have valid signatures.
2023-02-24T15:29:56Z Setting nMinimumChainWork=00000000000000000000000000000000000000001fa4663bbbe19f82de910280
2023-02-24T15:29:56Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2023-02-24T15:29:56Z Using RdSeed as additional entropy source
2023-02-24T15:29:56Z Using RdRand as an additional entropy source
2023-02-24T15:29:56Z Default data directory /root/.bitcoin
2023-02-24T15:29:56Z Using data directory /mnt/bitcoin
2023-02-24T15:29:56Z Config file: /mnt/bitcoin/bitcoin.conf (not found, skipping)
2023-02-24T15:29:56Z Config file arg: datadir=""/mnt/bitcoin/""
2023-02-24T15:29:56Z Config file arg: rpcpassword=****
2023-02-24T15:29:56Z Config file arg: rpcuser=****
2023-02-24T15:29:56Z Config file arg: txindex=""1""
2023-02-24T15:29:56Z Config file arg: walletdir=""/mnt/bitcoin/wallets/ak_wallet/""
2023-02-24T15:29:56Z Command-line arg: daemon=""""
2023-02-24T15:29:56Z Using at most 125 automatic connections (1024 file descriptors available)
2023-02-24T15:29:56Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2023-02-24T15:29:56Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2023-02-24T15:29:56Z Script verification uses 1 additional threads
```

Is there anyway for me to get debug info etc which could hopefully give me some hints on what exactly happened?"
bitcoin/bitcoin,2023-02-23 16:05:53,question,configure: error: cannot figure out how to use std::filesystem,"Got this error from `configure` when trying to build from tag `v24.0.1` on Ubuntu 18.

At first my gcc was version `gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)` so I tried installing clang 10: `clang version 10.0.0-4ubuntu1~18.04.2` and linked to `CXX`, etc... still got the same error.

```
$ uname -a
Linux party 4.15.0-163-generic #171-Ubuntu SMP Fri Nov 5 11:55:11 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 18.04.4 LTS
Release:        18.04
Codename:       bionic
```

Complete output:

```
$ ./configure --without-gui --with-incompatible-bdb
checking for pkg-config... /usr/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether to enable maintainer-specific portions of Makefiles... yes
checking whether make supports nested variables... (cached) yes
checking whether the C++ compiler works... yes
checking for C++ compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether /usr/bin/clang++-10 accepts -g... yes
checking for style of include used by make... GNU
checking dependency style of /usr/bin/clang++-10... gcc3
checking whether /usr/bin/clang++-10 supports C++17 features with -std=c++17... yes
checking whether std::filesystem can be used without link library... no
checking whether std::filesystem needs -lstdc++fs... no
checking whether std::filesystem needs -lc++fs... configure: error: in `/root/bitcoin':
configure: error: cannot figure out how to use std::filesystem
See `config.log' for more details
```"
bitcoin/bitcoin,2023-02-16 16:41:59,question,Intermittent issue in p2p_ibd_stalling.py                                        self.wait_until(lambda: self.total_bytes_recv_for_blocks() == 172761),"```
wget https://drahtbot.space/temp_scratch/p2p_ibd_stalling_96.tar.xz
tar -xvf p2p_ibd_stalling_96.tar.xz
test/functional/combine_logs.py -c  ./p2p_ibd_stalling_96
```

```
 test  2023-02-14T20:54:45.479000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 134, in main
                                       self.run_test()
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/p2p_ibd_stalling.py"", line 83, in run_test
                                       self.wait_until(lambda: self.total_bytes_recv_for_blocks() == 172761)
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 732, in wait_until
                                       return wait_until_helper(test_function, timeout=timeout, timeout_factor=self.options.timeout_factor)
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 281, in wait_until_helper
                                       raise AssertionError(""Predicate {} not true after {} seconds"".format(predicate_source, timeout))
                                   AssertionError: Predicate ''''
                                           self.wait_until(lambda: self.total_bytes_recv_for_blocks() == 172761)
                                   ''' not true after 2400.0 seconds
"
bitcoin/bitcoin,2023-02-02 08:17:41,question,changing the path of the .bitcoin folder,"hi, could someone please advise me how to change the path to the .bitcoin folder or what command to enter when running ./bitcoind to create a name other than .bitcoin"
bitcoin/bitcoin,2023-01-30 01:46:19,question,No wallet command found,"Hi Bitcoiners <3,

So my Ras.Pi 4 8gbs is coming in 2 days and tried to install bitcoin core on desktop, to practice a little. I downloaded 24.99, newest version, compiled it from code here.... and there is no bitcoin-cli createwallet command.

I wanted to create descriptor and play a little with bitcoin.conf walletnotify command.

What did I do wrong? Do you have any recommendations. Did I compile it wrong (I read some configuration needs to be good) or is it the problem with this new version?

Thanks for any help.

EDIT: which version would you recommend me ?"
bitcoin/bitcoin,2022-12-28 01:29:02,question,Building or Loading the MS Visual Studio Project issue,"To build MsVS 2022, in the folder of source ""build_msvc"" cannot load or locate projects.
Common error for all project files :   bitcoin-tx\\bitcoin-tx.vcxproj : error  : Project """" does not contain any configuration.

![image](https://user-images.githubusercontent.com/121049274/209743097-0102eab2-9197-4fd1-88d3-a3c142c76dec.png)

"
bitcoin/bitcoin,2022-11-24 15:37:40,question,`28E72909F1717FE9607754F8A7BEB2621678D37D` key not specified in builder keys and `24.0` release signed with it,"I would expect each key that signs new release to be included in `contrib/builder-keys/keys.txt` - am I wrong? `24.0` is signed with `28E72909F1717FE9607754F8A7BEB2621678D37D` <vertion@protonmail.com> but this key is not included in `keys.txt`.

If above is not true, where does one find all the keys that sign new releases ?
"
bitcoin/bitcoin,2022-11-02 08:13:57,question,"[MSVC] Bitcoin build failed due to bitcoin\\src\\fs.h(75,39): error C2039: 'u8string': is not a member of 'std' with msvc option /c++latest","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->
Recently, we updated the commit of bitcoin for MSVC RWC testing, it failed to build due to the error like below with option /c++latest, could you please help take a look? Thanks.
```
bitcoin\\src\\fs.h(75,39): error C2039: 'u8string': is not a member of 'std' [F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj]
       C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\filesystem(39,1): message : see declaration of 'std' [F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj]
```

**Expected behavior**

<!--- What behavior did you expect? -->
Build successfully.

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->
```
bitcoin\\src\\fs.h(75,39): error C2039: 'u8string': is not a member of 'std' [F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj]
       C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\filesystem(39,1): message : see declaration of 'std' [F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj]
```
**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->
1. git clone https://github.com/bitcoin/bitcoin F:\\gitP\\bitcoin\\bitcoin
2. git -C ""F:\\gitP\\bitcoin\\bitcoin"" fetch --recurse-submodules=no --force
3. git -C ""F:\\gitP\\bitcoin\\bitcoin"" reset --hard 43e813cab266eef42e622519836f171f6a18d426
4. set _CL_= /std:c++latest 
5. cd F:\\gitP\\bitcoin\\bitcoin\\build_msvc
6. py -3 msvc-autogen.py
7. set _CL_=%_CL_% /D_HAS_DEPRECATED_ALLOCATOR_MEMBERS /D_SILENCE_CXX20_U8PATH_DEPRECATION_WARNING /Zc:char8_t- /D_SILENCE_CXX23_ALIGNED_STORAGE_DEPRECATION_WARNING
8. set the vcpkg info and run `bootstrap-vcpkg.bat` and` vcpkg integrate install`, in my environment, as follows:
    cd F:\\gitP\\bitcoin\\tools\\vcpkg
    git -C ""F:\\gitP\\bitcoin\\tools\\vcpkg"" clean -xdf 
    bootstrap-vcpkg.bat 
    set path=%cd%;%path%
    vcpkg integrate install 
9. switch to the directory: cd F:\\gitP\\bitcoin\\bitcoin\\build_msvc
10. msbuild /m /p:Platform=x64 /p:Configuration=Release /p:PlatformToolset=v142 bitcoin.sln /t:Rebuild

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
the commit of Bitcoin: 43e813cab266eef42e622519836f171f6a18d426
<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->
OS: Windows Server 2022 Datacenter 21H2
<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
Detailed log: 
[build.log](https://github.com/bitcoin/bitcoin/files/9917851/build.log)
"
bitcoin/bitcoin,2022-11-01 11:49:08,question,getblockchaininfo.verificationprogress never reaching 1.0,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

`getblockchaininfo.verificationprogress` is 1.0 when it is done syncing

<!--- What behavior did you expect? -->

**Actual behavior**

The highest value that `getblockchaininfo.verificationprogress` gets to is something like `0.9999976273867589`. As a result, we can not rely on this value to determine if syncing is complete. We can not rely on `getblockchaininfo.initialblockdownload` either due to https://github.com/bitcoin/bitcoin/issues/26432

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

Call `getblockchaininfo` on a synced node and observe that `verificationprogress` is < 1.0. 


<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

Version 22 of the bitcoin core binary, downloaded from the website
<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-10-28 01:33:39,question,The “Bitcoin peers implement a reputation-based protocol” are exists?,"When I reading the paper 《Bitcoin over Tor isn’t a good idea》founding some introudce about Bitcoin policy for anti-Dos。

In this paper，seeing that Bitcoin peers implement a reputation-based protocol with eachnode keeping a penalty score for every other Bitcoin peer(identified by its IP address). Whenever a malformed message is sent to the node, the latter increases the penalty score of the sender and bans the “misbehaving” IP address for 24 hours when the penalty reaches the value of 100.

This paper published by 2015. I want konw it are outdated at now？

Thanks bro！
"
bitcoin/bitcoin,2022-10-15 17:30:22,question,MacOS: make issue libsecp256k1_precomputed.a: No such file or directory,"<!-- Describe the issue -->
While building bitcoin core on macOS Monterey 12.6 an issue with libsecp256k1 occurred. While going through [the steps in the instruction ](https://github.com/bitcoin-core/secp256k1#build-steps)`make` could not proceed properly.

**Expected behavior**
A regular building.

**Actual behavior**
Output of `make` is following:

```
./libtool: line 5941: cd: /Users/*****/*****/*****: No such file or directory
./libtool: line 1887: cd: .libs/libsecp256k1.lax/libsecp256k1_precomputed.a: No such file or directory
make[3]: *** [libsecp256k1.la] Error 1
make[2]: *** [secp256k1/libsecp256k1.la] Error 2
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1
```

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->



<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**
macOS Monterey 12.6


<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

I also had this output after running the command `./configure` (maybe it could be useful too)

```
checking for pkg-config... /usr/local/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking build system type... x86_64-apple-darwin21.6.0
checking host system type... x86_64-apple-darwin21.6.0
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a race-free mkdir -p... ./build-aux/install-sh -c -d
checking for gawk... no
checking for mawk... no
checking for nawk... no
checking for awk... awk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether to enable maintainer-specific portions of Makefiles... yes
checking whether make supports nested variables... (cached) yes
checking for g++... g++
checking whether the C++ compiler works... yes
checking for C++ compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether the compiler supports GNU C++... yes
checking whether g++ accepts -g... yes
checking for g++ option to enable C++11 features... none needed
checking whether make supports the include directive... yes (GNU style)
checking dependency style of g++... gcc3
checking whether g++ supports C++17 features with -std=c++17... yes
checking whether std::filesystem can be used without link library... yes
checking whether the compiler supports GNU Objective C++... yes
checking whether g++ -std=c++17 accepts -g... yes
checking dependency style of g++ -std=c++17... gcc3
configure: WARNING: Libtool does not cope well with whitespace in `pwd`
checking how to print strings... printf
checking for gcc... gcc
checking whether the compiler supports GNU C... yes
checking whether gcc accepts -g... yes
checking for gcc option to enable C11 features... none needed
checking whether gcc understands -c and -o together... yes
checking dependency style of gcc... gcc3
checking for a sed that does not truncate output... /usr/local/bin/gsed
checking for grep that handles long lines and -e... /usr/bin/grep
checking for egrep... /usr/bin/grep -E
checking for fgrep... /usr/bin/grep -F
checking for ld used by gcc... /Library/Developer/CommandLineTools/usr/bin/ld
checking if the linker (/Library/Developer/CommandLineTools/usr/bin/ld) is GNU ld... no
checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B
checking the name lister (/usr/bin/nm -B) interface... BSD nm
checking whether ln -s works... yes
checking the maximum length of command line arguments... 786432
checking how to convert x86_64-apple-darwin21.6.0 file names to x86_64-apple-darwin21.6.0 format... func_convert_file_noop
checking how to convert x86_64-apple-darwin21.6.0 file names to toolchain format... func_convert_file_noop
checking for /Library/Developer/CommandLineTools/usr/bin/ld option to reload object files... -r
checking for file... file
checking for objdump... objdump
checking how to recognize dependent libraries... pass_all
checking for dlltool... no
checking how to associate runtime and link libraries... printf %s\\n
checking for ar... ar
checking for archiver @FILE support... no
checking for strip... strip
checking for ranlib... ranlib
checking command to parse /usr/bin/nm -B output from gcc object... ok
checking for sysroot... no
checking for a working dd... /bin/dd
checking how to truncate binary pipes... /bin/dd bs=4096 count=1
checking for mt... no
checking if : is a manifest tool... no
checking for dsymutil... dsymutil
checking for nmedit... nmedit
checking for lipo... lipo
checking for otool... otool
checking for otool64... no
checking for -single_module linker flag... yes
checking for -exported_symbols_list linker flag... yes
checking for -force_load linker flag... yes
checking for stdio.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for strings.h... yes
checking for sys/stat.h... yes
checking for sys/types.h... yes
checking for unistd.h... yes
checking for dlfcn.h... yes
checking for objdir... .libs
checking if gcc supports -fno-rtti -fno-exceptions... yes
checking for gcc option to produce PIC... -fno-common -DPIC
checking if gcc PIC flag -fno-common -DPIC works... yes
checking if gcc static flag -static works... no
checking if gcc supports -c -o file.o... yes
checking if gcc supports -c -o file.o... (cached) yes
checking whether the gcc linker (/Library/Developer/CommandLineTools/usr/bin/ld) supports shared libraries... yes
checking dynamic linker characteristics... darwin21.6.0 dyld
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... yes
checking how to run the C++ preprocessor... g++ -std=c++17 -E
checking for ld used by g++ -std=c++17... /Library/Developer/CommandLineTools/usr/bin/ld
checking if the linker (/Library/Developer/CommandLineTools/usr/bin/ld) is GNU ld... no
checking whether the g++ -std=c++17 linker (/Library/Developer/CommandLineTools/usr/bin/ld) supports shared libraries... yes
checking for g++ -std=c++17 option to produce PIC... -fno-common -DPIC
checking if g++ -std=c++17 PIC flag -fno-common -DPIC works... yes
checking if g++ -std=c++17 static flag -static works... no
checking if g++ -std=c++17 supports -c -o file.o... yes
checking if g++ -std=c++17 supports -c -o file.o... (cached) yes
checking whether the g++ -std=c++17 linker (/Library/Developer/CommandLineTools/usr/bin/ld) supports shared libraries... yes
checking dynamic linker characteristics... darwin21.6.0 dyld
checking how to hardcode library paths into programs... immediate
checking for ar... /usr/bin/ar
checking for gcov... /usr/bin/gcov
checking for llvm-cov... no
checking for lcov... no
checking for python3.6... no
checking for python3.7... /Users/*****/.pyenv/shims/python3.7
checking for genhtml... no
checking for git... /usr/bin/git
checking for ccache... no
checking for xgettext... /usr/local/bin/xgettext
checking for hexdump... /usr/bin/hexdump
checking for objcopy... no
checking for doxygen... no
checking whether C++ compiler accepts -Werror... yes
checking whether the linker accepts -Wl,-fatal_warnings... yes
checking whether C++ compiler accepts -Wall... yes
checking whether C++ compiler accepts -Wextra... yes
checking whether C++ compiler accepts -Wgnu... yes
checking whether C++ compiler accepts -Wformat -Wformat-security... yes
checking whether C++ compiler accepts -Wvla... yes
checking whether C++ compiler accepts -Wshadow-field... yes
checking whether C++ compiler accepts -Wthread-safety... yes
checking whether C++ compiler accepts -Wloop-analysis... yes
checking whether C++ compiler accepts -Wredundant-decls... yes
checking whether C++ compiler accepts -Wunused-member-function... yes
checking whether C++ compiler accepts -Wdate-time... yes
checking whether C++ compiler accepts -Wconditional-uninitialized... yes
checking whether C++ compiler accepts -Wduplicated-branches... no
checking whether C++ compiler accepts -Wduplicated-cond... no
checking whether C++ compiler accepts -Wlogical-op... no
checking whether C++ compiler accepts -Woverloaded-virtual... yes
checking whether C++ compiler accepts -Wsuggest-override... yes
checking whether C++ compiler accepts -Wunreachable-code-loop-increment... yes
checking whether C++ compiler accepts -Wimplicit-fallthrough... yes
checking whether C++ compiler accepts -Wunused-parameter... yes
checking whether C++ compiler accepts -Wself-assign... yes
checking whether C++ compiler accepts -Wdeprecated-copy... yes
checking whether C++ compiler accepts -fno-extended-identifiers... no
checking whether C++ compiler accepts -msse4.2... yes
checking whether C++ compiler accepts -msse4.1... yes
checking whether C++ compiler accepts -mavx -mavx2... yes
checking whether C++ compiler accepts -msse4 -msha... yes
checking whether C++ compiler accepts -mpclmul... yes
checking for SSE4.2 intrinsics... yes
checking for SSE4.1 intrinsics... yes
checking for AVX2 intrinsics... yes
checking for x86 SHA-NI intrinsics... yes
checking whether C++ compiler accepts -march=armv8-a+crc... no
checking whether C++ compiler accepts -march=armv8-a+crypto... no
checking for ARMv8 CRC32 intrinsics... no
checking for ARMv8 SHA-NI intrinsics... no
checking for brew... brew
checking whether the linker accepts -Wl,-headerpad_max_install_names... yes
checking whether byte ordering is bigendian... no
checking how to run the C preprocessor... gcc -E
checking whether gcc is Clang... yes
checking whether pthreads work with ""-pthread"" and ""-lpthread""... yes
checking whether Clang needs flag to prevent ""argument unused"" warning when linking with -pthread... no
checking for joinable pthread attribute... PTHREAD_CREATE_JOINABLE
checking whether more special flags are required for pthreads... no
checking for PTHREAD_PRIO_INHERIT... yes
checking whether std::atomic can be used without link library... yes
checking for special C compiler options needed for large files... no
checking for _FILE_OFFSET_BITS value needed for large files... no
checking for g++ -std=c++17 options needed to detect all undeclared functions... none needed
checking whether strerror_r is declared... yes
checking whether strerror_r returns char *... no
checking for library containing clock_gettime... none required
checking whether C++ compiler accepts -fPIC... yes
checking whether C++ compiler accepts -fstack-reuse=none... no
checking whether C++ compiler accepts -Wstack-protector... yes
checking whether C++ compiler accepts -fstack-protector-all... yes
checking whether C++ compiler accepts -fcf-protection=full... yes
checking whether C++ compiler accepts -fstack-clash-protection... no
checking whether C++ preprocessor accepts -D_FORTIFY_SOURCE=2... yes
checking whether C++ preprocessor accepts -U_FORTIFY_SOURCE... yes
checking whether the linker accepts -Wl,--enable-reloc-section... no
checking whether the linker accepts -Wl,--dynamicbase... no
checking whether the linker accepts -Wl,--nxcompat... no
checking whether the linker accepts -Wl,--high-entropy-va... no
checking whether the linker accepts -Wl,-z,relro... no
checking whether the linker accepts -Wl,-z,now... no
checking whether the linker accepts -Wl,-z,separate-code... no
checking whether the linker accepts -fPIE -pie... no
checking whether the linker accepts -Wl,-dead_strip... yes
checking whether the linker accepts -Wl,-dead_strip_dylibs... yes
checking whether the linker accepts -Wl,-bind_at_load... yes
checking for endian.h... no
checking for sys/endian.h... no
checking for byteswap.h... no
checking for unistd.h... (cached) yes
checking for sys/types.h... (cached) yes
checking for sys/stat.h... (cached) yes
checking for sys/select.h... yes
checking for sys/prctl.h... no
checking for sys/sysctl.h... yes
checking for vm/vm_param.h... no
checking for sys/vmmeter.h... yes
checking for sys/resources.h... no
checking whether getifaddrs is declared... yes
checking whether ifaddrs funcs can be used without link library... yes
checking whether freeifaddrs is declared... yes
checking whether ifaddrs funcs can be used without link library... yes
checking whether fork is declared... yes
checking whether setsid is declared... yes
checking whether pipe2 is declared... no
checking for timingsafe_bcmp... yes
checking whether le16toh is declared... no
checking whether le32toh is declared... no
checking whether le64toh is declared... no
checking whether htole16 is declared... no
checking whether htole32 is declared... no
checking whether htole64 is declared... no
checking whether be16toh is declared... no
checking whether be32toh is declared... no
checking whether be64toh is declared... no
checking whether htobe16 is declared... no
checking whether htobe32 is declared... no
checking whether htobe64 is declared... no
checking whether bswap_16 is declared... no
checking whether bswap_32 is declared... no
checking whether bswap_64 is declared... no
checking for __builtin_clzl... yes
checking for __builtin_clzll... yes
checking for getmemoryinfo... no
checking for mallopt M_ARENA_MAX... no
checking for posix_fallocate... no
checking for default visibility attribute... yes
checking for dllexport attribute... no
checking for thread_local support... yes
checking for gmtime_r... yes
checking for Linux getrandom syscall... no
checking for getentropy via random.h... yes
checking for sysctl... yes
checking for sysctl KERN_ARND... no
checking for if type char equals int8_t... no
checking for fdatasync... no
checking for F_FULLFSYNC... yes
checking for O_CLOEXEC... yes
checking for __builtin_prefetch... yes
checking for _mm_prefetch... yes
checking for strong getauxval support in the system headers... no
checking for std::system... yes
checking for ::_wsystem... no
checking for Qt5Core >= 5.11.3... yes
checking for Qt5Gui >= 5.11.3... yes
checking for Qt5Widgets >= 5.11.3... yes
checking for Qt5Network >= 5.11.3... yes
checking for Qt5Test >= 5.11.3... yes
checking for Qt5DBus >= 5.11.3... yes
checking for static Qt... no
checking whether -fPIE can be used with this Qt config... yes
checking for moc-qt5... no
checking for moc5... no
checking for moc... /usr/local/Cellar/qt@5/5.15.6/bin/moc
checking for uic-qt5... no
checking for uic5... no
checking for uic... /usr/local/Cellar/qt@5/5.15.6/bin/uic
checking for rcc-qt5... no
checking for rcc5... no
checking for rcc... /usr/local/Cellar/qt@5/5.15.6/bin/rcc
checking for lrelease-qt5... no
checking for lrelease5... no
checking for lrelease... /usr/local/Cellar/qt@5/5.15.6/bin/lrelease
checking for lupdate-qt5... no
checking for lupdate5... no
checking for lupdate... /usr/local/Cellar/qt@5/5.15.6/bin/lupdate
checking for lconvert-qt5... no
checking for lconvert5... no
checking for lconvert... /usr/local/Cellar/qt@5/5.15.6/bin/lconvert
checking whether the linker accepts -framework Foundation -framework AppKit... yes
checking whether to build Bitcoin Core GUI... yes (Qt5)
checking whether main function is needed for fuzz binary... checking whether the linker accepts ... no
yes
checking for __builtin_mul_overflow... yes
checking for sqlite3 >= 3.7.17... yes
checking whether to build wallet with support for sqlite... yes
checking whether Userspace, Statically Defined Tracing tracepoints are supported... no
checking for miniupnpc/miniupnpc.h... no
checking for miniupnpc/upnpcommands.h... no
checking for miniupnpc/upnperrors.h... no
checking for natpmp.h... no
checking for boostlib >= 1.64.0 (106400)... yes
checking whether Boost.Process can be used... yes
checking for seccomp-bpf (Linux x86-64)... no
checking for libevent >= 2.1.8... yes
checking for libevent_pthreads >= 2.1.8... yes
checking if evhttp_connection_get_peer expects const char**... no
checking for libqrencode... yes
checking for libzmq >= 4... yes
checking for libmultiprocess... no
checking whether to build bitcoind... yes
checking whether to build bitcoin-cli... yes
checking whether to build bitcoin-tx... yes
checking whether to build bitcoin-wallet... yes
checking whether to build bitcoin-util... yes
checking whether to build experimental bitcoin-chainstate... no
checking whether to build libraries... yes
checking if ccache should be used... no
checking if wallet should be enabled... yes
checking whether to build with support for UPnP... no
checking whether to build with support for NAT-PMP... no
checking whether to build GUI with support for D-Bus... yes
checking whether to build GUI with support for QR codes... yes
checking whether to build test_bitcoin-qt... yes
checking whether to build test_bitcoin... yes
checking whether to reduce exports... no
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating libbitcoinconsensus.pc
config.status: creating Makefile
config.status: creating src/Makefile
config.status: creating doc/man/Makefile
config.status: creating share/setup.nsi
config.status: creating share/qt/Info.plist
config.status: creating test/config.ini
config.status: creating contrib/devtools/split-debug.sh
config.status: creating src/config/bitcoin-config.h
config.status: src/config/bitcoin-config.h is unchanged
config.status: executing depfiles commands
config.status: executing libtool commands
=== configuring in src/secp256k1 (/Users/*****/*****/*****/*****/src/secp256k1)
configure: running /bin/sh ./configure --disable-option-checking '--prefix=/usr/local'  'PYTHONPATH=/Users/******/******/spark-3.3.0-bin-hadoop3s/python/lib/py4j-0.10.9.5-src.zip:/Users/******/******/spark-3.3.0-bin-hadoop3s/python/:' '--disable-shared' '--with-pic' '--enable-benchmark=no' '--enable-module-recovery' '--enable-module-schnorrsig' --cache-file=/dev/null --srcdir=.
checking build system type... x86_64-apple-darwin21.6.0
checking host system type... x86_64-apple-darwin21.6.0
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a race-free mkdir -p... ./build-aux/install-sh -c -d
checking for gawk... no
checking for mawk... no
checking for nawk... no
checking for awk... awk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether make supports nested variables... (cached) yes
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether the compiler supports GNU C... yes
checking whether gcc accepts -g... yes
checking for gcc option to enable C11 features... none needed
checking whether gcc understands -c and -o together... yes
checking whether make supports the include directive... yes (GNU style)
checking dependency style of gcc... gcc3
checking dependency style of gcc... gcc3
checking for ar... ar
checking the archiver (ar) interface... ar
configure: WARNING: Libtool does not cope well with whitespace in `pwd`
checking how to print strings... printf
checking for a sed that does not truncate output... /usr/local/bin/gsed
checking for grep that handles long lines and -e... /usr/bin/grep
checking for egrep... /usr/bin/grep -E
checking for fgrep... /usr/bin/grep -F
checking for ld used by gcc... /Library/Developer/CommandLineTools/usr/bin/ld
checking if the linker (/Library/Developer/CommandLineTools/usr/bin/ld) is GNU ld... no
checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B
checking the name lister (/usr/bin/nm -B) interface... BSD nm
checking whether ln -s works... yes
checking the maximum length of command line arguments... 786432
checking how to convert x86_64-apple-darwin21.6.0 file names to x86_64-apple-darwin21.6.0 format... func_convert_file_noop
checking how to convert x86_64-apple-darwin21.6.0 file names to toolchain format... func_convert_file_noop
checking for /Library/Developer/CommandLineTools/usr/bin/ld option to reload object files... -r
checking for file... file
checking for objdump... objdump
checking how to recognize dependent libraries... pass_all
checking for dlltool... no
checking how to associate runtime and link libraries... printf %s\\n
checking for archiver @FILE support... no
checking for strip... strip
checking for ranlib... ranlib
checking command to parse /usr/bin/nm -B output from gcc object... ok
checking for sysroot... no
checking for a working dd... /bin/dd
checking how to truncate binary pipes... /bin/dd bs=4096 count=1
checking for mt... no
checking if : is a manifest tool... no
checking for dsymutil... dsymutil
checking for nmedit... nmedit
checking for lipo... lipo
checking for otool... otool
checking for otool64... no
checking for -single_module linker flag... yes
checking for -exported_symbols_list linker flag... yes
checking for -force_load linker flag... yes
checking for stdio.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for strings.h... yes
checking for sys/stat.h... yes
checking for sys/types.h... yes
checking for unistd.h... yes
checking for dlfcn.h... yes
checking for objdir... .libs
checking if gcc supports -fno-rtti -fno-exceptions... yes
checking for gcc option to produce PIC... -fno-common -DPIC
checking if gcc PIC flag -fno-common -DPIC works... yes
checking if gcc static flag -static works... no
checking if gcc supports -c -o file.o... yes
checking if gcc supports -c -o file.o... (cached) yes
checking whether the gcc linker (/Library/Developer/CommandLineTools/usr/bin/ld) supports shared libraries... yes
checking dynamic linker characteristics... darwin21.6.0 dyld
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... no
checking whether to build static libraries... yes
checking for brew... brew
checking if gcc supports -Werror=unknown-warning-option... yes
checking if gcc supports -std=c89 -pedantic -Wno-long-long -Wnested-externs -Wshadow -Wstrict-prototypes -Wundef... yes
checking if gcc supports -Wno-overlength-strings... yes
checking if gcc supports -Wall... yes
checking if gcc supports -Wno-unused-function... yes
checking if gcc supports -Wextra... yes
checking if gcc supports -Wcast-align... yes
checking if gcc supports -Wcast-align=strict... no
checking if gcc supports -Wconditional-uninitialized... yes
checking if gcc supports -fvisibility=hidden... yes
checking for x86_64 assembly availability... yes
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating Makefile
config.status: creating libsecp256k1.pc
config.status: creating src/libsecp256k1-config.h
config.status: src/libsecp256k1-config.h is unchanged
config.status: executing depfiles commands
config.status: executing libtool commands

Build Options:
  with external callbacks = no
  with benchmarks         = no
  with tests              = yes
  with coverage           = no
  with examples           = no
  module ecdh             = no
  module recovery         = yes
  module extrakeys        = yes
  module schnorrsig       = yes

  asm                     = x86_64
  ecmult window size      = 15
  ecmult gen prec. bits   = 4

  valgrind                = no
  CC                      = gcc
  CPPFLAGS                =  
  SECP_CFLAGS             = -O2  -std=c89 -pedantic -Wno-long-long -Wnested-externs -Wshadow -Wstrict-prototypes -Wundef -Wno-overlength-strings -Wall -Wno-unused-function -Wextra -Wcast-align -Wconditional-uninitialized -fvisibility=hidden 
  CFLAGS                  = -g -O2
  LDFLAGS                 = 

Options used to compile and link:
  external signer = yes
  multiprocess    = no
  with experimental syscall sandbox support = no
  with libs       = yes
  with wallet     = yes
    with sqlite   = yes
    with bdb      = yes
  with gui / qt   = yes
    with qr       = yes
  with zmq        = yes
  with test       = yes
  with fuzz binary = yes
  with bench      = yes
  with upnp       = no
  with natpmp     = no
  use asm         = yes
  USDT tracing    = no
  sanitizers      = 
  debug enabled   = no
  gprof enabled   = no
  werror          = no
  LTO             = no

  target os       = darwin21.6.0
  build os        = darwin21.6.0

  CC              = gcc
  CFLAGS          = -pthread -g -O2
  CPPFLAGS        =   -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=2  -DHAVE_BUILD_INFO -DMAC_OSX -DOBJC_OLD_DISPATCH_PROTOTYPES=0 -DPROVIDE_FUZZ_MAIN_FUNCTION 
  CXX             = g++ -std=c++17
  CXXFLAGS        =    -Wstack-protector -fstack-protector-all -fcf-protection=full  -Wall -Wextra -Wgnu -Wformat -Wformat-security -Wvla -Wshadow-field -Wthread-safety -Wrange-loop-analysis -Wredundant-decls -Wunused-member-function -Wdate-time -Wconditional-uninitialized -Woverloaded-virtual -Wsuggest-override -Wunreachable-code-loop-increment -Wimplicit-fallthrough  -Wno-unused-parameter -Wno-self-assign -Wno-deprecated-copy    -g -O2
  LDFLAGS         =  -lpthread  -Wl,-bind_at_load   -Wl,-headerpad_max_install_names -Wl,-dead_strip -Wl,-dead_strip_dylibs 
  AR              = /usr/bin/ar
  ARFLAGS         = cr

```

Any help would be appreciated!
"
bitcoin/bitcoin,2022-10-09 13:55:12,question,why createwallet command not work in compiled bitcoin core ,"i compiled bitcoin core version 24.99 with BerkeleyDB 4.8.30
when i call ./bitcoin-cli createwallet mywallet .
it show method not found .
am i missing wallet support ?
why?"
bitcoin/bitcoin,2022-10-03 20:37:08,question,Error code -4. How to fix is?,"Hi there. 
I installed bitcoin core version v.24.99.0 from source code, then create test wallet:
`bitcoin-cli createwallet ""TestWallet""`
It worked, wallet created. But, when i trying using command dumpwallet, sethdseed ...etc it's not work.
I see error:
`error code: -4
error message:
Only legacy wallets are supported by this command`
Why i see this error, and how to fix it?"
bitcoin/bitcoin,2022-09-28 15:15:38,question,bitcoin-cli does'nt have any method. Error -32601,"I have installed bitcoin core running on my local machine. (Linux Ubuntu 20.04 LTS)
I want to create a new address,wallet  using bitcoin-cli, executing a command as follows then an error message was displayed.
How can I solve it?

bitcoin-cli createwallet(or etc...)
error code: -32601
error message:
Method not found

I try to use bitcoin-cli and RPC curl, but nothing "
bitcoin/bitcoin,2022-09-06 11:55:46,question,bitcoin-cli: Compiled without bdb support (required for legacy wallets),"I have installed Bitcoin Core following the official Github docs. (https://github.com/bitcoin/bitcoin/blob/master/doc/build-osx.md) 
I have the berkeley-db4 installed.

However, when I run this command 
`./bitcoin-cli -named createwallet wallet_name=alice descriptors=false passphrase=alice`

I get this error:

```
error code: -4
error message:
Compiled without bdb support (required for legacy wallets)
```

Can someone help me understand what is being missed here?"
bitcoin/bitcoin,2022-09-03 03:57:31,question,"[Arch] Bitcoin stalls randomly, pegs a thread until manually stopped","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

I was told to open an issue here. I've been having a really annoying issue with my bitcoin client that i cant find the solution to. Randomly, my bitcoin peer will start failing to send requests. At the same time, it pegs an entire thread, and it will stay that way until i intervene and restart it. I've tried everything, even using a different computer with the default config (other than `debug=1`). I can't seem to find anyone else having my issue, and nothing in my logs stands out to me. Something to note, when i did my test with the default config, i used bitcoin-qt to monitor, and it was still saying it was connected to 11 peers, although i couldnt add or remove any.

**Expected behavior**

I expect my bitcoin node to stay up without me needing to intervene.

**Actual behavior**

My node instead stalls and pegs a thread.

**To reproduce**

I can reproduce it every time i start my client, although it will randomly occur within an observed 4hr-48hr window.

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
Main peer:
- OS: EndeavourOS Linux x86_64 
- Host: MacBookAir7,2 1.0 
- Kernel: 5.19.1-zen1-1-zen 
- CPU: Intel i5-5350U @ 2.900GHz 
- Bitcoin Core v23.0.0 (self-compiled), used bitcoind
- WM: none (headless)

Clean test peer:
- OS: EndeavourOS Linux x86_64 
- Host: MacBookPro16,2 1.0
- Kernel: 5.18.16-arch1
- CPU: Intel i5-1038NG7 @ 3.800GHz
- Bitcoin Core v23.0.0 (downloaded from github releases), used bitcoin-qt
- WM: i3
- Graphical Shell: Kitty
<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

**Logs**
[Full logs for both machines are here.](https://mega.nz/folder/dBd2zSyD#aq70A9Ga0uzpQlwv4dTgRQ) 
"
bitcoin/bitcoin,2022-08-22 15:28:19,question,"lambda function in src/script/miniscript.h:571: asserts() on invalid input script (if compiled in debug mode), but does not return a valid string otherwise","Some possibly invalid input miniscripts may cause the miniscript evaluator to misbehave:
- when compiling in debug mode, at least we get an assertion failure (the program will crash and exit).
- when compiling in the release, that function may return some random std::string reference, causing UB.

See the lamda fucntion starting on src/script/miniscript.h:571.

Detected by gcc during compilation with -Wreturn-type. But as it is in a lambda, itself hidden in a template, it was hard to see at the location indicated (which is just the 1st line of the lambda missing some valid return).

So the final `assert(false);` is insufficient, it should at least be an exception that can be caught, or some valid string such as """" (depending on the expected semantics).
"
bitcoin/bitcoin,2022-08-20 04:20:18,question,Continued from closed #25864 ./configure and installing llvm,"Thanks to everyone and especially _fanquake_ for the help. Here we go... this is a bug continued from ./configure when installing `brew install llvm` i get a string of this::
```

==> llvm: stable 14.0.6, HEAD [keg-only]
Next-gen compiler infrastructure
https://llvm.org/
Not installed
From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/llvm.rb
License: Apache-2.0 with LLVM-exception
==> Dependencies
Build: cmake ✔, swig ✔
Required: python@3.10 ✔, libffi ✔
==> Options
--HEAD
	Install HEAD version
==> Caveats
To use the bundled libc++ please add the following LDFLAGS:
  LDFLAGS=""-L/usr/local/opt/llvm/lib -Wl,-rpath,/usr/local/opt/llvm/lib""

```
llvm is keg-only, which means it was not symlinked into /usr/local,
because macOS already provides this software and installing another version in
parallel can cause all kinds of trouble.


i feel like i am getting nowhere . still unable to configure the bitcoin/bitcoin git...... its heartwrenching, but I am trying diligently and patiently to fix my issues. is there something wrong with my mac itself. it is very old 2011..... over ten years old!! very outdated. running Mac OS 10.13.6

do i need to go ahead and use` brew info llvm `'s out put of /usr/local/opt/llvm/lib or what sense it is not symlinked... ?? and just use that LDFLAG as the path? it says it can't symlink? should i use the keg's path /usr/local/opt/llvm or the other, or will this cause problems. i will get this thing worked out eventually. just need a little nudge in the right direction one more time."
bitcoin/bitcoin,2022-08-09 07:40:18,question,Wallet file verification failed error,"Hello,
""bitcoin-cli createwallet adminwallet"" ran on ubuntu server and admin_wallet folder was created in ""~/.bitcoin/"" directory.
Then when I type ""bitcoin-cli loadwallet adminwallet"" I get this error:

error code: -4
error message:
Wallet file verification failed. SQLiteDatabase: Unable to obtain an exclusive lock on the database, is it being used by another instance of Bitcoin Core?

 `ps-aux | grep bitcoin

root 56945 116 16.0 4178224 1308340 ? Ssl Aug08 1205:58 bitcoind --daemon

root 61210 0.0 0.0 8168 724 pts/1 S+ 07:39 0:00 grep --color=auto bitcoin`"
bitcoin/bitcoin,2022-08-07 13:41:15,question,"IBD stalls permanently with Bitcoin core v23 after ""ignoring getheaders from peer=0 because node is in initial block download""","With Bitcoin core v23 (Qt versin on Windows, or with the daemon from commandline), the IBD process can stall indefinitely after receiving a large block:
```
2022-08-07T13:24:50Z [net.cpp:3043] [CNode] Added connection to 104.248.143.83:8333 peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending version (102 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:1146] [PushNodeVersion] send version message: version 70016, blocks=239242, them=104.248.143.83:8333, txrelay=1, peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: version (102 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending verack (0 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending getaddr (0 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2754] [ProcessMessage] receive version message: /Satoshi:0.20.1/: version 70015, blocks=748403, us=104.248.143.83:8333, txrelay=1, peer=0, peeraddr=104.248.143.83:8333
2022-08-07T13:24:50Z [timedata.cpp:57] [AddTimeData] added time data, samples 2, offset -1 (+0 minutes)
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: verack (0 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2797] [ProcessMessage] New outbound peer connected: version: 70015, blocks=748403, peer=0, peeraddr=104.248.143.83:8333 (manual)
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending sendheaders (0 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending sendcmpct (9 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending sendcmpct (9 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending ping (8 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:4650] [SendMessages] initial getheaders (319216) to peer=0 (startheight:748403)
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending getheaders (997 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending feefilter (8 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: sendheaders (0 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: sendcmpct (9 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: sendcmpct (9 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: ping (8 bytes) peer=0
2022-08-07T13:24:50Z [net.cpp:3060] [PushMessage] sending pong (8 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: getheaders (1029 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:3223] [ProcessMessage] Ignoring getheaders from peer=0 because node is in initial block download
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: feefilter (8 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:4051] [ProcessMessage] received: feefilter of 0.00001000 BTC/kvB from peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: pong (8 bytes) peer=0
2022-08-07T13:24:50Z [net_processing.cpp:2558] [ProcessMessage] received: headers (162003 bytes) peer=0
```
Then any other attemps to connect to other peers (or accept incoming connections) stalls permanently (we can waitfior hours: no network activity, no disk activity, but some thread uses 100% of a CPU core, probably in a tight loop generated by a deadlock and infinite attemps to recover from it).

If we try to shutdown, al most all databases thread are closed, the RPC services are closed, But the conenction manager remains locked for about ~15 minutes, not doing anything.

In fact the received headers are not even starting to be parse until AFTER these ~15 minutes. At which time we see that in logs:
(note: I've enabled ```logtimestamps=1```, ```logsourcelocations=1``` and ```logthreadnames=1``` in bitcoin.conf to have more traceable traces; and run it with ```debug=all```).

It seems that a lock is kept somewhere in ""net_processing.cpp"" after the ProcessMessage() exists with ""Ignoring getheaders from peer=0 because node is in initial block download""

Only when we shut down the node we see instantly:
```
2022-08-07T13:26:17Z [qt/bitcoin.cpp:207] [DebugMessageHandler] GUI: requestShutdown : Requesting shutdown
2022-08-07T13:26:17Z [rpc/server.cpp:302] [operator()] Interrupting RPC
2022-08-07T13:26:17Z [rpc/server.cpp:314] [operator()] Stopping RPC
2022-08-07T13:26:17Z [init.cpp:371] [OnRPCStopped] RPC stopped.
2022-08-07T13:26:17Z [qt/bitcoin.cpp:207] [DebugMessageHandler] GUI: Running Shutdown in thread
2022-08-07T13:26:17Z [httpserver.cpp:436] [InterruptHTTPServer] Interrupting HTTP server
2022-08-07T13:26:17Z [httprpc.cpp:312] [InterruptHTTPRPC] Interrupting HTTP RPC server
2022-08-07T13:26:17Z [init.cpp:205] [Shutdown] Shutdown: In progress...
2022-08-07T13:26:17Z [httprpc.cpp:317] [StopHTTPRPC] Stopping HTTP RPC server
2022-08-07T13:26:17Z [httpserver.cpp:657] [UnregisterHTTPHandler] Unregistering HTTP handler for / (exactmatch 1)
2022-08-07T13:26:17Z [httpserver.cpp:657] [UnregisterHTTPHandler] Unregistering HTTP handler for /wallet/ (exactmatch 0)
2022-08-07T13:26:17Z [httpserver.cpp:448] [StopHTTPServer] Stopping HTTP server
2022-08-07T13:26:17Z [httpserver.cpp:450] [StopHTTPServer] Waiting for HTTP worker threads to exit
2022-08-07T13:26:17Z [util/thread.cpp:19] [TraceThread] opencon thread exit
2022-08-07T13:26:17Z [util/thread.cpp:19] [TraceThread] dnsseed thread exit
2022-08-07T13:26:17Z [util/thread.cpp:19] [TraceThread] addcon thread exit
2022-08-07T13:26:17Z [httpserver.cpp:291] [ThreadHTTP] Exited http event loop
2022-08-07T13:26:17Z [httpserver.cpp:463] [StopHTTPServer] Waiting for HTTP event thread to exit
2022-08-07T13:26:17Z [httpserver.cpp:475] [StopHTTPServer] Stopped HTTP server
2022-08-07T13:26:17Z [mapport.cpp:210] [ProcessUpnp] UPNP_DeletePortMapping() returned: 0
2022-08-07T13:26:17Z [util/thread.cpp:19] [TraceThread] mapport thread exit
```
And then we wait for about 15-20 minutes to finally see this: the received block headers are processed, an attempt for sending another requests for headers is emitted, but all peers are finally disconnected (because we were shutting down), and all remaining data in moemory is flushed to disk and the node finally exits correctly.

```
2022-08-07T13:43:47Z [validation.cpp:3606] [ProcessNewBlockHeaders] Synchronizing blockheaders, height: 321216 (~43.64%)
2022-08-07T13:43:47Z [net_processing.cpp:2198] [ProcessHeadersMessage] more getheaders (321216) to end to peer=0 (startheight:748403)
2022-08-07T13:43:47Z [net.cpp:1262] [CreateNodeFromAcceptedSocket] connection from [2604:d500:4:1::2]:63949 accepted
2022-08-07T13:43:47Z [util/thread.cpp:19] [TraceThread] net thread exit
2022-08-07T13:43:47Z [net.cpp:3060] [PushMessage] sending getheaders (997 bytes) peer=0
2022-08-07T13:43:47Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=txindex, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:47Z [util/thread.cpp:19] [TraceThread] msghand thread exit
2022-08-07T13:43:47Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=txindex, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:47Z [util/thread.cpp:19] [TraceThread] txindex thread exit
2022-08-07T13:43:47Z [random.cpp:520] [SeedPeriodic] Feeding 1009016 bytes of dynamic environment data into RNG
2022-08-07T13:43:47Z [net.cpp:1846] [DumpAddresses] Flushed 6626 addresses to peers.dat  701ms
2022-08-07T13:43:47Z [net.cpp:567] [CloseSocketDisconnect] disconnecting peer=0
2022-08-07T13:43:47Z [net_processing.cpp:1282] [FinalizeNode] Cleared nodestate for peer=0
2022-08-07T13:43:47Z [net.cpp:567] [CloseSocketDisconnect] disconnecting peer=1
2022-08-07T13:43:47Z [net_processing.cpp:1282] [FinalizeNode] Cleared nodestate for peer=1
2022-08-07T13:43:47Z [net.cpp:1846] [DumpAddresses] Flushed 6626 addresses to peers.dat  15ms
2022-08-07T13:43:47Z [util/thread.cpp:19] [TraceThread] scheduler thread exit
2022-08-07T13:43:47Z [validation.cpp:4644] [DumpMempool] Writing 0 unbroadcast transactions to disk.
2022-08-07T13:43:47Z [validation.cpp:4654] [DumpMempool] Dumped mempool: 0s to copy, 0.036892s to dump
2022-08-07T13:43:47Z [policy/fees.cpp:998] [FlushUnconfirmed] Recorded 0 unconfirmed txs from mempool in 0s
2022-08-07T13:43:47Z [logging/timer.h:57] [Log] FlushStateToDisk: write block and undo data to disk started
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block and undo data to disk completed (38.02ms)
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block index to disk started
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=index, before=0.0MiB, after=0.3MiB
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block index to disk completed (36.98ms)
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write coins cache to disk (0 coins, 0kB) started
2022-08-07T13:43:48Z [txdb.cpp:161] [BatchWrite] Writing final batch of 0.00 MiB
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=chainstate, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:48Z [txdb.cpp:163] [BatchWrite] Committed 0 changed transaction outputs (out of 0) to coin database...
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write coins cache to disk (0 coins, 0kB) completed (0.00ms)
2022-08-07T13:43:48Z [validationinterface.cpp:243] [ChainStateFlushed] Enqueuing ChainStateFlushed: block hash=000000000000012a873661c59cd6301ace5a9108786284693eb0e35038713f4d
2022-08-07T13:43:48Z [validationinterface.cpp:243] [operator()] ChainStateFlushed: block hash=000000000000012a873661c59cd6301ace5a9108786284693eb0e35038713f4d
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=db, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=db, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block and undo data to disk started
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block and undo data to disk completed (31.27ms)
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block index to disk started
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=index, before=0.3MiB, after=0.3MiB
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write block index to disk completed (15.63ms)
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write coins cache to disk (0 coins, 0kB) started
2022-08-07T13:43:48Z [txdb.cpp:161] [BatchWrite] Writing final batch of 0.00 MiB
2022-08-07T13:43:48Z [dbwrapper.cpp:198] [WriteBatch] WriteBatch memory usage: db=chainstate, before=0.0MiB, after=0.0MiB
2022-08-07T13:43:48Z [txdb.cpp:163] [BatchWrite] Committed 0 changed transaction outputs (out of 0) to coin database...
2022-08-07T13:43:48Z [logging/timer.h:57] [Log] FlushStateToDisk: write coins cache to disk (0 coins, 0kB) completed (0.00ms)
2022-08-07T13:43:48Z [validationinterface.cpp:243] [ChainStateFlushed] Enqueuing ChainStateFlushed: block hash=000000000000012a873661c59cd6301ace5a9108786284693eb0e35038713f4d
2022-08-07T13:43:48Z [init.cpp:322] [Shutdown] Shutdown: done
2022-08-07T13:43:48Z [qt/bitcoin.cpp:207] [DebugMessageHandler] GUI: Shutdown finished
2022-08-07T13:43:48Z [qt/bitcoin.cpp:207] [DebugMessageHandler] GUI: ~InitExecutor : Stopping thread
2022-08-07T13:43:48Z [qt/bitcoin.cpp:207] [DebugMessageHandler] GUI: ~InitExecutor : Stopped thread
```

If we relaunch the node, it loads all data successfully, connects to a new peer=0 and requests another set of headers, then stalls again exactly as above, and we have to shutdown and wait 15-20 minutes exactly the same way as above. A single connection is made instantly to download a single nex set of headers, and then stalls forever. Other connections are added to the list (the UPnP discovery occurs, other peers are also discovered), but will never start. These extra connections are locked again until....

We shutdown the UI and then wait for 15-20 minutes when the RPC services are closed. These pending connections are then open and closed immediately and the UI closes.

Result: we have been able to download a single set of headers at each run (less than 1 minute to start and then seeing the lock after the 1st download of headers, just about 180KB, then no network or disk activity at all dor an infinite time, then shutdown and wait 15-20 minutes).

**Expected behavior**

IBD should not stall. It should also connect to other peers, and not just to the initial one (the QT interface dispays constantly: ""connecting to peers..."". 
**System information**

Bitcoin v23 for Windows (from https://bitcoincore.org/en/download/, extracted from the ZIP file, because the current EXE installer does not run on a Windows system enforcing ASLT)

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->
Windows 11 x64 (Intel Core i7), 32 GB RAM

"
bitcoin/bitcoin,2022-07-18 20:16:44,question,Cannot do HTTP JSON RPC  request on wallet,"I'm trying to figure out how to query specific wallets.

snippet of my config 
datadir=/data
wallet=/data/wallets/test1/
wallet=/data/wallets/test2/

```
view of the wallets
user@container:/data# ls /data/wallets/test1/
database  db.log  wallet.dat
user@container:/data# ls /data/wallets/test2/
database  db.log  wallet.dat

user@container:/data# bitcoin-cli -conf=bitcoin.conf listwallets
[
  ""/data/wallets/test1/"",
  ""/data/wallets/test2/""
]

no errors or issues via cli. the wallets are empty as expected as they were just created for testing
user@container:/data# bitcoin-cli  -conf=bitcoin.conf  -rpcwallet=/data/wallets/test1/ listunspent
[
]
```

when I try an rpc curl request on it, I get the following error message
`curl -X POST --data-binary '{""jsonrpc"": ""1.0"", ""id"":""curltest"", ""method"": ""listunspent"", ""params"": [6, 9999999, [] , true, { ""minimumAmount"": 0.005 } ] }' -H 'Content-Type: application/json' 'https:/mynodeaddress.com/wallet/data/wallets/test2`

`{""result"":null,""error"":{""code"":-18,""message"":""Requested wallet does not exist or is not loaded""},""id"":""curltest""}
`

**Note**
this works if I place the wallet in /data/test1. the curl request will be successful
if my config was 
wallet=test1
user@container:/data# ls /data/test1/
database  db.log  wallet.dat

`curl -X POST --data-binary '{""jsonrpc"": ""1.0"", ""id"":""curltest"", ""method"": ""listunspent"", ""params"": [6, 9999999, [] , true, { ""minimumAmount"": 0.005 } ] }' -H 'Content-Type: application/json' 'https:/mynodeaddress.com/wallet/test2`

it will return success fully return albeit an empty response similar to bitcoin-cli


My question is, is it possible to have the wallet file in any directory within my datadir and still be able run rpc query against it using curl.
"
bitcoin/bitcoin,2022-06-17 09:26:10,question,Stop the GPG verification madness,"**Is your feature request related to a problem? Please describe.**
It's becoming increasingly difficult to automate verification of the releases. For ""gpg --verify SHA256SUMS.asc SHA256SUMS"" to succeed, all the keys have to be imported. Some keys are not present on public keyservers and keyservers are anyway commonly considered as unreliable. Currently for v23, after importing 27 (!) keys (others are not available) the SHA256SUMS.asc verification still fails.

**Describe the solution you'd like**
Reduce the signer list to a set of well known, trusted signers and have their keys optionally signed by whoever verified their identity and is willing to sign.
Alternatively, provide one signature file PER signer, not a global file that always fails to pass all checks.
"
bitcoin/bitcoin,2022-06-06 08:35:05,question,Ubuntu 20.04 Server - Build Error,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**
Building bitcoin v.0.21
<!--- What behavior did you expect? -->

**Actual behavior**
`root@ubuntu-s-1vcpu-1gb-ams3-01:~/bitcoin# make

Making all in src
make[1]: Entering directory '/root/bitcoin/src'
make[2]: Entering directory '/root/bitcoin/src'
make[3]: Entering directory '/root/bitcoin'
make[3]: Leaving directory '/root/bitcoin'
  CXXLD    bitcoind
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#2}>::~thread_data()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#3}>::~thread_data()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::sp_counted_impl_p<boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#3}> >::dispose()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::sp_counted_impl_p<boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#2}> >::dispose()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread_group::interrupt_all()':
/usr/include/boost/thread/detail/thread_group.hpp:132: undefined reference to `boost::thread::interrupt()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread_group::join_all()':
/usr/include/boost/thread/detail/thread_group.hpp:118: undefined reference to `boost::thread::joinable() const'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::get_id() const':
/usr/include/boost/thread/detail/thread.hpp:716: undefined reference to `boost::thread::native_handle()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::join()':
/usr/include/boost/thread/detail/thread.hpp:740: undefined reference to `boost::thread::join_noexcept()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::start_thread()':
/usr/include/boost/thread/detail/thread.hpp:182: undefined reference to `boost::thread::start_thread_noexcept()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::thread_data_base::thread_data_base()':
/usr/include/boost/thread/pthread/thread_data.hpp:162: undefined reference to `vtable for boost::detail::thread_data_base'
/usr/bin/ld: /usr/include/boost/thread/pthread/thread_data.hpp:162: undefined reference to `vtable for boost::detail::thread_data_base'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::start_thread()':
/usr/include/boost/thread/detail/thread.hpp:182: undefined reference to `boost::thread::start_thread_noexcept()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#2}>::~thread_data()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::thread_data<AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*)::{lambda()#3}>::~thread_data()':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `AppInitMain(util::Ref const&, NodeContext&, interfaces::BlockAndHeaderTipInfo*) [clone .cold]':
/usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: /usr/include/boost/thread/detail/thread.hpp:94: undefined reference to `boost::detail::thread_data_base::~thread_data_base()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::~thread()':
/usr/include/boost/thread/detail/thread.hpp:257: undefined reference to `boost::thread::detach()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::detail::interruption_checker::interruption_checker(pthread_mutex_t*, pthread_cond_t*)':
/usr/include/boost/thread/pthread/thread_data.hpp:207: undefined reference to `boost::detail::get_current_thread_data()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::shared_mutex::lock()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::condition_variable::wait(boost::unique_lock<boost::mutex>&)':
/usr/include/boost/thread/pthread/condition_variable.hpp:88: undefined reference to `boost::this_thread::interruption_point()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::shared_mutex::lock()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: /usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::shared_mutex::lock_shared()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::condition_variable::wait(boost::unique_lock<boost::mutex>&)':
/usr/include/boost/thread/pthread/condition_variable.hpp:88: undefined reference to `boost::this_thread::interruption_point()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::shared_mutex::lock_shared()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: /usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o): in function `boost::thread::~thread()':
/usr/include/boost/thread/detail/thread.hpp:257: undefined reference to `boost::thread::detach()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o):(.data.rel.ro+0x98): undefined reference to `typeinfo for boost::detail::thread_data_base'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-init.o):(.data.rel.ro+0xb0): undefined reference to `typeinfo for boost::detail::thread_data_base'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock_shared()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::condition_variable::wait(boost::unique_lock<boost::mutex>&)':
/usr/include/boost/thread/pthread/condition_variable.hpp:88: undefined reference to `boost::this_thread::interruption_point()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock_shared()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::condition_variable::wait(boost::unique_lock<boost::mutex>&)':
/usr/include/boost/thread/pthread/condition_variable.hpp:88: undefined reference to `boost::this_thread::interruption_point()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock_shared()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:171: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-sigcache.o): in function `boost::shared_mutex::lock()':
/usr/include/boost/thread/pthread/shared_mutex.hpp:269: undefined reference to `boost::this_thread::disable_interruption::~disable_interruption()'
/usr/bin/ld: libbitcoin_server.a(libbitcoin_server_a-validation.o): in function `boost::condition_variable::wait(boost::unique_lock<boost::mutex>&)':
/usr/include/boost/thread/pthread/condition_variable.hpp:88: undefined reference to `boost::this_thread::interruption_point()'
collect2: error: ld returned 1 exit status
make[2]: *** [Makefile:8343: bitcoind] Error 1
make[2]: Leaving directory '/root/bitcoin/src'
make[1]: *** [Makefile:19347: all-recursive] Error 1
make[1]: Leaving directory '/root/bitcoin/src'
make: *** [Makefile:801: all-recursive] Error 1
`
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**
1. Go to the server hosting provider (e.g. digitalocean.com)
2. Create a dropplet (e.g. Ubuntu 20.04 server)
3. Clone bitcoin
4. Use the commands below
```
sudo apt install build-essential libtool autotools-dev automake pkg-config bsdmainutils python3
sudo apt install libevent-dev libboost-dev libboost-system-dev libboost-filesystem-dev libboost-test-dev
```
 
```
sudo apt install libsqlite3-dev
./contrib/install_db4.sh $(pwd)
export BDB_PREFIX=$(pwd)/db4
```
 
`./autogen.sh`
`./configure BDB_LIBS=""-L${BDB_PREFIX}/lib -ldb_cxx-4.8"" BDB_CFLAGS=""-I${BDB_PREFIX}/include""`

`make`
<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**
Linux ubuntu-s-1vcpu-1gb-ams3-01 5.4.0-113-generic #127-Ubuntu SMP Wed May 18 14:30:56 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   40 bits physical, 48 bits virtual
CPU(s):                          2
On-line CPU(s) list:             0,1
Thread(s) per core:              1
Core(s) per socket:              2
Socket(s):                       1
NUMA node(s):                    1
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           63
Model name:                      DO-Regular
Stepping:                        2
CPU MHz:                         2294.608
BogoMIPS:                        4589.21
Virtualization:                  VT-x
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       64 KiB
L1i cache:                       64 KiB
L2 cache:                        8 MiB
NUMA node0 CPU(s):               0,1
Vulnerability Itlb multihit:     KVM: Mitigation: Split huge pages
Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled
Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown
Vulnerability Meltdown:          Mitigation; PTI
Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling
Vulnerability Srbds:             Not affected
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx rdtscp lm constant_tsc arch_perfmon rep_good
                                  nopl cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf
                                 _lm abm cpuid_fault invpcid_single pti ssbd ibrs ibpb tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveop
                                 t md_clear

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-05-25 08:27:42,question,MakeHost error using WSL including pictures.,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->
I have followed the steps to compile bitcoin on Windows and I am receiving an error that is syntax in nature. This happened when I use this command in the terminal: make HOST=x86_64-w64-mingw32 in the depends folder. I will attach an image to clarify this issue.
**Expected behavior**

<!--- What behavior did you expect? -->
A complete compilation of Bitcoin. 
**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
I am using Bitcoin Core 23.0

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->
I am using an ASUS x555y latop and my OS is Windows 11

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. --
![New Bitcoin issue](https://user-images.githubusercontent.com/42873006/170217362-23730376-c97b-494b-87c3-9490886897f2.png)
>
"
bitcoin/bitcoin,2022-05-11 05:10:25,question,when can use tap root ? is there any document for tap root developer ?,when can use tap root ? is there any document for tap root developer ?
bitcoin/bitcoin,2022-04-28 17:45:28,question,"On node startup, load mempool from peers ?","Let's say I shutdown my bitcoin node for one hour, the default behaviour at the startup is to load the file mempool.dat but all transactions that were broadcasted during this outage will not appear.
I tried to disable the saving and loading with `persistmempool=0` but it starts with an empty mempool on startup.
How can I get the full list of unconfirmed transactions on a restart?"
bitcoin/bitcoin,2022-04-22 08:03:10,question,RPC `getblockfrompeer` returns an error with bitcoin-cli,"I encounter an error with new command `getblockfrompeer`.

```
bitcoin-cli getblockfrompeer ""000000000000034a7dedef4a161fa058a2d67a173a90155f3a2fe6fc132e0ebf"" 0
error code: -1
error message:
JSON value is not an integer as expected
 ```

It's working with curl:
```
curl -v --user myusername --data-binary '{""jsonrpc"": ""1.0"", ""id"":""curltest"", ""method"": ""getblockfrompeer"", ""params"": [""000000000000034a7dedef4a161fa058a2d67a173a90155f3a2fe6fc132e0ebf"", 0] }' -H 'content-type: text/plain;' http://127.0.0.1:8332/
```

I use the version bitcoin-core-23.0/test.rc5/bitcoin-23.0rc5-arm64-apple-darwin.dmg"
bitcoin/bitcoin,2022-04-04 20:42:37,question,non-mandatory-script-verify-flag (Witness program hash mismatch) when trying to spend Taproot output with 32x 0x00 tapbranch,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

i made a taproot output with an internal key, a tapscript in a leaf and a nonexistent tapscript with a SHA256 of 32x 0x00
spending it doesnt work, but when i change the hash of the nonexistent tapscript to 31x 0x00 + 0x01 is does work.
there is no reason why with 0 it wouldnt work as this theoretically is a valid SHA256 hash

**Expected behavior**

<!--- What behavior did you expect? -->

the transaction got accepted

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

it got not

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

100% reliable with https://github.com/antonilol/btc_stuff/blob/461391d6ecbba98d361bca60b0e5f7d17349494a/p2tr_tapscript.ts

if you dont have time to make one yourself here is my TX:
taproot output (this one is in the testnet blockchain [eed3cb854b1faaf3a342d2738b356ba4c176cb0c96e06a378b7ee47d8d68a12f](https://mempool.space/testnet/tx/eed3cb854b1faaf3a342d2738b356ba4c176cb0c96e06a378b7ee47d8d68a12f))
```
02000000000101eb9ed40b43fc676e2d33d6059338b75850611ea7ec7480a1cc030d7997cc7e730000000000feffffff021027000000000000225120c730e130570fb56367fb31d9ab9d492e38be719d31a0b4602fda344caa4b3d03fee468000000000016001494634ec1184c82d6a12b984f0e5efab9f6b053ec0247304402203ba26bbd89ddb641e19bab67773652ea09512ed7996a591d94d197109bf794ea0220375b5614eb4eb6a5f123aa5abce257f0273a9b70a9d87e6087378fc3fd452b68012102ca71026e93dc7183350a3c7af0c38ced30fea24dd230ccb2ffa365c55ec3e5da71762100
```
trying to spend:
```
02000000000101b294960b0f1c6d185882abab322dab58cba995b77fac80afc13b5595a0add7810000000000ffffffff017a2600000000000016001411b07a052ffdb815bb7eb017631b23d13eba5af904401a5f24e4d351e59eb64b213e5ce5dbe8e610eca7ea9128fd2f99bcc72eae122b6517a4a689122301dfa21f31c002cd3e55289be36be80bfd910e3f5649529f54401a5f24e4d351e59eb64b213e5ce5dbe8e610eca7ea9128fd2f99bcc72eae122b6517a4a689122301dfa21f31c002cd3e55289be36be80bfd910e3f5649529f54462042c68328c229ddef4f0588bd952e05eaca009bee43d01ff6bcbb0b681c15c894ac2042c68328c229ddef4f0588bd952e05eaca009bee43d01ff6bcbb0b681c15c894ba528741c15be1beaf62c286f2eae1b60ee1f1f6dcc9b96104ede0f09f1497c5bae82ea5aa000000000000000000000000000000000000000000000000000000000000000000000000
```

**System information**

What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)?

bitcoin.org binary v22

What type of machine are you observing the error on (OS/CPU and disk type)?

not needed, this is consensus related

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-03-20 06:55:38,question,main.cpp error during make. Ubuntu 16.04 with boost 1.77,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

Does anyone know how to address the rpcserver.cpp error during the make process? 

I am using Ubuntu 16.04 with the default boost 1.58.

```
  CXX      rpcserver.o
rpcserver.cpp:447:102: error: wrong number of template arguments (2, should be 1)
 static void RPCAcceptHandler(boost::shared_ptr< basic_socket_acceptor<Protocol, SocketAcceptorService> > acceptor,
                                                                                                      ^
In file included from /usr/local/include/boost/asio.hpp:30,
                 from rpcprotocol.h:15,
                 from rpcserver.h:10,
                 from rpcserver.cpp:6:
/usr/local/include/boost/asio/basic_socket_acceptor.hpp:73:7: note: provided for ‘template<class Protocol> class boost::asio::basic_socket_acceptor’
 class basic_socket_acceptor
       ^~~~~~~~~~~~~~~~~~~~~
rpcserver.cpp:447:104: error: template argument 1 is invalid
 static void RPCAcceptHandler(boost::shared_ptr< basic_socket_acceptor<Protocol, SocketAcceptorService> > acceptor,
                                                                                                        ^
rpcserver.cpp:457:95: error: wrong number of template arguments (2, should be 1)
 static void RPCListen(boost::shared_ptr< basic_socket_acceptor<Protocol, SocketAcceptorService> > acceptor,

```

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**
Ubuntu 16.04, default boost 1.58 installed.

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-01-21 07:53:41,question,Apple Silicon massive performance degredation in function tests / rpc (m1) ,"I discovered this significant slowdown when functional tests took substancially longer than expected on my new M1 macbook pro. (completes in ~250 seconds on my 1950x, and 3000 seconds on m1)

To validate this issue, I deployed two new regtest networks (both compiled locally), one on an Amd 1950x CPU running ubuntu 21.10, and one on a 16 in m1 macbook pro 10 core.

I found that the m1 macbook had substantially better results in benchmarks compared to x86, ~32% faster in the AssembleBlock benchmark for example. However, the functional tests were very slow, and RPC was very slow. It is my current believe that there is some issues in RPC handling / code that is very heavily slowing down the functional tests.

I compared some rpc calls in [m1](https://gist.github.com/PastaPastaPasta/faca79b604611de68c0898181448989c) and [x86](https://gist.github.com/PastaPastaPasta/e71cf0b8bafb3f4313c6ebbd45d2515f).

I found m1 rpc calls were around 10x slower.

~For example, getblockchaininfo took 0.5 seconds on m1, 0.003 seconds on x86.~ I've not been able to replicate this specific issue now.. Non-db / disk involved rpc calls seem mostly comparable between the two systems.
Generating 100 blocks took 23 seconds on m1, and 2.4 seconds on x86.

If someone has any ideas as to why this is so slow, or how to fix it, I am all ears!

Also, if someone on mac (non-m1) could replicate this test and post their information (so we can compare m1 mac to non-m1 mac) that'd be highly appreciated.

If other developers using m1 could share their experiences, that'd be helpful."
bitcoin/bitcoin,2022-01-09 08:41:15,question,Can't Unlock Bitcoin Wallet With Correct Password ,"I encrypted my wallet. used special characters in the password. And now the password doesn't work (exactly correct). Maybe the problem is in special characters?

the first character of the password is - !
and the body of the password contains - ~"
bitcoin/bitcoin,2022-01-01 12:15:51,question,Error while windows build.," root@geraniumd:~/bitcoin# make deploy 
CXXLD    bitcoind.exe
/usr/bin/x86_64-w64-mingw32-ld: /usr/lib/gcc/x86_64-w64-mingw32/9.3-posix/../../../../x86_64-w64-mingw32/lib/libmingw32.a(lib64_libmingw32_a-crt0_c.o): in function `main':
./build/x86_64-w64-mingw32-x86_64-w64-mingw32-crt/./mingw-w64-crt/crt/crt0_c.c:18: undefined reference to `WinMain'
collect2: error: ld returned 1 exit status
make[2]: *** [Makefile:5949: bitcoin.exe] Error 1
make[2]: Leaving directory '/root/bitcoin/src'
make[1]: *** [Makefile:16202: all-recursive] Error 1
make[1]: Leaving directory '/root/bitcoin/src'
make: *** [Makefile:821: all-recursive] Error 1
root@geraniumd:~/bitcoin# make dep                                  make: *** No rule to make target 'dep'.  Stop.


Having such error."
bitcoin/bitcoin,2021-12-15 13:14:56,question,`sendtoaddress` tries to use unspendable UTXOs and fails,"**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] -->
problem: having to wait for a confirmation before able to spend them with `sendtoaddress`

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->
an optional argument to enable it for one time (i agree with https://github.com/bitcoin/bitcoin/issues/3288 but i think you should be able to override it)

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->
i could `createrawtransaction` but i have a lot of UTXOs i want to bundle in one coin and it is easier to send the wallet balance to one address

**Additional context**
<!-- Add any other context or screenshots about the feature request here. -->
on the testnet btw
"
bitcoin/bitcoin,2021-11-24 15:53:25,question,The weak getauxval in CentOS 6,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

The `./configure` of v22.0 defines two macros: `HAVE_STRONG_GETAUXVAL` &amp; `HAVE_WEAK_GETAUXVAL` 
Judging by the code, the file sys/auxv.h to be included when one from these macros are defined.
The CentOS 6.* doesn't have a file **`sys/auxv.h`**

<!--- What behavior did you expect? -->

The configure at CentOS 6 should define macros as: 

```
HAVE_STRONG_GETAUXVAL='0'
HAVE_WEAK_GETAUXVAL='0'
```

And compiling without errors

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

The `./configure` should define macros as:

```
HAVE_STRONG_GETAUXVAL='0'
HAVE_WEAK_GETAUXVAL='1'
```

Errors of compilation:

```
randomenv.cpp:57:10: fatal error: sys/auxv.h: No such file or directory
   57 | #include <sys/auxv.h>
      |          ^~~~~~~~~~~~
```

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

/etc/centos-release: CentOS release 6.10 (Final)

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2021-10-20 02:03:03,question,About restore the wallet,"Hi~
I just have some questions about restoring a wallet. Most users had showed how to use Bitcoin core, but few focus on how to restore the wallet. I've read the book ""grokking Bitcoin"", where Kalle said that we should keep `wallet.dat` file safe. My try is as following: 

First I create a test wallet:
```
$ bitcoin-cli createwallet testwallet false false -passphrase=test111 false false true false
```

Next, I started to backup the wallet and kept it safe. This step seemed that the passphrase was not necessary.
```
$  bitcoin-cli -rpcwallet=testwallet backupwallet ~/testwallet.dat
```

Get an adress for someone who would send me some bitcoins:
```
$ bitcoin-cli -rpcwallet=""testwallet"" -named getnewaddress address_type=bech32
bc1q7y3uj...unpn9

# Get private key
$ bitcoin-cli -rpcwallet=""testwallet"" dumpprivkey ""bc1q7y3uj...unpn9""  
KzqZ3UoP...98K
```

Supposed that I've got the pay, but suddenly my server was exploded. I decided to restore the wallet in a new Linux server. After install `Bitcoin core` software completely, I created a new wallet:
```
$ bitcoin-cli createwallet testwallet2 false false -passphrase=test444 false false true false 
```

Unlock it for 120s:
```
$ bitcoin-cli -rpcwallet=testwallet2 walletpassphrase -passphrase=test444 120 
```

Restored the wallet:
```
$ bitcoin-cli -rpcwallet=testwallet2 importwallet <testwallet.dat> 
# without any error information
```

However, it seemed that the wallet could not access the private key
```
$ bitcoin-cli -rpcwallet=""testwallet2"" dumpprivkey ""bc1q7y3uj...unpn9""  
error code: -4
error message:
Private key for address bc1q7y3uj...unpn9 is not known
```

Here are some questions:
a. Was I do the right thing to restore the wallet?
b. If I was right, it means that anyone (hackers of my servers, for example) who own the `testwallet.dat` file could get all the bitcoins. In the `importwallet` step, I didn't need to use any passphrase (test111) of the previous wallet. It seemed not very safe, because my server may be easy to be cracked. So I guess I was wrong, but I have no idea how to do this. Any suggestions?

My `Bitcoin core` version is :
```
""version"": 220000,
""subversion"": ""/Satoshi:22.0.0/""
```

"
bitcoin/bitcoin,2021-10-14 08:42:06,question,bitcoincore update   linux (tgz) or arm linux,"I am running a node on a raspberry pi 4 model B 8gb with linux as os, should I update bitcoincore with linux (tgz) or arm linux.

Thanks
"
bitcoin/bitcoin,2021-08-27 07:32:01,question,How to mine continuously in segtest of version  bitcoin-0.20?,"Hello, I am just learning the bitcoin codes of version bitcoin-0.20 and I make it successfully.
Additionlly, I have only one PC. I want to mine continuously in segtest ,however I only found methods below:

```
== Generating ==
generatetoaddress nblocks ""address"" ( maxtries )
generatetodescriptor num_blocks ""descriptor"" ( maxtries )
```
But these methods can't  mine continuously. Hence would you please tell my how to mine continuously in segtest of version  bitcoin-0.20?

Thank you very much."
bitcoin/bitcoin,2021-08-19 09:03:18,question,Script verification threads are idle during IBD,"System: ODROID-HC1, ARM 32-bit, Armbian 21.05.8 Focal
Client [v22.0rc2](https://bitcoincore.org/bin/bitcoin-core-22.0/test.rc2/arm-linux-gnueabihf/bitcoin-22.0rc2-arm-linux-gnueabihf.tar.gz):
```
$ bitcoind -version
Bitcoin Core version v22.0.0-g873fbc745d037ad43570f81e58334c397bc95477
```

Started IBD from scratch (with `txindex=1`), now synced up to height ~510000.

According to data provided by `htop` tool, all additional script verification threads were idle since IBD started:

![Screenshot from 2021-08-19 12-01-05](https://user-images.githubusercontent.com/32963518/130040671-af0dd7f3-1ab1-4fbe-8157-c54ee5a15e70.png)

Is such behavior expected?"
bitcoin/bitcoin,2021-08-09 09:16:04,question,Is it possible to disable bind on onion network listening port?,"This may also be a bug depending on understanding of `listenonion` option.

**What is happening:** 
We have upgraded bitcoin node from v0.19 to v0.21, bitcoind tries to bind to port 8334 (tor listening), which we use for another service.

**Possible solution:**
Move the port using `bind` option to some unused port.

**Preferred solution:**
Disable listening on this particular port. 

I have tried using `listenonion=0` but the only effect is that the tor service is not started, the port is however still used. Can binding of this port be disabled? And is `listenonion` working correctly?"
bitcoin/bitcoin,2021-07-17 04:36:23,question,[Question]how can i use ip:host not localhost:port in RPC?,"I want to use ip(LAN IP) in RPC, not 127.0.0.1 or localhost, but i try to change the configuration of conf with [this](https://github.com/bitcoin/bitcoin/blob/853ac47705c8abd9b8bda2da59eb043d9591491c/share/examples/bitcoin.conf) , but it didn't work

I have been try to close firewall, still not work, and try to add more rpcallowip one by one

my system is ubuntu 20.04

my conf now is:
```
server=1
testnet=1
rpcuser=test
rpcpassword=123456
rpcallowip=0.0.0.0
rpcallowip=127.0.0.1
rpcallowip=192.168.2.36
rpcallowip=192.168.2.36/255.255.255.0
```

command report:
```
2021/07/17 12:33:23 Post ""http://192.168.2.36:18332"": dial tcp 192.168.2.36:18332: connect: connection refused
```

(it work in localhost and 127.0.0.1)
what should i do?"
bitcoin/bitcoin,2021-07-10 18:32:38,question,Increase trust through Wallet Aliases and Transfer Confirmations,"**Is your feature request related to a problem? Please describe.**

Users still fear doing bitcoin transactions, scared that something might go wrong. This fear is based on 2 closely-related issues:
- First: ""Is the account I am transferring to the correct one? Maybe I didn't copy it correctly, maybe I still have a previous address in my clipboard?
- Second: Users only know 100% that they transferred correctly AFTER the transaction has gone through.

**Describe the solution you'd like**
I think this could be greatly improved by implementing 2 solutions:

- Help users know where they are transferring to: Allow users to give their wallet's an Alias. Something like [ MyAmazingAlias ]. Then, when someone is preparing a transactions, after he fills out the wallet address, it could display:
`""transferring to id: [xxxxxxx] label: [ MyAmazingAlias ], confirm?`

- Allow the receiver to confirm his wallet BEFORE the transfer-confirmation: When the destination address and ammount are already filled out, already display the transaction in the receiver's wallet, but with the status ""0 confirmations"" or ""Awaiting confirmation"". This way, a user can ask ""Did the transaction arrive at your wallet?"" and only confirm after hearing from the receiver (but in this mode, the fee would probably have be paid, at least in part, even if the transaction is canceled).

Solutions but that would make people feel MUCH safer when using cryptocurrencies (even safer then when doing bank-transfers, that don't offer this), and would help with adoption not only for new users, but even for users that might still not use bitcoin for purchases, out of fear of doing something wrong at some point.

**Describe alternatives you've considered**
Implementing just the alias system would already solve most of this

**Additional context**

The chance of typing a wrong wallet address that works is very low, but still, users feel very uneasy when having to transfer funds through bitcoin, and questions about this can be found in most forums about bitcoin. Coming up with a solution for this issue could greatly increase trust and make people relax more when using bitcoin, which I believe is important for greater adoption.

And no one is perfect, people are bound to making mistakes, and bitcoin should not be the network where one day a story surfaces saying ""...transferred 100million dollars to the wrong address and no way of recuperating them."""
bitcoin/bitcoin,2021-06-30 11:58:04,question,"RPC usage for ""logging"" unclear","It's not clear how to enable/include or disable/exclude debug settings from within the console. The usage is not clear.

For example, I have tried

logging exclude mempool
logging exclude [ mempool ]
logging exclude [ ""mempool"" ]
logging exclude [\\""mempool\\""]
logging exclude ""[\\\\""mempool\\\\""]""

none of them work!
"
bitcoin/bitcoin,2021-06-23 12:20:35,question,Unable to link with libc++-8 (undefined reference to symbol '_ZNSt18condition_variable10notify_oneEv@@GLIBCXX_3.4.11'),"Steps to reproduce on current master (567670bec5ecf9bc252e91370382be53fd81ccee):


```
# ./configure CC=clang-8 CXX=""clang++-8 -stdlib=libc++""
...


# make V=1
Making all in src
make[1]: Entering directory '/bitcoin/src'
make[2]: Entering directory '/bitcoin/src'
make[3]: Entering directory '/bitcoin'
make[3]: Leaving directory '/bitcoin'
/bin/bash ../libtool  --tag=CXX --preserve-dup-deps  --mode=link /usr/bin/ccache clang++-8 -stdlib=libc++ -std=c++17 -fdebug-prefix-map=/bitcoin/src=. -Wstack-protector -fstack-protector-all -fcf-protection=full -Wall -Wextra -Wgnu -Wformat -Wformat-security -Wvla -Wshadow-field -Wswitch -Wthread-safety -Wrange-loop-analysis -Wredundant-decls -Wunused-variable -Wunused-member-function -Wdate-time -Wconditional-uninitialized -Wsign-compare -Woverloaded-virtual -Wunreachable-code-loop-increment -Wno-unused-parameter -Wno-self-assign -Wno-unused-local-typedef -Wno-implicit-fallthrough    -fPIE -g -O2   -Wl,-z,relro -Wl,-z,now -Wl,-z,separate-code -pie     -pthread -lpthread  -o bitcoind bitcoind-bitcoind.o  init/bitcoind-bitcoind.o libbitcoin_server.a libbitcoin_wallet.a libbitcoin_common.a libbitcoin_util.a univalue/libunivalue.la  libbitcoin_consensus.a crypto/libbitcoin_crypto_base.a crypto/libbitcoin_crypto_sse41.a crypto/libbitcoin_crypto_avx2.a crypto/libbitcoin_crypto_shani.a leveldb/libleveldb.a crc32c/libcrc32c.a crc32c/libcrc32c_sse42.a   leveldb/libmemenv.a secp256k1/libsecp256k1.la -L/usr/lib/x86_64-linux-gnu -lboost_system -lboost_filesystem -ldb_cxx   -levent_pthreads -levent -levent  -lsqlite3 
libtool: link: /usr/bin/ccache clang++-8 -stdlib=libc++ -std=c++17 -fdebug-prefix-map=/bitcoin/src=. -Wstack-protector -fstack-protector-all -fcf-protection=full -Wall -Wextra -Wgnu -Wformat -Wformat-security -Wvla -Wshadow-field -Wswitch -Wthread-safety -Wrange-loop-analysis -Wredundant-decls -Wunused-variable -Wunused-member-function -Wdate-time -Wconditional-uninitialized -Wsign-compare -Woverloaded-virtual -Wunreachable-code-loop-increment -Wno-unused-parameter -Wno-self-assign -Wno-unused-local-typedef -Wno-implicit-fallthrough -fPIE -g -O2 -Wl,-z -Wl,relro -Wl,-z -Wl,now -Wl,-z -Wl,separate-code -pie -pthread -o bitcoind bitcoind-bitcoind.o init/bitcoind-bitcoind.o  -lpthread libbitcoin_server.a libbitcoin_wallet.a libbitcoin_common.a libbitcoin_util.a univalue/.libs/libunivalue.a libbitcoin_consensus.a crypto/libbitcoin_crypto_base.a crypto/libbitcoin_crypto_sse41.a crypto/libbitcoin_crypto_avx2.a crypto/libbitcoin_crypto_shani.a leveldb/libleveldb.a crc32c/libcrc32c.a crc32c/libcrc32c_sse42.a leveldb/libmemenv.a secp256k1/.libs/libsecp256k1.a -L/usr/lib/x86_64-linux-gnu -lboost_system -lboost_filesystem -ldb_cxx -levent_pthreads -levent -levent /usr/lib/x86_64-linux-gnu/libsqlite3.so -pthread
/usr/bin/ld: leveldb/libleveldb.a(leveldb_libleveldb_a-env_posix.o): undefined reference to symbol '_ZNSt18condition_variable10notify_oneEv@@GLIBCXX_3.4.11'
//usr/lib/x86_64-linux-gnu/libstdc++.so.6: error adding symbols: DSO missing from command line
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Makefile:5271: recipe for target 'bitcoind' failed
make[2]: *** [bitcoind] Error 1
make[2]: Leaving directory '/bitcoin/src'
Makefile:15489: recipe for target 'all-recursive' failed
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory '/bitcoin/src'
Makefile:820: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1
```


<!--

7 -> 8

export DEBIAN_FRONTEND=noninteractive && apt update && apt install curl wget htop git vim ccache -y && git clone https://github.com/bitcoin/bitcoin.git && cd bitcoin && git checkout master && apt install build-essential libtool autotools-dev automake pkg-config bsdmainutils python3-zmq     libevent-dev libboost-system-dev libboost-filesystem-dev libboost-test-dev libboost-thread-dev  libsqlite3-dev  libdb++-dev  -y   &&  ./autogen.sh && apt install libqt5gui5 libqt5core5a libqt5dbus5 qttools5-dev qttools5-dev-tools -y && apt install clang-7 llvm-7 libc++abi-7-dev libc++-7-dev -y && ./configure CC=clang-7 CXX=""clang++-7 -stdlib=libc++""  --with-incompatible-bdb && make -j $(nproc) src/bitcoind && make -j 9 "
bitcoin/bitcoin,2021-06-16 13:26:11,question,how to support Native SegWit (Bech32) address ,"I run the bitcoin node version is 0.17.1, and now we want to support the Native SegWit (Bech32) format, how do I need to upgrade the node or redeploy a new node, or what method can I use to convert the Nested SegWit (P2SH) format to Bech32 address"
bitcoin/bitcoin,2021-05-13 06:34:07,question,Compiling macOS environment on Ubuntu 18.04,"I have an issue compiling the macOS environment for Bitcoin 0.21 and Bitcoin 0.21.1 on Ubuntu 18.04.

I installed the following dependencies:

sudo apt-get install curl librsvg2-bin libtiff-tools bsdmainutils cmake imagemagick libcap-dev libz-dev libbz2-dev python3-setuptools libtinfo5
sudo apt-get install make automake cmake curl g++-multilib libtool binutils-gold bsdmainutils pkg-config python3 patch

The following commands where executed to compile the macOS environment:

wget https://github.com/bitcoin/bitcoin/archive/refs/tags/v0.21.0.tar.gz
tar -xzvf v0.21.0.tar.gz
cd bitcoin-0.21.0
mkdir -p depends/sdk-sources
mkdir -p depends/SDKs
curl https://bitcoincore.org/depends-sources/sdks/MacOSX10.14.sdk.tar.gz -o depends/sdk-sources/MacOSX10.14.sdk.tar.gz
tar -C depends/SDKs -xf depends/sdk-sources/MacOSX10.14.sdk.tar.gz
cd depends
make HOST=x86_64-apple-darwin16

The compiling results in multiple errors regarding ""./boost/config/detail/select_stdlib_config.hpp:18:12: fatal error: 'cstddef' file not found"", see attached txt file for the complete compiling output.

Compiling output: [output.txt](https://github.com/bitcoin/bitcoin/files/6470698/output.txt)

"
bitcoin/bitcoin,2021-04-18 17:16:42,question,Fatal LevelDB error: Corruption: block checksum mismatch,"Had this problem with one long running node. Tried to restart bitcoind with `-debug=leveldb -reindex`, ended up with:
```
2021-04-18T15:00:03Z UpdateTip: new best=0000000000000000001153fc4f4aee341eacc92b28450dfff90810b4d2df216e height=616461 version=0x2fffe000 log2_work=91.642167 tx=500946903 date='2020-02-08T04:25:07Z' progress=0.799912 cache=147.0MiB(649296txo)
2021-04-18T15:00:03Z leveldb: Generated table #215595: 44038 keys, 2177932 bytes
2021-04-18T15:00:04Z leveldb: Generated table #215596: 44998 keys, 2180094 bytes
2021-04-18T15:00:04Z leveldb: Generated table #215597: 23369 keys, 1204615 bytes
2021-04-18T15:00:04Z leveldb: Compacted 1@1 + 2@2 files => 5562641 bytes
2021-04-18T15:00:04Z leveldb: compacted to: files[ 0 10 124 455 1486 0 0 ]
2021-04-18T15:00:04Z leveldb: Delete type=2 #215401
2021-04-18T15:00:04Z leveldb: Delete type=2 #215292
2021-04-18T15:00:04Z leveldb: Delete type=2 #215579
2021-04-18T15:00:04Z leveldb: Delete type=2 #215580
2021-04-18T15:00:04Z leveldb: Compacting 1@2 + 0@3 files
2021-04-18T15:00:04Z LevelDB read failure: Corruption: block checksum mismatch
2021-04-18T15:00:04Z Fatal LevelDB error: Corruption: block checksum mismatch
2021-04-18T15:00:04Z You can use -debug=leveldb to get more complete diagnostic messages
2021-04-18T15:00:05Z leveldb: Generated table #215598: 28652 keys, 1426450 bytes
2021-04-18T15:00:05Z Error: Error reading from database, shutting down.
2021-04-18T15:00:05Z Error reading from database: Fatal LevelDB error: Corruption: block checksum mismatch
```

Any hints how to debug this?

```
# bitcoind -version
Bitcoin Core version v0.20.1.0-gentoo
```"
bitcoin/bitcoin,2021-04-01 22:02:04,question,ERROR: AcceptBlockHeader: block is marked invalid,"Compiled the latest git version (28.3.2021, Revision: c00852653f2bf9cd3ee53ab05d574fe4a9ff6dcc) with msvc 16.9 2019 of bitcoind (32bit) and get the following error:

> bitcoind -txindex=1
> ERROR: AcceptBlockHeader: block 000000000000000002bff65a2701c0b7f82d2d9c0a69bec94ca45f3fc4fdf4b6 is marked invalid

also getting:

> bitcoin-cli reconsiderblock
> ERROR: ConnectBlock: Consensus::CheckTxInputs: cb33f844ef77a7282389c515df3831d13771851a4cf2b63e5be406eebc578ceb, bad-txns-inputs-missingorspent, CheckTxInputs: inputs missing/spent
> InvalidChainFound: invalid block=000000000000000002bff65a2701c0b7f82d2d9c0a69bec94ca45f3fc4fdf4b6  height=445812  log2_work=85.747783  date=2016-12-30T13:52:38Z

> bitcoin-cli getblockchaininfo
> {
  ""chain"": ""main"",
  ""blocks"": 445811,
  ""headers"": 445811,
  ""bestblockhash"": ""00000000000000000050987b914811f687096790995b9aba7abf9c5b2db64e8e"",
  ""difficulty"": 317688400354.0338,
  ""mediantime"": 1483103061,
  ""verificationprogress"": 0.2916487827889263,
  ""initialblockdownload"": true,
  ""chainwork"": ""00000000000000000000000000000000000000000035bbcd1e85726a7ac80c38"",
  ""size_on_disk"": 110458261547,
  ""pruned"": false,
  ""softforks"": {
    ""bip34"": {
      ""type"": ""buried"",
      ""active"": true,
      ""height"": 227931
    },
    ""bip66"": {
      ""type"": ""buried"",
      ""active"": true,
      ""height"": 363725
    },
    ""bip65"": {
      ""type"": ""buried"",
      ""active"": true,
      ""height"": 388381
    },
    ""csv"": {
      ""type"": ""buried"",
      ""active"": true,
      ""height"": 419328
    },
    ""segwit"": {
      ""type"": ""buried"",
      ""active"": false,
      ""height"": 481824
    }
  },
  ""warnings"": ""This is a pre-release test build - use at your own risk - do not use for mining or merchant applications""
}

Is 000000000000000002bff65a2701c0b7f82d2d9c0a69bec94ca45f3fc4fdf4b6 actually a valid hash?
What can i do now?
"
bitcoin/bitcoin,2021-02-24 09:34:51,question,Block pre-allocation appears to have surprisingly poor performance ,"When syncing Bitcoin Core with extremely fast storage the pre-allocation of block files with `fallocate` looks to be a surprising portion for the wall time for IBD and appears to sometimes slow down block relay at the tip of the chain. It's not completely clear why this would be the case, 300ms to write 16MB to `tmpfs` would be excessively slow. 

Is this an artifact of the logging (altered in #21041), or an actual performance concern?

```
2021-02-24T08:49:34.637365Z UpdateTip: new best=00000000000000000020fc15108523bcb10b5b76a3fd61ce57fb8460f666f845 height=578868 version=0x20000000 log2_work=90.697662 tx=420028811 date='2019-06-01T23:46:52Z' progress=0.679947 cache=74.0MiB(372110txo)
2021-02-24T08:49:34.828910Z Pre-allocating up to position 0x6000000 in blk01663.dat

...

2021-02-24T08:49:42.683484Z Pre-allocating up to position 0x1000000 in blk01667.dat
2021-02-24T08:49:42.951362Z Pre-allocating up to position 0x2000000 in blk01667.dat
2021-02-24T08:49:43.257131Z Pre-allocating up to position 0x3000000 in blk01667.dat
2021-02-24T08:49:43.510151Z Pre-allocating up to position 0x4000000 in blk01667.dat
2021-02-24T08:49:43.752604Z Pre-allocating up to position 0x5000000 in blk01667.dat
2021-02-24T08:49:44.074489Z Pre-allocating up to position 0x6000000 in blk01667.dat
2021-02-24T08:49:44.341478Z Pre-allocating up to position 0x7000000 in blk01667.dat
2021-02-24T08:49:44.655581Z Pre-allocating up to position 0x8000000 in blk01667.dat

...

2021-02-24T08:49:44.886586Z Pre-allocating up to position 0x1000000 in blk01668.dat
2021-02-24T08:49:45.148651Z Pre-allocating up to position 0x2000000 in blk01668.dat
2021-02-24T08:49:45.515480Z Pre-allocating up to position 0x100000 in rev01668.dat
2021-02-24T08:49:45.525122Z UpdateTip: new best=0000000000000000000c7af325f257778530cd2127b0ceacb1f56ce4c98fa4dc height=578869 version=0x20c00000 log2_work=90.697685 tx=420031549 date='2019-06-02T00:18:16Z' progress=0.679951 cache=29.9MiB(12647txo)
```


"
bitcoin/bitcoin,2021-02-07 09:06:54,question,mandatory-script-verify-flag-failed (Public key is neither compressed or uncompressed),"""txdata"":""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"",""utxos"":[{""tx_hash"":""b2d1b5999a06596c21f46792ca3ed3a5bf65b81f30c09b26c1a1419b2f101434"",""vout_index"":1},{""tx_hash"":""d6915bc2b092e35a6df2e5190709f777313e6c74f5ea3b929276f6ae316a1d4a"",""vout_index"":1},{""tx_hash"":""3262eb956deb6e66d113cc14b20e7d0fcc00ac58a94c032ec4a2a7fbaa5fb52a"",""vout_index"":1},{""tx_hash"":""f3fb03285e6ddfc0c66e97b48565e11a1993bc38b3748b467434b8543631aaaa"",""vout_index"":1},{""tx_hash"":""14cc96b6d9fdba483ab297f6dd7ead3ac271b6074808ab3ade84c4a0d40a92db"",""vout_index"":1},{""tx_hash"":""96f9bf3534e5e0c4f65d9ace93f01fd9601660d145fc69c0a301dc673c903865"",""vout_index"":1},{""tx_hash"":""7aabe144e7d083302a9956d5fc927fa166187b6345277018233daab18e5dab82"",""vout_index"":1},{""tx_hash"":""73db04ca6848820803b530513b3432a0018defa6c80e9713703f8915a6885719"",""vout_index"":1},{""tx_hash"":""79b6661af25399593a24efb2c60b4fb9da795045bf115c9021224e92c6a9fa56"",""vout_index"":1},{""tx_hash"":""e801fef675aa45cdfea623cd24d0a5c37ab8b0a32e6323c319472936ac63ad32"",""vout_index"":1},{""tx_hash"":""352275bb5ea9153779c4acb62d8a1919d39618a412e587e5137fad30287da699"",""vout_index"":1},{""tx_hash"":""a8ac8efe4ad653642aea6e375ea9a0f06a0303308445d35970afe9df3a920d1e"",""vout_index"":1},{""tx_hash"":""c29b4529c43637c3c3cb86ff4d32980a055a8a58f8b0ff83abfac924bb7d5a53"",""vout_index"":1},{""tx_hash"":""652367e4b5879c2707eef5a27c82416102669f7ec784d63141962a4a1c73365d"",""vout_index"":1},{""tx_hash"":""5d58a62cb677727e1506aaf2ef5b492cee8995fb294c0c625c541907529d4097"",""vout_index"":1},{""tx_hash"":""6ef23e01b47245ea43481e9db8e1095417591067702a2221fb3cf4f036b156bc"",""vout_index"":1},{""tx_hash"":""19adadbbec36fc2c14d0965da5600c099b3f4a8ec19d549d48853d4ca439abf4"",""vout_index"":1},{""tx_hash"":""0f86ef78c86b565b8ffe9bccebcb609c89f55f43e2adbc4b88079898b3baecc0"",""vout_index"":1},{""tx_hash"":""220e99b4640f22dc561c3a9f5c92e0a2452a4912b7e75bbe957c1cb86cf6ef32"",""vout_index"":1},{""tx_hash"":""7e209e6d5e283ac80078586c3ce85ef74f7c4270d4e036cceef33daf0ff31307"",""vout_index"":1},{""tx_hash"":""325f9df0c9cac98e30b02ce9e35d02eda8f15be9f89cb302454a2bb6c6bc9aeb"",""vout_index"":1},{""tx_hash"":""84dc293f4d6afaa417978630e80cba6a29d3b057ab25e82b2010a4d2e39ef18c"",""vout_index"":1},{""tx_hash"":""2d4621ddcfdc7eae231007b14856688f00b248da7447d85f7b23297c7ac118fb"",""vout_index"":1},{""tx_hash"":""c73eace587dbde385675efe97604a4d1c04b84874e48b285bfd4eb344491dfa7"",""vout_index"":1},{""tx_hash"":""bd256eddb8bd570c66f101997ae41045306f1cea18f6d5f12d0e81f6383a0754"",""vout_index"":0},{""tx_hash"":""a1929453b13d356cc4c46af7a06b3a6bf70d6ea97e8665aa05e35b817e6571a6"",""vout_index"":1},{""tx_hash"":""206c52a5ea2cd1012fae3441d15fe101a1a03356d3399fd8febf35150e96a9f1"",""vout_index"":0},{""tx_hash"":""cd03c2c6526bac5149be85b7021d0a853f9e50242e9f6b21686cf291b8d52854"",""vout_index"":0},{""tx_hash"":""159d9697d9c542af4036e22897b7b4ba8ff986548f8be642b54faf948e9050c9"",""vout_index"":1},{""tx_hash"":""1d0a739cecae0277ed20ed51616435a86bf744add9b84b8bc7f1d31bb483e001"",""vout_index"":0},{""tx_hash"":""dc2c336d4eb6a3fbe1c9833c32a625413ce8845185660395d4aa4d443b03b5d7"",""vout_index"":1},{""tx_hash"":""af6826fe571668e6d83e05d7dcf7f6de45b6b642103db29ea5e25bedb9888f73"",""vout_index"":1},{""tx_hash"":""a1fb47719387e4f7594cde20b385b8b122d4080a0c4fc63ec33c0c4a41f8c4be"",""vout_index"":1},{""tx_hash"":""af6826fe571668e6d83e05d7dcf7f6de45b6b642103db29ea5e25bedb9888f73"",""vout_index"":0},{""tx_hash"":""1c9f7d81a663709cf0f57c2e6d134427f4ddc57163bdfaa471406a890e0f7073"",""vout_index"":0},{""tx_hash"":""bafef8acbe396a6b2216999713d04139d1ff58dcd5a9a7460e5244b80818a4c8"",""vout_index"":0},{""tx_hash"":""348d24086b7be3118b776d7e2e3651a8a37f8734f653de5a6bab7323911c8cb3"",""vout_index"":7},{""tx_hash"":""815988bfa1935d40d57fb0a125fd455676520bf1cf361b2c3e40065570a6f3c0"",""vout_index"":0},{""tx_hash"":""d72bf5ae04301be810cae7878306a8d2e0d0c323414b5b78974b528b18a2e4be"",""vout_index"":0},{""tx_hash"":""e12853c51e39f772743fea3448b04d4cc44f7de1435bd02b8b2096bde1ae8728"",""vout_index"":18},{""tx_hash"":""fca5a1e609c6bde3fc23e14fa92a3658f6bda05aaf1354af14ec452d4a1d871f"",""vout_index"":0},{""tx_hash"":""88a2e1f80556032551985011858b3f9f0c0d2e3e9cd7345daef4bdb926eab76b"",""vout_index"":0},{""tx_hash"":""9dc33588482af217c038024d593183eb3d227de85e1df6076b64b68dc60ed908"",""vout_index"":0},{""tx_hash"":""ec1f4e15d19460ac490c905e22340aad3af3e9836e267af0f4b0153536867b7b"",""vout_index"":1},{""tx_hash"":""3bd20e73c838a064cfea311294391f66940d43c0bebc1e7a7787a9386efd6aa5"",""vout_index"":1},{""tx_hash"":""fd0955bec3839124bc2947ef17472bf2e4aa05553965104dd91bc5c90de5a52a"",""vout_index"":0},{""tx_hash"":""56293e369826884fb9c857ace1ec7f49204dab939fd7214fbe43cdebe23c344b"",""vout_index"":0},{""tx_hash"":""e7c3a7f929cf927e45680f9d409f341167fa6d234f54f952f2d39d4b1924cf3d"",""vout_index"":0},{""tx_hash"":""ab07b0f430febee34372c3cfa10b7cbe44f8211aadb12d72353e8e47afc89edb"",""vout_index"":0},{""tx_hash"":""79c83e5f3b6d1985f653aeed9105c7f39097436bbba68c8e94e8ede3accead4e"",""vout_index"":2},{""tx_hash"":""5da82ef18664968630fe14440caaa4882d00141fb74e054c941866b985fd008e"",""vout_index"":2},{""tx_hash"":""04ab6edee514a0ee2aeff48ca672d6f47d6c40923abb0d78aa33ec7e2001eb09"",""vout_index"":1}]}"
bitcoin/bitcoin,2021-01-15 03:19:41,question,We should remove MAX_BLOCK_BASE_SIZE in functional tests ?,"I can see that @gmaxwell  pushed a commit to `Remove confusing MAX_BLOCK_BASE_SIZE` (3babbcb48), by the way, I still can see the use of it in a lot of functional tests (p2p_segwit, mempool_accept...), do these test cases still make any sense? Should we remove them as well?"
bitcoin/bitcoin,2020-12-14 03:52:08,question,Idea suggestion: allow using external DB (for instance RethinkDB) engines for block storage,"Considering the huge size of block database, maybe it is practical to allow users to use external DB engines for this.

Imho, RethinkDB, with it's GUI, is very easy to setup, including it's mirroring and clustering.

Because of size of blocks DB and of amount of time it is required to download and/or reindex existing backup, mirroring features of real DB engines may be more practical to use.

use case:
1. user downloads Bitcoin Core
2. user downloads and sets up RethinkDB (possibly with mirroring and/or clustering)
3. user selects usage of RethinkDB in bitcoin-qt options window
4. user restarts bitcoin-qt
5. bitcoin-qt checks DB structure and tables and indexes and creates/changes them as needed
6. bitcoin-qt does it's work on blocks DB using RethinkDB

* such db could be placed on external computer
* connection to db could be encrypted and secured
* less chances to destroy hdd/ssd on local PC and data on it (because of intensive use of drive by bitcoin software)
* less chances to lose blocks DB
* less load on network from those who lost their blocks DB and now re-downloading it
* easier for user to shutdown bitcoin-qt - less time to wait while bitcoin-qt syncs and closes DB
* easier to restart in case of power failures"
bitcoin/bitcoin,2020-12-08 17:57:40,question,"""Segmentation Fault: 11"" on MacOS when ""-rescan"" large (1 GB) wallet.dat","Its a custom testnet based on `v0.21.0rc1` But I couldn't test on real bitcoin, because I do not have such large BTC wallet.dat. I may test on regtest soon.

- Reproduce
Run bitcoind with `-rescan` large wallet.dat (~1 GB mining wallet).

![image](https://user-images.githubusercontent.com/60179867/101533896-6b05ff00-3964-11eb-84d8-3fb05628e242.png)

Strange thing is Linux64 is just fine, but MacOS and Win10 this error.
- MacOS
```
Segmentation Fault: 11
```
- Win10: Similar behavier like MacOS, but daemon just killed. No Segfault or any error log.

- FYI:
Win10 2cpu 4096ram
MacOS (High Sierra 10.13.3) 2cpu 4096ram (Macbook Air 13 Inch 2011)

- Related Issues
bitcoin/bitcoin#4607
bitcoin/bitcoin#12318
bitcoin/bitcoin#9816"
bitcoin/bitcoin,2020-11-25 10:12:34,question,Question: How to debug using docker & CI shell script?,"Hi,

my PR #19245 fails on several platforms so I have attempted to run on my Ubuntu 20.04:

```bash
FILE_ENV=""./ci/test/00_setup_env_native_tsan.sh"" ./ci/test_run_all.sh
```

as per instructions in https://github.com/bitcoin/bitcoin/tree/master/ci.

I can reproduce issue on CI which is great news. However, I would love to modify source code and re-run the `FILE_ENV=""./ci/test/00_setup_env_native_tsan.sh"" ./ci/test_run_all.sh` script or attach to docker to run a failing functional test again so that I can debug it easier.

Does anybody have a workflow for this? 

Notes:
* I've tried `docker attach <container ID>` but it's not possible to simply execute `test/functional/wallet_hd.py` as it fails in that docker instance because it is supposed to be run somehow differently (that's as much as I gather from studying `ci/test_run_all.sh`).
"
bitcoin/bitcoin,2020-11-06 08:40:40,question,"Processblock not accepted, leveldb batch commit failure: IO error: unsufficient space","Fatal error has occurred, BAD_Alloc runaway exception error when starting bitcoin core

Debug log says error on processblock 
Then creates orphanblocks and finally ends with:

LevelDB batch commit failure: IO error: winmapfile.append::unmapcurrent region or mapnewregion : unsufficient memory available to process this task

Error AcceptBlock : Addtoblockindex failed
Error ProcessBlock : Acceptblock failed
Processblock, block not accepted

> Orphanblock


Can anyone help me with this issue?

I did fix the bad alloc runaway error with this memory issue before by configuring the configuration file adding the following:

Listen=1
Bind=0.0.0.0
Port=8333
Maxconnections=64
Maxmempool=32
Upnp=1
Dnsseed=1
Discover=1

Dbcache=18
Dblogsize=50
Par=1
Checkblocks=2000
Checklevel=6

Disablewallet=0

Rpcbind= 127.0.0.1
Rpcport=8333
Rpcallowip=127.0.0.1

Maxuploadtargets=32
Rescan=1

Maxorphanblocksmib=80

Testnet=0




Can anyone help me solve the issue?

It keeps coming back at the memory issue as the underlying problem but why does it not accept the block? 

I think I need to fix the memory issue first. This is done on the latest bitcoin core wallet"
bitcoin/bitcoin,2020-10-28 18:03:25,question,Unable to compile on native macOS after 3caee16946575e71e90ead9ac531f5a3a1259307,"Introduced with 3caee16946575e71e90ead9ac531f5a3a1259307.

```
CXXLD    qt/test/test_bitcoin-qt
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [bitcoin-tx] Error 1
make[2]: *** Waiting for unfinished jobs....
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [bitcoin-wallet] Error 1
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
ld: symbol(s) not found for architecture x86_64
ld: symbol(s) not found for architecture x86_64
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [bench/bench_bitcoin] Error 1
clang: error: linker command failed with exit code 1 (use -v to see invocation)
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [bitcoind] Error 1
make[2]: *** [bitcoin-node] Error 1
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [test/test_bitcoin] Error 1
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
Undefined symbols for architecture x86_64:
  ""_secp256k1_xonly_pubkey_tweak_add_check"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
  ""_secp256k1_xonly_pubkey_parse"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::CheckPayToContract(XOnlyPubKey const&, uint256 const&, bool) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
  ""_secp256k1_schnorrsig_verify"", referenced from:
  ""_secp256k1_schnorrsig_verify"", referenced from:
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
      XOnlyPubKey::VerifySchnorr(uint256 const&, Span<unsigned char const>) const in libbitcoin_consensus.a(libbitcoin_consensus_a-pubkey.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [qt/test/test_bitcoin-qt] Error 1
ld: symbol(s) not found for architecture x86_64
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [bitcoin-gui] Error 1
make[2]: *** [qt/bitcoin-qt] Error 1
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1
```"
bitcoin/bitcoin,2020-10-02 20:59:04,question,BitcoinCore Fatal Internal Error when syncing,"Hey guys!
So I just downloaded Bitcoin Core on my MacBook Air today and it was syncing. It synced up to 6% and now says: ""Error: A fatal internal error occurred, see debug.log for details""

I don't know anything about coding so if some one can guide me through the process step by step it would be great.

Also I already transferred money into the wallet while it was syncing but now i cannot recover it and it never was even fully synced! Can someone please help me?"
bitcoin/bitcoin,2020-09-06 11:47:30,question,Fail to build on Fedora 32,"```
$ cat /etc/system-release
Fedora release 32 (Thirty Two)
$ gcc --version | grep gcc
gcc (GCC) 10.2.1 20200723 (Red Hat 10.2.1-1)
$ make --version
GNU Make 4.2.1
Built for x86_64-redhat-linux-gnu
Copyright (C) 1988-2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
$ git rev-parse HEAD
03689317021a72431762c1974530f2a980a7fffa
$ ./autogen.sh 
$ ./configure
$ make clean
$ make
...
  CXX      libbitcoin_common_a-warnings.o
  AR       libbitcoin_common.a
bfd plugin: script/libbitcoin_common_a-descriptor.o: file too short

/usr/bin/ranlib: out of memory allocating 7701297037312 bytes after a total of 774144 bytes
make[2]: *** [Makefile:7481: libbitcoin_common.a] Error 1
make[2]: Leaving directory '/home/hebasto/bitcoin/src'
make[1]: *** [Makefile:18954: all-recursive] Error 1
make[1]: Leaving directory '/home/hebasto/bitcoin/src'
make: *** [Makefile:797: all-recursive] Error 1
```"
bitcoin/bitcoin,2020-07-04 17:00:49,question,Bech32 address not provided when creating a new receiving address,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

<!--- What behavior did you expect? -->
I go to the Receive tab, ensuring that ""Generate native segwit (Bech32) address"" is selected. I fill the label as ""Test Bech32"" and then click ""Create new receiving address"". I expect the address to be a bech32 address

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->
The address is a p2pkh address.

![20200704_174715_bitcoin_no_work](https://user-images.githubusercontent.com/1679965/86517144-87829600-be1e-11ea-937e-b2725eaea523.png)


**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->
On my own wallet this is reproducible according to the steps above each time.

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
v0.20.0 from bitcoin.org/en/bitcoin-core/ (The issue was present in 0.19.0 too)

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->
Linux Debian 4.19.118-2+deb10u1 (x86-64) HDD
KDE plasmashell 5.14.5

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
This is using a pre-HD wallet 
"
bitcoin/bitcoin,2020-05-05 11:57:21,question,"On windows 10 most of the time the wallet is in the - (not responding) mode, please tell me what to do","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

<!--- What behavior did you expect? -->

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2020-03-15 18:25:43,question,"Built 0.19.1 from source, get the warning ""This is a pre-release..."" developers forgot to remove it.","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

<!--- What behavior did you expect? -->

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2020-01-12 13:40:28,question,bitcoin-core snap issues,"Following up the recent discussion about Bitcoin Core packaging (#17343, [IRC meeting](http://www.erisian.com.au/meetbot/bitcoin-core-dev/2020/bitcoin-core-dev.2020-01-02-19.00.log.html)) I've started to test [`bitcoin-core`](https://snapcraft.io/bitcoin-core) snap.

1. The most frighten issue, IMO, is automatic updates:
- https://snapcraft.io/docs/keeping-snaps-up-to-date
- https://snapcraft.io/blog/8-ways-snaps-are-different:
> However, some users do not wish to have their software updated immediately. ... Snaps enable users to control when updates are delivered. Users can postpone them to update outside the working day, overnight, or later in the month

It seems there is no way to disable updates at all.

2. The default datadir is `~/snap/bitcoin-core/common/.bitcoin`, which means it will be removed during removing of `bitcoin-core` snap or the entire snap framework.

3. The GUI settings ([currently](https://github.com/bitcoin/bitcoin/pull/15936), `QSettings`) are stored in `~/snap/bitcoin-core/<revision>/.config/Bitcoin` directory, which means they will be dropped after the snap update.

4. ~On Debian 10.2 I didn't find the way to launch `bitcoin-qt` with command line options. This makes it impossible to use `-choosedatadir` option (a minor issue).~ (`bitcoin-core.qt` [works](https://github.com/bitcoin/bitcoin/issues/17912#issuecomment-573825562))

5. On Debian 10.2 ""Start Bitcoin Core on system login"" does not work (a minor issue).

Should we clear inform users about mentioned pitfalls?

Refs:
- #15936 
- #17636 "
bitcoin/bitcoin,2019-12-19 17:44:37,question,p2p: GetBlocks message response,"IIUC, since #4468 (341735eb8f42e898cf9d4d130709471e5d01abe2) has been merged, i.e. v0.10, the Bitcoin Core has no means to send [`GetBlocks`](https://bitcoin.org/en/developer-reference#getblocks) messages to its peers.

How long should we support the means to handle `GetBlocks` messages from other peers?"
bitcoin/bitcoin,2019-12-03 23:06:40,question,Balance is 0 after send transaction in testnet v0.19 and blocksonly flag enabled,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

I tried to send 0.00469484 BTC from 0.02 BTC total in testnet3. The daemon reports a txid but this one is not visible in any explorer and the transaction is not processing it at all. Furthermore the balance is set to 0.00000000 (getbalance ""*"" 0). I can query the transaction via listtransactions. It is a transaction from and to the same wallet. Also maybe related, in regtest mode some invisible fee is reduced from total wallet balance but not logged in any receive-send info. The amount is just missing.

<!--- What behavior did you expect? -->

Just send a test transaction

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

Bitcoin Core version v0.19.0.1 (release build) -blocksonly=1 -prune=1024

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

Linux  4.15 (Ubuntu 18.04.1 LTS)

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

There is nothing special logged:
```
2019-12-03T20:54:13Z [test] CommitTransaction:
CTransaction(hash=896cc84018, ver=2, vin.size=2, vout.size=2, nLockTime=1611085)
    CTxIn(COutPoint(21866e5057, 1), scriptSig=160014a5a09ce3f5f5d78d39, nSequence=4294967294)
    CTxIn(COutPoint(5a847af857, 0), scriptSig=160014a5a09ce3f5f5d78d39, nSequence=4294967294)
    CScriptWitness(304402207f290913ac2c796278b9ae2271f17c39e9039bbbfe6e37e6a3d9b97fd19d7928022004e46bceec87b3195ca0fe1477366ef21c9f9a11ffd7acec039a3dea8cf2839501, 02baab1863533372a82ced2b84009217fd81e83d3325d44e865970f3ffe9c7a162)
    CScriptWitness(304402206ba62fc2a8a37c59e7474f318021fd09c67e6b6b38aaf7ac10ec60125c04555c02204608cd5764f1c38f814b55f8e5629f0a69a2f07ac094505a1249d0524fa3ec0b01, 02baab1863533372a82ced2b84009217fd81e83d3325d44e865970f3ffe9c7a162)
    CTxOut(nValue=0.00469484, scriptPubKey=a914da26d8b5010fb615fe442b678c)
    CTxOut(nValue=0.01530260, scriptPubKey=a914fafbd45c56f6a2a593b115b0fa)

2019-12-03T20:54:13Z [test] AddToWallet 896cc8401843a2f7d0682a43faa180a5b2f722db96039a9d30b2158e3b7e97ea  new
```
```

{
    ""address"": ""2ND8hqCu4r4Raj6paNk3FThSb9SZx8bad57"",
    ""category"": ""receive"",
    ""amount"": 0.00469484,
    ""label"": """",
    ""vout"": 0,
    ""confirmations"": 0,
    ""trusted"": false,
    ""txid"": ""896cc8401843a2f7d0682a43faa180a5b2f722db96039a9d30b2158e3b7e97ea"",
    ""walletconflicts"": [
    ],
    ""time"": 1575406453,
    ""timereceived"": 1575406453,
    ""bip125-replaceable"": ""unknown""
  },
  {
    ""address"": ""2ND8hqCu4r4Raj6paNk3FThSb9SZx8bad57"",
    ""category"": ""send"",
    ""amount"": -0.00469484,
    ""label"": """",
    ""vout"": 0,
    ""fee"": -0.00000256,
    ""confirmations"": 0,
    ""trusted"": false,
    ""txid"": ""896cc8401843a2f7d0682a43faa180a5b2f722db96039a9d30b2158e3b7e97ea"",
    ""walletconflicts"": [
    ],
    ""time"": 1575406453,
    ""timereceived"": 1575406453,
    ""bip125-replaceable"": ""unknown"",
    ""abandoned"": false
  }
```


Does maybe the peers do not accept my transaction? All peers have a lower version:
```
    ""subver"": ""/Satoshi:0.18.0/"",
    ""subver"": ""/Satoshi:0.18.1/"",
    ""subver"": ""/Satoshi:0.16.0/"",
    ""subver"": ""/Satoshi:0.15.2/"",
    ""subver"": ""/Satoshi:0.17.0/"",
    ""subver"": ""/Satoshi:0.17.0/"",
    ""subver"": ""/Satoshi:0.18.0/"",
    ""subver"": ""/Satoshi:0.13.2/"",
    ""subver"": ""/Satoshi:0.14.2/"",
```


"
bitcoin/bitcoin,2019-11-26 19:22:05,question,"""2"" address unexpectedly changed to ""m"" address in Testnet","Using Bitcoin Core 0.18

Testing a change I made to my local NOMP pool software.

- My Bitcoin Core **bitcoin-cli getnewaddress** gave me the address that NOMP used to communicate with the Bitcoin Test Network (2N5tv3gJzBD1zZ2hri2ffYbHsm1AEhpNQFz); 
- I began mining; 
- I found 6 blocks, but at the heights found, instead of having my ""2"" address, it showed all were found by the (random?) ""m"" address in the link below:

[Found these blocks on Testnet](https://tbtc.bitaps.com/mtAeMSyVpsdX3BrVdNMaKwA1G8G4GMWHpF) (other transactions shown are test transactions with faucet coins)

I have no idea where this ""m"" address came from or how or why it ""changed"" from my ""2"" address; it doesn't belong to my Bitcoin Core wallet and I have no access to the funds (not an issue but it would be nice to return faucet coins). 

Concern is whether something like this can occur on Mainnet."
bitcoin/bitcoin,2019-11-06 18:32:16,question,FlushStateToDisk() takes more than 10 minutes,"```
2019-11-06T18:09:57Z [qt-init] Shutdown: In progress...
2019-11-06T18:09:57Z [msghand] msghand thread exit
2019-11-06T18:09:57Z [net] net thread exit
2019-11-06T18:09:57Z [scheduler] scheduler thread interrupt
2019-11-06T18:09:58Z [shutoff] Dumped mempool: 0.016041s to copy, 0.043132s to dump
2019-11-06T18:09:58Z [shutoff] FlushStateToDisk: write coins cache to disk (65649194 coins, 9493571kB) started
2019-11-06T18:23:09Z [shutoff] FlushStateToDisk: write coins cache to disk (65649194 coins, 9493571kB) completed (791.57s)
2019-11-06T18:23:09Z [shutoff] FlushStateToDisk: write coins cache to disk (0 coins, 1027997kB) started
2019-11-06T18:23:09Z [shutoff] FlushStateToDisk: write coins cache to disk (0 coins, 1027997kB) completed (0.07s)
2019-11-06T18:23:09Z [shutoff] Shutdown: done
```

I understand that cache is about 9.5 GB, nevertheless, is it expected flush duration?

Configuration:
- datadir resides on SSD
- blocksdir resides on HDD
- master (86771d431054efb780a2be4a83a6952530c14875)"
bitcoin/bitcoin,2019-10-30 16:28:08,question,Private test network - Outgoing connections ?,"Hello,

I wanted to make a private network of bitcoin nodes (in testnet) but they are not connecting to each other. They receive ADDR messages but do not issue a connection afterwards.
I have 4 different hosts with 4 different IP adresses (but same sub network). 3 of them are connected to a fourth one (addnode rpc call). The fourth one issues ADDR messages to the nodes but they won't connect to each other.

###  Describe the issue
#### What behavior did you expect?
If a node learn about a new node (by receiving an ADDR message) and if its 8 outgoing connections are not filled it will try to connect to the new node

#### What was the actual behavior (provide screenshots if the issue is GUI-related)?
The node won't issue an outgoing connection

####  How reliably can you reproduce the issue, what are the steps to do so?
All host : 
`$ ./bitcoind -datadir=/home/jpe/bitcoin-test/datadir -conf=bitcoin.conf -debug=net -debug=rpc -debug=addrman -printtoconsole -logips`

Test the connection with bitcoin-cli getconnectioncount RPC call

#### What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)?
Bitcoin 0.18.1, from the website

#### Any extra information that might be useful in the debugging process.

##### Bitcoin.conf

```
# Generated by https://jlopp.github.io/bitcoin-core-config-generator/

# This config should be placed in following path:
# ~/.bitcoin/bitcoin.conf

# [network]
# Allow DNS lookups for -addnode, -seednode and -connect values.
dns=0
# Query for peer addresses via DNS lookup, if low on addresses.
dnsseed=0
# Automatically create Tor hidden service.
listenonion=0
listen=1

# [debug]
# Run this node on the Bitcoin Test Network.
testnet=1

# [rpc]
# Accept command line and JSON-RPC commands.
server=1
rpcuser=user
rpcpassword=qwerty

# [Sections]
# Most options automatically apply to mainnet, testnet, and regtest networks.
# If you want to confine an option to just one network, you should add it in the relevant section.
# EXCEPTIONS: The options addnode, connect, port, bind, rpcport, rpcbind and wallet
# only apply to mainnet unless they appear in the appropriate section below.

# Options only for mainnet
[main]

# Options only for testnet
[test]
# Listen for incoming connections on non-default port.
# Listen for JSON-RPC connections on this port
rpcport=16590
rpcallowip=[my_ip_here]
rpcbind=0.0.0.0

# Options only for regtest
[regtest]
```

##### Logs

The node connected to the 3 others
```
2019-10-29T16:39:12Z Bitcoin Core version v0.18.1 (release build)
2019-10-29T16:39:12Z Assuming ancestors of block 0000000000000037a8cd3e06cd5edbfe9dd1dbcc5dacab279376ef7cfc2b4c75 have valid signatures.
2019-10-29T16:39:12Z Setting nMinimumChainWork=00000000000000000000000000000000000000000000007dbe94253893cbd463
2019-10-29T16:39:12Z Using the 'sse4(1way),sse41(4way)' SHA256 implementation
2019-10-29T16:39:12Z Using RdRand as an additional entropy source
2019-10-29T16:39:12Z Default data directory /home/jpe/.bitcoin
2019-10-29T16:39:12Z Using data directory /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:39:12Z Config file: /home/jpe/bitcoin-test/datadir/bitcoin.conf
2019-10-29T16:39:12Z Using at most 125 automatic connections (1024 file descriptors available)
2019-10-29T16:39:12Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2019-10-29T16:39:12Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2019-10-29T16:39:12Z Using 16 threads for script verification
2019-10-29T16:39:12Z scheduler thread start
2019-10-29T16:39:12Z WARNING: the RPC server is not safe to expose to untrusted networks such as the public internet
2019-10-29T16:39:12Z HTTP: creating work queue of depth 16
2019-10-29T16:39:12Z Starting RPC
2019-10-29T16:39:12Z Starting HTTP RPC server
2019-10-29T16:39:12Z Config options rpcuser and rpcpassword will soon be deprecated. Locally-run instances may remove rpcuser to use cookie-based auth, or may be replaced with rpcauth. Please see share/rpcauth for rpcauth auth generation.
2019-10-29T16:39:12Z HTTP: starting 4 worker threads
2019-10-29T16:39:12Z Using wallet directory /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:39:12Z init message: Verifying wallet(s)...
2019-10-29T16:39:12Z Using BerkeleyDB version Berkeley DB 4.8.30: (April  9, 2010)
2019-10-29T16:39:12Z Using wallet /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:39:12Z BerkeleyEnvironment::Open: LogDir=/home/jpe/bitcoin-test/datadir/testnet3/database ErrorFile=/home/jpe/bitcoin-test/datadir/testnet3/db.log
2019-10-29T16:39:12Z init message: Loading banlist...
2019-10-29T16:39:12Z Loaded 0 banned node ips/subnets from banlist.dat  0ms
2019-10-29T16:39:12Z net: setting try another outbound peer=false
2019-10-29T16:39:12Z Cache configuration:
2019-10-29T16:39:12Z * Using 2.0 MiB for block index database
2019-10-29T16:39:12Z * Using 8.0 MiB for chain state database
2019-10-29T16:39:12Z * Using 440.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
2019-10-29T16:39:12Z init message: Loading block index...
2019-10-29T16:39:12Z Opening LevelDB in /home/jpe/bitcoin-test/datadir/testnet3/blocks/index
2019-10-29T16:39:12Z Opened LevelDB successfully
2019-10-29T16:39:12Z Using obfuscation key for /home/jpe/bitcoin-test/datadir/testnet3/blocks/index: 0000000000000000
2019-10-29T16:39:12Z LoadBlockIndexDB: last block file = 0
2019-10-29T16:39:12Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=1, size=293, heights=0...0, time=2011-02-02...2011-02-02)
2019-10-29T16:39:12Z Checking all blk files are present...
2019-10-29T16:39:12Z Opening LevelDB in /home/jpe/bitcoin-test/datadir/testnet3/chainstate
2019-10-29T16:39:12Z Opened LevelDB successfully
2019-10-29T16:39:12Z Using obfuscation key for /home/jpe/bitcoin-test/datadir/testnet3/chainstate: d94cbd34778bbbff
2019-10-29T16:39:12Z Loaded best chain: hashBestChain=000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943 height=0 date=2011-02-02T23:16:42Z progress=0.000000
2019-10-29T16:39:12Z init message: Rewinding blocks...
2019-10-29T16:39:12Z init message: Verifying blocks...
2019-10-29T16:39:12Z  block index             383ms
2019-10-29T16:39:12Z init message: Loading wallet...
2019-10-29T16:39:12Z BerkeleyEnvironment::Open: LogDir=/home/jpe/bitcoin-test/datadir/testnet3/database ErrorFile=/home/jpe/bitcoin-test/datadir/testnet3/db.log
2019-10-29T16:39:12Z [default wallet] nFileVersion = 180100
2019-10-29T16:39:12Z [default wallet] Keys: 2001 plaintext, 0 encrypted, 2001 w/ metadata, 2001 total. Unknown wallet records: 0
2019-10-29T16:39:13Z [default wallet] Wallet completed loading in             116ms
2019-10-29T16:39:13Z [default wallet] setKeyPool.size() = 2000
2019-10-29T16:39:13Z [default wallet] mapWallet.size() = 0
2019-10-29T16:39:13Z [default wallet] mapAddressBook.size() = 0
2019-10-29T16:39:13Z mapBlockIndex.size() = 1
2019-10-29T16:39:13Z nBestHeight = 0
2019-10-29T16:39:13Z Imported mempool transactions from disk: 0 succeeded, 0 failed, 0 expired, 0 already there
2019-10-29T16:39:13Z AddLocal([ip_address_host1]:18333,1)
2019-10-29T16:39:13Z Discover: IPv4 eno1: [ip_address_host1]
2019-10-29T16:39:13Z Bound to [ip_address_host1]:18333
2019-10-29T16:39:13Z AddLocal([ip_address_host1]:18333,2)
2019-10-29T16:39:13Z init message: Loading P2P addresses...
2019-10-29T16:39:13Z Loaded 7 addresses from peers.dat  1ms
2019-10-29T16:39:13Z init message: Starting network threads...
2019-10-29T16:39:13Z DNS seeding disabled
2019-10-29T16:39:13Z net thread start
2019-10-29T16:39:13Z addcon thread start
2019-10-29T16:39:13Z opencon thread start
2019-10-29T16:39:13Z init message: Done loading
2019-10-29T16:39:13Z msghand thread start
2019-10-29T16:42:25Z Added connection to [ip_address_host3]:49051 peer=0
2019-10-29T16:42:25Z connection from [ip_address_host3]:49051 accepted
2019-10-29T16:42:25Z received: version (102 bytes) peer=0
2019-10-29T16:42:25Z sending version (102 bytes) peer=0
2019-10-29T16:42:25Z send version message: version 70015, blocks=0, us=[::]:0, them=[ip_address_host3]:49051, peer=0
2019-10-29T16:42:25Z sending verack (0 bytes) peer=0
2019-10-29T16:42:25Z receive version message: /Satoshi:0.18.1/: version 70015, blocks=0, us=[ip_address_host1]:18333, peer=0, peeraddr=[ip_address_host3]:49051
2019-10-29T16:42:25Z added time data, samples 2, offset +0 (+0 minutes)
2019-10-29T16:42:25Z received: verack (0 bytes) peer=0
2019-10-29T16:42:25Z sending sendheaders (0 bytes) peer=0
2019-10-29T16:42:25Z sending sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z sending sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z sending ping (8 bytes) peer=0
2019-10-29T16:42:25Z initial getheaders (0) to peer=0 (startheight:0)
2019-10-29T16:42:25Z sending getheaders (69 bytes) peer=0
2019-10-29T16:42:25Z sending feefilter (8 bytes) peer=0
2019-10-29T16:42:25Z received: getaddr (0 bytes) peer=0
2019-10-29T16:42:25Z received: sendheaders (0 bytes) peer=0
2019-10-29T16:42:25Z received: sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z received: sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z received: ping (8 bytes) peer=0
2019-10-29T16:42:25Z sending pong (8 bytes) peer=0
2019-10-29T16:42:25Z received: getheaders (69 bytes) peer=0
2019-10-29T16:42:25Z Ignoring getheaders from peer=0 because node is in initial block download
2019-10-29T16:42:25Z received: feefilter (8 bytes) peer=0
2019-10-29T16:42:25Z received: feefilter of 0.00001000 BTC/kB from peer=0
2019-10-29T16:42:25Z received: pong (8 bytes) peer=0
2019-10-29T16:42:29Z sending addr (31 bytes) peer=0
2019-10-29T16:43:31Z Added connection to [ip_address_host2]:47834 peer=1
2019-10-29T16:43:31Z connection from [ip_address_host2]:47834 accepted
2019-10-29T16:43:31Z received: version (102 bytes) peer=1
2019-10-29T16:43:31Z sending version (102 bytes) peer=1
2019-10-29T16:43:31Z send version message: version 70015, blocks=0, us=[::]:0, them=[ip_address_host2]:47834, peer=1
2019-10-29T16:43:31Z sending verack (0 bytes) peer=1
2019-10-29T16:43:31Z receive version message: /Satoshi:0.18.1/: version 70015, blocks=0, us=[ip_address_host1]:18333, peer=1, peeraddr=[ip_address_host2]:47834
2019-10-29T16:43:31Z added time data, samples 3, offset +0 (+0 minutes)
2019-10-29T16:43:31Z received: verack (0 bytes) peer=1
2019-10-29T16:43:31Z sending sendheaders (0 bytes) peer=1
2019-10-29T16:43:31Z sending sendcmpct (9 bytes) peer=1
2019-10-29T16:43:31Z sending sendcmpct (9 bytes) peer=1
2019-10-29T16:43:31Z sending ping (8 bytes) peer=1
2019-10-29T16:43:31Z sending feefilter (8 bytes) peer=1
2019-10-29T16:43:31Z received: getaddr (0 bytes) peer=1
2019-10-29T16:43:31Z received: sendheaders (0 bytes) peer=1
2019-10-29T16:43:31Z received: sendcmpct (9 bytes) peer=1
2019-10-29T16:43:31Z received: sendcmpct (9 bytes) peer=1
2019-10-29T16:43:31Z received: ping (8 bytes) peer=1
2019-10-29T16:43:31Z sending pong (8 bytes) peer=1
2019-10-29T16:43:31Z received: getheaders (69 bytes) peer=1
2019-10-29T16:43:31Z Ignoring getheaders from peer=1 because node is in initial block download
2019-10-29T16:43:31Z received: feefilter (8 bytes) peer=1
2019-10-29T16:43:31Z received: feefilter of 0.00001000 BTC/kB from peer=1
2019-10-29T16:43:31Z received: pong (8 bytes) peer=1
2019-10-29T16:43:56Z sending addr (31 bytes) peer=1
2019-10-29T16:44:25Z received: ping (8 bytes) peer=0
2019-10-29T16:44:25Z sending pong (8 bytes) peer=0
2019-10-29T16:44:25Z sending ping (8 bytes) peer=0
2019-10-29T16:44:25Z received: pong (8 bytes) peer=0
2019-10-29T16:45:31Z received: ping (8 bytes) peer=1
2019-10-29T16:45:31Z sending pong (8 bytes) peer=1
2019-10-29T16:45:31Z sending ping (8 bytes) peer=1
2019-10-29T16:45:31Z received: pong (8 bytes) peer=1
2019-10-29T16:46:25Z sending ping (8 bytes) peer=0
2019-10-29T16:46:25Z received: pong (8 bytes) peer=0
2019-10-29T16:46:25Z received: ping (8 bytes) peer=0
2019-10-29T16:46:25Z sending pong (8 bytes) peer=0
2019-10-29T16:47:31Z received: ping (8 bytes) peer=1
2019-10-29T16:47:31Z sending pong (8 bytes) peer=1
2019-10-29T16:47:31Z sending ping (8 bytes) peer=1
2019-10-29T16:47:31Z received: pong (8 bytes) peer=1
2019-10-29T16:48:25Z received: ping (8 bytes) peer=0
2019-10-29T16:48:25Z sending pong (8 bytes) peer=0
2019-10-29T16:48:25Z sending ping (8 bytes) peer=0
2019-10-29T16:48:25Z received: pong (8 bytes) peer=0
2019-10-29T16:49:31Z received: ping (8 bytes) peer=1
2019-10-29T16:49:31Z sending pong (8 bytes) peer=1
2019-10-29T16:49:31Z sending ping (8 bytes) peer=1
2019-10-29T16:49:31Z received: pong (8 bytes) peer=1
2019-10-29T16:50:25Z received: ping (8 bytes) peer=0
2019-10-29T16:50:25Z sending pong (8 bytes) peer=0
2019-10-29T16:50:25Z sending ping (8 bytes) peer=0
2019-10-29T16:50:25Z received: pong (8 bytes) peer=0
2019-10-29T16:51:31Z sending ping (8 bytes) peer=1
2019-10-29T16:51:31Z received: pong (8 bytes) peer=1
2019-10-29T16:51:31Z received: ping (8 bytes) peer=1
2019-10-29T16:51:31Z sending pong (8 bytes) peer=1
2019-10-29T16:52:25Z received: ping (8 bytes) peer=0
2019-10-29T16:52:25Z sending pong (8 bytes) peer=0
2019-10-29T16:52:25Z sending ping (8 bytes) peer=0
2019-10-29T16:52:25Z received: pong (8 bytes) peer=0
2019-10-29T16:53:32Z sending ping (8 bytes) peer=1
2019-10-29T16:53:32Z received: pong (8 bytes) peer=1
2019-10-29T16:53:32Z received: ping (8 bytes) peer=1
2019-10-29T16:53:32Z sending pong (8 bytes) peer=1
2019-10-29T16:54:13Z Flushed 7 addresses to peers.dat  32ms
2019-10-29T16:54:25Z received: ping (8 bytes) peer=0
2019-10-29T16:54:25Z sending pong (8 bytes) peer=0
2019-10-29T16:54:25Z sending ping (8 bytes) peer=0
2019-10-29T16:54:25Z received: pong (8 bytes) peer=0
2019-10-29T16:54:38Z Added connection to [ip_address_host4]:51666 peer=2
2019-10-29T16:54:38Z connection from [ip_address_host4]:51666 accepted
2019-10-29T16:54:38Z received: version (102 bytes) peer=2
2019-10-29T16:54:38Z sending version (102 bytes) peer=2
2019-10-29T16:54:38Z send version message: version 70015, blocks=0, us=[::]:0, them=[ip_address_host4]:51666, peer=2
2019-10-29T16:54:38Z sending verack (0 bytes) peer=2
2019-10-29T16:54:38Z receive version message: /Satoshi:0.18.1/: version 70015, blocks=0, us=[ip_address_host1]:18333, peer=2, peeraddr=[ip_address_host4]:51666
2019-10-29T16:54:38Z added time data, samples 4, offset +0 (+0 minutes)
2019-10-29T16:54:38Z received: verack (0 bytes) peer=2
2019-10-29T16:54:38Z sending sendheaders (0 bytes) peer=2
2019-10-29T16:54:38Z sending sendcmpct (9 bytes) peer=2
2019-10-29T16:54:38Z sending sendcmpct (9 bytes) peer=2
2019-10-29T16:54:38Z sending ping (8 bytes) peer=2
2019-10-29T16:54:38Z sending feefilter (8 bytes) peer=2
2019-10-29T16:54:38Z received: getaddr (0 bytes) peer=2
2019-10-29T16:54:38Z received: sendheaders (0 bytes) peer=2
2019-10-29T16:54:38Z received: sendcmpct (9 bytes) peer=2
2019-10-29T16:54:38Z received: sendcmpct (9 bytes) peer=2
2019-10-29T16:54:38Z received: ping (8 bytes) peer=2
2019-10-29T16:54:38Z sending pong (8 bytes) peer=2
2019-10-29T16:54:38Z received: getheaders (69 bytes) peer=2
2019-10-29T16:54:38Z Ignoring getheaders from peer=2 because node is in initial block download
2019-10-29T16:54:38Z received: feefilter (8 bytes) peer=2
2019-10-29T16:54:38Z received: feefilter of 0.00001000 BTC/kB from peer=2
2019-10-29T16:54:38Z received: pong (8 bytes) peer=2
2019-10-29T16:55:18Z sending addr (31 bytes) peer=2
2019-10-29T16:55:32Z received: ping (8 bytes) peer=1
2019-10-29T16:55:32Z sending pong (8 bytes) peer=1
2019-10-29T16:55:32Z sending ping (8 bytes) peer=1
2019-10-29T16:55:32Z received: pong (8 bytes) peer=1
2019-10-29T16:56:25Z received: ping (8 bytes) peer=0
2019-10-29T16:56:25Z sending pong (8 bytes) peer=0
2019-10-29T16:56:25Z sending ping (8 bytes) peer=0
2019-10-29T16:56:25Z received: pong (8 bytes) peer=0
2019-10-29T16:56:38Z sending ping (8 bytes) peer=2
2019-10-29T16:56:38Z received: pong (8 bytes) peer=2
2019-10-29T16:56:38Z received: ping (8 bytes) peer=2
2019-10-29T16:56:38Z sending pong (8 bytes) peer=2
2019-10-29T16:57:32Z sending ping (8 bytes) peer=1
2019-10-29T16:57:32Z received: pong (8 bytes) peer=1
2019-10-29T16:57:32Z received: ping (8 bytes) peer=1
2019-10-29T16:57:32Z sending pong (8 bytes) peer=1
2019-10-29T16:58:25Z received: ping (8 bytes) peer=0
2019-10-29T16:58:25Z sending pong (8 bytes) peer=0
2019-10-29T16:58:25Z sending ping (8 bytes) peer=0
2019-10-29T16:58:25Z received: pong (8 bytes) peer=0
```

Logs of one of the 3 node connected to the fourth one :

```
2019-10-29T16:41:48Z Bitcoin Core version v0.18.1 (release build)
2019-10-29T16:41:48Z Assuming ancestors of block 0000000000000037a8cd3e06cd5edbfe9dd1dbcc5dacab279376ef7cfc2b4c75 have valid signatures.
2019-10-29T16:41:48Z Setting nMinimumChainWork=00000000000000000000000000000000000000000000007dbe94253893cbd463
2019-10-29T16:41:48Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2019-10-29T16:41:48Z Using RdRand as an additional entropy source
2019-10-29T16:41:49Z Default data directory /home/jpe/.bitcoin
2019-10-29T16:41:49Z Using data directory /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:41:49Z Config file: /home/jpe/bitcoin-test/datadir/bitcoin.conf
2019-10-29T16:41:49Z Using at most 125 automatic connections (1024 file descriptors available)
2019-10-29T16:41:49Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2019-10-29T16:41:49Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2019-10-29T16:41:49Z Using 12 threads for script verification
2019-10-29T16:41:49Z scheduler thread start
2019-10-29T16:41:49Z WARNING: the RPC server is not safe to expose to untrusted networks such as the public internet
2019-10-29T16:41:49Z HTTP: creating work queue of depth 16
2019-10-29T16:41:49Z Starting RPC
2019-10-29T16:41:49Z Starting HTTP RPC server
2019-10-29T16:41:49Z Config options rpcuser and rpcpassword will soon be deprecated. Locally-run instances may remove rpcuser to use cookie-based auth, or may be replaced with rpcauth. Please see share/rpcauth for rpcauth auth generation.
2019-10-29T16:41:49Z HTTP: starting 4 worker threads
2019-10-29T16:41:49Z Using wallet directory /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:41:49Z init message: Verifying wallet(s)...
2019-10-29T16:41:49Z Using BerkeleyDB version Berkeley DB 4.8.30: (April  9, 2010)
2019-10-29T16:41:49Z Using wallet /home/jpe/bitcoin-test/datadir/testnet3
2019-10-29T16:41:49Z BerkeleyEnvironment::Open: LogDir=/home/jpe/bitcoin-test/datadir/testnet3/database ErrorFile=/home/jpe/bitcoin-test/datadir/testnet3/db.log
2019-10-29T16:41:49Z init message: Loading banlist...
2019-10-29T16:41:49Z Loaded 0 banned node ips/subnets from banlist.dat  0ms
2019-10-29T16:41:49Z net: setting try another outbound peer=false
2019-10-29T16:41:49Z Cache configuration:
2019-10-29T16:41:49Z * Using 2.0 MiB for block index database
2019-10-29T16:41:49Z * Using 8.0 MiB for chain state database
2019-10-29T16:41:49Z * Using 440.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
2019-10-29T16:41:49Z init message: Loading block index...
2019-10-29T16:41:49Z Opening LevelDB in /home/jpe/bitcoin-test/datadir/testnet3/blocks/index
2019-10-29T16:41:49Z Opened LevelDB successfully
2019-10-29T16:41:49Z Using obfuscation key for /home/jpe/bitcoin-test/datadir/testnet3/blocks/index: 0000000000000000
2019-10-29T16:41:49Z LoadBlockIndexDB: last block file = 0
2019-10-29T16:41:49Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=1, size=293, heights=0...0, time=2011-02-02...2011-02-02)
2019-10-29T16:41:49Z Checking all blk files are present...
2019-10-29T16:41:49Z Opening LevelDB in /home/jpe/bitcoin-test/datadir/testnet3/chainstate
2019-10-29T16:41:49Z Opened LevelDB successfully
2019-10-29T16:41:49Z Using obfuscation key for /home/jpe/bitcoin-test/datadir/testnet3/chainstate: 690559680cb906bf
2019-10-29T16:41:49Z Loaded best chain: hashBestChain=000000000933ea01ad0ee984209779baaec3ced90fa3f408719526f8d77f4943 height=0 date=2011-02-02T23:16:42Z progress=0.000000
2019-10-29T16:41:49Z init message: Rewinding blocks...
2019-10-29T16:41:49Z init message: Verifying blocks...
2019-10-29T16:41:49Z  block index             128ms
2019-10-29T16:41:49Z init message: Loading wallet...
2019-10-29T16:41:49Z BerkeleyEnvironment::Open: LogDir=/home/jpe/bitcoin-test/datadir/testnet3/database ErrorFile=/home/jpe/bitcoin-test/datadir/testnet3/db.log
2019-10-29T16:41:49Z [default wallet] nFileVersion = 180100
2019-10-29T16:41:49Z [default wallet] Keys: 2001 plaintext, 0 encrypted, 2001 w/ metadata, 2001 total. Unknown wallet records: 0
2019-10-29T16:41:49Z [default wallet] Wallet completed loading in              64ms
2019-10-29T16:41:49Z [default wallet] setKeyPool.size() = 2000
2019-10-29T16:41:49Z [default wallet] mapWallet.size() = 0
2019-10-29T16:41:49Z [default wallet] mapAddressBook.size() = 0
2019-10-29T16:41:49Z mapBlockIndex.size() = 1
2019-10-29T16:41:49Z nBestHeight = 0
2019-10-29T16:41:49Z Imported mempool transactions from disk: 0 succeeded, 0 failed, 0 expired, 0 already there
2019-10-29T16:41:49Z AddLocal([ip_address_host2]:18333,1)
2019-10-29T16:41:49Z Discover: IPv4 em1: [ip_address_host2]
2019-10-29T16:41:49Z Bound to [ip_address_host2]:18333
2019-10-29T16:41:49Z AddLocal([ip_address_host2]:18333,2)
2019-10-29T16:41:49Z init message: Loading P2P addresses...
2019-10-29T16:41:49Z Loaded 7 addresses from peers.dat  0ms
2019-10-29T16:41:49Z init message: Starting network threads...
2019-10-29T16:41:49Z DNS seeding disabled
2019-10-29T16:41:49Z net thread start
2019-10-29T16:41:49Z init message: Done loading
2019-10-29T16:41:49Z addcon thread start
2019-10-29T16:41:49Z opencon thread start
2019-10-29T16:41:49Z msghand thread start
2019-10-29T16:42:23Z ThreadRPCServer method=addnode user=user peeraddr=[ip_address_host_bitcoin-cli]:60266
2019-10-29T16:42:25Z trying connection [ip_address_host1] lastseen=0.0hrs
2019-10-29T16:42:25Z Added connection to [ip_address_host1] peer=0
2019-10-29T16:42:25Z sending version (102 bytes) peer=0
2019-10-29T16:42:25Z send version message: version 70015, blocks=0, us=[::]:0, them=[ip_address_host1]:18333, peer=0
2019-10-29T16:42:25Z received: version (102 bytes) peer=0
2019-10-29T16:42:25Z sending verack (0 bytes) peer=0
2019-10-29T16:42:25Z sending getaddr (0 bytes) peer=0
2019-10-29T16:42:25Z receive version message: /Satoshi:0.18.1/: version 70015, blocks=0, us=[ip_address_host2]:49051, peer=0, peeraddr=[ip_address_host1]:18333
2019-10-29T16:42:25Z added time data, samples 2, offset +0 (+0 minutes)
2019-10-29T16:42:25Z received: verack (0 bytes) peer=0
2019-10-29T16:42:25Z New outbound peer connected: version: 70015, blocks=0, peer=0, peeraddr=[ip_address_host1]:18333
2019-10-29T16:42:25Z sending sendheaders (0 bytes) peer=0
2019-10-29T16:42:25Z sending sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z sending sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z sending ping (8 bytes) peer=0
2019-10-29T16:42:25Z initial getheaders (0) to peer=0 (startheight:0)
2019-10-29T16:42:25Z sending getheaders (69 bytes) peer=0
2019-10-29T16:42:25Z sending feefilter (8 bytes) peer=0
2019-10-29T16:42:25Z received: sendheaders (0 bytes) peer=0
2019-10-29T16:42:25Z received: sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z received: sendcmpct (9 bytes) peer=0
2019-10-29T16:42:25Z received: ping (8 bytes) peer=0
2019-10-29T16:42:25Z sending pong (8 bytes) peer=0
2019-10-29T16:42:25Z received: getheaders (69 bytes) peer=0
2019-10-29T16:42:25Z Ignoring getheaders from peer=0 because node is in initial block download
2019-10-29T16:42:25Z received: feefilter (8 bytes) peer=0
2019-10-29T16:42:25Z received: feefilter of 0.00001000 BTC/kB from peer=0
2019-10-29T16:42:25Z received: pong (8 bytes) peer=0
2019-10-29T16:42:29Z received: addr (31 bytes) peer=0
2019-10-29T16:44:25Z sending ping (8 bytes) peer=0
2019-10-29T16:44:25Z received: pong (8 bytes) peer=0
2019-10-29T16:44:25Z received: ping (8 bytes) peer=0
2019-10-29T16:44:25Z sending pong (8 bytes) peer=0
2019-10-29T16:46:25Z received: ping (8 bytes) peer=0
2019-10-29T16:46:25Z sending pong (8 bytes) peer=0
2019-10-29T16:46:25Z sending ping (8 bytes) peer=0
2019-10-29T16:46:25Z received: pong (8 bytes) peer=0
2019-10-29T16:48:25Z sending ping (8 bytes) peer=0
2019-10-29T16:48:25Z received: pong (8 bytes) peer=0
2019-10-29T16:48:25Z received: ping (8 bytes) peer=0
2019-10-29T16:48:25Z sending pong (8 bytes) peer=0
2019-10-29T16:50:25Z sending ping (8 bytes) peer=0
2019-10-29T16:50:25Z received: pong (8 bytes) peer=0
2019-10-29T16:50:25Z received: ping (8 bytes) peer=0
2019-10-29T16:50:25Z sending pong (8 bytes) peer=0
2019-10-29T16:52:25Z sending ping (8 bytes) peer=0
2019-10-29T16:52:25Z received: pong (8 bytes) peer=0
2019-10-29T16:52:25Z received: ping (8 bytes) peer=0
2019-10-29T16:52:25Z sending pong (8 bytes) peer=0
2019-10-29T16:54:25Z sending ping (8 bytes) peer=0
2019-10-29T16:54:25Z received: pong (8 bytes) peer=0
2019-10-29T16:54:25Z received: ping (8 bytes) peer=0
2019-10-29T16:54:25Z sending pong (8 bytes) peer=0
2019-10-29T16:56:25Z sending ping (8 bytes) peer=0
2019-10-29T16:56:25Z received: pong (8 bytes) peer=0
2019-10-29T16:56:25Z received: ping (8 bytes) peer=0
2019-10-29T16:56:25Z sending pong (8 bytes) peer=0
2019-10-29T16:56:49Z Flushed 7 addresses to peers.dat  14ms
2019-10-29T16:58:25Z sending ping (8 bytes) peer=0
2019-10-29T16:58:25Z received: pong (8 bytes) peer=0
2019-10-29T16:58:25Z received: ping (8 bytes) peer=0
2019-10-29T16:58:25Z sending pong (8 bytes) peer=0
2019-10-29T17:00:25Z sending ping (8 bytes) peer=0
2019-10-29T17:00:25Z received: pong (8 bytes) peer=0
2019-10-29T17:00:25Z received: ping (8 bytes) peer=0
2019-10-29T17:00:25Z sending pong (8 bytes) peer=0
2019-10-29T17:02:25Z sending ping (8 bytes) peer=0
2019-10-29T17:02:25Z received: pong (8 bytes) peer=0
2019-10-29T17:02:25Z received: ping (8 bytes) peer=0
2019-10-29T17:02:25Z sending pong (8 bytes) peer=0
2019-10-29T17:04:25Z received: ping (8 bytes) peer=0
2019-10-29T17:04:25Z sending pong (8 bytes) peer=0
2019-10-29T17:04:25Z sending ping (8 bytes) peer=0
2019-10-29T17:04:25Z received: pong (8 bytes) peer=0
2019-10-29T17:06:25Z received: ping (8 bytes) peer=0
2019-10-29T17:06:25Z sending pong (8 bytes) peer=0
2019-10-29T17:06:25Z sending ping (8 bytes) peer=0
2019-10-29T17:06:25Z received: pong (8 bytes) peer=0
2019-10-29T17:08:25Z received: ping (8 bytes) peer=0
2019-10-29T17:08:25Z sending pong (8 bytes) peer=0
2019-10-29T17:08:25Z sending ping (8 bytes) peer=0
2019-10-29T17:08:25Z received: pong (8 bytes) peer=0
2019-10-29T17:10:25Z received: ping (8 bytes) peer=0
2019-10-29T17:10:25Z sending pong (8 bytes) peer=0
2019-10-29T17:10:25Z sending ping (8 bytes) peer=0
2019-10-29T17:10:25Z received: pong (8 bytes) peer=0
2019-10-29T17:11:49Z Flushed 7 addresses to peers.dat  15ms
2019-10-29T17:12:25Z received: ping (8 bytes) peer=0
2019-10-29T17:12:25Z sending pong (8 bytes) peer=0
2019-10-29T17:12:25Z sending ping (8 bytes) peer=0
2019-10-29T17:12:25Z received: pong (8 bytes) peer=0
2019-10-29T17:14:04Z Potential stale tip detected, will try using extra outbound peer (last tip update: 1890 seconds ago)
2019-10-29T17:14:04Z net: setting try another outbound peer=true
2019-10-29T17:14:25Z received: ping (8 bytes) peer=0
2019-10-29T17:14:25Z sending pong (8 bytes) peer=0
2019-10-29T17:14:25Z sending ping (8 bytes) peer=0
2019-10-29T17:14:25Z received: pong (8 bytes) peer=0
2019-10-29T17:16:25Z received: ping (8 bytes) peer=0
2019-10-29T17:16:25Z sending pong (8 bytes) peer=0
2019-10-29T17:16:25Z sending ping (8 bytes) peer=0
2019-10-29T17:16:25Z received: pong (8 bytes) peer=0
2019-10-29T17:18:25Z received: ping (8 bytes) peer=0
2019-10-29T17:18:25Z sending pong (8 bytes) peer=0
2019-10-29T17:18:25Z sending ping (8 bytes) peer=0
2019-10-29T17:18:25Z received: pong (8 bytes) peer=0
2019-10-29T17:20:25Z received: ping (8 bytes) peer=0
2019-10-29T17:20:25Z sending pong (8 bytes) peer=0
2019-10-29T17:20:25Z sending ping (8 bytes) peer=0
2019-10-29T17:20:25Z received: pong (8 bytes) peer=0
2019-10-29T17:22:25Z received: ping (8 bytes) peer=0
2019-10-29T17:22:25Z sending pong (8 bytes) peer=0
2019-10-29T17:22:25Z sending ping (8 bytes) peer=0
2019-10-29T17:22:25Z received: pong (8 bytes) peer=0
2019-10-29T17:24:25Z received: ping (8 bytes) peer=0
2019-10-29T17:24:25Z sending pong (8 bytes) peer=0
2019-10-29T17:24:25Z sending ping (8 bytes) peer=0
2019-10-29T17:24:25Z received: pong (8 bytes) peer=0
2019-10-29T17:24:34Z Potential stale tip detected, will try using extra outbound peer (last tip update: 2520 seconds ago)
2019-10-29T17:24:34Z net: setting try another outbound peer=true

```"
bitcoin/bitcoin,2019-10-25 10:08:11,question,Outgoing P2P connections are reset immediately,"```
2019-10-25T09:39:28Z Bitcoin Core version v0.17.0.1 (release build)
2019-10-25T09:39:28Z InitParameterInteraction: parameter interaction: -whitelistforcerelay=1 -> setting -whitelistrelay=1
2019-10-25T09:39:28Z Assuming ancestors of block 0000000000000000002e63058c023a9a1de233554f28c7b21380b6c9003f36a8 have valid signatu
res.
2019-10-25T09:39:28Z Setting nMinimumChainWork=0000000000000000000000000000000000000000028822fef1c230963535a90d
2019-10-25T09:39:28Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2019-10-25T09:39:28Z Using RdRand as an additional entropy source
2019-10-25T09:39:28Z Default data directory /home/mpsp/.bitcoin
2019-10-25T09:39:28Z Using data directory /data/project/online_project/node/bitcoin_test/bitcoin
2019-10-25T09:39:28Z Using config file /data/project/online_project/node/bitcoin_test/bitcoin/bitcoin.conf
2019-10-25T09:39:28Z Using at most 125 automatic connections (1024 file descriptors available)
2019-10-25T09:39:28Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2019-10-25T09:39:28Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2019-10-25T09:39:28Z Using 16 threads for script verification
2019-10-25T09:39:28Z scheduler thread start
2019-10-25T09:39:28Z Allowing HTTP connections from: 127.0.0.0/8 ::1/128 0.0.0.0/0 
2019-10-25T09:39:28Z Binding RPC on address 10.8.64.178 port 8232
2019-10-25T09:39:28Z Initialized HTTP server
2019-10-25T09:39:28Z HTTP: creating work queue of depth 128
2019-10-25T09:39:28Z Starting RPC
2019-10-25T09:39:28Z Starting HTTP RPC server
2019-10-25T09:39:28Z Config options rpcuser and rpcpassword will soon be deprecated. Locally-run instances may remove rpcuser to use
 cookie-based auth, or may be replaced with rpcauth. Please see share/rpcauth for rpcauth auth generation.
2019-10-25T09:39:28Z Registering HTTP handler for / (exactmatch 1)
2019-10-25T09:39:28Z Registering HTTP handler for /wallet/ (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/tx/ (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/block/notxdetails/ (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/block/ (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/chaininfo (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/mempool/info (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/mempool/contents (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/headers/ (exactmatch 0)
2019-10-25T09:39:28Z Registering HTTP handler for /rest/getutxos (exactmatch 0)
2019-10-25T09:39:28Z Starting HTTP server
2019-10-25T09:39:28Z HTTP: starting 100 worker threads
2019-10-25T09:39:28Z Entering http event loop
2019-10-25T09:39:28Z Using wallet directory /data/project/online_project/node/bitcoin_test/bitcoin
2019-10-25T09:39:28Z init message: Verifying wallet(s)...
2019-10-25T09:39:28Z Using BerkeleyDB version Berkeley DB 4.8.30: (April  9, 2010)
2019-10-25T09:39:28Z Using wallet wallet.dat
2019-10-25T09:39:28Z BerkeleyEnvironment::Open: LogDir=/data/project/online_project/node/bitcoin_test/bitcoin/database ErrorFile=/da
ta/project/online_project/node/bitcoin_test/bitcoin/db.log
2019-10-25T09:39:28Z net: setting try another outbound peer=false
2019-10-25T09:39:28Z Cache configuration:
2019-10-25T09:39:28Z * Using 2.0MiB for block index database
2019-10-25T09:39:28Z * Using 56.0MiB for transaction index database
2019-10-25T09:39:28Z * Using 8.0MiB for chain state database
2019-10-25T09:39:28Z * Using 384.0MiB for in-memory UTXO set (plus up to 286.1MiB of unused mempool space)
2019-10-25T09:39:28Z init message: Loading block index...
2019-10-25T09:39:28Z LevelDB using max_open_files=1000 (default=1000)
2019-10-25T09:39:28Z Opening LevelDB in /data/project/online_project/node/bitcoin_test/bitcoin/blocks/index
2019-10-25T09:39:28Z leveldb: Delete type=3 #1
2019-10-25T09:39:28Z Opened LevelDB successfully
2019-10-25T09:39:28Z Using obfuscation key for /data/project/online_project/node/bitcoin_test/bitcoin/blocks/index: 0000000000000000
2019-10-25T09:39:28Z LoadBlockIndexDB: last block file = 0
2019-10-25T09:39:28Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=0, size=0, heights=0...0, time=1970-01-01...1970-
01-01)
2019-10-25T09:39:28Z Checking all blk files are present...
2019-10-25T09:39:28Z Initializing databases...
2019-10-25T09:39:28Z Pre-allocating up to position 0x1000000 in blk00000.dat
2019-10-25T09:39:28Z LevelDB using max_open_files=1000 (default=1000)
2019-10-25T09:39:28Z Opening LevelDB in /data/project/online_project/node/bitcoin_test/bitcoin/chainstate
2019-10-25T09:39:28Z leveldb: Delete type=3 #1
2019-10-25T09:39:28Z Opened LevelDB successfully
2019-10-25T09:39:28Z WriteBatch memory usage: db=chainstate, before=0.0MiB, after=0.0MiB
2019-10-25T09:39:28Z Wrote new obfuscate key for /data/project/online_project/node/bitcoin_test/bitcoin/chainstate: 28879900dc24e41c
2019-10-25T09:39:28Z Using obfuscation key for /data/project/online_project/node/bitcoin_test/bitcoin/chainstate: 28879900dc24e41c
2019-10-25T09:39:28Z init message: Rewinding blocks...
2019-10-25T09:39:28Z  block index               3ms
2019-10-25T09:39:28Z LevelDB using max_open_files=1000 (default=1000)
2019-10-25T09:39:28Z Opening LevelDB in /data/project/online_project/node/bitcoin_test/bitcoin/indexes/txindex
2019-10-25T09:39:28Z leveldb: Delete type=3 #1
2019-10-25T09:39:28Z Opened LevelDB successfully
2019-10-25T09:39:28Z Using obfuscation key for /data/project/online_project/node/bitcoin_test/bitcoin/indexes/txindex: 0000000000000
000
2019-10-25T09:39:28Z init message: Loading wallet...
2019-10-25T09:39:28Z txindex thread start
2019-10-25T09:39:28Z txindex is enabled
2019-10-25T09:39:28Z txindex thread exit
2019-10-25T09:39:28Z [default wallet] nFileVersion = 170001
2019-10-25T09:39:28Z [default wallet] Keys: 0 plaintext, 0 encrypted, 0 w/ metadata, 0 total. Unknown wallet records: 0
2019-10-25T09:39:28Z [default wallet] Performing wallet upgrade to 169900
2019-10-25T09:39:29Z [default wallet] keypool added 2000 keys (1000 internal), size=2000 (1000 internal)
2019-10-25T09:39:29Z [default wallet] Wallet completed loading in            1103ms
2019-10-25T09:39:29Z [default wallet] setKeyPool.size() = 2000
2019-10-25T09:39:29Z [default wallet] mapWallet.size() = 0
2019-10-25T09:39:29Z [default wallet] mapAddressBook.size() = 0
2019-10-25T09:39:29Z   - Load block from disk: 0.11ms [0.00s]
2019-10-25T09:39:29Z   - Connect total: 0.06ms [0.00s (infms/blk)]
2019-10-25T09:39:29Z   - Flush: 0.01ms [0.00s (infms/blk)]
2019-10-25T09:39:29Z   - Writing chainstate: 0.01ms [0.00s (infms/blk)]
2019-10-25T09:39:29Z UpdateTip: new best=000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f height=0 version=0x0000000
1 log2_work=32.000022 tx=1 date='2009-01-03T18:15:05Z' progress=0.000000 cache=0.0MiB(0txo)
2019-10-25T09:39:29Z   - Connect postprocess: 5.37ms [0.01s (infms/blk)]
2019-10-25T09:39:29Z - Connect block: 5.57ms [0.01s (infms/blk)]
2019-10-25T09:39:29Z mapBlockIndex.size() = 1
2019-10-25T09:39:29Z Failed to open mempool file from disk. Continuing anyway.
2019-10-25T09:39:29Z nBestHeight = 0
2019-10-25T09:39:29Z torcontrol thread start
2019-10-25T09:39:29Z WriteBatch memory usage: db=txindex, before=0.0MiB, after=0.0MiB
2019-10-25T09:39:29Z tor: Error connecting to Tor control socket
2019-10-25T09:39:29Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:29Z Bound to [::]:8233
2019-10-25T09:39:29Z Bound to 0.0.0.0:8233
2019-10-25T09:39:29Z init message: Loading P2P addresses...
2019-10-25T09:39:29Z ERROR: DeserializeFileDB: Failed to open file /data/project/online_project/node/bitcoin_test/bitcoin/peers.dat
2019-10-25T09:39:29Z Invalid or missing peers.dat; recreating
2019-10-25T09:39:29Z Flushed 0 addresses to peers.dat  0ms
2019-10-25T09:39:29Z init message: Loading banlist...
2019-10-25T09:39:29Z ERROR: DeserializeFileDB: Failed to open file /data/project/online_project/node/bitcoin_test/bitcoin/banlist.da
t
2019-10-25T09:39:29Z Invalid or missing banlist.dat; recreating
2019-10-25T09:39:29Z Flushed 0 banned node ips/subnets to banlist.dat  1ms
2019-10-25T09:39:29Z init message: Starting network threads...
2019-10-25T09:39:29Z net thread start
2019-10-25T09:39:29Z dnsseed thread start
2019-10-25T09:39:29Z Loading addresses from DNS seeds (could take a while)
2019-10-25T09:39:29Z init message: Done loading
2019-10-25T09:39:29Z addcon thread start
2019-10-25T09:39:29Z opencon thread start
2019-10-25T09:39:29Z msghand thread start
2019-10-25T09:39:29Z Added 40 addresses from sxa35bqgjufzajdt.internal: 0 tried, 39 new
2019-10-25T09:39:29Z Added 33 addresses from 3ruqazlhqrx4izs4.internal: 0 tried, 72 new
2019-10-25T09:39:29Z Added 34 addresses from fc46cby4cw6bvgfz.internal: 0 tried, 106 new
2019-10-25T09:39:29Z Added 40 addresses from nzcecuuaj4fa5dcl.internal: 0 tried, 146 new
2019-10-25T09:39:30Z trying connection [2a01:4f8:1c17:7e2a::1]:8333 lastseen=75.2hrs
2019-10-25T09:39:30Z connect() to [2a01:4f8:1c17:7e2a::1]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:30Z tor: Error connecting to Tor control socket
2019-10-25T09:39:30Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:30Z trying connection 174.138.51.147:8333 lastseen=152.4hrs
2019-10-25T09:39:31Z Added connection peer=0
2019-10-25T09:39:31Z sending version (104 bytes) peer=0
2019-10-25T09:39:31Z send version message: version 70015, blocks=0, us=[::]:0, peer=0
2019-10-25T09:39:31Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:31Z disconnecting peer=0
2019-10-25T09:39:31Z Cleared nodestate for peer=0
2019-10-25T09:39:31Z Added 37 addresses from epu7hertnka6c2px.internal: 0 tried, 183 new
2019-10-25T09:39:31Z trying connection [2001:760:2c0c:202:f4bb:ff:fe80:2b42]:8333 lastseen=81.1hrs
2019-10-25T09:39:31Z connect() to [2001:760:2c0c:202:f4bb:ff:fe80:2b42]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:31Z Added 23 addresses from mr3yswg4s4rrqkm4.internal: 0 tried, 206 new
2019-10-25T09:39:32Z trying connection 90.187.18.209:8333 lastseen=145.3hrs
2019-10-25T09:39:32Z Flushing wallet.dat
2019-10-25T09:39:32Z tor: Error connecting to Tor control socket
2019-10-25T09:39:32Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:32Z Flushed wallet.dat 2ms
2019-10-25T09:39:32Z Added connection peer=1
2019-10-25T09:39:32Z sending version (104 bytes) peer=1
2019-10-25T09:39:32Z send version message: version 70015, blocks=0, us=[::]:0, peer=1
2019-10-25T09:39:32Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:32Z disconnecting peer=1
2019-10-25T09:39:32Z Cleared nodestate for peer=1
2019-10-25T09:39:32Z trying connection 13.250.32.177:8333 lastseen=74.9hrs
2019-10-25T09:39:33Z Added connection peer=2
2019-10-25T09:39:33Z sending version (104 bytes) peer=2
2019-10-25T09:39:33Z send version message: version 70015, blocks=0, us=[::]:0, peer=2
2019-10-25T09:39:33Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:33Z disconnecting peer=2
2019-10-25T09:39:33Z Cleared nodestate for peer=2
2019-10-25T09:39:33Z Added 34 addresses from 7v4ngmag7nmkcv4l.internal: 0 tried, 240 new
2019-10-25T09:39:33Z 245 addresses found from DNS seeds
2019-10-25T09:39:33Z dnsseed thread exit
2019-10-25T09:39:33Z trying connection 13.229.152.67:8333 lastseen=125.0hrs
2019-10-25T09:39:33Z Added connection peer=3
2019-10-25T09:39:33Z sending version (104 bytes) peer=3
2019-10-25T09:39:33Z send version message: version 70015, blocks=0, us=[::]:0, peer=3
**2019-10-25T09:39:33Z socket recv error Connection reset by peer (104)**
2019-10-25T09:39:33Z disconnecting peer=3
2019-10-25T09:39:33Z Cleared nodestate for peer=3
2019-10-25T09:39:34Z trying connection 116.62.64.237:8333 lastseen=109.1hrs
2019-10-25T09:39:34Z Added connection peer=4
2019-10-25T09:39:34Z sending version (104 bytes) peer=4
2019-10-25T09:39:34Z send version message: version 70015, blocks=0, us=[::]:0, peer=4
**2019-10-25T09:39:34Z socket recv error Connection reset by peer (104)**
2019-10-25T09:39:34Z disconnecting peer=4
2019-10-25T09:39:34Z Cleared nodestate for peer=4
2019-10-25T09:39:34Z tor: Error connecting to Tor control socket
2019-10-25T09:39:34Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:34Z trying connection 5.196.65.205:8333 lastseen=136.8hrs
2019-10-25T09:39:34Z Added connection peer=5
2019-10-25T09:39:34Z sending version (104 bytes) peer=5
2019-10-25T09:39:34Z send version message: version 70015, blocks=0, us=[::]:0, peer=5
2019-10-25T09:39:34Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:34Z disconnecting peer=5
2019-10-25T09:39:34Z Cleared nodestate for peer=5
2019-10-25T09:39:35Z trying connection 52.197.62.118:8333 lastseen=116.4hrs
2019-10-25T09:39:35Z Added connection peer=6
2019-10-25T09:39:35Z sending version (104 bytes) peer=6
2019-10-25T09:39:35Z send version message: version 70015, blocks=0, us=[::]:0, peer=6
2019-10-25T09:39:35Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:35Z disconnecting peer=6
2019-10-25T09:39:35Z Cleared nodestate for peer=6
2019-10-25T09:39:35Z trying connection 45.45.30.90:8333 lastseen=138.0hrs
2019-10-25T09:39:36Z Added connection peer=7
2019-10-25T09:39:36Z sending version (104 bytes) peer=7
2019-10-25T09:39:36Z send version message: version 70015, blocks=0, us=[::]:0, peer=7
2019-10-25T09:39:36Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:36Z disconnecting peer=7
2019-10-25T09:39:36Z Cleared nodestate for peer=7
2019-10-25T09:39:36Z trying connection 75.136.216.43:8333 lastseen=140.0hrs
2019-10-25T09:39:36Z Added connection peer=8
2019-10-25T09:39:36Z sending version (104 bytes) peer=8
2019-10-25T09:39:36Z send version message: version 70015, blocks=0, us=[::]:0, peer=8
2019-10-25T09:39:36Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:36Z disconnecting peer=8
2019-10-25T09:39:36Z Cleared nodestate for peer=8
2019-10-25T09:39:37Z trying connection [2a01:4f8:c2c:1b21::1]:8333 lastseen=143.3hrs
2019-10-25T09:39:37Z connect() to [2a01:4f8:c2c:1b21::1]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:37Z tor: Error connecting to Tor control socket
2019-10-25T09:39:37Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:37Z trying connection 18.136.51.188:8333 lastseen=138.7hrs
2019-10-25T09:39:38Z Added connection peer=9
2019-10-25T09:39:38Z sending version (104 bytes) peer=9
2019-10-25T09:39:38Z send version message: version 70015, blocks=0, us=[::]:0, peer=9
2019-10-25T09:39:38Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:38Z disconnecting peer=9
2019-10-25T09:39:38Z Cleared nodestate for peer=9
2019-10-25T09:39:38Z trying connection 60.191.106.148:8333 lastseen=99.3hrs
2019-10-25T09:39:38Z Added connection peer=10
2019-10-25T09:39:38Z sending version (104 bytes) peer=10
2019-10-25T09:39:38Z send version message: version 70015, blocks=0, us=[::]:0, peer=10
2019-10-25T09:39:38Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:38Z disconnecting peer=10
2019-10-25T09:39:38Z Cleared nodestate for peer=10
2019-10-25T09:39:39Z trying connection 31.43.140.190:8333 lastseen=111.4hrs
2019-10-25T09:39:39Z Added connection peer=11
2019-10-25T09:39:39Z sending version (104 bytes) peer=11
2019-10-25T09:39:39Z send version message: version 70015, blocks=0, us=[::]:0, peer=11
2019-10-25T09:39:39Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:39Z disconnecting peer=11
2019-10-25T09:39:39Z Cleared nodestate for peer=11
2019-10-25T09:39:39Z trying connection [2a01:4f8:c010:2f11::1]:8333 lastseen=138.8hrs
2019-10-25T09:39:39Z connect() to [2a01:4f8:c010:2f11::1]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:40Z trying connection [2001:67c:22fc:1337::5]:8333 lastseen=102.4hrs
2019-10-25T09:39:40Z connect() to [2001:67c:22fc:1337::5]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:40Z trying connection [2001:0:9d38:6ab8:141b:2d1:b83d:5257]:8333 lastseen=162.3hrs
2019-10-25T09:39:40Z connect() to [2001:0:9d38:6ab8:141b:2d1:b83d:5257]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:41Z trying connection [2001:0:4137:9e76:30c6:355c:b954:92d9]:8333 lastseen=142.0hrs
2019-10-25T09:39:41Z connect() to [2001:0:4137:9e76:30c6:355c:b954:92d9]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:41Z trying connection 173.212.220.9:8333 lastseen=131.6hrs
2019-10-25T09:39:42Z Added connection peer=12
2019-10-25T09:39:42Z sending version (104 bytes) peer=12
2019-10-25T09:39:42Z send version message: version 70015, blocks=0, us=[::]:0, peer=12
2019-10-25T09:39:42Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:42Z disconnecting peer=12
2019-10-25T09:39:42Z Cleared nodestate for peer=12
2019-10-25T09:39:42Z trying connection 145.239.133.16:8333 lastseen=141.8hrs
2019-10-25T09:39:43Z tor: Error connecting to Tor control socket
2019-10-25T09:39:43Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:43Z Added connection peer=13
2019-10-25T09:39:43Z sending version (104 bytes) peer=13
2019-10-25T09:39:43Z send version message: version 70015, blocks=0, us=[::]:0, peer=13
2019-10-25T09:39:43Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:43Z disconnecting peer=13
2019-10-25T09:39:43Z Cleared nodestate for peer=13
2019-10-25T09:39:44Z trying connection 195.242.93.189:8333 lastseen=133.7hrs
2019-10-25T09:39:44Z Added connection peer=14
2019-10-25T09:39:44Z sending version (104 bytes) peer=14
2019-10-25T09:39:44Z send version message: version 70015, blocks=0, us=[::]:0, peer=14
2019-10-25T09:39:44Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:44Z disconnecting peer=14
2019-10-25T09:39:44Z Cleared nodestate for peer=14
2019-10-25T09:39:45Z trying connection [2001:0:5ef5:79fb:1090:2e42:b3bc:41c8]:8333 lastseen=145.1hrs
2019-10-25T09:39:45Z connect() to [2001:0:5ef5:79fb:1090:2e42:b3bc:41c8]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:45Z trying connection [2a01:4f8:c2c:27fc::1]:8333 lastseen=106.5hrs
2019-10-25T09:39:45Z connect() to [2a01:4f8:c2c:27fc::1]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:46Z trying connection [2a01:cb00:7cd:b000:fa1f:bd1:fe0:62a6]:8333 lastseen=88.2hrs
2019-10-25T09:39:46Z connect() to [2a01:cb00:7cd:b000:fa1f:bd1:fe0:62a6]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:46Z trying connection 3.1.203.33:8333 lastseen=133.5hrs
2019-10-25T09:39:46Z Added connection peer=15
2019-10-25T09:39:46Z sending version (104 bytes) peer=15
2019-10-25T09:39:46Z send version message: version 70015, blocks=0, us=[::]:0, peer=15
2019-10-25T09:39:46Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:46Z disconnecting peer=15
2019-10-25T09:39:46Z Cleared nodestate for peer=15
2019-10-25T09:39:47Z trying connection [2a00:d2a0:a:3d00:405:3f71:b2e:34f9]:8333 lastseen=109.3hrs
2019-10-25T09:39:47Z connect() to [2a00:d2a0:a:3d00:405:3f71:b2e:34f9]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:47Z trying connection 94.242.187.147:8333 lastseen=152.1hrs
2019-10-25T09:39:50Z tor: Error connecting to Tor control socket
2019-10-25T09:39:50Z tor: Not connected to Tor control port 127.0.0.1:9051, trying to reconnect
2019-10-25T09:39:52Z connection to 94.242.187.147:8333 timeout
2019-10-25T09:39:53Z trying connection 167.86.105.128:8333 lastseen=77.1hrs
2019-10-25T09:39:53Z Added connection peer=16
2019-10-25T09:39:53Z sending version (104 bytes) peer=16
2019-10-25T09:39:53Z send version message: version 70015, blocks=0, us=[::]:0, peer=16
2019-10-25T09:39:53Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:53Z disconnecting peer=16
2019-10-25T09:39:53Z Cleared nodestate for peer=16
2019-10-25T09:39:54Z trying connection 217.160.185.97:8333 lastseen=126.7hrs
2019-10-25T09:39:54Z Added connection peer=17
2019-10-25T09:39:54Z sending version (104 bytes) peer=17
2019-10-25T09:39:54Z send version message: version 70015, blocks=0, us=[::]:0, peer=17
2019-10-25T09:39:54Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:54Z disconnecting peer=17
2019-10-25T09:39:54Z Cleared nodestate for peer=17
2019-10-25T09:39:54Z trying connection 62.210.124.142:8333 lastseen=75.1hrs
2019-10-25T09:39:54Z Added connection peer=18
2019-10-25T09:39:54Z sending version (104 bytes) peer=18
2019-10-25T09:39:54Z send version message: version 70015, blocks=0, us=[::]:0, peer=18
2019-10-25T09:39:55Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:55Z disconnecting peer=18
2019-10-25T09:39:55Z Cleared nodestate for peer=18
2019-10-25T09:39:55Z trying connection [2001:470:6c80:101::1]:8333 lastseen=73.1hrs
2019-10-25T09:39:55Z connect() to [2001:470:6c80:101::1]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:56Z trying connection 18.138.255.8:8333 lastseen=163.6hrs
2019-10-25T09:39:56Z Added connection peer=19
2019-10-25T09:39:56Z sending version (104 bytes) peer=19
2019-10-25T09:39:56Z send version message: version 70015, blocks=0, us=[::]:0, peer=19
2019-10-25T09:39:56Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:56Z disconnecting peer=19
2019-10-25T09:39:56Z Cleared nodestate for peer=19
2019-10-25T09:39:56Z trying connection 31.179.204.142:8333 lastseen=145.7hrs
2019-10-25T09:39:56Z Added connection peer=20
2019-10-25T09:39:56Z sending version (104 bytes) peer=20
2019-10-25T09:39:56Z send version message: version 70015, blocks=0, us=[::]:0, peer=20
2019-10-25T09:39:56Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:56Z disconnecting peer=20
2019-10-25T09:39:56Z Cleared nodestate for peer=20
2019-10-25T09:39:57Z trying connection 139.180.161.239:8333 lastseen=119.7hrs
2019-10-25T09:39:57Z Added connection peer=21
2019-10-25T09:39:57Z sending version (104 bytes) peer=21
2019-10-25T09:39:57Z send version message: version 70015, blocks=0, us=[::]:0, peer=21
2019-10-25T09:39:57Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:57Z disconnecting peer=21
2019-10-25T09:39:57Z Cleared nodestate for peer=21
2019-10-25T09:39:58Z trying connection 62.212.83.41:8333 lastseen=90.7hrs
2019-10-25T09:39:58Z Added connection peer=22
2019-10-25T09:39:58Z sending version (104 bytes) peer=22
2019-10-25T09:39:58Z send version message: version 70015, blocks=0, us=[::]:0, peer=22
2019-10-25T09:39:58Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:58Z disconnecting peer=22
2019-10-25T09:39:58Z Cleared nodestate for peer=22
2019-10-25T09:39:58Z trying connection [2a01:4f8:191:418f::2]:8333 lastseen=85.5hrs
2019-10-25T09:39:58Z connect() to [2a01:4f8:191:418f::2]:8333 failed: Network is unreachable (101)
2019-10-25T09:39:59Z trying connection 167.99.224.63:8333 lastseen=149.8hrs
2019-10-25T09:39:59Z Added connection peer=23
2019-10-25T09:39:59Z sending version (104 bytes) peer=23
2019-10-25T09:39:59Z send version message: version 70015, blocks=0, us=[::]:0, peer=23
2019-10-25T09:39:59Z socket recv error Connection reset by peer (104)
2019-10-25T09:39:59Z disconnecting peer=23
```
When my node starts, it always wraps “socket recv error Connection reset by peer (104)” error，does not synchronize blocks，Why？"
bitcoin/bitcoin,2019-10-24 02:33:24,question,How to ensure there is no version mismatch?,"### Motivation

We would like to ship `bitcoind` with our software, however it's problematic if the user has already Bitcoin Core in her machine, because of the disk requirements.

In order to avoid API/database/etc incompatibilities and various potential disastrous scenarios we would like to prevent the user to use Bitcoin Core with our software if the version of her own Core and our shipped Core differs.  

### Question

How do similar software resolve this issue?

### Idea 1

Should I play a setup procedure with the user like this:

```
1. Do you want to use Bitcoin Core with Wasabi? Yes -> 2. / No -> END
2. Do you have already Bitcoin Core installed? Yes -> 3. / No -> GOOD
3. What version of Bitcoin Core do you have? ... -> Same -> 4. / Different -> BAD
4. Try to detect data folder automatically. Success -> GOOD / Fail -> 5.
5. What's your current Bitcoin Core's data folder? ... -> GOOD

- What if we update Bitcoin Core? Replay step 3.
- What if the user updates Bitcoin Core? It's a problem, there's no way we can detect it.
```

### Idea 2

What I can do is to try to detect Bitcoin Core's default data folder and also get some input from the user if she's using a custom data folder, but is there a way to acquire the version from the data folder somehow?  

### Idea 3

I could call `bitcoind --version`. In this case the user should tell us the location of `bitcoind`. In that case, is it possible to recognize programmatically her custom data folder if she's using one?

### Reiterating The Question

What are the common strategies of doing this? Is it possible to detect if Bitcoin Core is installed and what data folder is used?"
bitcoin/bitcoin,2019-10-10 20:38:31,question,build depends: How to use with ccache?,"Is it possible to compile depends with ccache enabled? If so, what is the recommended way of doing so?"
bitcoin/bitcoin,2019-09-15 21:27:16,question,"armhf GUI: Permission denied: ""/database/lost+found/wallet.dat"" Aborted","Bitcoin Core version v0.18.99.0-4bfef0dae-dirty
Compiled natively on ""Linux Debian-Desktop 4.4.185 #1 SMP Fri Jul 19 14:34:55 EDT 2019 aarch64 GNU/Linux""

```
./autogen.sh
./configure --disable-debug CFLAGS=""-g0 -O3"" CXXFLAGS=""-g0 -O3"" BDB_LIBS=""-L${BDB_PREFIX}/lib -ldb_cxx-4.8"" BDB_CFLAGS=""-I${BDB_PREFIX}/include"" --enable-werror --with-qrencode --enable-cxx --disable-shared --with-pic --with-boost-libdir=/usr/lib/arm-linux-gnueabihf
```
GUI runs OK. And noticeable faster than previously compiled (with debug enabled) v0.18.99.0-495db72ee-dirty. But both

- Bitcoin Core version v0.18.99.0-495db72ee-dirty (debug enabled, default optimization)
- Bitcoin Core version v0.18.99.0-4bfef0dae-dirty (debug disabled, -O3)

are crashing when mouse cursor is pointed to File/OpenWallet menu item. Starting parameters are the same:

bitcoin-qt -datadir=/bdb -nodebug -dbcache=1536

/bdb is dedicated RAID1 volume with bitcoin database inside. Console output:

```
terminate called after throwing an instance of 'boost::filesystem::filesystem_error'
  what():  boost::filesystem::status: Permission denied: ""/bdb/lost+found/wallet.dat""
```
There is nothing about the crash in debug.log before sudden application abort.
I'm wondering for what purpose to access the wallets in lost+found folder? I thought there may be a problems with access rights for that folder. I have deleted lost+found and executed fsck -f /dev/md0 for /bdb directory volume. The problem still here.
While configuring the build, I had to add
```
--with-boost-libdir=/usr/lib/arm-linux-gnueabihf
```
because configure script was unable to find boost executables."
bitcoin/bitcoin,2019-08-01 08:46:05,question,Accessing function in anonymous namespace from a unit test,"Looking at script/descriptor.cpp, and the associated test file descriptor_tests.cpp, much of the code in descriptor.cpp is inside an anonymous namespace (AN). If I'm not mistaken, this makes functions accessible only within the translation unit corresponding to the source file they're defined in. Functions that are used in the test in descriptor_tests.cpp (such as the `Parse` function) are outside of the AN. I would like to use the `DescriptorChecksum` function from descriptor.cpp in a unit test, but it's inside the AN and thus inaccessible from descriptor_tests.cpp. I currently see three options:
- `#include ""descriptor.cpp""` at the top of descriptor_tests.cpp, which is ugly
- Move the `DescriptorChecksum` function outside of the AN.
- Leave it alone and find something else to unit test

What's the best way to approach this?

Also - a lot of the code is inside ANs. Is there a special reason for this? Wouldn't it make sense to gradually move the code outside of ANs to make it more accessible to unit tests?"
bitcoin/bitcoin,2019-07-24 03:05:54,question,New to Bitcoin Core,"Hello there I'm a little lost here and wondering if you can help. 

I thought BitcoinCore was a plug and play set up for receiving BTC for validation of a bitcoin transaction between two people. 

However I did not know there was more steps to this. Wonder if anyone have insight and can lend me there time and helping me through this? Not sure where to look or what to read. 

I download BitcoinCore and downloaded the whole network. And obtained my wallet. What's next in order to validate BTC transactions?

I currently have this set up through windows 10"
bitcoin/bitcoin,2019-07-04 00:37:41,question,How to create HD address?,"I am wondering if it's possible to create an HD address with bitcoind's getnewaddress RPC call?
If not how does one create an HD address?

"
bitcoin/bitcoin,2019-06-24 09:29:45,question,bumpfee not returning hash of newly created transaction,"
I'm trying to implement replace by fee functionality with bictoind (v18) and bumpfee rpc call described here

It works by mean that new transaction is created with higher fee, but result should be:

```
{
  ""txid"":    ""value"",   (string)  The id of the new transaction
  ""origfee"":  n,         (numeric) Fee of the replaced transaction
  ""fee"":      n,         (numeric) Fee of the new transaction
  ""errors"":  [ str... ] (json array of strings) Errors encountered during processing (may be empty)
}
```

but I'm getting basically all null response

```
{
  ""id"": ""BumpFeeRequest 1561247020661"",
  ""error"": null,
  ""txid"": null,
  ""origfee"": null,
  ""fee"": null,
  ""errors"": null
}
```

Which makes it hard to identify the new transaction in down-steam system.

I believe that hash of newly created tx should be returned

I posted this also here:
https://bitcoin.stackexchange.com/questions/88560/bitcoind-jsonrpc-bumpfee-not-returning-hash-of-newly-created-transaction
"
bitcoin/bitcoin,2019-06-14 02:04:39,question,Can't bind rpcport inside docker,"I dont really know if it is a bitcoin core problem or not , but I'm trying hard and I can't get it

> Binding RPC on address :: port 18332 failed.

More info: https://github.com/kylemanna/docker-bitcoind/issues/42"
bitcoin/bitcoin,2019-06-09 23:22:46,question,Bitcoin testnet rpc is binding to localhost ,"Hello , Thanks for the efforts here . currently, i have installed couple of time and at different server bitcoind and i have even pull up github at server and compiled it still bitcoind rpc is binding to localhost even after i have added rpcallowip=0.0.0.0/0

i saw 

root@arraymachinetest-desktop:~/.bitcoin# sudo netstat --ip -lpa|grep bitcoin
tcp        0      0 localhost:29000         0.0.0.0:*               LISTEN      20921/bitcoind      
tcp        0      0 localhost:18332         0.0.0.0:*               LISTEN      20921/bitcoind      
tcp        0      0 0.0.0.0:18333           0.0.0.0:*               LISTEN      20921/bitcoind    

i used this config file :

server=1
testnet=1
rpcallowip=0.0.0.0/0
port=18332
rpcport=18333
rpcuser=foo
rpcpassword=bar

it was working for me before . any solution for this ? 






"
bitcoin/bitcoin,2019-05-16 07:21:38,question,Is it possible to downgrade from 0.18.0 to 0.17.0?,"I am running a full node on a Ubuntu server.

Yesterday, I upgraded the bitcoin package from 0.17.0 to 0.18.0 using apt-get.
However, I really need the account RPCs.

Is it possible to get back to 0.17.0 without the full sync from the scratch?"
bitcoin/bitcoin,2019-04-12 16:02:56,question,Signing transactions locked with OP_CHECKLOCKTIMEVERIFY,"I've been trying to send and sign a transaction paying to a multisig locked with `<locktime> OP_CHECKLOCKTIMEVERIFY OP_DROP` and noticed that this is considered nonstandard in bitcoin and thus not possible.

Are there any plans or interest in implementing something like this?

I've implemented this in a fork using a new transaction type (TX_LOCKED_MULTISIG), where the Solver first checks for the lock script above and then proceeds to ""MatchMultisig"". Similarly this new type can be recognised when signing the transaction by simply ignoring the preceding lock script, which is checked when verifying the script anyway. 

It would be quite straightforward to implement for the remaining standard transaction types as well or maybe even better with some manipulation use a single transaction type if possible."
bitcoin/bitcoin,2019-04-03 21:51:32,question,Unable to importprivkey,"Have been trying to import some recovered private keys into bitcoin core for the past 2 days. After syncing the entire bitcoin blockchain, I've verified the same using `bitcoin-cli getnetworkinfo`.

Then, I've tried to run `bitcoin-cli importprivkey the_key """" true` to enable a rescan after import (since I wanted to do one at a time, to verify spend-ability of balances).

However, while looking at the debug.log, I notice that the rescan is complete, yet the balances are not updated across my wallets.

Since I am unable to determine why the balances are not updating, I am trying to import the address into bitcoin core first before importing the private key.

Note, that the private key I tried to import starts with a K and has 52 characters. The beginning part of the recovered path is: `m/44'/0'/0'/0/427`.

I expect to be able to import these balances and make them spendable. However, I'm not even able to see the balances when running `bitcoin-cli getwalletinfo`."
bitcoin/bitcoin,2019-03-04 07:15:34,question,"Fixed some times can not remove ""$SUFFIX-dirty"" on version number cor…","Fixed some times can not remove ""$SUFFIX-dirty"" on version number correctly with git tag using Gitian.

Even you use git annotated tag encounter with such problem."
bitcoin/bitcoin,2019-02-28 05:16:43,question,"error: #error ""Bitcoin cannot be compiled without assertions.""","Hi, all:

**My ENV:** 
_Ubuntu 18.04.2
Qt 5.12.1
bitcoin: git master of the day Feb 27, 2019._ 


My configuration is:
```console
$ export PKG_CONFIG_PATH=/opt/Qt/Current/gcc_64/lib/pkgconfig
$ ./configure --with-incompatible-bdb --enable-gprof=yes -with-gui=yes  --with-boost-libdir=/usr/lib --with-boost-system=boost_system --with-boost-filesystem=boost_filesystem --with-boost-thread=boost_thread --with-boost-chrono=boost_chrono --with-boost-unit-test-framework=boost_unit_test_framework
```

If `--with-gui=no`,  I can successfully build **bitcoin**. However, by enabling gui/qt, I obtained the following **ERROR** messages:


```
In file included from ./util/system.h:19,
                 from ./dbwrapper.h:12,
                 from ./txdb.h:10,
                 from ./test/test_bitcoin.h:14,
                 from ./wallet/test/wallet_test_fixture.h:8,
                 from wallet/test/wallet_test_fixture.cpp:5:
./compat/assumptions.h:17:3: error: #error ""Bitcoin cannot be compiled without assertions.""
 # error ""Bitcoin cannot be compiled without assertions.""
   ^~~~~
In file included from ./util/system.h:19,
                 from ./dbwrapper.h:12,
                 from ./txdb.h:10,
                 from ./test/test_bitcoin.h:14,
                 from test/test_bitcoin.cpp:5:
./compat/assumptions.h:17:3: error: #error ""Bitcoin cannot be compiled without assertions.""
 # error ""Bitcoin cannot be compiled without assertions.""
   ^~~~~
  CXX      crypto/libbitcoinconsensus_la-siphash.lo
  CXX      crypto/libbitcoinconsensus_la-sha256_sse4.lo
  CXX      libbitcoinconsensus_la-arith_uint256.lo
  CXX      consensus/libbitcoinconsensus_la-merkle.lo
  CXX      libbitcoinconsensus_la-hash.lo
  CXX      primitives/libbitcoinconsensus_la-block.lo
  CXX      primitives/libbitcoinconsensus_la-transaction.lo
  CXX      libbitcoinconsensus_la-pubkey.lo
In file included from ./wallet/walletdb.h:12,
                 from ./wallet/wallet.h:23,
                 from ./wallet/test/wallet_test_fixture.h:12,
                 from wallet/test/wallet_test_fixture.cpp:5:
./wallet/db.h: In destructor ‘BerkeleyDatabase::~BerkeleyDatabase()’:
./wallet/db.h:129:20: warning: unused variable ‘erased’ [-Wunused-variable]
             size_t erased = env->m_databases.erase(strFile);
                    ^~~~~~
  CXX      script/libbitcoinconsensus_la-bitcoinconsensus.lo
  CXX      script/libbitcoinconsensus_la-interpreter.lo
make[2]: *** [Makefile:10957: test/qt_test_test_bitcoin_qt-test_bitcoin.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[2]: *** [Makefile:10999: wallet/test/qt_test_test_bitcoin_qt-wallet_test_fixture.o] Error 1
make[2]: Leaving directory '....../bitcoingui/src'
make[1]: *** [Makefile:12939: all-recursive] Error 1
make[1]: Leaving directory '....../bitcoingui/src'
make: *** [Makefile:774: all-recursive] Error 1
```

Any suggestions?
"
bitcoin/bitcoin,2019-02-07 04:44:48,question,MacOS wallet compilation on Linux Ubuntu ,"Can anyone provide step by step instructions, how to compile bitcoin wallet for MacOS / XOS on an **Ubuntu 16.04 or 18.04** machine?"
bitcoin/bitcoin,2019-01-16 20:11:04,question,void,
bitcoin/bitcoin,2018-12-26 08:14:18,question,Rpc not working on 0.17.1,"```
curl --user test:test --data-binary '{""jsonrpc"": ""1.0"", ""id"":""curltest"", ""method"": ""getbalance"", ""params"": [""*"", 1] }' -H 'content-type: text/plain;' http://127.0.0.1:8332/
curl: (7) Failed to connect to 127.0.0.1 port 8332: Connection refused

```
But version 0.16 works "
bitcoin/bitcoin,2018-11-09 03:57:30,question,Why did not return?,"I use bitcoind -daemon -datadir=/mydata/btc/storage -rpcport=1234 to start BTC wallet.

and use code: as follows (python)

import requests
params={""method"": ""GetBestBlockHash"",""params"": [],""id"": ""foo""}
url=""http://127.0.0.1:1234""
print(requests.post(url=url,json=params).text)

but no return"
bitcoin/bitcoin,2018-11-04 10:16:04,question,Error when building binary files under Windows 64/32 bit,"Hello,

I collect Bitcoin client under Windows 64/32 and an error occurred.

Staging miniupnpc...
Postprocessing miniupnpc...
Caching miniupnpc...
copying packages: native_ccache native_protobuf boost openssl libevent zeromq qrencode protobuf zlib qt bdb miniupnpc
to: /root/bitcoin/depends/x86_64-w64-mingw32
bash: ./configure: No such file or directory

I tried to collect on ubunt 14.04; 16.04; 18.04 64 bit
"
bitcoin/bitcoin,2018-10-30 12:11:46,question,RPC Error: error -8,"i just setup btc wallet and it show this error:
RPC Error: error -8: dummy value must be set to ""*""

i have tried run ./bitcoind --deprecatedrpc=accounts
but the error still showing
"
bitcoin/bitcoin,2023-08-17 20:25:26,bug,rpc: remove one more quote from non-string oneline description,"This fixes a silent conflict between https://github.com/bitcoin/bitcoin/pull/28123 (which removed all `\\""options\\""`) and https://github.com/bitcoin/bitcoin/pull/27460 (which added a new one). 

It should fix the current CI failures."
bitcoin/bitcoin,2023-06-20 01:37:29,bug,wallets created on master get corrupted when processed with v25,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

I randomly encountered this by loading a wallet created from master with an older version. I could trace it down to bd13dc2f46ea10302a928fcf0f53b7aed77ad260 (#26076), but I don't know descriptor wallets enough to understand what's going on (cc @Sjors @achow101).

### Expected behaviour

`bitcoind` starts, or at least tells me to use a later version if #26076 was a breaking change instead of corrupting `wallet.dat`.

### Steps to reproduce

(master, clean chain): 
`createwallet -regtest """"`
(v25.0rc2):
first `bitcoind -regtest` succeeds
then stop and restart the node: The second `bitcoind -regtest` leads to an InitError:
`Error: Error: Unable to expand wallet descriptor from cache`
now, the wallet cannot be loaded by either version.

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Compiled from source

### What version of Bitcoin Core are you using?

v25.0rc2 / master

### Operating system and version

Ubuntu

### Machine specifications

_No response_"
bitcoin/bitcoin,2023-06-10 05:18:46,bug,"Indefinite ""Bitcoin Core is shutting down...""","### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

After shutting down Bitcoin Core Qt, the window ""Bitcoin Core is shutting down... Do not shut down the computer until this window disappears."" remains open indefinitely (or at least 24h).

### Expected behaviour

Quick shutdown.

### Steps to reproduce

Try to stop Bitcoin-Qt

### Relevant log output

2023-06-09T01:32:58Z UPnP Port Mapping successful.
2023-06-09T01:37:38Z tor: Thread interrupt
2023-06-09T01:37:38Z Shutdown: In progress...
2023-06-09T01:37:38Z addcon thread exit
2023-06-09T01:37:38Z torcontrol thread exit
2023-06-09T01:37:38Z opencon thread exit
2023-06-09T01:37:38Z net thread exit
2023-06-09T01:37:38Z UPNP_DeletePortMapping() returned: 0
2023-06-09T01:37:38Z mapport thread exit
2023-06-09T01:37:38Z msghand thread exit
2023-06-09T01:48:37Z Potential stale tip detected, will try using extra outbound peer (last tip update: 1804 seconds ago)
2023-06-09T01:59:07Z Potential stale tip detected, will try using extra outbound peer (last tip update: 2434 seconds ago)
2023-06-09T02:09:37Z Potential stale tip detected, will try using extra outbound peer (last tip update: 3064 seconds ago)

...

2023-06-10T05:06:39Z Potential stale tip detected, will try using extra outbound peer (last tip update: 100086 seconds ago)


### How did you obtain Bitcoin Core

Pre-built binaries

### What version of Bitcoin Core are you using?

v25.0

### Operating system and version

debian 5.10.0-22-amd64

### Machine specifications

24 core
32 GB RAM
at times flaky but fast internet
secondary 2TB SSD with symlinks under ~/.bitcoin/{blocks,chainstate,indexes}"
bitcoin/bitcoin,2023-05-23 05:10:37,bug,Validation of malformed address fails with a peculiar message,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behaviour

Open RPC console and execute this command:

`validateaddress bc1qqrq69gfzvvqcxs6rgg3crqjzcw369sxzyp3v9sspursx9gmzyv32x7xa5z`

Following message is the result:

```
Internal bug detected: 'isValid == error_msg.empty()'
rpc/misc.cpp:75 (operator())
You may report this issue here: https://github.com/bitcoin/bitcoin/issues
 (code -1)
```


### Expected behaviour

Well it should tell me something, like what is wrong exactly. Though I believe that too much is wrong with this particular address.

### Steps to reproduce

Run validateaddress method using `bc1qqrq69gfzvvqcxs6rgg3crqjzcw369sxzyp3v9sspursx9gmzyv32x7xa5z` as its argument.

### Relevant log output

_No response_

### How did you obtain Bitcoin Core

Pre-built binaries

### What version of Bitcoin Core are you using?

/Satoshi:23.0.0/

### Operating system and version

MacOS Ventura 13.4 (22F66)

### Machine specifications

MacBook Pro 13-inch, M1, 2020
With 16GB of RAM"
bitcoin/bitcoin,2023-04-19 13:16:07,bug,ci: failure in Docker build step,"Failure here: https://cirrus-ci.com/task/4581145136857088 in #27479. Does this just need a rebase? cc @MarcoFalke 
```bash
docker build --tag gcr.io/cirrus-ci-community/bitcoin/bitcoin/ci/test_imagefile:d050f5b1ebc8c0ebdf779e9eda85ea03bfd0fca46391a14e57fbe478652e6623 --file ci/test_imagefile --build-arg CI_IMAGE_NAME_TAG=""ubuntu:focal"" --build-arg FILE_ENV=""./ci/test/00_setup_env_native_nowallet_libbitcoinkernel.sh"" ${CIRRUS_DOCKER_CONTEXT:-$CIRRUS_WORKING_DIR}
Sending build context to Docker daemon  53.57MB
Step 1/7 : ARG CI_IMAGE_NAME_TAG
Step 2/7 : FROM ${CI_IMAGE_NAME_TAG}
focal: Pulling from library/ubuntu
ca1778b69356: Pulling fs layer
ca1778b69356: Verifying Checksum
ca1778b69356: Download complete
ca1778b69356: Pull complete
Digest: sha256:db8bf6f4fb351aa7a26e27ba2686cf35a6a409f65603e59d4c203e58387dc6b3
Status: Downloaded newer image for ubuntu:focal
 ---> 88bd68917189
Step 3/7 : ARG FILE_ENV
 ---> Running in 77bc178b543a
Removing intermediate container 77bc178b543a
 ---> a52b7bf6ebc8
Step 4/7 : ENV FILE_ENV=${FILE_ENV}
 ---> Running in 2d35ad9e7577
Removing intermediate container 2d35ad9e7577
 ---> 6bdfd4b02a40
Step 5/7 : COPY ./ci/retry/retry /usr/bin/retry
 ---> 8d3f67387e0d
Step 6/7 : COPY ./ci/test/00_setup_env.sh ./${FILE_ENV} ./ci/test/01_base_install.sh /ci_base_install/ci/test/
 ---> ca5c1a6cf9b3
Step 7/7 : RUN [""bash"", ""-c"", ""cd /ci_base_install/ && set -o errexit && source ./ci/test/00_setup_env.sh && ./ci/test/01_base_install.sh""]
 ---> Running in bda994800fa2
Setting specific values in env
Fallback to default values in env (if not yet set)
./ci/test/00_setup_env.sh: line 33: /ci_base_install/depends/config.guess: No such file or directory
./ci/test/01_base_install.sh: line 11: git: command not found
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
Get:5 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]
Get:6 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]
Get:7 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]
Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]
Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3111 kB]
Get:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1329 kB]
Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]
Get:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2277 kB]
Get:13 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]
Get:14 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]
Get:15 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2139 kB]
Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1033 kB]
Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2629 kB]
Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #1: sleeping 0.3 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #2: sleeping 0.6 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #3: sleeping 1.2 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #4: sleeping 2.4 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #5: sleeping 4.8 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #6: sleeping 9.6 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #7: sleeping 19.2 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #8: sleeping 38.4 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #9: sleeping 60 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease
Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Before retry #10: sleeping 60 seconds
Get:1 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]
Err:1 http://deb.debian.org/debian buster-backports InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease
Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Reading package lists...
W: GPG error: http://deb.debian.org/debian buster-backports InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 648ACFD622F3D138 NO_PUBKEY 0E98404D386FA1D9
E: The repository 'http://deb.debian.org/debian buster-backports InRelease' is not signed.
Retries exhausted
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #1: sleeping 0.3 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #2: sleeping 0.6 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #3: sleeping 1.2 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #4: sleeping 2.4 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #5: sleeping 4.8 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #6: sleeping 9.6 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #7: sleeping 19.2 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #8: sleeping 38.4 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #9: sleeping 60 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Before retry #10: sleeping 60 seconds
Reading package lists...
E: The value 'buster-backports' is invalid for APT::Default-Release as such a release is not available in the sources
Retries exhausted
./ci/test/01_base_install.sh: line 87: git: command not found
The command 'bash -c cd /ci_base_install/ && set -o errexit && source ./ci/test/00_setup_env.sh && ./ci/test/01_base_install.sh' returned a non-zero code: 127
Exit status: 127
```"
bitcoin/bitcoin,2023-03-08 21:03:00,bug,Issue with `wallet_importdescriptors.py --descriptors` under valgrind,"This  was running #27226 rebased on master (at the time 8d12127a9c19cb218d661a88ab9b6871c9d853b9).
```bash
256/256 - wallet_importdescriptors.py --descriptors failed, Duration: 3065 s 

stdout:
2023-03-08T19:51:27.647000Z TestFramework (INFO): PRNG seed is: 266766692547859291
2023-03-08T19:51:27.648000Z TestFramework (INFO): Initializing test directory /home/ubuntu/ci_scratch/ci/scratch/test_runner/test_runner_₿_🏃_20230308_165259/wallet_importdescriptors_86
2023-03-08T19:51:44.422000Z TestFramework (INFO): Setting up wallets
2023-03-08T19:51:48.643000Z TestFramework (INFO): Mining coins
2023-03-08T19:52:01.196000Z TestFramework (INFO): Import should fail if a descriptor is not provided
2023-03-08T19:52:01.232000Z TestFramework (INFO): Should import a p2pkh descriptor
2023-03-08T19:52:02.257000Z TestFramework (INFO): Test can import same descriptor with public key twice
2023-03-08T19:52:02.481000Z TestFramework (INFO): Test can update descriptor label
2023-03-08T19:52:02.732000Z TestFramework (INFO): Internal addresses cannot have labels
2023-03-08T19:52:02.746000Z TestFramework (INFO): Internal addresses should be detected as such
2023-03-08T19:52:02.987000Z TestFramework (INFO): Should not import a p2sh-p2wpkh descriptor without checksum
2023-03-08T19:52:03.000000Z TestFramework (INFO): Should not import a p2sh-p2wpkh descriptor that has range specified
2023-03-08T19:52:03.049000Z TestFramework (INFO): Should not import a p2sh-p2wpkh descriptor and have it set to active
2023-03-08T19:52:03.065000Z TestFramework (INFO): Should import a (non-active) p2sh-p2wpkh descriptor
2023-03-08T19:52:03.976000Z TestFramework (INFO): Should import a 1-of-2 bare multisig from descriptor
2023-03-08T19:52:04.260000Z TestFramework (INFO): Should not treat individual keys from the imported bare multisig as watchonly
2023-03-08T19:52:04.284000Z TestFramework (INFO): Ranged descriptors cannot have labels
2023-03-08T19:52:04.346000Z TestFramework (INFO): Private keys required for private keys enabled wallet
2023-03-08T19:52:04.407000Z TestFramework (INFO): Ranged descriptor import should warn without a specified range
2023-03-08T19:52:04.902000Z TestFramework (INFO): Should not import a ranged descriptor that includes xpriv into a watch-only wallet
2023-03-08T19:52:04.959000Z TestFramework (INFO): Should not import a descriptor with hardened derivations when private keys are disabled
2023-03-08T19:52:05.233000Z TestFramework (INFO): Verify we can only extend descriptor's range
2023-03-08T19:52:07.965000Z TestFramework (INFO): Check we can change descriptor internal flag
2023-03-08T19:52:09.524000Z TestFramework (INFO): Key ranges should be imported in order
2023-03-08T19:52:13.027000Z TestFramework (INFO): Check we can change next_index
2023-03-08T19:52:16.359000Z TestFramework (INFO): Check imported descriptors are not active by default
2023-03-08T19:52:16.960000Z TestFramework (INFO): Check can activate inactive descriptor
2023-03-08T19:52:17.750000Z TestFramework (INFO): Check can deactivate active descriptor
2023-03-08T19:52:18.480000Z TestFramework (INFO): Verify activation state is persistent
2023-03-08T19:52:19.073000Z TestFramework (INFO): Should import a descriptor with a WIF private key as spendable
2023-03-08T19:52:19.323000Z TestFramework (INFO): Test can import same descriptor with private key twice
2023-03-08T19:52:25.121000Z TestFramework (INFO): Test that multisigs can be imported, signed for, and getnewaddress'd
2023-03-08T19:53:23.322000Z TestFramework (INFO): Multisig with distributed keys
2023-03-08T19:53:58.924000Z TestFramework (INFO): We can create and use a huge multisig under P2WSH
2023-03-08T19:55:33.870000Z TestFramework (INFO): Under P2SH, multisig are standard with up to 15 compressed keys
2023-03-08T19:56:49.926000Z TestFramework (INFO): Amending multisig with new private keys
2023-03-08T19:57:09.705000Z TestFramework (INFO): Combo descriptors cannot be active
2023-03-08T19:57:09.748000Z TestFramework (INFO): Descriptors with no type cannot be active
2023-03-08T19:57:10.542000Z TestFramework (INFO): Test importing a descriptor to an encrypted wallet
2023-03-08T20:02:32.611000Z TestFramework (INFO): Stopping nodes
2023-03-08T20:42:32.654000Z TestFramework.utils (ERROR): wait_until() failed. Predicate: ''''
    def is_node_stopped(self):
        """"""Checks whether the node has stopped.

        Returns True if the node has stopped. False otherwise.
        This method is responsible for freeing resources (self.process).""""""
        if not self.running:
            return True
        return_code = self.process.poll()
        if return_code is None:
            return False

        # process has stopped. Assert that it didn't return an error code.
        assert return_code == 0, self._node_msg(
            ""Node returned non-zero exit code (%d) when stopping"" % return_code)
        self.running = False
        self.process = None
        self.rpc_connected = False
        self.rpc = None
        self.log.debug(""Node stopped"")
        return True
'''
[node 1] Cleaning up leftover process
[node 0] Cleaning up leftover process


stderr:
Traceback (most recent call last):
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/wallet_importdescriptors.py"", line 699, in <module>
    ImportDescriptorsTest().main()
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 157, in main
    exit_code = self.shutdown()
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 313, in shutdown
    self.stop_nodes()
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 581, in stop_nodes
    node.wait_until_stopped()
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_node.py"", line 388, in wait_until_stopped
    wait_until_helper(self.is_node_stopped, timeout=timeout, timeout_factor=self.timeout_factor)
  File ""/home/ubuntu/ci_scratch/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 281, in wait_until_helper
    raise AssertionError(""Predicate {} not true after {} seconds"".format(predicate_source, timeout))
AssertionError: Predicate ''''
    def is_node_stopped(self):
        """"""Checks whether the node has stopped.

        Returns True if the node has stopped. False otherwise.
        This method is responsible for freeing resources (self.process).""""""
        if not self.running:
            return True
        return_code = self.process.poll()
        if return_code is None:
            return False

        # process has stopped. Assert that it didn't return an error code.
        assert return_code == 0, self._node_msg(
            ""Node returned non-zero exit code (%d) when stopping"" % return_code)
        self.running = False
        self.process = None
        self.rpc_connected = False
        self.rpc = None
        self.log.debug(""Node stopped"")
        return True
''' not true after 2400.0 seconds
```

See here for combined log: https://gist.github.com/fanquake/b9e9669a2023118908d7a91e9cb117b7"
bitcoin/bitcoin,2023-03-04 16:37:46,bug,Release 0.24.0.1 File system error.,"Release 0.24.0.1
Executable Buildet at debian 9 (MINGW)  give error at windows file system. 

```
EXCEPTION: NSt10filesystem7__cxx1116filesystem_errorE

filesystem error: cannot remove [D:\\Chains\\Bitcoin22\\anchors.dat]

H:\\Testing\\dssdds\\bin\\bitcoin-qt.exe in Runaway exception 
```

no matter installer or setup .

what commit is stable for that version for self build ?"
bitcoin/bitcoin,2023-02-23 13:00:40,bug,miniscript_stable fuzz timeout,https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=56270
bitcoin/bitcoin,2023-02-20 16:32:21,bug,failure in feature_bip68_sequence.py,"https://cirrus-ci.com/task/5552802257174528?logs=ci#L3998
```bash
 node0 2023-02-16T16:55:28.505405Z [httpworker.3] [rpc/request.cpp:179] [parse] [rpc] ThreadRPCServer method=sendrawtransaction user=__cookie__ 
 node0 2023-02-16T16:55:28.506156Z [httpworker.3] [txmempool.cpp:644] [check] [mempool] Checking mempool with 58 transactions and 123 inputs 
 test  2023-02-16T16:55:28.513000Z TestFramework (ERROR): JSONRPC error 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 134, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_bip68_sequence.py"", line 73, in run_test
                                       self.test_sequence_lock_unconfirmed_inputs()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_bip68_sequence.py"", line 225, in test_sequence_lock_unconfirmed_inputs
                                       tx1 = self.wallet.send_self_transfer(from_node=self.nodes[0])[""tx""]
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/wallet.py"", line 249, in send_self_transfer
                                       self.sendrawtransaction(from_node=from_node, tx_hex=tx['hex'])
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/wallet.py"", line 356, in sendrawtransaction
                                       txid = from_node.sendrawtransaction(hexstring=tx_hex, maxfeerate=maxfeerate, **kwargs)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/coverage.py"", line 49, in __call__
                                       return_val = self.auth_service_proxy_instance.__call__(*args, **kwargs)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/authproxy.py"", line 146, in __call__
                                       raise JSONRPCException(response['error'], status)
                                   test_framework.authproxy.JSONRPCException: bad-txns-premature-spend-of-coinbase, tried to spend coinbase at depth 7 (-26)
 test  2023-02-16T16:55:28.514000Z TestFramework (DEBUG): Closing down network thread 
 test  2023-02-16T16:55:28.564000Z TestFramework (INFO): Stopping nodes 
 test  2023-02-16T16:55:28.564000Z TestFramework.node0 (DEBUG): Stopping node 
```"
bitcoin/bitcoin,2023-02-17 16:08:59,bug,"on OSX, bitcoind chooses different data directory than Bitcoin-Qt","bitcoind uses `~/Library/Application Support/Bitcoin` as base data directory path but Bitcoin-Qt uses `~/.bitcoin`

I actually much prefer the latter since it is more UNIX-like and I assume users running the daemon from the command line probably do as well. It might make sense if this behavior was swapped and the GUI chose the user-friendly library/support path, but my preference would be using `~/.bitcoin` on OSX all the time.

The issue most likely comes down to this logic in system.cpp:

https://github.com/bitcoin/bitcoin/blob/fe1b3256888bd0e70d0c9655f565e139ec87b606/src/util/system.cpp#L859-L864

... but I have no idea why Bitcoin-Qt would be compiled without the `MAC_OSX` flag while bitcoind would be. I confirmed the behavior building bitcoind and -Qt both from master and also by downloading the v24.0.1 GUI for OSX from bitcoincore.org

It's also possible that this issue only affects M1 (arm64) Macs like mine:


```
--> uname -a
Darwin   21.4.0 Darwin Kernel Version 21.4.0: Fri Mar 18 00:47:26 PDT 2022; root:xnu-8020.101.4~15/RELEASE_ARM64_T8101 arm64
```"
bitcoin/bitcoin,2023-02-11 16:26:17,bug,Failing to fetch `cfheader` corresponding to block header in `headers` message,"<!-- Describe the issue -->

Occasionally when I receive a `headers` message on the p2p network, and attempt to fetch the `cfheader` that corresponds to a block header inside of a `headers`, i get this error message inside of my `debug.log`

>2023-02-07T16:47:15Z [net] Failed to find block filter hashes in index: filter_type=basic, start_height=150, stop_hash=3ca9030aeb6c2721cfbab0116b8d96e2d3c7e00738238010e0bc622566dc2aed


**Expected behavior**

I should be able to fetch a `cfheader` for a block after the block has been relayed to me over the p2p network via `headers` p2p message.

<!--- What behavior did you expect? -->

**Actual behavior**

It fails to fetch the `cfheader` that corresponds to the block.

**To reproduce**

I am unable to reproduce reliably, but this does occur ~1/5 times when running test suites on bitcoin-s. 

**System information**

24.0 (although i have seen this behavior for awhile and believe this issue exists in older versions of `bitcoind`)

The OS/CPU arch does not seem to be a factor in this bug. I can reproduce on Linux/mac machines with older/newer versions of bitcoind

I've attached the `debug.log` for my `bitcoind` instance. The block hash in question is `3ca9030aeb6c2721cfbab0116b8d96e2d3c7e00738238010e0bc622566dc2aed`.


[debug.log](https://github.com/bitcoin/bitcoin/files/10714034/debug.log)


This is related to: https://github.com/bitcoin-s/bitcoin-s/issues/4976
"
bitcoin/bitcoin,2023-02-10 16:18:01,bug,"Private key import via RPC importdescriptors shouldn't fail with the error code -5, ""Missing checksum""","**Expected behavior**

Descriptor is imported successfully.

**Actual behavior**

Descriptor is not imported. There is the error:
```
[
  {
    ""success"": false,
    ""error"": {
      ""code"": -5,
      ""message"": ""Missing checksum""
    }
  }
]
```
**To reproduce**

1. Execute the RPC command, e.g.: `importdescriptors '[{""desc"": ""tr(cUTFbLPUaBAPmTKwjcDs4rWHUSEUbUBfkPMogrbTmQFnJA3vgrLE)"", ""timestamp"": ""now""}]'`

**System information**

Bitcoin Core 24.0.1

Excerpt from the BIP-380 specification: 

> The top level expression is a SCRIPT. This expression **may** be followed by #CHECKSUM, where CHECKSUM is an 8 character alphanumeric descriptor checksum."
bitcoin/bitcoin,2023-02-09 09:59:45,bug,Intermittent issue in wallet_pruning.py,"```
wget https://drahtbot.space/temp_scratch/wallet_pruning_258.tar.xz
tar -xvf ./wallet_pruning_258.tar.xz
test/functional/combine_logs.py -c ./wallet_pruning_258
```


```
 test  2023-02-08T07:00:29.787000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 134, in main
                                       self.run_test()
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/wallet_pruning.py"", line 127, in run_test
                                       self.mine_large_blocks(self.nodes[0], 50)
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/wallet_pruning.py"", line 63, in mine_large_blocks
                                       self.sync_all()
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 728, in sync_all
                                       self.sync_blocks(nodes)
                                     File ""/root/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 697, in sync_blocks
                                       assert (all([len(x.getpeerinfo()) for x in rpc_connections]))
                                   AssertionError
"
bitcoin/bitcoin,2023-02-03 09:40:17,bug,macos depends build does not cache in the CI,"At least in the CI, the depends build does not cache. E.g. https://cirrus-ci.com/task/5094611354386432?logs=ci#L789

This causes the build to take an hour when it should be done in less than 10 minutes witch a ccache."
bitcoin/bitcoin,2023-02-01 16:27:05,bug,"signrawtransactionwithkey command shouldn't output the ""Witness program was passed an empty witness"" error for a TapRoot transaction","There is the irrelevant error message output by the signrawtransactionwithkey command.

**Expected behavior**

Hex string of the raw transaction with signature OR meaningful message about an alternative way to achieve one.

**Actual behavior**

```
{
  ""hex"": ""0200000001a2c0d82460883696219dbca6f545f72963b2b3ee085d832eb5ef9a69a374af160000000000fdffffff01e011000000000000225120052e44f45a6e381be8e06d3f3362b58034a68ba98081e24de7bfc5795420a90b00000000"",
  ""complete"": false,
  ""errors"": [
    {
      ""txid"": ""16af74a3699aefb52e835d08eeb3b26329f745f5a6bc9d219636886024d8c0a2"",
      ""vout"": 0,
      ""witness"": [
      ],
      ""scriptSig"": """",
      ""sequence"": 4294967293,
      ""error"": ""Witness program was passed an empty witness""
    }
  ]
}
```
**To reproduce**

```

$ signrawtransactionwithkey ""02000000011157667b81a1a4e688938c42ee7cdea23761cb7622a3476f0bc8ace7d0ec523100000000000000000001e8030000000000002251203b82b2b2a9185315da6f80da5f06d0440d8a5e1457fa93387c2d919c86ec878600000000"" '[""cUTFbLPUaBAPmTKwjcDs4rWHUSEUbUBfkPMogrbTmQFnJA3vgrLE""]' '[{""txid"": ""3152ecd0e7acc80b6f47a32276cb6137a2de7cee428c9388e6a4a1817b665711"", ""vout"": 0, ""scriptPubKey"": ""5120c38859777bc9c3294d3587035fc3823a146dabaab1fa250bc04e92f16887a065"", ""amount"": 0.00004242}]' ""DEFAULT""
```

**System information**

Console in Bitcoin Core 24.0.1 portable, Windows 11. (Occurs in both cases of a descriptor wallet loaded and not loaded).
Hint: signrawtransactionwithwallet works fine if preceded with the importdescriptors '[{""desc"": ""tr(cUTFbLPUaBAPmTKwjcDs4rWHUSEUbUBfkPMogrbTmQFnJA3vgrLE)#tdkpah70"", ""timestamp"": ""now""}]'
"
bitcoin/bitcoin,2023-01-27 19:07:28,bug,RPC request gives an exception on more than two wallet has been loaded. ,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->
RPC request gives an exception on more than two wallet has been loaded.  But it is working as normal when only one of them opened. Therefore, it makes it difficult to operate using multiple api on Bitcoincore. 
**Expected behavior**
working with no exception when we call the wallet using API.
<!--- What behavior did you expect? -->

**Actual behavior**
 the exception message : Wallet file not specified (must request wallet RPC through /wallet/<filename> uri-path). || 27-Jan-23 15:21:46
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2023-01-24 20:55:25,bug,Intermittent failure in mining_getblocktemplate_longpoll.py,"Seen on master, https://cirrus-ci.com/task/6706434163867648?logs=ci#L5577
```
2023-01-24T20:06:14.173000Z TestFramework (INFO): Test that introducing a new transaction into the mempool will terminate the longpoll
2023-01-24T20:07:34.182000Z TestFramework (ERROR): Assertion failed
Traceback (most recent call last):
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 134, in main
    self.run_test()
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/mining_getblocktemplate_longpoll.py"", line 74, in run_test
    assert not thr.is_alive()
AssertionError
2023-01-24T20:07:

```
"
bitcoin/bitcoin,2023-01-12 03:44:54,bug,"signmessage and verifymessage : ""Address does not refer to key""","<!-- This issue tracker is only for technical issues related to Bitcoin Core.


General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

sign and verify message functions not working with (probobaly bech32 type addresse like: ""bc1qg0kmtkmgf4angrmcr9wz2fslxjj4jzj95tkg6r"".

I was able to sign using signmessagewithprivkey ""privkey"" ""my message""  and the result if it is correct : ""H+QaukT8FP/TFh3bchd9oRj00S7rzuxzSXNXH10y4tgkWkYmKFsfFxOs50bo9510ngNhgsUbE1yG7bbldc2qVGY=""

however using signmessage ""bc1qg0kmtkmgf4angrmcr9wz2fslxjj4jzj95tkg6r"" ""my message"" and verifymessage ""bc1qg0kmtkmgf4angrmcr9wz2fslxjj4jzj95tkg6r"" ""signature"" ""my message"" examples gives error : ""Address does not refer to key""

**Expected behavior**
Signing and verifying messages using bech32 addresses.
<!--- What behavior did you expect? -->

**Actual behavior**
""Address does not refer to key"" error
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2023-01-09 18:01:32,bug,contrib/install_db4.sh script fails on OpenBSD 7.2 (RPi 3) (error: Unable to find a mutex implementation),"When trying to compile Bitcoin Core with legacy wallet on OpenBSD 7.2 stable running on RPI3, the process fails while running

install_db4.sh script

checking for getopt optreset variable... yes
checking for mutexes... UNIX/fcntl
configure: WARNING: NO SHARED LATCH IMPLEMENTATION FOUND FOR THIS PLATFORM.
configure: error: Unable to find a mutex implementation"
bitcoin/bitcoin,2023-01-05 17:40:53,bug,s390x fails to compile (Werror=free-nonheap-object),"Steps to reproduce on a fresh install of `debian:bookworm`:

```
export DEBIAN_FRONTEND=noninteractive && apt update && apt install htop git vim -y && git clone https://github.com/bitcoin/bitcoin.git --depth=1 ./bitcoin-core && cd ./bitcoin-core
NO_QT=1 DIR_QA_ASSETS=/qa_assets CCACHE_DIR=/ccache_dir CCACHE_SIZE=500M DANGER_RUN_CI_ON_HOST=""1"" MAKEJOBS=""-j9"" FILE_ENV=""./ci/test/00_setup_env_s390x.sh"" ./ci/test_run_all.sh
```


```
/usr/bin/ccache s390x-linux-gnu-g++ -std=c++17 -DHAVE_CONFIG_H -I. -I../src/config  -fmacro-prefix-map=/tmp/cirrus-ci-build/bitcoin-core/ci/scratch/build/bitcoin-s390x-linux-gnu=. -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=2 -DHAVE_BUILD_INFO -DPROVIDE_FUZZ_MAIN_FUNCTION -I. -I./minisketch/include -I./secp256k1/include -I./univalue/include -I./leveldb/include -isystem /tmp/cirrus-ci-build/bitcoin-core/depends/s390x-linux-gnu/include -DBOOST_MULTI_INDEX_DISABLE_SERIALIZATION -DBOOST_NO_CXX98_FUNCTION_BASE   -isystem /tmp/cirrus-ci-build/bitcoin-core/depends/s390x-linux-gnu/include -I/tmp/cirrus-ci-build/bitcoin-core/depends/s390x-linux-gnu/include   -I/tmp/cirrus-ci-build/bitcoin-core/depends/s390x-linux-gnu/include/  -fdebug-prefix-map=/tmp/cirrus-ci-build/bitcoin-core/ci/scratch/build/bitcoin-s390x-linux-gnu=. -fstack-reuse=none -Wstack-protector -fstack-protector-all -fstack-clash-protection   -Werror    -fno-extended-identifiers -fvisibility=hidden -fPIE -pipe -std=c++17 -O2  -c -o libbitcoin_node_a-txmempool.o `test -f 'txmempool.cpp' || echo './'`txmempool.cpp
In file included from /usr/s390x-linux-gnu/include/c++/12/s390x-linux-gnu/bits/c++allocator.h:33,
                 from /usr/s390x-linux-gnu/include/c++/12/bits/allocator.h:46,
                 from /usr/s390x-linux-gnu/include/c++/12/bits/stl_tree.h:64,
                 from /usr/s390x-linux-gnu/include/c++/12/map:60,
                 from ./txmempool.h:10,
                 from txmempool.cpp:6:
In member function ‘void std::__new_allocator<_Tp>::deallocate(_Tp*, size_type) [with _Tp = char]’,
    inlined from ‘static void std::allocator_traits<std::allocator<_Tp1> >::deallocate(allocator_type&, pointer, size_type) [with _Tp = char]’ at /usr/s390x-linux-gnu/include/c++/12/bits/alloc_traits.h:496:23,
    inlined from ‘void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_destroy(size_type) [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/s390x-linux-gnu/include/c++/12/bits/basic_string.h:292:34,
    inlined from ‘void std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::_M_dispose() [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/s390x-linux-gnu/include/c++/12/bits/basic_string.h:286:14,
    inlined from ‘std::__cxx11::basic_string<_CharT, _Traits, _Alloc>::~basic_string() [with _CharT = char; _Traits = std::char_traits<char>; _Alloc = std::allocator<char>]’ at /usr/s390x-linux-gnu/include/c++/12/bits/basic_string.h:795:19,
    inlined from ‘bilingual_str::~bilingual_str()’ at ./util/translation.h:18:8,
    inlined from ‘constexpr void std::_Destroy(_Tp*) [with _Tp = bilingual_str]’ at /usr/s390x-linux-gnu/include/c++/12/bits/stl_construct.h:151:22,
    inlined from ‘std::__detail::__variant::_Variant_storage<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)> mutable [with auto:2 = bilingual_str&]’ at /usr/s390x-linux-gnu/include/c++/12/variant:472:19,
    inlined from ‘constexpr _Res std::__invoke_impl(__invoke_other, _Fn&&, _Args&& ...) [with _Res = void; _Fn = __detail::__variant::_Variant_storage<false, bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)>; _Args = {bilingual_str&}]’ at /usr/s390x-linux-gnu/include/c++/12/bits/invoke.h:61:36,
    inlined from ‘constexpr std::enable_if_t<is_invocable_r_v<_Res, _Callable, _Args ...>, _Res> std::__invoke_r(_Callable&&, _Args&& ...) [with _Res = void; _Callable = __detail::__variant::_Variant_storage<false, bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)>; _Args = {bilingual_str&}]’ at /usr/s390x-linux-gnu/include/c++/12/bits/invoke.h:111:28,
    inlined from ‘static constexpr decltype(auto) std::__detail::__variant::__gen_vtable_impl<std::__detail::__variant::_Multi_array<_Result_type (*)(_Visitor, _Variants ...)>, std::integer_sequence<long unsigned int, __indices ...> >::__visit_invoke(_Visitor&&, _Variants ...) [with _Result_type = void; _Visitor = std::__detail::__variant::_Variant_storage<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)>&&; _Variants = {std::variant<bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >&}; long unsigned int ...__indices = {0}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:1035:40,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = void; _Visitor = __detail::__variant::_Variant_storage<false, bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)>; _Variants = {variant<bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >&}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:1783:5,
    inlined from ‘constexpr decltype(auto) std::__do_visit(_Visitor&&, _Variants&& ...) [with _Result_type = void; _Visitor = __detail::__variant::_Variant_storage<false, bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::_M_reset()::<lambda(auto:2&&)>; _Variants = {variant<bilingual_str, set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >&}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:1729:5,
    inlined from ‘constexpr void std::__detail::__variant::_Variant_storage<false, _Types ...>::_M_reset() [with _Types = {bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > >}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:470:23,
    inlined from ‘std::__detail::__variant::_Variant_storage<false, _Types ...>::~_Variant_storage() [with _Types = {bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > >}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:480:17,
    inlined from ‘std::__detail::__variant::_Copy_ctor_base<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::~_Copy_ctor_base()’ at /usr/s390x-linux-gnu/include/c++/12/variant:554:12,
    inlined from ‘std::__detail::__variant::_Move_ctor_base<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::~_Move_ctor_base()’ at /usr/s390x-linux-gnu/include/c++/12/variant:591:12,
    inlined from ‘std::__detail::__variant::_Copy_assign_base<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::~_Copy_assign_base()’ at /usr/s390x-linux-gnu/include/c++/12/variant:629:12,
    inlined from ‘std::__detail::__variant::_Move_assign_base<false, bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::~_Move_assign_base()’ at /usr/s390x-linux-gnu/include/c++/12/variant:681:12,
    inlined from ‘std::__detail::__variant::_Variant_base<bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > > >::~_Variant_base()’ at /usr/s390x-linux-gnu/include/c++/12/variant:735:12,
    inlined from ‘std::variant<_Types>::~variant() [with _Types = {bilingual_str, std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash, std::allocator<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag> > >}]’ at /usr/s390x-linux-gnu/include/c++/12/variant:1407:28,
    inlined from ‘util::Result<std::set<boost::multi_index::detail::hashed_index_iterator<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::hashed_index_node<boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::ordered_index_node<boost::multi_index::detail::null_augment_policy, boost::multi_index::detail::index_node_base<CTxMemPoolEntry, std::allocator<CTxMemPoolEntry> > > > > > >, boost::multi_index::detail::bucket_array<std::allocator<CTxMemPoolEntry> >, boost::multi_index::detail::hashed_unique_tag, boost::multi_index::detail::hashed_index_global_iterator_tag>, CompareIteratorByHash> >::~Result()’ at ./util/result.h:35:7,
    inlined from ‘CTxMemPool::setEntries CTxMemPool::AssumeCalculateMemPoolAncestors(std::string_view, const CTxMemPoolEntry&, const Limits&, bool) const’ at txmempool.cpp:263:17:
/usr/s390x-linux-gnu/include/c++/12/bits/new_allocator.h:158:33: error: ‘void operator delete(void*, std::size_t)’ called on unallocated object ‘<anonymous>’ [-Werror=free-nonheap-object]
  158 |         _GLIBCXX_OPERATOR_DELETE(_GLIBCXX_SIZED_DEALLOC(__p, __n));
      |                                 ^
In file included from txmempool.cpp:17:
txmempool.cpp: In member function ‘CTxMemPool::setEntries CTxMemPool::AssumeCalculateMemPoolAncestors(std::string_view, const CTxMemPoolEntry&, const Limits&, bool) const’:
txmempool.cpp:263:49: note: declared here
  263 |     auto result{Assume(CalculateMemPoolAncestors(entry, limits, fSearchForParents))};
      |                        ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
./util/check.h:83:51: note: in definition of macro ‘Assume’
   83 | #define Assume(val) inline_assertion_check<false>(val, __FILE__, __LINE__, __func__, #val)
      |                                                   ^~~
cc1plus: all warnings being treated as errors
make[2]: *** [Makefile:10502: libbitcoin_node_a-txmempool.o] Error 1
make[2]: Leaving directory '/tmp/cirrus-ci-build/bitcoin-core/ci/scratch/build/bitcoin-s390x-linux-gnu/src'
make[1]: *** [Makefile:19146: install-recursive] Error 1
make[1]: Leaving directory '/tmp/cirrus-ci-build/bitcoin-core/ci/scratch/build/bitcoin-s390x-linux-gnu/src'
make: *** [Makefile:817: install-recursive] Error 1"
bitcoin/bitcoin,2023-01-04 11:35:40,bug,p2p_disconnect_ban intermittent issue,"https://cirrus-ci.com/task/5426348117196800?logs=ci#L4502

```
 test  2022-12-17T13:12:20.699000Z TestFramework.utils (ERROR): wait_until() failed. Predicate: '''' 
                                           self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                   '''
 test  2022-12-17T13:12:20.699000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 134, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/p2p_disconnect_ban.py"", line 112, in run_test
                                       self.connect_nodes(0, 1)  # reconnect the node
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 608, in connect_nodes
                                       self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 732, in wait_until
                                       return wait_until_helper(test_function, timeout=timeout, timeout_factor=self.options.timeout_factor)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 281, in wait_until_helper
                                       raise AssertionError(""Predicate {} not true after {} seconds"".format(predicate_source, timeout))
                                   AssertionError: Predicate ''''
                                           self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                   ''' not true after 2400.0 seconds"
bitcoin/bitcoin,2023-01-02 16:40:48,bug,`test/lint/lint-python.py` thinks flake8 is not installed when it actually is,"```
$ ./test/lint/lint-python.py 
Skipping Python linting since flake8 is not installed.
$ flake8 --version
5.0.4 (mccabe: 0.7.0, pycodestyle: 2.9.1, pyflakes: 2.5.0) CPython 3.10.9 on Linux
```

If I try [old `lint-python.sh`](https://github.com/bitcoin/bitcoin/blob/41720a1f540ef3c16a283a6cce6f0a63552a4937/test/lint/lint-python.sh), it works."
bitcoin/bitcoin,2022-12-26 18:16:48,bug,I2P: Limit transient addresses,"The per-connection transient address feature in 24.0.1 is, we are pretty sure, putting a large load on the I2P network. Starting on Dec. 19 the number of tunnels in the network started to increase, and as measured at one router, it peaked at about 3x normal levels on Dec. 26. While the load is manageable for now, if the transient feature becomes popular, it has the potential to get much much worse.

I2P isn't really designed to work that way, and some limit on the number of addresses (aka destinations or tunnels) built by the bitcoin client is necessary. I2P Destinations are designed to be long-lived, it's not like in Tor where you can create tons of circuits. Waiting for tunnels to be built for each connection also adds a huge amount of delay to connection setup time. 

A high number of addresses will also result in excessive CPU and bandwidth usage by your router to build the tunnels for each.
Additionally, other routers will reject your excessive tunnel building, which will increase your resource usage even more as your router retries. And, of course, the overhead applies to every connection attempt, not just every successful connection.

Depending on your security goals, and the max number of i2p connections you wish to support, there's a few options for you to consider:

- Use a single transient address, created at startup
- Hard limit the number of i2p connections if configured for transient
- Group several connections into one of several addresses in a pool
- Rate limit new connections or creation of new addresses

The i2pd router does not currently limit the number of local destinations, I don't think.
The Java router limits to 100 by default and will reject addresses over that limit.

If you do choose to continue using multiple addresses, we recommend a reasonable maximum number of addresses of around 16 or so, together with a rate limit on how frequently you create new ones.


Unfortunately I did not realize this was happening when I reviewed your transient changes, but your 24.0.1 release notes confirm it.
We ask that you please fix and include this in your next point release.

In the meantime, please update your i2p doc to discourage the transient option.
Also in that doc, in the bandwidth section at the bottom, please encourage people to share as much bandwidth and transittunnels as they can, to increase their anonymity with the cover traffic, and so that bitcoin users contribute more to the i2p network than they consume.

If a popular application uses more network resources than it contributes, it has the potential to take down the entire network.

Reference files:

https://github.com/bitcoin/bitcoin/blob/master/src/net.cpp#L499
https://github.com/bitcoin/bitcoin/blob/master/doc/i2p.md



Thank you very much for considering this change.

Graph of number of tunnels on a typical high-speed i2p router, Dec. 15-26:
(edit, replaced link) https://i2pd.xyz/partTunnels-12days.png
"
bitcoin/bitcoin,2022-12-17 22:37:23,bug,Codespell errors in `src/wallet/test/walletload_tests.cpp`,"On current master (cb32328d1b80d0ccd6eb9532bd8fe4e0a4de385e).
```
$ ./test/lint/all-lint.py 
Skipping Python linting since flake8 is not installed.
src/wallet/test/walletload_tests.cpp:107: crypted ==> encrypted
src/wallet/test/walletload_tests.cpp:108: crypted ==> encrypted
src/wallet/test/walletload_tests.cpp:127: crypted ==> encrypted
^ Warning: codespell identified likely spelling errors. Any false positives? Add them to the list of ignored words in test/lint/spelling.ignore-words.txt
```"
bitcoin/bitcoin,2022-12-15 08:48:37,bug,ci: clang-tidy does not check header files?,"Inside the CI env `FILE_ENV=""./ci/test/00_setup_env_native_tidy.sh""` clang-tidy only checks our cpp files and not our header files?


Currently it passes on master, however if manually invoked on a header file, it will fail:


```
# clang-tidy-15 -p=/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu -quiet /bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.cpp && echo $?
543 warnings generated.
1086 warnings generated.
0

# clang-tidy-15 -p=/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu -quiet /bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h && echo $?
408 warnings generated.
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:192:19: error: std::move of the const variable 'name' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
        : m_names{std::move(name)},
                  ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:193:18: error: std::move of the const variable 'type' of the trivially-copyable type 'const RPCArg::Type' has no effect; remove std::move() [performance-move-const-arg,-warnings-as-errors]
          m_type{std::move(type)},
                 ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:194:22: error: std::move of the const variable 'fallback' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_fallback{std::move(fallback)},
                     ^~~~~~~~~~        ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:195:25: error: std::move of the const variable 'description' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_description{std::move(description)},
                        ^~~~~~~~~~           ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:208:19: error: std::move of the const variable 'name' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
        : m_names{std::move(name)},
                  ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:209:18: error: std::move of the const variable 'type' of the trivially-copyable type 'const RPCArg::Type' has no effect; remove std::move() [performance-move-const-arg,-warnings-as-errors]
          m_type{std::move(type)},
                 ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:210:19: error: std::move of the const variable 'inner' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_inner{std::move(inner)},
                  ^~~~~~~~~~     ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:211:22: error: std::move of the const variable 'fallback' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_fallback{std::move(fallback)},
                     ^~~~~~~~~~        ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:212:25: error: std::move of the const variable 'description' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_description{std::move(description)},
                        ^~~~~~~~~~           ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:275:18: error: std::move of the const variable 'type' of the trivially-copyable type 'const RPCResult::Type' has no effect; remove std::move() [performance-move-const-arg,-warnings-as-errors]
        : m_type{std::move(type)},
                 ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:276:22: error: std::move of the const variable 'm_key_name' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_key_name{std::move(m_key_name)},
                     ^~~~~~~~~~          ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:277:19: error: std::move of the const variable 'inner' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_inner{std::move(inner)},
                  ^~~~~~~~~~     ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:280:25: error: std::move of the const variable 'description' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_description{std::move(description)},
                        ^~~~~~~~~~           ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:281:18: error: std::move of the const variable 'cond' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_cond{std::move(cond)}
                 ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:302:18: error: std::move of the const variable 'type' of the trivially-copyable type 'const RPCResult::Type' has no effect; remove std::move() [performance-move-const-arg,-warnings-as-errors]
        : m_type{std::move(type)},
                 ^~~~~~~~~~    ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:303:22: error: std::move of the const variable 'm_key_name' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_key_name{std::move(m_key_name)},
                     ^~~~~~~~~~          ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:304:19: error: std::move of the const variable 'inner' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_inner{std::move(inner)},
                  ^~~~~~~~~~     ~
/bitcoin-core/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src/rpc/util.h:307:25: error: std::move of the const variable 'description' has no effect; remove std::move() or make the variable non-const [performance-move-const-arg,-warnings-as-errors]
          m_description{std::move(description)},
                        ^~~~~~~~~~           ~
"
bitcoin/bitcoin,2022-12-12 15:33:38,bug,Unable to create PSBT for legacy watchonly wallets in the GUI,"A user reported to me that they are unable to create a PSBT with a legacy watchonly wallet when they use the GUI. They get an error ""The amount exceeds your balance"".

I think the issue is that the GUI will retrieve the wallet's balance via `AvailableCoins` before calling `CreateTransaction`. This call to `AvailableCoins` is resulting in fewer coins being returned with recent changes to how `AvailableCoins` works, particularly with additional filtering of spendable coins and preset coins.

***

Note that this does not effect descriptor wallets. A workaround for this issue is to switch to using descriptor wallets. An existing legacy wallet can be turned into a descriptor wallet using the `migratewallet` RPC. However many watchonly legacy wallets are likely the result of importing descriptors via `importmulti`. For such wallets, I recommend creating a new descriptor wallet and re-importing the descriptors."
bitcoin/bitcoin,2022-12-08 00:14:23,bug,Pruned node unable to start when a wallet is present,"This is the weirdest bug I have seen in a while, as I can't reproduce, but it happens persistently on a server of one of our users.

I tried to delete wallet and recreate it, it doesn't work, still stuck.
I tried to remove the wallet and not recreate it, and bitcoin core starts properly.

I uploaded a wallet generated there: https://aois.blob.core.windows.net/public/wallet.tar.gz

I tried to load the wallet on my local node (non-pruned), it works and there is no rescan.

```
2022-12-08T00:11:47Z No coin database inconsistencies in last 6 blocks (503 transactions)
2022-12-08T00:11:47Z  block index           10522ms
2022-12-08T00:11:47Z init message: Loading wallet…
2022-12-08T00:11:47Z BerkeleyEnvironment::Open: LogDir=C:\\Users\\NicolasDorier\\tmp\\database ErrorFile=C:\\Users\\NicolasDorier\\tmp\\db.log
2022-12-08T00:11:47Z [default wallet] Wallet File Version = 169900
2022-12-08T00:11:47Z [default wallet] Keys: 2001 plaintext, 0 encrypted, 2001 w/ metadata, 2001 total. Unknown wallet records: 0
2022-12-08T00:11:47Z [default wallet] Wallet completed loading in              60ms
2022-12-08T00:11:47Z [default wallet] setKeyPool.size() = 2000
2022-12-08T00:11:47Z [default wallet] mapWallet.size() = 0
2022-12-08T00:11:47Z [default wallet] m_address_book.size() = 0
2022-12-08T00:11:47Z Unsetting NODE_NETWORK on prune mode
20
```


On the server of the user the node is pruned, and there is a rescan with error

```
2022-12-08T00:01:33Z [default wallet] Wallet completed loading in             152ms
2022-12-08T00:01:33Z Error: Prune: last wallet synchronisation goes beyond pruned data. You need to -reindex (download the whole blockchain again in case of pruned node)
Error: Prune: last wallet synchronisation goes beyond pruned data. You need to -reindex (download the whole blockchain again in case of pruned node)
```
Even though the wallet just got created.

The pruneheight should also be big enough:
```bash
getblockchaininfo
```
```json
{
  ""chain"": ""test"",
  ""blocks"": 2410292,
  ""headers"": 2410292,
  ""bestblockhash"": ""0000000000000022a51915a9d3473d6185744de708a1bf008a471e702dfc43fc"",
  ""difficulty"": 82946068.46057868,
  ""time"": 1670458413,
  ""mediantime"": 1670455720,
  ""verificationprogress"": 0.9999988037011049,
  ""initialblockdownload"": false,
  ""chainwork"": ""000000000000000000000000000000000000000000000839f6768de759ba571f"",
  ""size_on_disk"": 8702871138,
  ""pruned"": true,
  ""pruneheight"": 1441677,
  ""automatic_pruning"": true,
  ""prune_target_size"": 104857600000,
  ""warnings"": ""Unknown new rules activated (versionbit 28)""
}
```

Anything else I can provide you? I am really at loss about why this specific node is asking for a rescan.

**Expected behavior**

Node starts properly when a new wallet is just created.

**Actual behavior**

Node stuck

```
2022-12-08T00:00:24Z Bitcoin Core version v24.0.0 (release build)
2022-12-08T00:00:24Z Using the 'sse4(1way),sse41(4way)' SHA256 implementation
2022-12-08T00:00:24Z Default data directory /home/bitcoin/.bitcoin
2022-12-08T00:00:24Z Using data directory /home/bitcoin/.bitcoin/testnet3
2022-12-08T00:00:24Z Config file: /home/bitcoin/.bitcoin/bitcoin.conf
2022-12-08T00:00:24Z Config file arg: testnet=""1""
2022-12-08T00:00:24Z Config file arg: [test] maxmempool=""500""
2022-12-08T00:00:24Z Config file arg: [test] onion=""tor:9050""
2022-12-08T00:00:24Z Config file arg: [test] port=""39388""
2022-12-08T00:00:24Z Config file arg: [test] printtoconsole=""1""
2022-12-08T00:00:24Z Config file arg: [test] prune=""100000""
2022-12-08T00:00:24Z Config file arg: [test] rpcallowip=""::/0""
2022-12-08T00:00:24Z Config file arg: [test] rpcallowip=""0.0.0.0/0""
2022-12-08T00:00:24Z Config file arg: [test] rpcauth=****
2022-12-08T00:00:24Z Config file arg: [test] rpcauth=****
2022-12-08T00:00:24Z Config file arg: [test] rpcbind=****
2022-12-08T00:00:24Z Config file arg: [test] rpcport=""43782""
2022-12-08T00:00:24Z Config file arg: [test] walletdir=""/walletdata/testnet""
2022-12-08T00:00:24Z Config file arg: [test] whitelist=""0.0.0.0/0""
2022-12-08T00:00:24Z Config file arg: [test] zmqpubhashblock=""tcp://0.0.0.0:28334""
2022-12-08T00:00:24Z Config file arg: [test] zmqpubrawblock=""tcp://0.0.0.0:28332""
2022-12-08T00:00:24Z Config file arg: [test] zmqpubrawtx=""tcp://0.0.0.0:28333""
2022-12-08T00:00:24Z Using at most 125 automatic connections (1048576 file descriptors available)
2022-12-08T00:00:24Z Using 16 MiB out of 16 MiB requested for signature cache, able to store 524288 elements
2022-12-08T00:00:24Z Using 16 MiB out of 16 MiB requested for script execution cache, able to store 524288 elements
2022-12-08T00:00:24Z Script verification uses 1 additional threads
2022-12-08T00:00:24Z scheduler thread start
2022-12-08T00:00:24Z WARNING: the RPC server is not safe to expose to untrusted networks such as the public internet
2022-12-08T00:00:24Z [http] creating work queue of depth 16
2022-12-08T00:00:24Z Using random cookie authentication.
2022-12-08T00:00:24Z Generated RPC authentication cookie /home/bitcoin/.bitcoin/testnet3/.cookie
2022-12-08T00:00:24Z Using rpcauth authentication.
2022-12-08T00:00:24Z [http] starting 4 worker threads
2022-12-08T00:00:24Z Using wallet directory /walletdata/testnet
2022-12-08T00:00:24Z init message: Verifying wallet(s)…
2022-12-08T00:00:24Z Using BerkeleyDB version Berkeley DB 4.8.30: (April  9, 2010)
2022-12-08T00:00:24Z Using wallet /walletdata/testnet/wallet.dat
2022-12-08T00:00:24Z BerkeleyEnvironment::Open: LogDir=/walletdata/testnet/database ErrorFile=/walletdata/testnet/db.log
2022-12-08T00:00:24Z Using /16 prefix for IP bucketing
2022-12-08T00:00:24Z init message: Loading P2P addresses…
2022-12-08T00:00:25Z Loaded 22930 addresses from peers.dat  393ms
2022-12-08T00:00:25Z init message: Loading banlist…
2022-12-08T00:00:25Z SetNetworkActive: true
2022-12-08T00:00:25Z Cache configuration:
2022-12-08T00:00:25Z * Using 2.0 MiB for block index database
2022-12-08T00:00:25Z * Using 8.0 MiB for chain state database
2022-12-08T00:00:25Z * Using 440.0 MiB for in-memory UTXO set (plus up to 476.8 MiB of unused mempool space)
2022-12-08T00:00:25Z init message: Loading block index…
2022-12-08T00:00:25Z Assuming ancestors of block 0000000000000004877fa2d36316398528de4f347df2f8a96f76613a298ce060 have valid signatures.
2022-12-08T00:00:25Z Setting nMinimumChainWork=00000000000000000000000000000000000000000000076f6e7cbd0beade5d20
2022-12-08T00:00:25Z Prune configured to target 100000 MiB on disk for block and undo files.
2022-12-08T00:00:25Z Switching active chainstate to Chainstate [ibd] @ height -1 (null)
2022-12-08T00:00:25Z Opening LevelDB in /home/bitcoin/.bitcoin/testnet3/blocks/index
2022-12-08T00:00:25Z Opened LevelDB successfully
2022-12-08T00:00:25Z Using obfuscation key for /home/bitcoin/.bitcoin/testnet3/blocks/index: 0000000000000000
2022-12-08T00:01:15Z LoadBlockIndexDB: last block file = 211
2022-12-08T00:01:15Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=3150, size=89162763, heights=2407141...2410289, time=2022-11-19...2022-12-07)
2022-12-08T00:01:15Z Checking all blk files are present...
2022-12-08T00:01:16Z LoadBlockIndexDB(): Block files have previously been pruned
2022-12-08T00:01:30Z Opening LevelDB in /home/bitcoin/.bitcoin/testnet3/chainstate
2022-12-08T00:01:30Z Opened LevelDB successfully
2022-12-08T00:01:30Z Using obfuscation key for /home/bitcoin/.bitcoin/testnet3/chainstate: dd2d995e0b39a1e4
2022-12-08T00:01:32Z Loaded best chain: hashBestChain=000000000000002e323100923fd63334a2b4e3e087609da60e07bfcb41fc3e66 height=2410289 date=2022-12-07T23:46:07Z progress=0.999998
2022-12-08T00:01:32Z init message: Verifying blocks…
2022-12-08T00:01:32Z Verifying last 6 blocks at level 3
2022-12-08T00:01:32Z [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
2022-12-08T00:01:32Z No coin database inconsistencies in last 6 blocks (433 transactions)
2022-12-08T00:01:32Z  block index           67622ms
2022-12-08T00:01:32Z init message: Loading wallet…
2022-12-08T00:01:32Z BerkeleyEnvironment::Open: LogDir=/walletdata/testnet/database ErrorFile=/walletdata/testnet/db.log
2022-12-08T00:01:32Z [default wallet] Wallet file version = 10500, last client version = 240000
2022-12-08T00:01:33Z [default wallet] Keys: 2001 plaintext, 0 encrypted, 2001 w/ metadata, 2001 total. Unknown wallet records: 0
2022-12-08T00:01:33Z [default wallet] Wallet completed loading in             152ms
2022-12-08T00:01:33Z Error: Prune: last wallet synchronisation goes beyond pruned data. You need to -reindex (download the whole blockchain again in case of pruned node)
Error: Prune: last wallet synchronisation goes beyond pruned data. You need to -reindex (download the whole blockchain again in case of pruned node)
2022-12-08T00:01:33Z Shutdown: In progress...
2022-12-08T00:01:33Z scheduler thread exit
2022-12-08T00:01:34Z Shutdown: done
```

**To reproduce**

I can only reproduce on a single instance, I tried to reproduce on my own server and didn't managed to.

```bash
bitcoin-wallet ""-datadir=/walletdata/testnet"" ""-legacy"" ""-wallet="" create
```

Config

```ini
testnet=1
[test]
walletdir=/walletdata/testnet

printtoconsole=1
rpcallowip=::/0
rpcport=43782
rpcbind=0.0.0.0:43782
rpcallowip=0.0.0.0/0
port=39388
whitelist=0.0.0.0/0
maxmempool=500

prune=100000
zmqpubrawblock=tcp://0.0.0.0:28332
zmqpubrawtx=tcp://0.0.0.0:28333
zmqpubhashblock=tcp://0.0.0.0:28334

onion=tor:9050
```

**System information**

Bitcoin Core 24.0 amd64
Docker Hub `btcpayserver/bitcoin:24.0`.
"
bitcoin/bitcoin,2022-11-30 22:23:47,bug,Intermittent failure in validation_chainstatemanager unit test,"https://cirrus-ci.com/task/4951273196224512?logs=ci#L3217

`test_bitcoin: ./chain.h:266: uint256 CBlockIndex::GetBlockHash() const: Assertion phashBlock != nullptr failed.`

Observed in #26584, but I don't see how this crash (which looks related to AssumeUTXO loading of a chainstate) could be related to the changes from that PR. 
FYI @jamesob"
bitcoin/bitcoin,2022-11-28 00:37:08,bug,"CJDNS doesn't resolve inbound/outbound to same addr, resulting in duplicated connections","For two peers connecting to each other over CJDNS via `addnode`, it results in 2 connections. We would expect them to determine they are connecting to each other and not duplicate the connection. `getpeerinfo` gives the following:
inbound connection - `""addr"": ""[xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:<port>]:<random_port>"",`, 
outbound connection - `""addr"": ""xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:<port>"",`

Not sure if it would be possible to determine that the inbound connection is the same as our outbound, and don't make an outbound if requested with that same cjdns address."
bitcoin/bitcoin,2022-11-26 01:32:40,bug,bitcoin-cli -netinfo ignores inbound peer from local network,"I have bitcoind serving cfilters to an LND node on my local network on separate machines. Both have `192.168.1.x` IP addresses and they connect fine using neutrino. However, the LND node does not appear in the `bitcoin-cli -netinfo` table.

The reason is because if you look at the `getpeerinfo` response:

```
{
  ""id"": 11728,
  ""addr"": ""192.168.1.153:64367"",
  ""addrbind"": ""192.168.1.13:8333"",
  ""addrlocal"": ""192.168.1.13:8333"",
  ""network"": ""not_publicly_routable"",
  ""services"": ""0000000000000048"",
  ""servicesnames"": [
    ""WITNESS"",
    ""COMPACT_FILTERS""
  ],
...
```

...so bitcoin-cli ignores the peer here:

https://github.com/bitcoin/bitcoin/blob/9c47eb450346937b3d7ee21b9e669b5c91a7ed6c/src/bitcoin-cli.cpp#L477-L479

...expecting one of these strings only:

https://github.com/bitcoin/bitcoin/blob/9c47eb450346937b3d7ee21b9e669b5c91a7ed6c/src/bitcoin-cli.cpp#L58

I'm happy to fix / add this myself unless there's a reason? Maybe instead of skipping the peer we can just add an ""unknown network"" category, or ""other"" ...?

cc: @jonatack "
bitcoin/bitcoin,2022-11-19 22:46:47,bug,Multiple connections to same i2p address,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

This behavior is okay for ipv4 based on https://bitcoin.stackexchange.com/questions/87739/can-a-bitcoin-node-create-an-outgoing-connection-to-a-inbound-node

However, making multiple connections to same i2p peer makes no sense IMO

**Expected behavior**

No connections (outbound/inbound) should be initiated with i2p address that already exists as one of the peer.

**Actual behavior**

![image](https://user-images.githubusercontent.com/94559964/202874210-8e2a6d27-6516-4e13-9b89-73ae19893f03.png)

![Screenshot 2022-11-20 035027](https://user-images.githubusercontent.com/94559964/202874219-21d48a2f-9ceb-4f29-9003-d8ad63de0a6c.png)


**To reproduce**

- Setup 3 nodes such that node2 and node3 works with i2p
- Connect node2 manually with `addnode` in config to node1
- Connect node3 manually with node2 using i2p address
- Observe result for `getpeerinfo` in a few minutes or check peers list in bitcoin-qt

**System information**

Bitcoin Core Master Branch, v24.0rc2 and v24.0rc4
"
bitcoin/bitcoin,2022-11-13 16:16:35,bug,[systemd] Use of discouraged network target,"Here are the current network hooks used in Bitcoin Core systemd integration template:

https://github.com/bitcoin/bitcoin/blob/7ef730ca84bd71a06f986ae7070e7b2ac8e47582/contrib/init/bitcoind.service#L16-L18

Semantically, it makes sense, so I never questioned those lines when deploying new nodes, but here are some quotes from the official systemd website:

https://systemd.io/NETWORK_ONLINE/

> $network / network-online.target is a mechanism that is required only to deal with software that assumes continuous network is available (i.e. of the simple not-well-written kind)

> Services using the network should hence simply place an After=network.target stanza in their unit files, without Wants=network.target or Requires=network.target

> After=network.target can be sure that it is stopped before the network is shut down when the system is going down. This allows services to cleanly terminate connections before going down, instead of losing ongoing connections leaving the other side in an undefined state.

> If you are a developer, instead of wondering what to do about network.target, please just fix your program to be friendly to dynamically changing network configuration.

> Watch rtnetlink and react to network configuration changes as they happen. This is usually the nicest solution, but not always the easiest.

It looks like the use of `network-online.target` is discouraged and `network.target` is a preferred option for any program which knows how to adapt to changing network conditions. Are there any good reasons not to follow the official recommendation?"
bitcoin/bitcoin,2022-11-10 13:16:46,bug,[24.0rc4] `--torcontrol` without `--onion` connects to wrong Tor proxy,"**Actual behavior**

I'm using `--torcontrol` without `--onion`, which should be possible in 24.0.
I get these errors in the log, repeated every couple of secs:
`connect() to 127.0.0.1:9050 failed after wait: Connection refused (111)`
If I use `--onion=tor:9050`, it works. `tor` is the hostname of my Tor proxy.

**Expected behavior**

Bitcoind to fetch the correct tor proxy address/port from the control port and use it.

**System information**

bitcoind 24.0rc4 (self-built Docker image), tor 0.4.7.10 (Docker image from docker.io), Docker 20.10.12 (from Ubuntu 22.04.1)

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

My startup log until the error occurs (Tor hidden service addresses redacted):

```
bitcoind_1          | Bitcoin Core version v24.0rc4 (release build)
bitcoind_1          | Using the 'x86_shani(1way,2way)' SHA256 implementation
bitcoind_1          | Using RdSeed as an additional entropy source
bitcoind_1          | Using RdRand as an additional entropy source
bitcoind_1          | Startup time: 2022-11-10T13:13:48Z
bitcoind_1          | Default data directory /home/bitcoin/.bitcoin
bitcoind_1          | Using data directory /home/bitcoin/.bitcoin
bitcoind_1          | Config file: /home/bitcoin/.bitcoin/bitcoin.conf (not found, skipping)
bitcoind_1          | Command-line arg: bind=""[::]:8333""
bitcoind_1          | Command-line arg: bind=""0.0.0.0:8333""
bitcoind_1          | Command-line arg: bind=""[::]:8334=onion""
bitcoind_1          | Command-line arg: bind=""0.0.0.0:8334=onion""
bitcoind_1          | Command-line arg: connect=""redacted:8333""
bitcoind_1          | Command-line arg: debug=""rpc""
bitcoind_1          | Command-line arg: debug=""tor""
bitcoind_1          | Command-line arg: disablewallet=""1""
bitcoind_1          | Command-line arg: discover=""1""
bitcoind_1          | Command-line arg: dnsseed=""0""
bitcoind_1          | Command-line arg: listen=""1""
bitcoind_1          | Command-line arg: listenonion=""1""
bitcoind_1          | Command-line arg: logtimestamps=""0""
bitcoind_1          | Command-line arg: maxconnections=""32""
bitcoind_1          | Command-line arg: mempoolexpiry=""8760""
bitcoind_1          | Command-line arg: onlynet=""onion""
bitcoind_1          | Command-line arg: par=""2""
bitcoind_1          | Command-line arg: prune=""1024""
bitcoind_1          | Command-line arg: rpcallowip=""0.0.0.0""
bitcoind_1          | Command-line arg: rpcallowip=""[::]""
bitcoind_1          | Command-line arg: rpcbind=****
bitcoind_1          | Command-line arg: rpcthreads=""2""
bitcoind_1          | Command-line arg: server=""1""
bitcoind_1          | Command-line arg: torcontrol=""tor:9051""
bitcoind_1          | Command-line arg: torpassword=****
bitcoind_1          | Using at most 32 automatic connections (1048576 file descriptors available)
bitcoind_1          | Using 16 MiB out of 16 MiB requested for signature cache, able to store 524288 elements
bitcoind_1          | Using 16 MiB out of 16 MiB requested for script execution cache, able to store 524288 elements
bitcoind_1          | Script verification uses 1 additional threads
bitcoind_1          | No wallet support compiled in!
bitcoind_1          | scheduler thread start
bitcoind_1          | WARNING: the RPC server is not safe to expose to untrusted networks such as the public internet
bitcoind_1          | [http] creating work queue of depth 16
bitcoind_1          | [rpc] Starting RPC
bitcoind_1          | [rpc] Starting HTTP RPC server
bitcoind_1          | Using random cookie authentication.
bitcoind_1          | Generated RPC authentication cookie /home/bitcoin/.bitcoin/.cookie
bitcoind_1          | [http] starting 2 worker threads
bitcoind_1          | Using /16 prefix for IP bucketing
bitcoind_1          | init message: Loading P2P addresses…
bitcoind_1          | Loaded 14239 addresses from peers.dat  27ms
bitcoind_1          | init message: Loading banlist…
bitcoind_1          | SetNetworkActive: true
bitcoind_1          | Cache configuration:
bitcoind_1          | * Using 2.0 MiB for block index database
bitcoind_1          | * Using 8.0 MiB for chain state database
bitcoind_1          | * Using 440.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
bitcoind_1          | init message: Loading block index…
bitcoind_1          | Assuming ancestors of block 00000000000000000009c97098b5295f7e5f183ac811fb5d1534040adb93cabd have valid signatures.
bitcoind_1          | Setting nMinimumChainWork=00000000000000000000000000000000000000003404ba0801921119f903495e
bitcoind_1          | Prune configured to target 1024 MiB on disk for block and undo files.
bitcoind_1          | Switching active chainstate to Chainstate [ibd] @ height -1 (null)
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/blocks/index
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/blocks/index: 0000000000000000
bitcoind_1          | LoadBlockIndexDB: last block file = 3267
bitcoind_1          | LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=102, size=124550955, heights=762330...762431, time=2022-11-08...2022-11-09)
bitcoind_1          | Checking all blk files are present...
bitcoind_1          | LoadBlockIndexDB(): Block files have previously been pruned
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/chainstate
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/chainstate: cec422b42319b403
bitcoind_1          | Loaded best chain: hashBestChain=0000000000000000000380536903d286f9010f25b6582c80a892c3e95bf80d06 height=762431 date=2022-11-09T14:14:43Z progress=0.999689
bitcoind_1          | init message: Verifying blocks…
bitcoind_1          | Verifying last 6 blocks at level 3
bitcoind_1          | [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
bitcoind_1          | No coin database inconsistencies in last 6 blocks (11634 transactions)
bitcoind_1          |  block index            3243ms
bitcoind_1          | init message: Pruning blockstore…
bitcoind_1          | Leaving InitialBlockDownload (latching to false)
bitcoind_1          | block tree size = 762433
bitcoind_1          | nBestHeight = 762431
bitcoind_1          | Warning: More than one onion bind address is provided. Using [::]:8334 for the automatically created Tor onion service.
bitcoind_1          | Warning: More than one onion bind address is provided. Using [::]:8334 for the automatically created Tor onion service.
bitcoind_1          | Bound to [::]:8333
bitcoind_1          | Bound to 0.0.0.0:8333
bitcoind_1          | Bound to [::]:8334
bitcoind_1          | Bound to 0.0.0.0:8334
bitcoind_1          | init message: Starting network threads…
bitcoind_1          | loadblk thread start
bitcoind_1          | DNS seeding disabled
bitcoind_1          | torcontrol thread start
bitcoind_1          | Imported mempool transactions from disk: 0 succeeded, 0 failed, 0 expired, 0 already there, 0 waiting for initial broadcast
bitcoind_1          | loadblk thread exit
bitcoind_1          | init message: Done loading
bitcoind_1          | msghand thread start
bitcoind_1          | addcon thread start
bitcoind_1          | opencon thread start
bitcoind_1          | Cannot create socket for redacted.onion:8333: unsupported network
bitcoind_1          | net thread start
bitcoind_1          | [tor] Reading cached private key from /home/bitcoin/.bitcoin/onion_v3_private_key
bitcoind_1          | [tor] Successfully connected!
bitcoind_1          | [tor] Connected to Tor version 0.4.7.10
bitcoind_1          | [tor] Supported authentication method: HASHEDPASSWORD
bitcoind_1          | [tor] Using HASHEDPASSWORD authentication
bitcoind_1          | [tor] Authentication successful
bitcoind_1          | [tor] Get SOCKS port command yielded [::]:9050
bitcoind_1          | [tor] Configuring onion proxy for 127.0.0.1:9050
bitcoind_1          | [tor] ADD_ONION successful
bitcoind_1          | [tor] Got service ID redacted, advertising service redacted.onion:8333
bitcoind_1          | [tor] Cached service private key to /home/bitcoin/.bitcoin/onion_v3_private_key
bitcoind_1          | AddLocal(redacted.onion:8333,4)
bitcoind_1          | connect() to 127.0.0.1:9050 failed after wait: Connection refused (111)
bitcoind_1          | connect() to 127.0.0.1:9050 failed after wait: Connection refused (111)
```"
bitcoin/bitcoin,2022-11-06 15:37:17,bug,"[wallet] when unloading a wallet which still rescans the wallet, the command should fail but bitcoind gets in a buggy state","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

I am loading descriptors into bitcoind using the rpcclient with the function: `importdescriptors`, it automatically starts rescanning the blockchain. Now when I try to unload the wallet, bitcoin-cli will allow me to do it (wallet disappears from the `listwallet` section, but it will not abort the rescan. Thats a problem bc now when trying to reload the wallet, the wallet sqlite db is locked by bitcoind and this wallet is unusable unitl the rescan is over. `abortrescan` will not work bc the wallet is not loaded anymore according to `bitcoin-cli listwallets`. The only solution for me was to delete the wallet dir and create a new wallet with the same name. Now everything works as expected, but as soon as I want to unload the wallet again, bitcoind crashes

<!--- What behavior did you expect? -->

The `unloadwallet` function should internally abort the rescan and unload the wallet, so that it can be reloaded afterwards without waiting for the rescan. Or it should fail and leading the user to abort the rescan with `abortrescan`

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

* Create a descriptor wallet
* Import a descriptor and unload the walllet while bitcoind is still rescanning
* Now you are stuck bc abortrescan will not work bc wallet is not loaded and the only solution is to delete the wallet dir
* Now Create a new wallet with the same name and import the descriptors again. When the wallet is faster synced then the old process still syncing, and you unload the wallet, bitcoind crashes

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

` Linux 5.10.103-v8+ #1529 SMP PREEMPT Tue Mar 8 12:26:46 GMT 2022 aarch64 GNU/Linux`

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

bitcoind v.23.0 (release binary for raspberry pi)

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-10-31 14:22:58,bug,`prevector` is basically doing UB if on C++17,"Things like this are technically UB in C++17: https://github.com/bitcoin/bitcoin/blob/4766cd198172225c66a0adb01f6cb9513c3d0e66/src/prevector.h#L187

- You cannot start the lifetime of an array like this. You need to do some compiler magic (such as perhaps call `std::launder`? Or actually some weird placement-new that does nothing?) here to properly start the lifetime of the `T` array object, as far as the C++ abstract machine is concerned.  See: https://www.youtube.com/watch?v=pbkQG09grFw
  -  C++20 does *now* allows this -- implicit lifetimes can be started when you use some backing store in this way for trivial types only, but in C++17 this is technically UB.
  - The fact that this works is a happy accident because most major compiler support such (mis)-use of the language for trivial types owing to its C roots. So much so that they actually changed the standard for C++20 to support this, but for '17 this is UB (but works in practice on all major compilers).  
- The following is not fixed in C++20 though: Memory is not guaranteed to be aligned to whatever T requires here. (This is not fixed by C++20 -- unaligned access is UB, even if it happens to work on your platform). So `prevector` only really works without UB for `uint8_t` and similar types, if on C++20.
"
bitcoin/bitcoin,2022-10-28 10:31:51,bug,Failure in rpc_getblockfrompeer.py,"https://cirrus-ci.com/task/6199503433760768?logs=ci#L4414
https://cirrus-ci.com/task/5628523837652992?logs=ci#L4519
```bash
hash=5005fd299f78e023c07fa4cce23642e5a689c535f8adbc379825b7604b928b64)]) 
 test  2022-10-28T10:04:36.776000Z TestFramework.p2p (DEBUG): Received message from 127.0.0.1:15393: msg_sendheaders() 
 test  2022-10-28T10:04:36.776000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 139, in try_rpc
                                       fun(*args, **kwds)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/coverage.py"", line 49, in __call__
                                       return_val = self.auth_service_proxy_instance.__call__(*args, **kwargs)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/authproxy.py"", line 144, in __call__
                                       raise JSONRPCException(response['error'], status)
                                   test_framework.authproxy.JSONRPCException: Block header missing (-1)
                                   During handling of the above exception, another exception occurred:
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/rpc_getblockfrompeer.py"", line 112, in run_test
                                       assert_raises_rpc_error(-1, error_msg, self.nodes[1].getblockfrompeer, blockhash, node1_interface_id)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 130, in assert_raises_rpc_error
                                       assert try_rpc(code, message, fun, *args, **kwds), ""No exception raised""
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 147, in try_rpc
                                       message, e.error['message']))
                                   AssertionError: Expected substring not found in error message:
                                   substring: 'In prune mode, only blocks that the node has already synced previously can be fetched from a peer'
                                   error message: 'Block header missing'.
```"
bitcoin/bitcoin,2022-10-27 12:40:32,bug,v22.0 wallet freezes on Mac OS Ventura on arm64,"I upgraded to the newest Mac OS Ventura. The Bitcoin Core application was deleted automatically from my computer. I downloaded Bitcoin core again fro my Mac. and when I tried to open Bitcoin core it has been stuck on Verifying Wallet(s) for over an hour. 

<img width=""476"" alt=""Screenshot 2022-10-27 at 08 39 24"" src=""https://user-images.githubusercontent.com/116812829/198286720-66e119e2-1c2b-453d-928c-9b4abc794240.png"">







<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**

<!--- What behavior did you expect? -->

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-10-22 19:29:54,bug,"""system_tests/run_command"" unit test fails on MSVC","I'm building v24.0rc2 on Windows 10 using Visual Studio 2022. The build succeeds without errors and the binary works fine, but there is one failing unit test:

```
C:\\bitcoin\\src>test_bitcoin.exe
Running 528 test cases...
C:/bitcoin/src/test/system_tests.cpp(64): error: in ""system_tests/run_command"": check what.find(expected) != std::string::npos has failed

*** 1 failure is detected in the test module ""Bitcoin Core Test Suite""
```

Also tried with v23.0 and latest master, same error (for v23.0 the line number is different).
"
bitcoin/bitcoin,2022-10-19 17:29:32,bug,bitcoin-qt crashes if it can't listen on the port it wants to,"I created new folders ~/.bitcoin2 and ~/.bitcoin3, then ran bitcoin-qt using both of them as data directories.

The first one ran fine:

    qt/bitcoin-qt -datadir=/home/chris/.bitcoin2

I left the first one running. The second one crashed in various ways:

    $ qt/bitcoin-qt -datadir=/home/chris/.bitcoin3
    Error: Unable to bind to 0.0.0.0:8333 on this computer. Bitcoin Core is probably already running.
    Error: Failed to listen on any port. Use -listen=0 if you want this.
    Segmentation fault

    $ qt/bitcoin-qt -datadir=/home/chris/.bitcoin3
    Error: Unable to bind to 0.0.0.0:8333 on this computer. Bitcoin Core is probably already running.
    Error: Failed to listen on any port. Use -listen=0 if you want this.
    free(): double free detected in tcache 2
    Aborted

    $ qt/bitcoin-qt -datadir=/home/chris/.bitcoin3
    Error: Unable to bind to 0.0.0.0:8333 on this computer. Bitcoin Core is probably already running.
    Error: Failed to listen on any port. Use -listen=0 if you want this.
    corrupted size vs. prev_size in fastbins
    Aborted

I would expect it to complain about the unavailable port, but not to crash in such random ways.

This is bitcoin built against git tag `v24.0rc2`."
bitcoin/bitcoin,2022-10-19 15:49:06,bug,doc: Internal bug detected running listtransactions on OP_RETURN output,"I saw an internal bug detected message running `listtransactions`:

    $ bitcoin-cli listtransactions '*' 500
    error code: -1
    error message:
    Internal bug detected: ""std::any_of(m_results.m_results.begin(), m_results.m_results.end(), [&ret](const RPCResult& res) { return res.MatchesType(ret); })""
    rpc/util.cpp:587 (HandleRequest)
    Please report this issue here: https://github.com/bitcoin/bitcoin/issues

I narrowed it down to a particular transaction in my history:

    $ for i in {453..455}; do echo ""--- $i ---""; bitcoin-cli listtransactions '*' 1 $i; done
    --- 453 ---
    [
      {
        ""address"": ""bc1qnle0kjvz4wyju49m00krxztdqu5ygak00nft37"",
        ""category"": ""send"",
        ""amount"": -1.26545600,
        ""label"": """",
        ""vout"": 0,
        ""fee"": -0.00006450,
        ""confirmations"": 253723,
        ""blockhash"": ""00000000000000000025a92c80f5c259ca5d9d36d407906dbefbd8075c3ae77b"",
        ""blockheight"": 505672,
        ""blockindex"": 857,
        ""blocktime"": 1516693344,
        ""txid"": ""b6439e1c9eb3915b3cc89871d2c2479f3f1847f0c7bab252c3ebc503b8f6d344"",
        ""wtxid"": ""393cfba86cb0d3274850aa461dab433b5d62c1dce68dc10d43de969b8dec2359"",
        ""walletconflicts"": [
        ],
        ""time"": 1516689935,
        ""timereceived"": 1516689935,
        ""bip125-replaceable"": ""no"",
        ""abandoned"": false
      }
    ]
    --- 454 ---
    error code: -1
    error message:
    Internal bug detected: ""std::any_of(m_results.m_results.begin(), m_results.m_results.end(), [&ret](const RPCResult& res) { return res.MatchesType(ret); })""
    rpc/util.cpp:587 (HandleRequest)
    Please report this issue here: https://github.com/bitcoin/bitcoin/issues

    --- 455 ---
    [
      {
        ""address"": ""bc1qnle0kjvz4wyju49m00krxztdqu5ygak00nft37"",
        ""parent_descs"": [
        ],
        ""category"": ""receive"",
        ""amount"": 1.26545600,
        ""label"": """",
        ""vout"": 0,
        ""confirmations"": 253723,
        ""blockhash"": ""00000000000000000025a92c80f5c259ca5d9d36d407906dbefbd8075c3ae77b"",
        ""blockheight"": 505672,
        ""blockindex"": 857,
        ""blocktime"": 1516693344,
        ""txid"": ""b6439e1c9eb3915b3cc89871d2c2479f3f1847f0c7bab252c3ebc503b8f6d344"",
        ""wtxid"": ""393cfba86cb0d3274850aa461dab433b5d62c1dce68dc10d43de969b8dec2359"",
        ""walletconflicts"": [
        ],
        ""time"": 1516689935,
        ""timereceived"": 1516689935,
        ""bip125-replaceable"": ""no""
      }
    ]

Note how the txid on the two transactions surrounding the bad one are the same. [the block explorer](https://blockstream.info/tx/b6439e1c9eb3915b3cc89871d2c2479f3f1847f0c7bab252c3ebc503b8f6d344) shows that this transaction has an OP_RETURN output, which is probably what is triggering this internal bug detection.

**System information**

I built bitcoin core from the git tag `v24.0rc1`. It's running on a Debian Linux system, Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz, SSD."
bitcoin/bitcoin,2022-10-17 09:40:37,bug,Bitcoin failed to build with error MSB3073 on MSVC,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**
Build successfully.
<!--- What behavior did you expect? -->

**Actual behavior**
Hi all,
Bitcoin fails to build on MSVC due to error MSB3073, the same problem also appeared in vs2019, I mentioned a same issue but it was closed. Could you help look?
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->
**error message**
23>F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj(161,5): error MSB3073: The command ""C:\\Qt_static\\bin\\moc.exe  ""..\\..\\src\\qt\\bitcoinamountfield.cpp"" -o .\\QtGeneratedFiles\\qt\\bitcoinamountfield.moc"" exited with code 3.
23>Done Building Project ""F:\\gitP\\bitcoin\\bitcoin\\build_msvc\\libbitcoin_qt\\libbitcoin_qt.vcxproj"" (Rebuild target(s)) -- FAILED.
**detail log**
[build (27).log](https://github.com/bitcoin/bitcoin/files/9798888/build.27.log)

**To reproduce**
1. set VSCMD_SKIP_SENDTELEMETRY=1 & ""C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\Enterprise\\Common7\\Tools\\VsDevCmd.bat"" -host_arch=amd64 -arch=amd64 
2. git clone https://github.com/bitcoin/bitcoin F:\\bitcoin\\bitcoin 
3. cd F:\\bitcoin\\bitcoin\\build_msvc  
4. py -3 msvc-autogen.py
5. cd F:\\bitcoin
6.  mkdir tools\\vcpkg
7.  git clone https://github.com/microsoft/vcpkg F:\\bitcoin\\tools\\vcpkg
8. cd F:\\bitcoin\\tools\\vcpkg
9. bootstrap-vcpkg.bat 2>&1
10. set path=%cd%;%path%
11. vcpkg integrate install 2>&1
12. cd F:\\bitcoin\\bitcoin\\build_msvc
13. msbuild /m /p:Platform=x64 /p:Configuration=Release bitcoin.sln /t:Rebuild 2>&1
<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**
VS version: VS2022(17.3.6)
Operating system: windows server 2019
the commit of Bitcoin we use is 5174a13
<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-10-14 18:33:15,bug,Intermittent failure in `feature_fee_estimation.py`,https://cirrus-ci.com/task/5348833621180416?logs=ci#L3306
bitcoin/bitcoin,2022-10-13 14:11:46,bug,Testnet difficulty was reset at height 2350655,"<!-- Describe the issue -->

It appears that the testnet difficulty was reset from `83,269,806.51979084` at block height [2350654](https://www.blockchain.com/explorer/blocks/btc-testnet/2350654) down to `1.00` in the next block at height [2350655](https://www.blockchain.com/explorer/blocks/btc-testnet/2350655).

I'm seeing this on my own testnet nodes (`v23.0`) as well as on several blockchain explorers (e.g. [blockchain.com](https://www.blockchain.com/explorer/blocks/btc-testnet/2350655) and [blockchair](https://blockchair.com/bitcoin/testnet/block/2350655))

At the time of writing this post, the network has produced about 16k blocks in 6 hours, while the difficulty is crawling back up slowly (it's at `65,536.00` at the time of writing).

Was this a scheduled testnet reset of some kind or maybe a bug?
"
bitcoin/bitcoin,2022-10-05 18:43:26,bug,test: `minisketch_tests.cpp` can fail when built with MSVC,"To trigger the [failure](https://cirrus-ci.com/task/4513724340895744), consider the [diff](https://github.com/hebasto/bitcoin/commit/c218353ca987e6f6888cccaf8b547d1aed68c333) as follows:
```diff
--- a/src/test/minisketch_tests.cpp
+++ b/src/test/minisketch_tests.cpp
@@ -27,6 +27,11 @@ BOOST_AUTO_TEST_CASE(minisketch_test)
         uint32_t start_b = start_a + a_not_b;
         uint32_t end_b = start_b + both + b_not_a;
 
+#ifdef _MSC_VER
+        // FIXME
+        BOOST_TEST_MESSAGE(""This message *breaks* this test when running with '-l test_suite'"");
+#endif
+
         Minisketch sketch_a = MakeMinisketch32(10);
         for (uint32_t a = start_a; a < end_a; ++a) sketch_a.Add(a);
         Minisketch sketch_b = MakeMinisketch32(10);
```

The observing behavior:
```
>build_msvc\\x64\\Release\\test_bitcoin.exe -t minisketch_tests
Running 1 test case...
unknown location(0): fatal error: in ""minisketch_tests/minisketch_test"": memory access violation occurred at address 0xffffffff, while attempting to  read inaccessible data
C:\\Users\\hebasto\\bitcoin\\src\\test\\minisketch_tests.cpp(18): last checkpoint: ""minisketch_test"" test entry

*** 1 failure is detected in the test module ""Bitcoin Core Test Suite""

```

Assuming either MSVC bug or non-portable code?"
bitcoin/bitcoin,2022-10-02 22:35:21,bug,retry add onion after tor's controller closed connection,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->
I am running Bitcoind on Qubes-Whonix.
Whonix-Gateway is where tor resides, and every tor controller's command made by Whonix-Workstation to Whonix-Gateway is filtered by a proxy called onion-grater (upstream development is made by Tails OS)

From this point on I will be referring to tor's controller simply by ""control"" or ""controller"".

If the control connection is closed by restarting onion-grater to apply new rules (even not regarding the bitcoind.yml) file, bitcoind will remove the current onion and won't be able to set it as address again.
```
RemoveLocal(myonion.onion:8333)
tor: No supported authentication method
```
and `bitcoin-cli -netinfo`
```
Local addresses: n/a
```
and of course none of my nodes listening to this address will continue working.

ADD_ONION command works the first time but not the second.
The problem occurs because bitcoind does not give time for the controller to restart the second time, so the authentication is not ""available yet"".

In fact, tor is not being reloaded or restarted, only the proxy, which is what bitcoind sees as the controller.

This happens because I am using bitcoind tor controller feature to send ADD_ONION.
If I set the onion service manually on the gateway, I wouldn't have this problem, but then the problem would be:
- it wouldn't be pragmatically deleted, although I am rewriting it with `Flag=DiscardPK` anyway
- it would stay as a file on the Gateway and the onion address would need to be manually inserted into the bitcoin.conf

**Expected behavior**

<!--- What behavior did you expect? -->
I expect that after bitcoind can't authenticate to the controller, it tries again after some time.
This way, I can restart my controller without needing to restart bitcoind for it to get a onion listening address again.

**Actual behavior**

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->
Bitcoind removes current listening onion hostname and not retry to authenticate to the controller.

**To reproduce**

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->
This is difficult to reproduce on normal systems.
First, Qubes may not interfere, so you could try to reproduce on Whonix KVM or Virtualbox or Tails OS (probably).

If you can't test on that setup, I would try to simple close the controller connection on a normal system and then see what bitcoind does to the listening onion address.

Also you need to add the bitcoind onion-grater profile, I don't know if Tails has, but for Whonix, I am using https://github.com/Whonix/onion-grater/blob/9addd1d6dd9671b18e5415a196b92d7f6ee5846c/usr/share/doc/onion-grater-merger/examples/40_bitcoind.yml

It is not the current Whonix onion-grater profile but it is the correct one and will be merged upstream soon.

If that is the case, add that profile to `/usr/local/etc/onion-grater-merger.d/40_bitcoind.yml`

Also set in the `bitcoin.conf` the onion binding to `bind=0.0.0.0:8334=onion` and not `127.0.0.1`. This makes the service listen on the Whonix internal interface and does not affect security. Restart bitcoind to apply changes.

You will also have to open the port on the Whonix-Workstation firewall.
```
$ sudo mkdir -p /usr/local/etc/whonix_firewall.d
$ echo ""EXTERNAL_OPEN_PORTS+="" 8334 "" | tee -a /usr/local/etc/whonix_firewall.d/50_user.conf`
$ sudo whonix_firewall
```

To view onion-grater logs, you need to enable debug mode on the Whonix-Gateway:
```
$ echo ""[Service]
## Clear onion-grater default file '/lib/systemd/system/onion-grater.service'.
ExecStart=
## Enable debug mode.
ExecStart=/usr/lib/onion-grater --listen-interface eth1 --debug
"" | tee /usr/lib/systemd/system/onion-grater.service.d/50_user.conf
$ sudo systemctl daemon-reload
$ sudo systemctl restart onion-grater
```


Then after adding the profile, enabling onion-grater debug mode, restarting onion-grater and bitcoind, test if the address is reachable externally. Use the bitcoid option `connect=address.onion`, so you are sure it is connecting.

If it is, restart onion-grater.
```
$ sudo systemctl restart onion-grater
```

On the bitcoin debug.logs, you should see the same error I saw above about removing onion address and not able to authenticate to the controller.

But that is not entirely correct, because I can issue an ADD_ONION manually and it will accept, but bitcoind does not try to issue ADD_ONION a second time.


```
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): -> PROTOCOLINFO 1
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): <- 250-PROTOCOLINFO 1
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): <- 250-AUTH METHODS=NULL
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): <- 250-VERSION Tor=""0.4.7.10""
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): <- 250 OK
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): -> AUTHENTICATE
host onion-grater[149562]: 10.137.0.31:54044 (filter: 30_autogenerated): <- 250 OK
```
The controller actually lets bitcoind authenticate, as seen from the onion-grater logs above. But I don't understand why bitcoind says it can't authenticate.
The ip doesn't matter, it is internal ip.

What you should expect from onion-grater logs is the following:
```
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): -> AUTHENTICATE
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): <- 250 OK
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): -> ADD_ONION NEW:ED25519-V3 Port=8333,0.0.0.0:8334
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): rewrote command:
host onion-grater[149562]:     ADD_ONION NEW:ED25519-V3 Port=8333,0.0.0.0:8334
host onion-grater[149562]: to:
host onion-grater[149562]:     ADD_ONION NEW:ED25519-V3 Port=8333,10.137.0.31:8334 Flags=DiscardPK
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): <- (multi-line)
host onion-grater[149562]:     250-ServiceID=myonionhostnamewithoutonion
host onion-grater[149562]:     250 OK
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): -> QUIT
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated): <- 250 closing connection
host onion-grater[149562]: 10.137.0.31:43566 (filter: 30_autogenerated) disconnected: client quit
```

You could try on Whonix with `tor-ctrl` to authenticate after restarting onion-grater and it will work:
```
$ tor-ctrl ADD_ONION NEW:ED25519-V3 Port=8333,0.0.0.0:8334
```
and it will authenticate.

But for some unknown reason bitcoind does not reauthenticate to the controller.

I understand that once the contoller connection closes, the commands send to the controller are not valid anymore. But bitcoind should be able to reauthenticate once it loses connection.

**System information**

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

System information was already described above.
I understand this may be difficult debug because it requires a system with onion-grater.
But it does not rely on Whonix or Tails or onion-grater package to fix this, as they are only restarting to read the new configuration, but bitcoind understand that as a connection that must be closed forever.

If you read all this and is open to make bitcoind retry to authenticate cleanly, I am open to help debug and instruct how to reproduce."
bitcoin/bitcoin,2022-09-24 15:30:08,bug,Opening macOS DMG does not open Finder window,"When I open `bitcoin-24.0rc1-arm64-apple-darwin.dmg` the volume is mounted silently, but does not open Finder window leading to very confusing user experience.

OS: macOS 12.6 (21G115) on MacBook Air (M1, 2020)

This is a regression from `bitcoin-23.0-arm64-apple-darwin.dmg` which opens the following Finder window:

<img width=""527"" alt=""Screenshot 2022-09-24 at 17 31 58"" src=""https://user-images.githubusercontent.com/42201/192106347-7f738f20-f80f-4f74-aca0-7cbde2146e3a.png"">

PS: I downloaded the file from https://bitcoincore.org/bin/bitcoin-core-24.0/test.rc1/bitcoin-24.0rc1-arm64-apple-darwin.dmg"
bitcoin/bitcoin,2022-09-21 10:20:13,bug,fuzz: miniscript_string: ASSERT: constructed[0]->ScriptSize() == script_size ,"Probably after commit 55e1deb745531a0749f668ed7265770c70a58563

https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=51636

"
bitcoin/bitcoin,2022-09-15 08:41:19,bug,test: failure in interface_rest.py ,"https://cirrus-ci.com/task/6131758964932608?logs=functional_tests#L43

```
Traceback (most recent call last):
  File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\test_framework\\test_framework.py"", line 133, in main
    self.run_test()
  File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\interface_rest.py"", line 293, in run_test
    json_obj = self.test_rest_request(f""/blockfilterheaders/basic/{bb_hash}"", query_params={""count"": 5})
  File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\interface_rest.py"", line 85, in test_rest_request
    assert_equal(resp.status, status)
  File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\test_framework\\util.py"", line 56, in assert_equal
    raise AssertionError(""not(%s)"" % "" == "".join(str(arg) for arg in (thing1, thing2) + args))
AssertionError: not(404 == 200)
```"
bitcoin/bitcoin,2022-09-12 11:28:13,bug,Segmentation fault in the scheduler thread when an index fails to commit to the db,"See https://cirrus-ci.com/task/5413417262514176?logs=ci#L3188 :

* First, it fails to commit (for some unknown reason): `ERROR: Commit: Failed to commit latest coinstatsindex state`
* Then, it considers itself synced: `coinstatsindex is enabled at height 100`, `coinstatsindex thread exit`
* Then, the scheduler thread segfaults.

```
...

2022-09-12T10:55:06.800147Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2060] [ConnectBlock] [bench]     - Sanity checks: 0.00ms [0.00s (0.01ms/blk)]
2022-09-12T10:55:06.800227Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2159] [ConnectBlock] [bench]     - Fork checks: 0.08ms [0.23s (0.57ms/blk)]
2022-09-12T10:55:06.800311Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2244] [ConnectBlock] [bench]       - Connect 1 transactions: 0.08ms (0.076ms/tx, 0.000ms/txin) [0.04s (0.10ms/blk)]
2022-09-12T10:55:06.800399Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2257] [ConnectBlock] [bench]     - Verify 0 txins: 0.17ms (0.000ms/txin) [0.09s (0.23ms/blk)]
2022-09-12T10:55:06.800528Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2267] [ConnectBlock] [bench]     - Write undo data: 0.13ms [0.03s (0.07ms/blk)]
2022-09-12T10:55:06.800590Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2278] [ConnectBlock] [bench]     - Index writing: 0.07ms [0.02s (0.04ms/blk)]
2022-09-12T10:55:06.800692Z (mocktime: 2020-08-31T15:34:11Z) [test] [validationinterface.cpp:252] [BlockChecked] [validation] BlockChecked: block hash=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 state=Valid
2022-09-12T10:55:06.800760Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2729] [ConnectTip] [bench]   - Connect total: 0.71ms [0.18s (0.45ms/blk)]
2022-09-12T10:55:06.800820Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2734] [ConnectTip] [bench]   - Flush: 0.06ms [0.05s (0.12ms/blk)]
2022-09-12T10:55:06.800920Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2740] [ConnectTip] [bench]   - Writing chainstate: 0.09ms [0.02s (0.04ms/blk)]
2022-09-12T10:55:06.801517Z (mocktime: 2020-08-31T15:34:11Z) [test] [policy/fees.cpp:667] [processBlock] [estimatefee] Blockpolicy estimates updated by 0 of 0 block txs, since last block 0 of 0 tracked, mempool map size 0, max target 0 from current
2022-09-12T10:55:06.801640Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2519] [UpdateTipLog] UpdateTip: new best=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 height=100 version=0x20000000 log2_work=7.658211 tx=101 date='2020-08-31T15:34:11Z' progress=1.000000 cache=0.0MiB(100txo)
2022-09-12T10:55:06.801701Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2751] [ConnectTip] [bench]   - Connect postprocess: 0.78ms [0.34s (0.83ms/blk)]
2022-09-12T10:55:06.801755Z (mocktime: 2020-08-31T15:34:11Z) [test] [validation.cpp:2752] [ConnectTip] [bench] - Connect block: 1.72ms [0.75s (1.85ms/blk)]
2022-09-12T10:55:06.801819Z (mocktime: 2020-08-31T15:34:11Z) [test] [txmempool.cpp:706] [check] [mempool] Checking mempool with 0 transactions and 0 inputs
2022-09-12T10:55:06.801952Z (mocktime: 2020-08-31T15:34:11Z) [test] [validationinterface.cpp:229] [BlockConnected] [validation] Enqueuing BlockConnected: block hash=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 block height=100
2022-09-12T10:55:06.802069Z (mocktime: 2020-08-31T15:34:11Z) [scheduler] [validationinterface.cpp:229] [operator()] [validation] BlockConnected: block hash=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 block height=100
2022-09-12T10:55:06.802183Z (mocktime: 2020-08-31T15:34:11Z) [test] [validationinterface.cpp:202] [UpdatedBlockTip] [validation] Enqueuing UpdatedBlockTip: new block hash=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 fork block hash=68aef0c7c1c2cc15ca20a558ea1d6e66aecc1d6398bddea75e4c031cb79cc07e (in IBD=false)
2022-09-12T10:55:06.802295Z (mocktime: 2020-08-31T15:34:11Z) [scheduler] [validationinterface.cpp:202] [operator()] [validation] UpdatedBlockTip: new block hash=571d80a9967ae599cec0448b0b0ba1cfb606f584d8069bd7166b86854ba7a191 fork block hash=68aef0c7c1c2cc15ca20a558ea1d6e66aecc1d6398bddea75e4c031cb79cc07e (in IBD=false)
2022-09-12T10:55:06.803313Z (mocktime: 2020-08-31T15:34:12Z) [test] [dbwrapper.cpp:158] [CDBWrapper] Opened LevelDB successfully
2022-09-12T10:55:06.803450Z (mocktime: 2020-08-31T15:34:12Z) [test] [dbwrapper.cpp:183] [CDBWrapper] Using obfuscation key for /tmp/test_common_Bitcoin Core/7b9ce9462bbe925416385de4a76d6cf557607ae446c058cb3eba456ddb39ce92/regtest/indexes/coinstats/db: 0000000000000000
2022-09-12T10:55:06.804336Z (mocktime: 2020-08-31T15:34:12Z) [coinstatsindex] [util/thread.cpp:18] [TraceThread] coinstatsindex thread start
2022-09-12T10:55:06.804462Z (mocktime: 2020-08-31T15:34:12Z) [coinstatsindex] [index/base.cpp:186] [ThreadSync] Syncing coinstatsindex with block chain from height 0
2022-09-12T10:55:06.804547Z (mocktime: 2020-08-31T15:34:12Z) [coinstatsindex] [util/system.h:50] [error] ERROR: Commit: Failed to commit latest coinstatsindex state
2022-09-12T10:55:09.607921Z (mocktime: 2020-08-31T15:34:12Z) [coinstatsindex] [index/base.cpp:215] [ThreadSync] coinstatsindex is enabled at height 100
2022-09-12T10:55:09.608000Z (mocktime: 2020-08-31T15:34:12Z) [coinstatsindex] [util/thread.cpp:20] [TraceThread] coinstatsindex thread exit
2022-09-12T10:55:09.608444Z (mocktime: 2020-08-31T15:34:12Z) [test] [node/miner.cpp:163] [CreateNewBlock] CreateNewBlock(): block weight: 940 txs: 0 fees: 0 sigops 400
2022-09-12T10:55:09.608651Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2060] [ConnectBlock] [bench]     - Sanity checks: 0.01ms [0.00s (0.01ms/blk)]
2022-09-12T10:55:09.608752Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2159] [ConnectBlock] [bench]     - Fork checks: 0.11ms [0.23s (0.57ms/blk)]
2022-09-12T10:55:09.608860Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2244] [ConnectBlock] [bench]       - Connect 1 transactions: 0.09ms (0.088ms/tx, 0.000ms/txin) [0.04s (0.10ms/blk)]
2022-09-12T10:55:09.608974Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2257] [ConnectBlock] [bench]     - Verify 0 txins: 0.22ms (0.000ms/txin) [0.10s (0.23ms/blk)]
2022-09-12T10:55:09.609051Z (mocktime: 2020-08-31T15:34:12Z) [test] [node/miner.cpp:178] [CreateNewBlock] [bench] CreateNewBlock() packages: 0.05ms (0 packages, 0 updated descendants), validity: 0.69ms (total 0.74ms)
2022-09-12T10:55:09.609767Z (mocktime: 2020-08-31T15:34:12Z) [test] [validationinterface.cpp:257] [NewPoWValidBlock] [validation] NewPoWValidBlock: block hash=0e146357c1f82a8b9e2c54cb53a619a0ffbcace90d33ad228dc0ef44752b6844
2022-09-12T10:55:09.610524Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2710] [ConnectTip] [bench]   - Using cached block
2022-09-12T10:55:09.610601Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2717] [ConnectTip] [bench]   - Load block from disk: 0.07ms [0.16s (0.41ms/blk)]
2022-09-12T10:55:09.610702Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2060] [ConnectBlock] [bench]     - Sanity checks: 0.00ms [0.00s (0.01ms/blk)]
2022-09-12T10:55:09.610794Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2159] [ConnectBlock] [bench]     - Fork checks: 0.09ms [0.23s (0.57ms/blk)]
2022-09-12T10:55:09.610920Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2244] [ConnectBlock] [bench]       - Connect 1 transactions: 0.10ms (0.096ms/tx, 0.000ms/txin) [0.04s (0.10ms/blk)]
2022-09-12T10:55:09.611012Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2257] [ConnectBlock] [bench]     - Verify 0 txins: 0.21ms (0.000ms/txin) [0.10s (0.23ms/blk)]
2022-09-12T10:55:09.611151Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2267] [ConnectBlock] [bench]     - Write undo data: 0.14ms [0.03s (0.07ms/blk)]
2022-09-12T10:55:09.611217Z (mocktime: 2020-08-31T15:34:12Z) [test] [validation.cpp:2278] [ConnectBlock] [bench]     - Index writing: 0.07ms [0.02s (0.04ms/blk)]
2022-09-12T10:55:09.611296Z (mocktime: 2020-08-31T15:34:12Z) [test] [valiThreadSanitizer:DEADLYSIGNAL
make[3]: *** [Makefile:21202: test/coinstatsindex_tests.cpp.test] Error 1
make[3]: *** Waiting for unfinished jobs....
PASS: qt/test/test_bitcoin-qt
============================================================================
Testsuite summary for Bitcoin Core 23.99.0
============================================================================
# TOTAL: 4
# PASS:  4
# SKIP:  0
# XFAIL: 0
# FAIL:  0
# XPASS: 0
# ERROR: 0
============================================================================
make[4]: Leaving directory '/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src'
make[3]: Leaving directory '/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src'
make[2]: *** [Makefile:19260: check-am] Error 2
make[2]: Leaving directory '/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src'
make[1]: *** [Makefile:18926: check-recursive] Error 1
make[1]: Leaving directory '/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/src'
make: *** [Makefile:824: check-recursive] Error 1
==25198==ERROR: ThreadSanitizer: SEGV on unknown address 0x000000000068 (pc 0x55e8c6500aeb bp 0x7b1400009548 sp 0x7fdf23d0ec80 T25670)
==25198==The signal is caused by a READ memory access.
==25198==Hint: address points to the zero page.
    #0 BaseIndex::SetBestBlockIndex(CBlockIndex const*)::$_1::operator()() const src/index/base.cpp:418:9 (test_bitcoin+0xd78aeb)
    #1 BaseIndex::SetBestBlockIndex(CBlockIndex const*) src/index/base.cpp:418:9 (test_bitcoin+0xd78aeb)
    #2 BaseIndex::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*) src/index/base.cpp:298:9 (test_bitcoin+0xd7ae5b)
    #3 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)::operator()(CValidationInterface&) const src/validationinterface.cpp:225:79 (test_bitcoin+0x10f0f54)
    #4 void MainSignalsImpl::Iterate<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)&&) src/validationinterface.cpp:86:17 (test_bitcoin+0x10f0f54)
    #5 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const src/validationinterface.cpp:225:22 (test_bitcoin+0x10f0f54)
    #6 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9::operator()() const src/validationinterface.cpp:227:5 (test_bitcoin+0x10f0f54)
    #7 decltype(static_cast<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(fp)()) std::__1::__invoke<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0x10f0f54)
    #8 void std::__1::__invoke_void_return_wrapper<void, true>::__call<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0x10f0f54)
    #9 std::__1::__function::__alloc_func<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9, std::__1::allocator<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0x10f0f54)
    #10 std::__1::__function::__func<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9, std::__1::allocator<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0x10f0f54)
    #11 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x118bc01)
    #12 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x118bc01)
    #13 SingleThreadedSchedulerClient::ProcessQueue() src/scheduler.cpp:175:5 (test_bitcoin+0x118bc01)
    #14 SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1::operator()() const src/scheduler.cpp:144:41 (test_bitcoin+0x118d915)
    #15 decltype(static_cast<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(fp)()) std::__1::__invoke<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0x118d915)
    #16 void std::__1::__invoke_void_return_wrapper<void, true>::__call<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0x118d915)
    #17 std::__1::__function::__alloc_func<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1, std::__1::allocator<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0x118d915)
    #18 std::__1::__function::__func<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1, std::__1::allocator<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0x118d915)
    #19 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x118abec)
    #20 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x118abec)
    #21 CScheduler::serviceQueue() src/scheduler.cpp:62:17 (test_bitcoin+0x118abec)
    #22 ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0::operator()() const src/test/util/setup_common.cpp:186:110 (test_bitcoin+0xab5148)
    #23 decltype(static_cast<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(fp)()) std::__1::__invoke<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0xab5148)
    #24 void std::__1::__invoke_void_return_wrapper<void, true>::__call<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0xab5148)
    #25 std::__1::__function::__alloc_func<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, std::__1::allocator<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0xab5148)
    #26 std::__1::__function::__func<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, std::__1::allocator<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0xab5148)
    #27 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x124b1af)
    #28 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x124b1af)
    #29 util::TraceThread(char const*, std::__1::function<void ()>) src/util/thread.cpp:19:9 (test_bitcoin+0x124b1af)
    #30 decltype(static_cast<void (*>(fp)(static_cast<char const*>(fp0), static_cast<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>(fp0))) std::__1::__invoke<void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>(void (*&&)(char const*, std::__1::function<void ()>), char const*&&, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0xab4d41)
    #31 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>&, std::__1::__tuple_indices<2ul, 3ul>) /usr/lib/llvm-13/bin/../include/c++/v1/thread:280:5 (test_bitcoin+0xab4d41)
    #32 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0> >(void*) /usr/lib/llvm-13/bin/../include/c++/v1/thread:291:5 (test_bitcoin+0xab4d41)
    #33 __tsan_thread_start_func <null> (test_bitcoin+0x13319c)
    #34 <null> <null> (libc.so.6+0x94b42)
    #35 <null> <null> (libc.so.6+0x1269ff)
ThreadSanitizer can not provide additional info.
SUMMARY: ThreadSanitizer: SEGV src/index/base.cpp:418:9 in BaseIndex::SetBestBlockIndex(CBlockIndex const*)::$_1::operator()() const
==25198==ABORTING
Exit status: 2"
bitcoin/bitcoin,2022-09-09 15:10:14,bug,intermittent failure in rpc_invalidateblock.py,"Seen in doc-only PR: https://cirrus-ci.com/task/4697486185988096?logs=ci


```
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main
    self.run_test()
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/rpc_invalidateblock.py"", line 90, in run_test
    assert_equal(chain_tips, self.nodes[1].getchaintips())
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 56, in assert_equal
    raise AssertionError(""not(%s)"" % "" == "".join(str(arg) for arg in (thing1, thing2) + args))
AssertionError: not([{'height': 24, 'hash': '1e8c7b54ab7c4556c4602fc89c0796d104260df2e2f8d2de9006d2446676d71c', 'branchlen': 0, 'status': 'active'}, {'height': 6, 'hash': '46727213ae639f43edd7fde6d5c2dcd54cc182f7c3a5c7a652784a7ece5aa0cf', 'branchlen': 2, 'status': 'invalid'}, {'height': 4, 'hash': '0fefbfc08f4e5e4ae4dd1a74113c580530bda9bfa85ed2c40e59ee8ff5bc352d', 'branchlen': 4, 'status': 'headers-only'}] == [{'height': 24, 'hash': '1e8c7b54ab7c4556c4602fc89c0796d104260df2e2f8d2de9006d2446676d71c', 'branchlen': 0, 'status': 'active'}, {'height': 6, 'hash': '46727213ae639f43edd7fde6d5c2dcd54cc182f7c3a5c7a652784a7ece5aa0cf', 'branchlen': 2, 'status': 'invalid'}, {'height': 4, 'hash': '0fefbfc08f4e5e4ae4dd1a74113c580530bda9bfa85ed2c40e59ee8ff5bc352d', 'branchlen': 4, 'status': 'headers-only'}, {'height': 3, 'hash': '4ff28db3f1e2007c2a8f321dc6759ffdb7ff0c81ea9334d982049b9c8ae674a9', 'branchlen': 1, 'status': 'headers-only'}])
```"
bitcoin/bitcoin,2022-09-07 07:53:53,bug,test: BDB-only build fails wallet_basic.py,"Steps to reproduce:

* Compile bdb-only
* Run wallet_basic.py

```
 test  2022-09-05T10:20:34.866000Z TestFramework (ERROR): JSONRPC error 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/test_framework.py"", line 133, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/wallet_basic.py"", line 708, in run_test
                                       self.nodes[0].createwallet(wallet_name=""wo"", descriptors=True, disable_private_keys=True)
                                     File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/test_node.py"", line 754, in createwallet
                                       return self.__getattr__('createwallet')(wallet_name, disable_private_keys, blank, passphrase, avoid_reuse, descriptors, load_on_startup, external_signer)
                                     File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/coverage.py"", line 49, in __call__
                                       return_val = self.auth_service_proxy_instance.__call__(*args, **kwargs)
                                     File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/authproxy.py"", line 144, in __call__
                                       raise JSONRPCException(response['error'], status)
                                   test_framework.authproxy.JSONRPCException: Compiled without sqlite support (required for descriptor wallets) (-4)
"
bitcoin/bitcoin,2022-09-05 09:11:57,bug,sendall creates tx that fails tx-size mempool check,"Steps to reproduce:
* Create many inputs
* Call `sendall`

Error: `2022-09-05T09:07:57Z [httpworker.1] [wallet/wallet.h:827] [WalletLogPrintf] [default wallet] CommitTransaction(): Transaction cannot be broadcast immediately, tx-size`"
bitcoin/bitcoin,2022-09-02 08:01:00,bug,Stop loading wallet at startup,"**Describe the issue**
Bitcoin Core QT auto loads wallet at startup
[](https://i.ibb.co/FKDJSLn/img1.png)

**What behavior did you expect?**
I did not specify to load a wallet at startup.

**What was the actual behavior (provide screenshots if the issue is GUI-related)?**
Starting Bitcoin Core QT with double click on bitcoin-qt.exe, then it auto loads a wallet at startup
screenshot: https://i.ibb.co/FKDJSLn/img1.png
I must click on OK.

My configuration file (bitcoin.conf):
```
server=1
testnet=0
rpcuser=<redacted>
rpcpassword=<redacted>
daemon=1
```

**System information**
<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
Bitcoin Core 22.0
Windows 11 x64

**debug.log:**

2022-09-02T07:49:13Z Default data directory XXX
2022-09-02T07:49:13Z Using data directory XXX
2022-09-02T07:49:13Z Config file: XXX\\bitcoin.conf
2022-09-02T07:49:13Z Config file arg: daemon=""1""
2022-09-02T07:49:13Z Config file arg: rpcpassword=****
2022-09-02T07:49:13Z Config file arg: rpcuser=****
2022-09-02T07:49:13Z Config file arg: server=""1""
2022-09-02T07:49:13Z Config file arg: testnet=""0""
**2022-09-02T07:49:13Z Setting file arg: wallet = [""XXXXX""]**
2022-09-02T07:49:13Z Using at most 125 automatic connections (2048 file descriptors available)
2022-09-02T07:49:13Z GUI: ""registerShutdownBlockReason: Successfully registered: Bitcoin Core didn't yet exit safely...""
2022-09-02T07:49:13Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2022-09-02T07:49:13Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2022-09-02T07:49:13Z Script verification uses 3 additional threads
2022-09-02T07:49:13Z scheduler thread start
2022-09-02T07:49:13Z HTTP: creating work queue of depth 16
2022-09-02T07:49:13Z Using wallet directory XXX
2022-09-02T07:49:13Z init message: Verifying wallet(s)...
**2022-09-02T07:49:13Z Warning: Skipping -wallet path that doesn't exist. Failed to load database path 'XXX'. Path does not exist.**

**Issue looks like to be at line ""2022-09-02T07:49:13Z Setting file arg: wallet = [""XXXX""]""**

**But why Bitcoin Core loads it? I did NOT specify it in bitcoin.conf**

Thanks."
bitcoin/bitcoin,2022-08-29 19:40:57,bug,Compilation / build failed - Mac 12.4 M1 - ld: unknown option: -soname - make[2]: *** [libbitcoinconsensus.la] Error 1,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**
Build should not failed 
<!--- What behavior did you expect? -->

**Actual behavior**
Build is failing

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**
M1 - Mac OS Monterey - version 12.4
XCODE Version 13.4.1 (13F100)

1.   sh ./autogen.sh ( working fine)
2. ./configure --prefix=/Users/amit/Downloads/androidProject/wow/doit/bitcoin/depends/aarch64-linux-android --without-gui --disable-zmq --with-miniupnpc=no --with-incompatible-bdb --disable-bench --disable-tests --enable-module-ecdh CC=/opt/homebrew/Cellar/llvm/14.0.6_1/bin/clang CXX=/opt/homebrew/Cellar/llvm/14.0.6_1/bin/clang++


**./configure is failing with following errors 
ld: unknown option: -soname - make[2]: *** [libbitcoinconsensus.la] Error 1**

Making all in src
  CXX      node/libbitcoin_node_a-interface_ui.o
  CXX      node/libbitcoin_node_a-interfaces.o
  CXX      node/libbitcoin_node_a-miner.o
  CXX      node/libbitcoin_node_a-mempool_persist_args.o
  CXX      node/libbitcoin_node_a-transaction.o
  CXX      libbitcoin_node_a-noui.o
  CXX      policy/libbitcoin_node_a-fees.o
  CXX      policy/libbitcoin_node_a-rbf.o
  CXX      libbitcoin_node_a-rest.o
  CXX      rpc/libbitcoin_node_a-blockchain.o
  CXX      rpc/libbitcoin_node_a-fees.o
  CXX      rpc/libbitcoin_node_a-mempool.o
  CXX      rpc/libbitcoin_node_a-mining.o
  CXX      rpc/libbitcoin_node_a-net.o
  CXX      rpc/libbitcoin_node_a-rawtransaction.o
  CXX      rpc/libbitcoin_node_a-server.o
  CXX      rpc/libbitcoin_node_a-server_util.o
  CXX      rpc/libbitcoin_node_a-txoutproof.o
  CXX      libbitcoin_node_a-torcontrol.o
  CXX      libbitcoin_node_a-txmempool.o
  CXX      libbitcoin_node_a-txrequest.o
  CXX      libbitcoin_node_a-validation.o
  CXX      wallet/libbitcoin_node_a-init.o
  CXX      wallet/libbitcoin_wallet_a-dump.o
  CXX      wallet/libbitcoin_wallet_a-external_signer_scriptpubkeyman.o
  CXX      wallet/libbitcoin_wallet_a-feebumper.o
  CXX      wallet/libbitcoin_wallet_a-fees.o
  CXX      wallet/libbitcoin_wallet_a-interfaces.o
  CXX      wallet/libbitcoin_wallet_a-load.o
  CXX      wallet/libbitcoin_wallet_a-receive.o
  CXX      wallet/rpc/libbitcoin_wallet_a-addresses.o
  CXX      wallet/rpc/libbitcoin_wallet_a-backup.o
  CXX      wallet/rpc/libbitcoin_wallet_a-coins.o
  CXX      wallet/rpc/libbitcoin_wallet_a-encrypt.o
  CXX      wallet/rpc/libbitcoin_wallet_a-spend.o
  CXX      wallet/rpc/libbitcoin_wallet_a-signmessage.o
  CXX      wallet/rpc/libbitcoin_wallet_a-transactions.o
  CXX      wallet/rpc/libbitcoin_wallet_a-util.o
  CXX      wallet/rpc/libbitcoin_wallet_a-wallet.o
  CXX      wallet/libbitcoin_wallet_a-scriptpubkeyman.o
  CXX      wallet/libbitcoin_wallet_a-spend.o
  CXX      wallet/libbitcoin_wallet_a-wallet.o
  CXX      wallet/libbitcoin_wallet_a-walletdb.o
  CXX      wallet/libbitcoin_wallet_a-sqlite.o
  CXX      wallet/libbitcoin_wallet_a-bdb.o
  CXX      wallet/libbitcoin_wallet_a-salvage.o
  CXX      interfaces/libbitcoin_util_a-handler.o
  CXX      util/libbitcoin_util_a-system.o
  CXX      util/libbitcoin_util_a-time.o
  CXX      util/libbitcoin_util_a-url.o
  CXX      bitcoin_cli-bitcoin-cli.o
  CXX      bitcoin_tx-bitcoin-tx.o
  CXX      wallet/libbitcoin_wallet_tool_a-wallettool.o
  CXX      wallet/test/fuzz/test_fuzz_fuzz-coinselection.o
  CXX      wallet/test/fuzz/test_fuzz_fuzz-notifications.o
  CXX      test/fuzz/fuzz-addition_overflow.o
  CXX      test/fuzz/fuzz-addrman.o
  CXX      test/fuzz/fuzz-autofile.o
  CXX      test/fuzz/fuzz-banman.o
  CXX      test/fuzz/fuzz-block.o
  CXX      test/fuzz/fuzz-block_header.o
  CXX      test/fuzz/fuzz-blockfilter.o
  CXX      test/fuzz/fuzz-bloom_filter.o
  CXX      test/fuzz/fuzz-buffered_file.o
  CXX      test/fuzz/fuzz-chain.o
  CXX      test/fuzz/fuzz-checkqueue.o
  CXX      test/fuzz/fuzz-coins_view.o
  CXX      test/fuzz/fuzz-connman.o
  CXX      test/fuzz/fuzz-crypto.o
  CXX      test/fuzz/fuzz-crypto_aes256.o
  CXX      test/fuzz/fuzz-crypto_aes256cbc.o
  CXX      test/fuzz/fuzz-crypto_chacha20.o
  CXX      test/fuzz/fuzz-crypto_chacha20_poly1305_aead.o
  CXX      test/fuzz/fuzz-crypto_common.o
  CXX      test/fuzz/fuzz-crypto_diff_fuzz_chacha20.o
  CXX      test/fuzz/fuzz-crypto_hkdf_hmac_sha256_l32.o
  CXX      test/fuzz/fuzz-crypto_poly1305.o
  CXX      test/fuzz/fuzz-cuckoocache.o
  CXX      test/fuzz/fuzz-deserialize.o
  CXX      test/fuzz/fuzz-fee_rate.o
  CXX      test/fuzz/fuzz-fees.o
  CXX      test/fuzz/fuzz-flatfile.o
  CXX      test/fuzz/fuzz-float.o
  CXX      test/fuzz/fuzz-golomb_rice.o
  CXX      test/fuzz/fuzz-http_request.o
  CXX      test/fuzz/fuzz-i2p.o
  CXX      test/fuzz/fuzz-integer.o
  CXX      test/fuzz/fuzz-kitchen_sink.o
  CXX      test/fuzz/fuzz-load_external_block_file.o
  CXX      test/fuzz/fuzz-merkleblock.o
  CXX      test/fuzz/fuzz-message.o
  CXX      test/fuzz/fuzz-miniscript.o
  CXX      test/fuzz/fuzz-minisketch.o
  CXX      test/fuzz/fuzz-muhash.o
  CXX      test/fuzz/fuzz-multiplication_overflow.o
  CXX      test/fuzz/fuzz-net.o
  CXX      test/fuzz/fuzz-net_permissions.o
  CXX      test/fuzz/fuzz-netaddress.o
  CXX      test/fuzz/fuzz-netbase_dns_lookup.o
  CXX      test/fuzz/fuzz-node_eviction.o
  CXX      test/fuzz/fuzz-parse_hd_keypath.o
  CXX      test/fuzz/fuzz-policy_estimator.o
  CXX      test/fuzz/fuzz-policy_estimator_io.o
  CXX      test/fuzz/fuzz-pow.o
  CXX      test/fuzz/fuzz-primitives_transaction.o
  CXX      test/fuzz/fuzz-process_message.o
  CXX      test/fuzz/fuzz-process_messages.o
  CXX      test/fuzz/fuzz-protocol.o
  CXX      test/fuzz/fuzz-random.o
  CXX      test/fuzz/fuzz-rbf.o
  CXX      test/fuzz/fuzz-rolling_bloom_filter.o
  CXX      test/fuzz/fuzz-rpc.o
  CXX      test/fuzz/fuzz-script.o
  CXX      test/fuzz/fuzz-script_bitcoin_consensus.o
  CXX      test/fuzz/fuzz-script_descriptor_cache.o
  CXX      test/fuzz/fuzz-script_format.o
  CXX      test/fuzz/fuzz-script_interpreter.o
  CXX      test/fuzz/fuzz-script_ops.o
  CXX      test/fuzz/fuzz-script_sigcache.o
  CXX      test/fuzz/fuzz-script_sign.o
  CXX      test/fuzz/fuzz-scriptnum_ops.o
  CXX      test/fuzz/fuzz-secp256k1_ec_seckey_import_export_der.o
  CXX      test/fuzz/fuzz-secp256k1_ecdsa_signature_parse_der_lax.o
  CXX      test/fuzz/fuzz-signature_checker.o
  CXX      test/fuzz/fuzz-signet.o
  CXX      test/fuzz/fuzz-socks5.o
  CXX      test/fuzz/fuzz-span.o
  CXX      test/fuzz/fuzz-string.o
  CXX      test/fuzz/fuzz-strprintf.o
  CXX      test/fuzz/fuzz-system.o
  CXX      test/fuzz/fuzz-timedata.o
  CXX      test/fuzz/fuzz-torcontrol.o
  CXX      test/fuzz/fuzz-transaction.o
  CXX      test/fuzz/fuzz-tx_pool.o
  CXX      test/fuzz/fuzz-txorphan.o
  CXX      test/fuzz/fuzz-utxo_snapshot.o
  CXX      test/fuzz/fuzz-validation_load_mempool.o
  CXX      test/fuzz/fuzz-versionbits.o
  CXX      test/util/libtest_util_a-blockfilter.o
  CXX      test/util/libtest_util_a-mining.o
  CXX      test/util/libtest_util_a-setup_common.o
  CXX      test/util/libtest_util_a-validation.o
  CXX      test/util/libtest_util_a-wallet.o
  CXX      test/fuzz/libtest_fuzz_a-fuzz.o
  CXX      test/fuzz/libtest_fuzz_a-util.o
  **CXXLD    libbitcoinconsensus.la
  CXX      libbitcoin_node_a-blockencodings.o
ld: unknown option: -soname
clang-14: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [libbitcoinconsensus.la] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1**



  
<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

**System information**

M1 - Mac OS Monterey - version 12.4 
XCODE Version 13.4.1 (13F100)
<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-08-25 00:08:08,bug,qt.qpa.xcb: could not connect to display,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->
ubuntu 20.04 desktop with necessary build tools for bitcoin core

QFactoryLoader::QFactoryLoader() ignoring ""org.qt-project.Qt.QPA.QPlatformIntegrationFactoryInterface.5.3"" since plugins are disabled in static builds
qt.qpa.xcb: could not connect to display 
qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in """" even though it was found

bitcoin core v23.0 built using depends on ubuntu 20.04
NOTE: same issue is present with 22.0 binaries built on ubuntu 18.04
the binaries run just fine on the 18.04 server

This appears to be a dependency issue in the build tree

**Expected behavior**
<!--- What behavior did you expect? -->
bitcoin-qt to start
**Actual behavior**
bitcoin-qt failed to start, error message above
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**
install fresh ubuntu 20.04 or lubuntu 20.04 desktop
apt-get install ..... the build stuff for bitcoin core
cd depends
make
cd ..
./autogen.sh
configure --- with lots of stuff normally used
run binary with normal config

<!--- How reliably can you reproduce the issue, what are the steps to do so? -->
100% every time
**System information**
lubuntu 20.04 and ubuntu 20.04

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->
github
checkout tag v23.0
<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->
proliant GL380-G7  192G/ram 2T disk

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->
see above
<!-- Any extra information that might be useful in the debugging process. -->
tried all the solutions found with google, about a half dozen of them
The error message about platforms not included in static build appears to be the key,
do not know how to resolve that.

<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-08-21 02:16:12,bug,c++ compiler wont make,"```
errirayech:bitcoin folderhub$  ./configure CC=/opt/homebrew/opt/llvm/bin/clang@ CXX=/opt/homebrew/opt/llvm/bin/clang++  
checking for pkg-config... /usr/local/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking build system type... i386-apple-darwin17.7.0
checking host system type... i386-apple-darwin17.7.0
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a race-free mkdir -p... ./build-aux/install-sh -c -d
checking for gawk... no
checking for mawk... no
checking for nawk... no
checking for awk... awk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether to enable maintainer-specific portions of Makefiles... yes
checking whether make supports nested variables... (cached) yes
checking whether the C++ compiler works... no
configure: error: in `/Users/folderhub/heard/bitcoin':
configure: error: C++ compiler cannot create executables
See `config.log' for more details
```"
bitcoin/bitcoin,2022-08-13 02:48:34,bug,"Mac OS latest Bitcoin core latest release will not run as Mac OS, ""you can't open the application 'Bitcoin core'""","similar to #24140, but different error message. 

<img width=""648"" alt=""Screen Shot 2022-08-12 at 9 08 46 PM"" src=""https://user-images.githubusercontent.com/83173948/184465625-340a49e5-0616-42e6-bc29-18a572817289.png"">
<img width=""778"" alt=""Screen Shot 2022-08-12 at 9 08 16 PM"" src=""https://user-images.githubusercontent.com/83173948/184465622-35f2748c-fb5e-4cc7-a11c-9db3938cff4c.png"">
<img width=""881"" alt=""Screen Shot 2022-08-12 at 9 08 30 PM"" src=""https://user-images.githubusercontent.com/83173948/184465624-0b782382-32a6-4732-ba9a-87c639704736.png"">
"
bitcoin/bitcoin,2022-08-10 21:48:51,bug,1 test fails when running make check (Windows Visual Studio WSL Ubuntu),"See log below.. bottom line up front: The test fails because test_bitcoin-qt.exe fails to launch. I attempted just running the executable manually in my C:\\ drive's bitcoin\\bin folder and that also does not launch. bitcoin-qt.exe launches though and runs nicely.

From test-suite.log:

==============================================
   Bitcoin Core 23.99.0: src/test-suite.log
==============================================

# TOTAL: 4
# PASS:  3
# SKIP:  0
# XFAIL: 0
# FAIL:  1
# XPASS: 0
# ERROR: 0

.. contents:: :depth: 2

FAIL: qt/test/test_bitcoin-qt
=============================

A crash occurred in \\\\wsl$\\Ubuntu\\home\\jglann\\bitcoin\\src\\qt\\test\\test_bitcoin-qt.exe.
Function time: 836ms Total time: 836ms

Exception address: 0x0000000000bc5988
Exception code   : 0xc0000005

Stack:
#  1: Unable to obtain symbol
#  2: UnhandledExceptionFilter() - 0x00007ffe7fdbfe70
#  3: memset() - 0x00007ffe82634000
#  4: _C_specific_handler() - 0x00007ffe8261c6d0
#  5: _chkstk() - 0x00007ffe82632180
#  6: RtlRaiseException() - 0x00007ffe825e1020
#  7: KiUserExceptionDispatcher() - 0x00007ffe82630da0
#  8: Unable to obtain symbol
#  9: Unable to obtain symbol
# 10: Unable to obtain symbol
# 11: Unable to obtain symbol
# 12: Unable to obtain symbol
# 13: Unable to obtain symbol
# 14: Unable to obtain symbol
# 15: Unable to obtain symbol
# 16: Unable to obtain symbol
# 17: Unable to obtain symbol
# 18: Unable to obtain symbol
# 19: Unable to obtain symbol
# 20: Unable to obtain symbol
# 21: Unable to obtain symbol
# 22: Unable to obtain symbol
# 23: Unable to obtain symbol
# 24: Unable to obtain symbol
# 25: Unable to obtain symbol
# 26: Unable to obtain symbol
# 27: Unable to obtain symbol
# 28: Unable to obtain symbol
# 29: Unable to obtain symbol
# 30: Unable to obtain symbol
# 31: Unable to obtain symbol
# 32: Unable to obtain symbol
# 33: Unable to obtain symbol
# 34: BaseThreadInitThunk() - 0x00007ffe82357020
# 35: RtlUserThreadStart() - 0x00007ffe825e2630

FAIL qt/test/test_bitcoin-qt.exe (exit status: 5)

"
bitcoin/bitcoin,2022-07-30 14:56:44,bug,"pubkey.cpp:368:18: runtime error: implicit conversion from type 'int' of value 256 (32-bit, signed) to type 'unsigned char' changed the value to 0 (8-bit, unsigned)","Steps to reproduce:

* Build with `integer` sanitizer
* Start: `UBSAN_OPTIONS=""suppressions=$(pwd)/test/sanitizer_suppressions/ubsan:print_stacktrace=1:halt_on_error=1:report_error_type=1"" ./src/qt/bitcoin-qt -chain=main`
* Enter `getdescriptorinfo ""sh(multi(1,xprv9uPDJpEQgRQfDcW7BkF7eTya6RPxXeJCqCJGHuCJ4GiRVLzkTXBAJMu2qaMWPrS7AANYqdq6vcBcBUdJCVVFceUvJFjaPdGZ2y9WACViL4L/5/5/5/2/8/5/8/8/8/2/2/5/8/5/2/5/85/58/5/2/5/5/5/5/2/58/2/5/8/0000/00/8/5/2/001/1/246/8/501/46/8/5/8/5/000/8/52/5/8/5/2/8/2/5/8/5/246/8/5/2/8/2/5/8/5/2/5/8/8/2/5/5/5/58/88/2/6/8/5/2/8/2/5/8/2/588/5/2/5/8/5/2/8/5/1/5/4/5/5/5/2/6/8/5/2/8/2/5/8/5/2/5/58/58/2/5/8/58/588/2/52/5/5/5/58/8/2/6/8/5/2/8/2/5/8/2/588/5/2/5/8/5/2/8/5/15/58/58/2/6/8/5/2/8/2/5/8/5/2/5/8/58/2/5/58/58/588/2/5/2/8/5/8/5/4/5/58/588/2/6/8/5/2/8/2/5/8/2/8/5/2/8/2/5/8/5/2/5/58/58/2/5/5/5/58/588/2/6/8/5/2/8/2/5/8/2/8/588/2/6/8/5/2/8/2/5/5/8/5/2/0/8/5/2/00/4/5/5/588/2/6/8/5/2/8/25/5/8/2/5/58/58/8/4/1,[00000000]xprv9uPDJpEQgRQfDcW7BkF7eTya6RPxXeJCqCJGHuCJ4GiRVLzkTXBAJMu2qaMWPrS7AANYqdq6vcBcBUdJCVVFceUvJFjaPdGZ2y9WACViL4L/58)0)""` into the console"
bitcoin/bitcoin,2022-07-29 15:18:04,bug,test: failure in rpc_net.py,"https://cirrus-ci.com/task/5789129014247424
```bash
 test  2022-07-29T10:31:54.510000Z TestFramework.utils (ERROR): wait_until() failed. Predicate: '''' 
                                           self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                   '''
 test  2022-07-29T10:31:54.511000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/rpc_net.py"", line 68, in run_test
                                       self.test_getnetworkinfo()
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/rpc_net.py"", line 144, in test_getnetworkinfo
                                       self.connect_nodes(0, 1)
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 594, in connect_nodes
                                       self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 717, in wait_until
                                       return wait_until_helper(test_function, timeout=timeout, timeout_factor=self.options.timeout_factor)
                                     File ""/tmp/cirrus-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/util.py"", line 277, in wait_until_helper
                                       raise AssertionError(""Predicate {} not true after {} seconds"".format(predicate_source, timeout))
                                   AssertionError: Predicate ''''
                                           self.wait_until(lambda: sum(peer['version'] != 0 for peer in to_connection.getpeerinfo()) == to_num_peers)
                                   ''' not true after 2400.0 seconds
```"
bitcoin/bitcoin,2022-07-27 21:30:51,bug,"MacOS: `importdescriptors` RPC call very slow with release and source-built binaries, but fast for homebrew binaries","I'm working on a project on MacOS that makes RPC calls using the Rust [bitcoincore-rpc crate](https://crates.io/crates/bitcoincore-rpc), against the bitcoind prebuilt binary available [here](https://bitcoincore.org/bin/bitcoin-core-23.0/bitcoin-23.0-x86_64-apple-darwin.tar.gz).

Some RPC calls succeed, but some fail with this error:

```
Error: Rpc(JsonRpc(Transport(SocketError(Os { code: 35, kind: WouldBlock, message: ""Resource temporarily unavailable"" }))))
```

After much debugging, we found that when we used the version of `bitcoind` provided by [homebrew](https://brew.sh/), the exact same sequence of RPC calls would succeed. We were able to reproduce this on both an x86 mac and an ARM mac.

We haven't been able to figure out what the disparity is, but looking at the homebrew [bitcoind formula defiinition](https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/bitcoin.rb) might provide some clues.

In particular:

```
  def install
    system ""./autogen.sh""
    system ""./configure"", *std_configure_args,
                          ""--disable-dependency-tracking"",
                          ""--disable-silent-rules"",
                          ""--with-boost-libdir=#{Formula[""boost@1.76""].opt_lib}""
    system ""make"", ""install""
    pkgshare.install ""share/rpcauth""
  end
```

It looks like homebrew is passing additional arguments to `configure`, `std_configure_args`, and is using a version of boost built by homebrew. I tried using `brew install -s boost@1.76 bitcoind`, to see if this also produced a binary that worked, and it did. The logs from that build are available [here](https://github.com/casey/homebrew-bitcoind-build-logs).

I've run into similar problems like this one a few times before, i.e., RPC calls to bitcoind failing on MacOS due to inscrutable I/O errors. One time, I was able to fix it by increasing the open file limit using `ulimit -n BIG_NUMBER`, but that doesn't seem to work here.

My best guess is that the homebrew poeople have gotten pretty good at configuring packages on MacOS, and they're passing some flag, or using a patched version of boost, or linking it differently, such that the binary they produce isn't affected by this issue."
bitcoin/bitcoin,2022-07-20 16:58:45,bug,"wallet_backup.py fails with AssertionError: not(50 == 0) [assert_equal(self.nodes[2].getbalance(), 0)]","From https://cirrus-ci.com/task/5098379825905664?logs=ci#L3267

```
 node2 2022-07-20T16:54:37.082831Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Wallet file version = 10500, last client version = 239900 
 node2 2022-07-20T16:54:37.082858Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Keys: 0 plaintext, 0 encrypted, 0 w/ metadata, 0 total. Unknown wallet records: 0 
 node2 2022-07-20T16:54:37.363230Z [httpworker.0] [wallet/scriptpubkeyman.h:248] [WalletLogPrintf] [default wallet] keypool added 200 keys (100 internal), size=200 (100 internal) 
 node2 2022-07-20T16:54:37.372743Z [httpworker.0] [wallet/scriptpubkeyman.h:248] [WalletLogPrintf] [default wallet] LegacyScriptPubKeyMan::NewKeyPool rewrote keypool 
 node2 2022-07-20T16:54:37.374228Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Wallet completed loading in             301ms 
 node2 2022-07-20T16:54:37.374283Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] setKeyPool.size() = 200 
 node2 2022-07-20T16:54:37.374293Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] mapWallet.size() = 0 
 node2 2022-07-20T16:54:37.374302Z [httpworker.0] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] m_address_book.size() = 0 
 node2 2022-07-20T16:54:37.375129Z [http] [httpserver.cpp:241] [http_request_cb] [http] Received a POST request for / from 127.0.0.1:43832 
 node2 2022-07-20T16:54:37.375178Z [httpworker.2] [rpc/request.cpp:179] [parse] [rpc] ThreadRPCServer method=getwalletinfo user=__cookie__ 
 node2 2022-07-20T16:54:37.375852Z [http] [httpserver.cpp:241] [http_request_cb] [http] Received a POST request for / from 127.0.0.1:43832 
 node2 2022-07-20T16:54:37.375896Z [httpworker.3] [rpc/request.cpp:179] [parse] [rpc] ThreadRPCServer method=importprivkey user=__cookie__ 
 node2 2022-07-20T16:54:37.384691Z [httpworker.3] [wallet/scriptpubkeyman.h:248] [WalletLogPrintf] [default wallet] Already have script 00144ff785b8221dc206314ca12e65773a876dff30ff, skipping 
 node2 2022-07-20T16:54:37.385674Z [httpworker.3] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] RescanFromTime: Rescanning last 215 blocks 
 node2 2022-07-20T16:54:37.385688Z [httpworker.3] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Rescan started from block 0f9188f13cb7b2c71f2a335e3a4fc328bf5beb436012afca590b1a11466e2206... 
 node2 2022-07-20T16:54:37.386000Z [httpworker.3] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] AddToWallet 4cddeee95a635bbf1efa02cd90f11f3385981ba1461309d7e4787e8f3c95d2df  new 
 node2 2022-07-20T16:54:37.391170Z [httpworker.3] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Scanning current mempool transactions. 
 node2 2022-07-20T16:54:37.391187Z [httpworker.3] [wallet/wallet.h:818] [WalletLogPrintf] [default wallet] Rescan completed in               5ms 
 node0 2022-07-20T16:54:37.391944Z [http] [httpserver.cpp:241] [http_request_cb] [http] Received a POST request for / from 127.0.0.1:42220 
 node0 2022-07-20T16:54:37.391992Z [httpworker.1] [rpc/request.cpp:179] [parse] [rpc] ThreadRPCServer method=getbalance user=__cookie__ 
 node1 2022-07-20T16:54:37.392518Z [http] [httpserver.cpp:241] [http_request_cb] [http] Received a POST request for / from 127.0.0.1:34582 
 node1 2022-07-20T16:54:37.392564Z [httpworker.2] [rpc/request.cpp:179] [parse] [rpc] ThreadRPCServer method=getbalance user=__cookie__ 
 test  2022-07-20T16:54:37.393000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/wallet_backup.py"", line 234, in run_test
                                       assert_equal(self.nodes[2].getbalance(), 0)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/util.py"", line 51, in assert_equal
                                       raise AssertionError(""not(%s)"" % "" == "".join(str(arg) for arg in (thing1, thing2) + args))
                                   AssertionError: not(50.00000000 == 0)"
bitcoin/bitcoin,2022-07-18 13:44:26,bug,SUMMARY: ThreadSanitizer: SEGV src/index/base.cpp:388:9 in BaseIndex::SetBestBlockIndex(CBlockIndex const*)::$_1::operator()() const,"On current master (d806407173926e46421ad807750a06e49afbbdbd):

```
==92915==ERROR: ThreadSanitizer: SEGV on unknown address 0x000000000068 (pc 0x563f9856361b bp 0x7b140000ae48 sp 0x7f2a8bcfec90 T93437)
==92915==The signal is caused by a READ memory access.
==92915==Hint: address points to the zero page.
    #0 BaseIndex::SetBestBlockIndex(CBlockIndex const*)::$_1::operator()() const src/index/base.cpp:388:9 (test_bitcoin+0xcf161b)
    #1 BaseIndex::SetBestBlockIndex(CBlockIndex const*) src/index/base.cpp:388:9 (test_bitcoin+0xcf161b)
    #2 BaseIndex::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*) src/index/base.cpp:273:9 (test_bitcoin+0xcf3899)
    #3 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)::operator()(CValidationInterface&) const src/validationinterface.cpp:225:79 (test_bitcoin+0x104ac34)
    #4 void MainSignalsImpl::Iterate<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const::'lambda'(CValidationInterface&)&&) src/validationinterface.cpp:86:17 (test_bitcoin+0x104ac34)
    #5 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_8::operator()() const src/validationinterface.cpp:225:22 (test_bitcoin+0x104ac34)
    #6 CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9::operator()() const src/validationinterface.cpp:227:5 (test_bitcoin+0x104ac34)
    #7 decltype(static_cast<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(fp)()) std::__1::__invoke<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0x104ac34)
    #8 void std::__1::__invoke_void_return_wrapper<void, true>::__call<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&>(CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0x104ac34)
    #9 std::__1::__function::__alloc_func<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9, std::__1::allocator<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0x104ac34)
    #10 std::__1::__function::__func<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9, std::__1::allocator<CMainSignals::BlockConnected(std::__1::shared_ptr<CBlock const> const&, CBlockIndex const*)::$_9>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0x104ac34)
    #11 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x10e3661)
    #12 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x10e3661)
    #13 SingleThreadedSchedulerClient::ProcessQueue() src/scheduler.cpp:175:5 (test_bitcoin+0x10e3661)
    #14 SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1::operator()() const src/scheduler.cpp:144:41 (test_bitcoin+0x10e5365)
    #15 decltype(static_cast<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(fp)()) std::__1::__invoke<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0x10e5365)
    #16 void std::__1::__invoke_void_return_wrapper<void, true>::__call<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&>(SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0x10e5365)
    #17 std::__1::__function::__alloc_func<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1, std::__1::allocator<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0x10e5365)
    #18 std::__1::__function::__func<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1, std::__1::allocator<SingleThreadedSchedulerClient::MaybeScheduleProcessQueue()::$_1>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0x10e5365)
    #19 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x10e264c)
    #20 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x10e264c)
    #21 CScheduler::serviceQueue() src/scheduler.cpp:62:17 (test_bitcoin+0x10e264c)
    #22 ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0::operator()() const src/test/util/setup_common.cpp:177:110 (test_bitcoin+0xa733c8)
    #23 decltype(static_cast<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(fp)()) std::__1::__invoke<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0xa733c8)
    #24 void std::__1::__invoke_void_return_wrapper<void, true>::__call<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&>(ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (test_bitcoin+0xa733c8)
    #25 std::__1::__function::__alloc_func<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, std::__1::allocator<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (test_bitcoin+0xa733c8)
    #26 std::__1::__function::__func<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, std::__1::allocator<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (test_bitcoin+0xa733c8)
    #27 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (test_bitcoin+0x11a229f)
    #28 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (test_bitcoin+0x11a229f)
    #29 util::TraceThread(char const*, std::__1::function<void ()>) src/util/thread.cpp:19:9 (test_bitcoin+0x11a229f)
    #30 decltype(static_cast<void (*>(fp)(static_cast<char const*>(fp0), static_cast<ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>(fp0))) std::__1::__invoke<void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>(void (*&&)(char const*, std::__1::function<void ()>), char const*&&, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0&&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (test_bitcoin+0xa72fc1)
    #31 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0>&, std::__1::__tuple_indices<2ul, 3ul>) /usr/lib/llvm-13/bin/../include/c++/v1/thread:280:5 (test_bitcoin+0xa72fc1)
    #32 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, ChainTestingSetup::ChainTestingSetup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<char const*, std::__1::allocator<char const*> > const&)::$_0> >(void*) /usr/lib/llvm-13/bin/../include/c++/v1/thread:291:5 (test_bitcoin+0xa72fc1)
    #33 __tsan_thread_start_func <null> (test_bitcoin+0x12d2bc)
    #34 <null> <null> (libc.so.6+0x94b42)
    #35 <null> <null> (libc.so.6+0x1269ff)
ThreadSanitizer can not provide additional info.
SUMMARY: ThreadSanitizer: SEGV src/index/base.cpp:388:9 in BaseIndex::SetBestBlockIndex(CBlockIndex const*)::$_1::operator()() const
==92915==ABORTING
Exit status: 2
```

https://cirrus-ci.com/task/5494683034976256?logs=ci#L3229"
bitcoin/bitcoin,2022-07-06 16:20:08,bug,build failure on M1 (arm64): error: unknown target CPU 'armv8-a+crc+crypto',"System:

```
--> uname -a
Darwin   21.4.0 Darwin Kernel Version 21.4.0: Fri Mar 18 00:47:26 PDT 2022; root:xnu-8020.101.4~15/RELEASE_ARM64_T8101 arm64

--> gcc -v
Apple clang version 13.1.6 (clang-1316.0.21.2.5)
Target: arm64-apple-darwin21.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```

Bitcoin: (master branch)

```
--> git rev-parse HEAD
aeab1b42e67cc8146bfc7d127d15633bd652fe60
```

Build command:

```
./autogen.sh
./configure --without-gui --with-incompatible-bdb
make -j 4
```


Error output:

```
...
30 warnings generated.
  CXX      zmq/libbitcoin_zmq_a-zmqutil.o
  CXX      primitives/libbitcoin_consensus_a-block.o
  CXX      primitives/libbitcoin_consensus_a-transaction.o
  CXX      crypto/libbitcoin_crypto_base_la-aes.lo
  CXX      crypto/libbitcoin_crypto_base_la-chacha_poly_aead.lo
  CXX      crypto/libbitcoin_crypto_base_la-chacha20.lo
  CXX      crypto/libbitcoin_crypto_base_la-hkdf_sha256_32.lo
  CXX      crypto/libbitcoin_crypto_base_la-hmac_sha256.lo
  CXX      crypto/libbitcoin_crypto_base_la-hmac_sha512.lo
  CXX      crypto/libbitcoin_crypto_base_la-poly1305.lo
  CXX      crypto/libbitcoin_crypto_base_la-muhash.lo
  CXX      crypto/libbitcoin_crypto_base_la-ripemd160.lo
  CXX      crypto/libbitcoin_crypto_base_la-sha1.lo
  CXX      crypto/libbitcoin_crypto_base_la-sha256.lo
  CXX      crypto/libbitcoin_crypto_base_la-sha3.lo
  CXX      crypto/libbitcoin_crypto_base_la-sha512.lo
  CXX      crypto/libbitcoin_crypto_base_la-siphash.lo
  CXX      crypto/libbitcoin_crypto_base_la-sha256_sse4.lo
  CXX      crypto/libbitcoin_crypto_arm_shani_la-sha256_arm_shani.lo
  CXX      leveldb/db/libleveldb_la-builder.lo
error: unknown target CPU 'armv8-a+crc+crypto'
note: valid target CPU values are: nocona, core2, penryn, bonnell, atom, silvermont, slm, goldmont, goldmont-plus, tremont, nehalem, corei7, westmere, sandybridge, corei7-avx, ivybridge, core-avx-i, haswell, core-avx2, broadwell, skylake, skylake-avx512, skx, cascadelake, cooperlake, cannonlake, icelake-client, rocketlake, icelake-server, tigerlake, sapphirerapids, alderlake, knl, knm, k8, athlon64, athlon-fx, opteron, k8-sse3, athlon64-sse3, opteron-sse3, amdfam10, barcelona, btver1, btver2, bdver1, bdver2, bdver3, bdver4, znver1, znver2, znver3, x86-64, x86-64-v2, x86-64-v3, x86-64-v4
make[2]: *** [crypto/libbitcoin_crypto_arm_shani_la-sha256_arm_shani.lo] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1
```"
bitcoin/bitcoin,2022-07-03 07:23:10,bug,Build failing on Ubuntu 22.04,"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

<!-- Describe the issue -->

**Expected behavior**
To run bitcoin-core in Ubuntu 22.04 after following all the steps in [build-unix.md](https://github.com/bitcoin/bitcoin/blob/master/doc/build-unix.md#linux-distribution-specific-instructions)

**Actual behavior**

Even after carefully following all the steps the build is failing in Ubuntu 22.04 version but by following the same steps again in Ubuntu 20.04 and 18.04 I am able to successfully run bitcoin-core.

**To reproduce**

To verify this first follow all the instructions in the [build-unix.md](https://github.com/bitcoin/bitcoin/blob/master/doc/build-unix.md#linux-distribution-specific-instructions) in Ubuntu 22.04 and then try to run the following command:

```
python3 test/functional/feature_taproot.py
```

You will be met with the following error of some sort:
```
 2022-06-11T19:04:12.075000Z TestFramework (INFO): Initializing test directory /tmp/bitcoin_func_test_2wz38651
2022-06-11T19:04:16.476000Z TestFramework (INFO): Post-activation tests...
2022-06-11T19:04:16.832000Z TestFramework (ERROR): Unexpected exception caught during testing
Traceback (most recent call last):
  File ""/usr/lib/python3.10/hashlib.py"", line 160, in __hash_new
    return _hashlib.new(name, data, **kwargs)
ValueError: [digital envelope routines] unsupported
```

This error is not in particular for running tests but shows a problem with the installation of bitcoin-core in general.

I searched for the issues and came across a few articles and conversations regarding this particular issue in Ubuntu 22.04. Will be providing links to them.
"
bitcoin/bitcoin,2022-06-29 15:29:50,bug,build: autoconf 2.71 warns that `%.raw.h` was already defined,"The master branch (cc22bd7f708fc3f5e793bf0138cd340f71c0feb9) on Ubuntu 22.04:
```
$ ./autogen.sh 
...
src/Makefile.bench.include:97: warning: %.raw.h was already defined in condition TRUE, which includes condition ENABLE_BENCH ...
src/Makefile.am:1065:   'src/Makefile.bench.include' included from here
src/Makefile.test.include:420: ... '%.raw.h' previously defined here
src/Makefile.am:1062:   'src/Makefile.test.include' included from here
...
```"
bitcoin/bitcoin,2022-06-22 19:55:23,bug,importdescriptors does not scan the mempool,"Just noticed this while debugging a watch-only wallet.

Step 1: create watch-only wallet
Step 2: import some descriptor
Step 3: notice that if a transaction is currently in the mempool, it won't show up in the wallet
Step 4: restart Bitcoin Core, it'll show up

cc @achow101 "
bitcoin/bitcoin,2022-06-13 16:18:46,bug,Mac: app GUI freezes when synchronizing with network,"When app is synchronising with network, often app becomes not responsive: Its window is not focused when you press the app icon and it doesn't handle mouse clicks, always showing loader instead of mouse arrow.

It still has some progress loading blocks though. After some time while being focused using F3 button on Mac, it may become responsive.

**Expected behavior**

It should always be responsive.

**Actual behavior**

It is not responsive.

**To reproduce**

Just start synchronizing, probably reproduced better on the last percents.

**System information**

Bitocoin Core 23.0
MacBook Air (M1, 2020)
Mac 12.4"
bitcoin/bitcoin,2022-05-30 19:20:51,bug,CVE-2021-31876 child transactions do not inherit rbf signaling,"The original issue reported at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018893.html and at https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-31876 seems to be unresolved in the codebase, and there are no open issues to track this, so I'm opening one.


"
bitcoin/bitcoin,2022-05-27 13:02:42,bug,nlocktime bug,**Block height** for `nlocktime` doesn't work. 
bitcoin/bitcoin,2022-05-20 14:48:19,bug,"""Estimated fee out of range"" intermittent failure in feature_fee_estimation.py","https://cirrus-ci.com/task/4707498732027904 on [nowallet libbitcoinkernel] [bionic]

```bash
 node1 2022-05-20T14:25:48.635128Z [httpworker.3] [policy/fees.cpp:365] [EstimateMedianVal] FeeEst: 1 > 95% decay 0.99520: feerate: 307809 from (301403 - 348912) 100.00% 34.4/(34.4 0 mem 0.0 out) Fail: (247965 - 301403) 86.60% 32.3/(32.3 5 mem 0.0 out) 
 node1 2022-05-20T14:25:48.635669Z [httpworker.3] [policy/fees.cpp:365] [EstimateMedianVal] FeeEst: 1 > 95% decay 0.99931: feerate: 732088 from (515502 - 1e+99) 100.00% 156.5/(156.5 0 mem 0.0 out) Fail: (247965 - 515502) 93.92% 154.6/(154.6 10 mem 0.0 out) 
 test  2022-05-20T14:25:48.638000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main
                                       self.run_test()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 297, in run_test
                                       self.sanity_check_estimates_range()
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 205, in sanity_check_estimates_range
                                       check_estimates(self.nodes[1], self.fees_per_kb)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 114, in check_estimates
                                       check_raw_estimates(node, fees_seen)
                                     File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 79, in check_raw_estimates
                                       f""Estimated fee ({feerate}) out of range ({min(fees_seen)},{max(fees_seen)})""
                                   AssertionError: Estimated fee (0.00732088) out of range (3.9370078740157485e-05,0.007271477272727272)
 test  2022-05-20T14:25:48.639000Z TestFramework (DEBUG): Closing down network thread 
 test  2022-05-20T14:25:48.690000Z TestFramework (INFO): Stopping nodes 
 test  2022-05-20T14:25:48.690000Z TestFramework.node0 (DEBUG): Stopping node 
```

I initially thought this might be the same as #23165 (and https://github.com/bitcoin/bitcoin/issues/11800 and https://github.com/bitcoin/bitcoin/issues/20725) that return *Estimated fee (a) larger than last fee (b) for lower number of confirms.*

But this one is *Estimated fee out of range*, so opening this issue and linking to the other ones."
bitcoin/bitcoin,2022-05-13 14:54:25,bug,subtractfeefromamount=true fails with insufficient funds,"Was: https://github.com/bitcoin/bitcoin/issues/25030

Steps to reproduce:

```
wget https://github.com/bitcoin/bitcoin/files/8688722/datadir_node0.tar.gz
tar -xvf datadir_node0.tar.gz
export DD='-datadir=./node0/'
bitcoin-qt $DD &
bitcoin-cli $DD loadwallet rpc_online
bitcoin-cli $DD -named sendtoaddress address=bcrt1qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq3xueyj amount=1966.66779818 subtractfeefromamount=true"
bitcoin/bitcoin,2022-05-10 20:15:19,bug,TapLeaf hash is calculated differently from BIP340 reference implementation,"Due to using the bitcoin serialization routines within CHashWriter, TapLeaf hasher is fed with an excessive byte, which contains the length of a hashed script.

**Expected behavior**

BIP341 refers TapLeaf hash calculation as tagged_hash function having reference implementation as https://github.com/bitcoin/bips/blob/b1791c24aa163eb6578d0bfaadcf44997484eeaf/bip-0340/reference.py#L25 :

```
def tagged_hash(tag: str, msg: bytes) -> bytes:
    tag_hash = hashlib.sha256(tag.encode()).digest()
    return hashlib.sha256(tag_hash + tag_hash + msg).digest()
```

**Actual behavior**

CScript, and its parent - prevector, are fed to CHashWriter as \\<compact length\\>||\\<prevector bytes\\> which leads to the wrong hash calculation compared to the reference implementation from BIP.

**To reproduce**

Here is the sample test which shows the behavior:
https://github.com/layer1dot5/l15-core/blob/barebones/test/src/test_taptree.cpp

**System information**

Bitcoin v23.0 and master

"
bitcoin/bitcoin,2022-05-06 14:25:41,bug,Fix chain tip data race and corrupt rest response,"This fixes two issues:

* A data race in `ActiveChain`, which returns a reference to the chain (a `std::vector`), which is not thread safe. See also below traceback.
* A corrupt rest response, which returns a blockheight and blockhash, which are unrelated to each other and to the result, as the chain might advance between each call without cs_main held.

The issues are fixed by taking cs_main and holding it for the required time.



```
==================
WARNING: ThreadSanitizer: data race (pid=32335)
  Write of size 8 at 0x7b3c000008f0 by thread T22 (mutexes: write M131626, write M151, write M131553):
    #0 std::__1::enable_if<(is_move_constructible<CBlockIndex**>::value) && (is_move_assignable<CBlockIndex**>::value), void>::type std::__1::swap<CBlockIndex**>(CBlockIndex**&, CBlockIndex**&) /usr/lib/llvm-13/bin/../include/c++/v1/__utility/swap.h:39:7 (bitcoind+0x501239)
    #1 std::__1::vector<CBlockIndex*, std::__1::allocator<CBlockIndex*> >::__swap_out_circular_buffer(std::__1::__split_buffer<CBlockIndex*, std::__1::allocator<CBlockIndex*>&>&) /usr/lib/llvm-13/bin/../include/c++/v1/vector:977:5 (bitcoind+0x501239)
    #2 std::__1::vector<CBlockIndex*, std::__1::allocator<CBlockIndex*> >::__append(unsigned long) /usr/lib/llvm-13/bin/../include/c++/v1/vector:1117:9 (bitcoind+0x501239)
    #3 std::__1::vector<CBlockIndex*, std::__1::allocator<CBlockIndex*> >::resize(unsigned long) /usr/lib/llvm-13/bin/../include/c++/v1/vector:2046:15 (bitcoind+0x4ffe29)
    #4 CChain::SetTip(CBlockIndex*) src/chain.cpp:19:12 (bitcoind+0x4ffe29)
    #5 CChainState::ConnectTip(BlockValidationState&, CBlockIndex*, std::__1::shared_ptr<CBlock const> const&, ConnectTrace&, DisconnectedBlockTransactions&) src/validation.cpp:2748:13 (bitcoind+0x475d00)
    #6 CChainState::ActivateBestChainStep(BlockValidationState&, CBlockIndex*, std::__1::shared_ptr<CBlock const> const&, bool&, ConnectTrace&) src/validation.cpp:2884:18 (bitcoind+0x47739e)
    #7 CChainState::ActivateBestChain(BlockValidationState&, std::__1::shared_ptr<CBlock const>) src/validation.cpp:3011:22 (bitcoind+0x477baf)
    #8 node::ThreadImport(ChainstateManager&, std::__1::vector<fs::path, std::__1::allocator<fs::path> >, ArgsManager const&) src/node/blockstorage.cpp:883:30 (bitcoind+0x23cd74)
    #9 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7::operator()() const src/init.cpp:1657:9 (bitcoind+0x15863e)
    #10 decltype(static_cast<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(fp)()) std::__1::__invoke<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (bitcoind+0x15863e)
    #11 void std::__1::__invoke_void_return_wrapper<void, true>::__call<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (bitcoind+0x15863e)
    #12 std::__1::__function::__alloc_func<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, std::__1::allocator<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (bitcoind+0x15863e)
    #13 std::__1::__function::__func<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, std::__1::allocator<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (bitcoind+0x15863e)
    #14 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (bitcoind+0x88891f)
    #15 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (bitcoind+0x88891f)
    #16 util::TraceThread(char const*, std::__1::function<void ()>) src/util/thread.cpp:18:9 (bitcoind+0x88891f)
    #17 decltype(static_cast<void (*>(fp)(static_cast<char const*>(fp0), static_cast<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>(fp0))) std::__1::__invoke<void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>(void (*&&)(char const*, std::__1::function<void ()>), char const*&&, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (bitcoind+0x157e6a)
    #18 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>&, std::__1::__tuple_indices<2ul, 3ul>) /usr/lib/llvm-13/bin/../include/c++/v1/thread:280:5 (bitcoind+0x157e6a)
    #19 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7> >(void*) /usr/lib/llvm-13/bin/../include/c++/v1/thread:291:5 (bitcoind+0x157e6a)
  Previous read of size 8 at 0x7b3c000008f0 by main thread:
    #0 std::__1::vector<CBlockIndex*, std::__1::allocator<CBlockIndex*> >::size() const /usr/lib/llvm-13/bin/../include/c++/v1/vector:680:61 (bitcoind+0x15179d)
    #1 CChain::Tip() const src/./chain.h:449:23 (bitcoind+0x15179d)
    #2 ChainstateManager::ActiveTip() const src/./validation.h:927:59 (bitcoind+0x15179d)
    #3 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*) src/init.cpp:1841:35 (bitcoind+0x15179d)
    #4 AppInit(node::NodeContext&, int, char**) src/bitcoind.cpp:231:43 (bitcoind+0x133fd2)
    #5 main src/bitcoind.cpp:275:13 (bitcoind+0x133fd2)
  Location is heap block of size 232 at 0x7b3c00000870 allocated by main thread:
    #0 operator new(unsigned long) <null> (bitcoind+0x132668)
    #1 ChainstateManager::InitializeChainstate(CTxMemPool*, std::__1::optional<uint256> const&) src/validation.cpp:4851:21 (bitcoind+0x48e26b)
    #2 node::LoadChainstate(bool, ChainstateManager&, CTxMemPool*, bool, Consensus::Params const&, bool, long, long, long, bool, bool, std::__1::function<bool ()>, std::__1::function<void ()>) src/node/chainstate.cpp:31:14 (bitcoind+0x24de07)
    #3 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*) src/init.cpp:1438:32 (bitcoind+0x14e994)
    #4 AppInit(node::NodeContext&, int, char**) src/bitcoind.cpp:231:43 (bitcoind+0x133fd2)
    #5 main src/bitcoind.cpp:275:13 (bitcoind+0x133fd2)
  Mutex M131626 (0x7b3c00000898) created at:
    #0 pthread_mutex_lock <null> (bitcoind+0xda898)
    #1 std::__1::mutex::lock() <null> (libc++.so.1+0x49f35)
    #2 node::ThreadImport(ChainstateManager&, std::__1::vector<fs::path, std::__1::allocator<fs::path> >, ArgsManager const&) src/node/blockstorage.cpp:883:30 (bitcoind+0x23cd74)
    #3 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7::operator()() const src/init.cpp:1657:9 (bitcoind+0x15863e)
    #4 decltype(static_cast<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(fp)()) std::__1::__invoke<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (bitcoind+0x15863e)
    #5 void std::__1::__invoke_void_return_wrapper<void, true>::__call<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&>(AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&) /usr/lib/llvm-13/bin/../include/c++/v1/__functional/invoke.h:61:9 (bitcoind+0x15863e)
    #6 std::__1::__function::__alloc_func<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, std::__1::allocator<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:171:16 (bitcoind+0x15863e)
    #7 std::__1::__function::__func<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, std::__1::allocator<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>, void ()>::operator()() /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:345:12 (bitcoind+0x15863e)
    #8 std::__1::__function::__value_func<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:498:16 (bitcoind+0x88891f)
    #9 std::__1::function<void ()>::operator()() const /usr/lib/llvm-13/bin/../include/c++/v1/__functional/function.h:1175:12 (bitcoind+0x88891f)
    #10 util::TraceThread(char const*, std::__1::function<void ()>) src/util/thread.cpp:18:9 (bitcoind+0x88891f)
    #11 decltype(static_cast<void (*>(fp)(static_cast<char const*>(fp0), static_cast<AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>(fp0))) std::__1::__invoke<void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>(void (*&&)(char const*, std::__1::function<void ()>), char const*&&, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&&) /usr/lib/llvm-13/bin/../include/c++/v1/type_traits:3918:1 (bitcoind+0x157e6a)
    #12 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, 2ul, 3ul>(std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7>&, std::__1::__tuple_indices<2ul, 3ul>) /usr/lib/llvm-13/bin/../include/c++/v1/thread:280:5 (bitcoind+0x157e6a)
    #13 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void (*)(char const*, std::__1::function<void ()>), char const*, AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7> >(void*) /usr/lib/llvm-13/bin/../include/c++/v1/thread:291:5 (bitcoind+0x157e6a)
  Mutex M151 (0x55aacb8ea030) created at:
    #0 pthread_mutex_init <null> (bitcoind+0xbed2f)
    #1 std::__1::recursive_mutex::recursive_mutex() <null> (libc++.so.1+0x49fb3)
    #2 __libc_start_main <null> (libc.so.6+0x29eba)
  Mutex M131553 (0x7b4c000042e0) created at:
    #0 pthread_mutex_init <null> (bitcoind+0xbed2f)
    #1 std::__1::recursive_mutex::recursive_mutex() <null> (libc++.so.1+0x49fb3)
    #2 std::__1::__unique_if<CTxMemPool>::__unique_single std::__1::make_unique<CTxMemPool, CBlockPolicyEstimator*, int const&>(CBlockPolicyEstimator*&&, int const&) /usr/lib/llvm-13/bin/../include/c++/v1/__memory/unique_ptr.h:728:32 (bitcoind+0x15c81d)
    #3 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*) src/init.cpp:1426:24 (bitcoind+0x14e7b4)
    #4 AppInit(node::NodeContext&, int, char**) src/bitcoind.cpp:231:43 (bitcoind+0x133fd2)
    #5 main src/bitcoind.cpp:275:13 (bitcoind+0x133fd2)
  Thread T22 'b-loadblk' (tid=32370, running) created by main thread at:
    #0 pthread_create <null> (bitcoind+0xbd5bd)
    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) /usr/lib/llvm-13/bin/../include/c++/v1/__threading_support:443:10 (bitcoind+0x155e06)
    #2 std::__1::thread::thread<void (*)(char const*, std::__1::function<void ()>), char const (&) [8], AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7, void>(void (*&&)(char const*, std::__1::function<void ()>), char const (&) [8], AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*)::$_7&&) /usr/lib/llvm-13/bin/../include/c++/v1/thread:307:16 (bitcoind+0x155e06)
    #3 AppInitMain(node::NodeContext&, interfaces::BlockAndHeaderTipInfo*) src/init.cpp:1656:29 (bitcoind+0x150164)
    #4 AppInit(node::NodeContext&, int, char**) src/bitcoind.cpp:231:43 (bitcoind+0x133fd2)
    #5 main src/bitcoind.cpp:275:13 (bitcoind+0x133fd2)
SUMMARY: ThreadSanitizer: data race /usr/lib/llvm-13/bin/../include/c++/v1/__utility/swap.h:39:7 in std::__1::enable_if<(is_move_constructible<CBlockIndex**>::value) && (is_move_assignable<CBlockIndex**>::value), void>::type std::__1::swap<CBlockIndex**>(CBlockIndex**&, CBlockIndex**&)
==================
```

From https://cirrus-ci.com/task/5612886578954240?logs=ci#L4868"
bitcoin/bitcoin,2022-05-03 23:15:14,bug,CI failure in feature_fee_estimation.py,"https://cirrus-ci.com/task/5178042396966912?logs=ci#L3381

```
Traceback (most recent call last):
File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 133, in main self.run_test()
File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 297, in run_test self.sanity_check_estimates_range()
File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 213, in sanity_check_estimates_range check_estimates(self.nodes[1], self.fees_per_kb)
File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 115, in check_estimates check_smart_estimates(node, fees_seen)
File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-x86_64-pc-linux-gnu/test/functional/feature_fee_estimation.py"", line 102, in check_smart_estimates raise AssertionError(
AssertionError: Estimated fee (0.00108829) larger than last fee (0.00096573) for lower number of confirms
```
"
bitcoin/bitcoin,2022-05-01 13:30:27,bug,"Node won't serve blocks when connect=0, listen=1 set due to IBD logic","I have a 2 machine setup on my local network that I use for testing and I've performed dozens of syncs without any issues until recently.

I have a node on a laptop that is configured not to make any outbound connections via connect=0 but does listen for connections via listen=1. This node, as a result, never downloads new blocks.

I then run a node on a desktop that it set to ONLY connect to the laptop node via connect=<local network IP> 

**Expected behavior**

This node then downloads all of the blocks it can from the laptop node and then stops. This has been the case for many months of testing I've performed.

**Actual behavior**

However, when trying to run a test recently, the desktop node wasn't syncing. After enabling net debugging I see that the laptop node is deciding not to serve any blocks because it considers itself to be in IBD. This is odd because it's unclear WHY it would know that there are more blocks available out on the network when it is explicitly told not to connect to any peers.

**To reproduce**

Hard to reproduce this far. Easy to fix by having the main node connect to other peers and sync to chain tip.

**System information**

Bitcoin Core 23.0 on Debian, official build

<!-- Any extra information that might be useful in the debugging process. -->
Example startup log from previous testing runs that went as expected. I find it interesting that it says ""progress=0.999996""

<pre>
2022-02-12T17:35:39Z Bitcoin Core version v22.0.0 (release build)
2022-02-12T17:35:39Z Assuming ancestors of block 00000000000000000008a89e854d57e5667df88f1cdef6fde2fbca1de5b639ad have valid signatures.
2022-02-12T17:35:39Z Setting nMinimumChainWork=00000000000000000000000000000000000000001fa4663bbbe19f82de910280
2022-02-12T17:35:39Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2022-02-12T17:35:39Z Using RdSeed as additional entropy source
2022-02-12T17:35:39Z Using RdRand as an additional entropy source
2022-02-12T17:35:39Z Default data directory /home/jameson/.bitcoin
2022-02-12T17:35:39Z Using data directory /media/jameson/Blockchain/bitcoin
2022-02-12T17:35:39Z Config file: /media/jameson/Blockchain/bitcoin/bitcoin.conf (not found, skipping)
2022-02-12T17:35:39Z Config file arg: blockfilterindex=""1""
2022-02-12T17:35:39Z Config file arg: connect=""0""
2022-02-12T17:35:39Z Config file arg: datadir=""/media/jameson/Blockchain/bitcoin""
2022-02-12T17:35:39Z Config file arg: dns=""0""
2022-02-12T17:35:39Z Config file arg: dnsseed=""0""
2022-02-12T17:35:39Z Config file arg: listen=""1""
2022-02-12T17:35:39Z Config file arg: rpcauth=****
2022-02-12T17:35:39Z Config file arg: rpcthreads=""12""
2022-02-12T17:35:39Z Config file arg: rpcworkqueue=""48""
2022-02-12T17:35:39Z Config file arg: server=""1""
2022-02-12T17:35:39Z Config file arg: txindex=""1""
2022-02-12T17:35:39Z Config file arg: wallet=""0""
2022-02-12T17:35:39Z Config file arg: zmqpubrawblock=""tcp://127.0.0.1:28332""
2022-02-12T17:35:39Z Config file arg: zmqpubrawtx=""tcp://127.0.0.1:28333""
2022-02-12T17:35:39Z Command-line arg: daemon=""""
2022-02-12T17:35:39Z Using at most 125 automatic connections (1024 file descriptors available)
2022-02-12T17:35:39Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2022-02-12T17:35:39Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2022-02-12T17:35:39Z Script verification uses 11 additional threads
2022-02-12T17:35:39Z scheduler thread start
2022-02-12T17:35:39Z HTTP: creating work queue of depth 48
2022-02-12T17:35:39Z Using random cookie authentication.
2022-02-12T17:35:39Z Generated RPC authentication cookie /media/jameson/Blockchain/bitcoin/.cookie
2022-02-12T17:35:39Z Using rpcauth authentication.
2022-02-12T17:35:39Z HTTP: starting 12 worker threads
2022-02-12T17:35:39Z Using wallet directory /media/jameson/Blockchain/bitcoin
2022-02-12T17:35:39Z init message: Verifying wallet(s)…
2022-02-12T17:35:39Z Warning: Skipping -wallet path that doesn't exist. Failed to load database path '/media/jameson/Blockchain/bitcoin/0'. Path does not exist.
2022-02-12T17:35:39Z init message: Loading banlist…
2022-02-12T17:35:39Z SetNetworkActive: true
2022-02-12T17:35:39Z Using /16 prefix for IP bucketing
2022-02-12T17:35:39Z Cache configuration:
2022-02-12T17:35:39Z * Using 2.0 MiB for block index database
2022-02-12T17:35:39Z * Using 56.0 MiB for transaction index database
2022-02-12T17:35:39Z * Using 49.0 MiB for basic block filter index database
2022-02-12T17:35:39Z * Using 8.0 MiB for chain state database
2022-02-12T17:35:39Z * Using 335.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
2022-02-12T17:35:39Z init message: Loading block index…
2022-02-12T17:35:39Z Switching active chainstate to Chainstate [ibd] @ height -1 (null)
2022-02-12T17:35:39Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/blocks/index
2022-02-12T17:35:39Z Opened LevelDB successfully
2022-02-12T17:35:39Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/blocks/index: 0000000000000000
2022-02-12T17:35:42Z LoadBlockIndexDB: last block file = 2921
2022-02-12T17:35:42Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=32, size=50279941, heights=722928...722965, time=2022-02-12...2022-02-12)
2022-02-12T17:35:42Z Checking all blk files are present...
2022-02-12T17:35:42Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/chainstate
2022-02-12T17:35:42Z Opened LevelDB successfully
2022-02-12T17:35:42Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/chainstate: 05e83c6994e2e4d1
2022-02-12T17:35:42Z Loaded best chain: hashBestChain=00000000000000000008c2190b3b7f475db5a19123e8e19da20af0629e3aeaa1 height=722965 date=2022-02-12T17:14:26Z progress=0.999996
2022-02-12T17:35:42Z init message: Verifying blocks…
2022-02-12T17:35:42Z Verifying last 6 blocks at level 3
2022-02-12T17:35:42Z [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
2022-02-12T17:35:42Z No coin database inconsistencies in last 6 blocks (9180 transactions)
2022-02-12T17:35:42Z  block index            3455ms
2022-02-12T17:35:42Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/indexes/txindex
2022-02-12T17:35:42Z Opened LevelDB successfully
2022-02-12T17:35:42Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/indexes/txindex: 0000000000000000
2022-02-12T17:35:42Z txindex thread start
2022-02-12T17:35:42Z txindex is enabled at height 722965
2022-02-12T17:35:42Z txindex thread exit
2022-02-12T17:35:42Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/indexes/blockfilter/basic/db
2022-02-12T17:35:42Z Opened LevelDB successfully
2022-02-12T17:35:42Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/indexes/blockfilter/basic/db: 0000000000000000
2022-02-12T17:35:42Z basic block filter index thread start
2022-02-12T17:35:42Z basic block filter index is enabled at height 722965
2022-02-12T17:35:42Z basic block filter index thread exit
2022-02-12T17:35:42Z block tree size = 722968
2022-02-12T17:35:42Z nBestHeight = 722965
2022-02-12T17:35:42Z loadblk thread start
2022-02-12T17:35:42Z Bound to 127.0.0.1:8334
2022-02-12T17:35:42Z Bound to [::]:8333
2022-02-12T17:35:42Z Bound to 0.0.0.0:8333
2022-02-12T17:35:42Z torcontrol thread start
2022-02-12T17:35:42Z init message: Loading P2P addresses…
2022-02-12T17:35:42Z Leaving InitialBlockDownload (latching to false)
2022-02-12T17:35:42Z Loaded 62684 addresses from peers.dat  122ms
2022-02-12T17:35:42Z init message: Starting network threads…
2022-02-12T17:35:42Z DNS seeding disabled
2022-02-12T17:35:42Z init message: Done loading
2022-02-12T17:35:42Z addcon thread start
2022-02-12T17:35:42Z net thread start
2022-02-12T17:35:42Z msghand thread start
2022-02-12T17:35:44Z Imported mempool transactions from disk: 781 succeeded, 0 failed, 0 expired, 0 already there, 0 waiting for initial broadcast
2022-02-12T17:35:44Z loadblk thread exit
</pre>

Here's the network debug log; note it says ""progress=0.982959""
Why did this change if the node was instructed to never connect to any other nodes outside of my LAN?
<pre>
2022-04-29T14:11:22Z Bitcoin Core version v23.0.0 (release build)
2022-04-29T14:11:22Z Assuming ancestors of block 000000000000000000052d314a259755ca65944e68df6b12a067ea8f1f5a7091 have valid signatures.
2022-04-29T14:11:22Z Setting nMinimumChainWork=00000000000000000000000000000000000000002927cdceccbd5209e81e80db
2022-04-29T14:11:22Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2022-04-29T14:11:22Z Using RdSeed as additional entropy source
2022-04-29T14:11:22Z Using RdRand as an additional entropy source
2022-04-29T14:11:22Z Default data directory /home/jameson/.bitcoin
2022-04-29T14:11:22Z Using data directory /media/jameson/Blockchain/bitcoin
2022-04-29T14:11:22Z Config file: /media/jameson/Blockchain/bitcoin/bitcoin.conf (not found, skipping)
2022-04-29T14:11:22Z Config file arg: blockfilterindex=""1""
2022-04-29T14:11:22Z Config file arg: connect=""0""
2022-04-29T14:11:22Z Config file arg: datadir=""/media/jameson/Blockchain/bitcoin""
2022-04-29T14:11:22Z Config file arg: debug=""net""
2022-04-29T14:11:22Z Config file arg: dns=""0""
2022-04-29T14:11:22Z Config file arg: dnsseed=""0""
2022-04-29T14:11:22Z Config file arg: listen=""1""
2022-04-29T14:11:22Z Config file arg: rpcauth=****
2022-04-29T14:11:22Z Config file arg: rpcthreads=""12""
2022-04-29T14:11:22Z Config file arg: rpcworkqueue=""48""
2022-04-29T14:11:22Z Config file arg: server=""1""
2022-04-29T14:11:22Z Config file arg: txindex=""1""
2022-04-29T14:11:22Z Config file arg: wallet=""0""
2022-04-29T14:11:22Z Config file arg: zmqpubrawblock=""tcp://127.0.0.1:28332""
2022-04-29T14:11:22Z Config file arg: zmqpubrawtx=""tcp://127.0.0.1:28333""
2022-04-29T14:11:22Z Command-line arg: daemon=""""
2022-04-29T14:11:22Z Using at most 125 automatic connections (1024 file descriptors available)
2022-04-29T14:11:22Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2022-04-29T14:11:22Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2022-04-29T14:11:22Z Script verification uses 11 additional threads
2022-04-29T14:11:22Z scheduler thread start
2022-04-29T14:11:22Z HTTP: creating work queue of depth 48
2022-04-29T14:11:22Z Using random cookie authentication.
2022-04-29T14:11:22Z Generated RPC authentication cookie /media/jameson/Blockchain/bitcoin/.cookie
2022-04-29T14:11:22Z Using rpcauth authentication.
2022-04-29T14:11:22Z HTTP: starting 12 worker threads
2022-04-29T14:11:22Z Using wallet directory /media/jameson/Blockchain/bitcoin
2022-04-29T14:11:22Z init message: Verifying wallet(s)…
2022-04-29T14:11:22Z Warning: Skipping -wallet path that doesn't exist. Failed to load database path '/media/jameson/Blockchain/bitcoin/0'. Path does not exist.
2022-04-29T14:11:22Z Using /16 prefix for IP bucketing
2022-04-29T14:11:22Z init message: Loading P2P addresses…
2022-04-29T14:11:22Z Loaded 0 addresses from peers.dat  0ms
2022-04-29T14:11:22Z init message: Loading banlist…
2022-04-29T14:11:22Z banlist.dat ignored because it can only be read by Bitcoin Core version 22.x. Remove ""/media/jameson/Blockchain/bitcoin/banlist.dat"" to silence this warning.
2022-04-29T14:11:22Z Loaded 0 banned node addresses/subnets  0ms
2022-04-29T14:11:22Z net: setting try another outbound peer=false
2022-04-29T14:11:22Z SetNetworkActive: true
2022-04-29T14:11:22Z Cache configuration:
2022-04-29T14:11:22Z * Using 2.0 MiB for block index database
2022-04-29T14:11:22Z * Using 56.0 MiB for transaction index database
2022-04-29T14:11:22Z * Using 49.0 MiB for basic block filter index database
2022-04-29T14:11:22Z * Using 8.0 MiB for chain state database
2022-04-29T14:11:22Z * Using 335.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
2022-04-29T14:11:22Z init message: Loading block index…
2022-04-29T14:11:22Z Switching active chainstate to Chainstate [ibd] @ height -1 (null)
2022-04-29T14:11:22Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/blocks/index
2022-04-29T14:11:22Z Opened LevelDB successfully
2022-04-29T14:11:22Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/blocks/index: 0000000000000000
2022-04-29T14:11:25Z LoadBlockIndexDB: last block file = 2954
2022-04-29T14:11:25Z LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=50, size=67330220, heights=726115...726749, time=2022-03-06...2022-03-10)
2022-04-29T14:11:25Z Checking all blk files are present...
2022-04-29T14:11:25Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/chainstate
2022-04-29T14:11:25Z Opened LevelDB successfully
2022-04-29T14:11:25Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/chainstate: 05e83c6994e2e4d1
2022-04-29T14:11:25Z Loaded best chain: hashBestChain=000000000000000000014ccfdf28b7b062aec08384456eba68e2070fc61ec811 height=726750 date=2022-03-10T20:26:32Z progress=0.982959
2022-04-29T14:11:25Z init message: Verifying blocks…
2022-04-29T14:11:25Z Verifying last 6 blocks at level 3
2022-04-29T14:11:25Z [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
2022-04-29T14:11:26Z No coin database inconsistencies in last 6 blocks (9585 transactions)
2022-04-29T14:11:26Z  block index            3744ms
2022-04-29T14:11:26Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/indexes/txindex
2022-04-29T14:11:26Z Opened LevelDB successfully
2022-04-29T14:11:26Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/indexes/txindex: 0000000000000000
2022-04-29T14:11:26Z txindex thread start
2022-04-29T14:11:26Z txindex is enabled at height 726750
2022-04-29T14:11:26Z txindex thread exit
2022-04-29T14:11:26Z Opening LevelDB in /media/jameson/Blockchain/bitcoin/indexes/blockfilter/basic/db
2022-04-29T14:11:26Z Opened LevelDB successfully
2022-04-29T14:11:26Z Using obfuscation key for /media/jameson/Blockchain/bitcoin/indexes/blockfilter/basic/db: 0000000000000000
2022-04-29T14:11:26Z block tree size = 726753
2022-04-29T14:11:26Z nBestHeight = 726750
2022-04-29T14:11:26Z basic block filter index thread start
2022-04-29T14:11:26Z basic block filter index is enabled at height 726750
2022-04-29T14:11:26Z basic block filter index thread exit
2022-04-29T14:11:26Z loadblk thread start
2022-04-29T14:11:26Z torcontrol thread start
2022-04-29T14:11:26Z Imported mempool transactions from disk: 0 succeeded, 0 failed, 0 expired, 0 already there, 0 waiting for initial broadcast
2022-04-29T14:11:26Z loadblk thread exit
2022-04-29T14:11:26Z Bound to 127.0.0.1:8334
2022-04-29T14:11:26Z Bound to [::]:8333
2022-04-29T14:11:26Z Bound to 0.0.0.0:8333
2022-04-29T14:11:26Z init message: Starting network threads…
2022-04-29T14:11:26Z DNS seeding disabled
2022-04-29T14:11:26Z init message: Done loading
2022-04-29T14:11:26Z net thread start
2022-04-29T14:11:26Z addcon thread start
2022-04-29T14:11:26Z msghand thread start
2022-04-29T14:11:52Z Added connection peer=0
2022-04-29T14:11:52Z connection from 192.168.2.42:53516 accepted
2022-04-29T14:11:52Z received: version (102 bytes) peer=0
2022-04-29T14:11:52Z sending version (102 bytes) peer=0
2022-04-29T14:11:52Z send version message: version 70016, blocks=726750, txrelay=1, peer=0
2022-04-29T14:11:52Z sending wtxidrelay (0 bytes) peer=0
2022-04-29T14:11:52Z sending sendaddrv2 (0 bytes) peer=0
2022-04-29T14:11:52Z sending verack (0 bytes) peer=0
2022-04-29T14:11:52Z receive version message: /Satoshi:0.21.2/: version 70016, blocks=0, us=[::]:0, txrelay=1, peer=0
2022-04-29T14:11:52Z received: wtxidrelay (0 bytes) peer=0
2022-04-29T14:11:52Z received: sendaddrv2 (0 bytes) peer=0
2022-04-29T14:11:52Z received: verack (0 bytes) peer=0
2022-04-29T14:11:52Z sending sendheaders (0 bytes) peer=0
2022-04-29T14:11:52Z sending sendcmpct (9 bytes) peer=0
2022-04-29T14:11:52Z sending sendcmpct (9 bytes) peer=0
2022-04-29T14:11:52Z sending ping (8 bytes) peer=0
2022-04-29T14:11:52Z initial getheaders (726749) to peer=0 (startheight:0)
2022-04-29T14:11:52Z sending getheaders (1029 bytes) peer=0
2022-04-29T14:11:52Z sending feefilter (8 bytes) peer=0
2022-04-29T14:11:52Z received: getaddr (0 bytes) peer=0
2022-04-29T14:11:52Z received: sendheaders (0 bytes) peer=0
2022-04-29T14:11:52Z received: sendcmpct (9 bytes) peer=0
2022-04-29T14:11:52Z received: sendcmpct (9 bytes) peer=0
2022-04-29T14:11:52Z received: ping (8 bytes) peer=0
2022-04-29T14:11:52Z sending pong (8 bytes) peer=0
2022-04-29T14:11:52Z received: getheaders (69 bytes) peer=0
2022-04-29T14:11:52Z Ignoring getheaders from peer=0 because node is in initial block download
2022-04-29T14:11:52Z received: feefilter (8 bytes) peer=0
2022-04-29T14:11:52Z received: feefilter of 0.09170997 BTC/kvB from peer=0
2022-04-29T14:11:52Z received: pong (8 bytes) peer=0
2022-04-29T14:12:48Z socket closed for peer=0
2022-04-29T14:12:48Z disconnecting peer=0
2022-04-29T14:12:48Z Cleared nodestate for peer=0
</pre>"
bitcoin/bitcoin,2022-04-29 15:30:33,bug,Intermittent win64 CI failure in feature_index_prune.py,"https://cirrus-ci.com/task/6532449864777728

Win64 native  [msvc] task

```
 test  2022-04-29T14:22:15.843000Z TestFramework (WARNING): Not cleaning up dir C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\test_runner_₿_🏃_20220429_140215\\feature_index_prune_242 

 test  2022-04-29T14:22:15.843000Z TestFramework (ERROR): Test failed. Test logging available at C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\test_runner_₿_🏃_20220429_140215\\feature_index_prune_242/test_framework.log 

 test  2022-04-29T14:22:15.843000Z TestFramework (ERROR): 
 node0 stderr Error: basic block filter index best block of the index goes beyond pruned data. Please disable the index or reindex (which will download the whole blockchain again) 
 node1 stderr Error: coinstatsindex best block of the index goes beyond pruned data. Please disable the index or reindex (which will download the whole blockchain again) 
 node2 stderr Error: basic block filter index best block of the index goes beyond pruned data. Please disable the index or reindex (which will download the whole blockchain again) 
"
bitcoin/bitcoin,2022-04-27 15:36:53,bug,wallet_createwallet.py --legacy-wallet fails on a system with only bdb installed,"
```
Traceback (most recent call last):
  File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/test_framework.py"", line 133, in main
    self.run_test()
  File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/wallet_createwallet.py"", line 31, in run_test
    assert_raises_rpc_error(-4, ""Passphrase provided but private keys are disabled. A passphrase is only used to encrypt private keys, so cannot be used for wallets with private keys disabled."",
  File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/util.py"", line 125, in assert_raises_rpc_error
    assert try_rpc(code, message, fun, *args, **kwds), ""No exception raised""
  File ""/tmp/cirrus-ci-build/bitcoin-core/test/functional/test_framework/util.py"", line 140, in try_rpc
    raise AssertionError(
AssertionError: Expected substring not found in error message:
substring: 'Passphrase provided but private keys are disabled. A passphrase is only used to encrypt private keys, so cannot be used for wallets with private keys disabled.'
error message: 'Compiled without sqlite support (required for descriptor wallets)'."
bitcoin/bitcoin,2022-04-25 18:47:54,bug,"`-torcontrol` asks tor to connect to port 0, and tor refuses","<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->

I'm using the `-torcontrol` and `-torpassword` directives to let bitcoind set up a tor onion service.

<!-- Describe the issue -->

**Expected behavior**

Bitcoind sets up an onion service and incoming connections will be received via tor.

**Actual behavior**

In the tor log, I see:

> Application asked to connect to port 0. Refusing.

Bitcoind doesn't receive any incoming connections via tor.

<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**System information**

Bitcoin Core 23.0 on signet or testnet chains, self-compiled Alpine based image, running in Docker 20.10.12 on Ubuntu 22.04 LTS.

tor 0.4.6.10, self-compiled image, running in Docker 20.10.12 on Ubuntu 22.04 LTS.

Both are composed via docker-compose 1.29.2.
<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->

tor.log:

```
tor_1               | Apr 25 18:22:13.607 [notice] Tor can't help you if you use it wrong! Learn how to be safe at https://support.torproject.org/faq/staying-anonymous/
tor_1               | Apr 25 18:22:13.607 [notice] Read configuration file ""/etc/tor/torrc"".
tor_1               | Apr 25 18:22:13.609 [warn] You have a ControlPort set to accept connections from a non-local address.  This means that programs not running on your computer can reconfigure your Tor.  That's pretty bad, since the controller protocol isn't encrypted!  Maybe you should just listen on 127.0.0.1 and use a tool like stunnel or ssh to encrypt remote connections to your control port.
tor_1               | Apr 25 18:22:13.631 [warn] You specified a public address '0.0.0.0:9050' for SocksPort. Other people on the Internet might find your computer and use it as an open proxy. Please don't allow this unless you have a good reason.
tor_1               | Apr 25 18:22:13.631 [warn] You have a ControlPort set to accept connections from a non-local address.  This means that programs not running on your computer can reconfigure your Tor.  That's pretty bad, since the controller protocol isn't encrypted!  Maybe you should just listen on 127.0.0.1 and use a tool like stunnel or ssh to encrypt remote connections to your control port.
tor_1               | Apr 25 18:22:13.631 [notice] Opening Socks listener on 0.0.0.0:9050
tor_1               | Apr 25 18:22:13.631 [notice] Opened Socks listener connection (ready) on 0.0.0.0:9050
tor_1               | Apr 25 18:22:13.631 [notice] Opening Control listener on 0.0.0.0:9051
tor_1               | Apr 25 18:22:13.631 [notice] Opened Control listener connection (ready) on 0.0.0.0:9051
tor_1               | Apr 25 18:22:13.000 [notice] Parsing GEOIP IPv4 file /usr/local/share/tor/geoip.
tor_1               | Apr 25 18:22:13.000 [notice] Parsing GEOIP IPv6 file /usr/local/share/tor/geoip6.
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 0% (starting): Starting
tor_1               | Apr 25 18:22:14.000 [notice] Starting with guard context ""default""
tor_1               | Apr 25 18:22:14.000 [notice] New control connection opened from 192.168.32.4.
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 5% (conn): Connecting to a relay
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 10% (conn_done): Connected to a relay
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 14% (handshake): Handshaking with a relay
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 15% (handshake_done): Handshake with a relay done
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 20% (onehop_create): Establishing an encrypted directory connection
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 25% (requesting_status): Asking for networkstatus consensus
tor_1               | Apr 25 18:22:14.000 [notice] Bootstrapped 30% (loading_status): Loading networkstatus consensus
tor_1               | Apr 25 18:22:15.000 [notice] I learned some more directory information, but not enough to build a circuit: We have no usable consensus.
tor_1               | Apr 25 18:22:15.000 [notice] Bootstrapped 40% (loading_keys): Loading authority key certs
tor_1               | Apr 25 18:22:15.000 [notice] The current consensus has no exit nodes. Tor can only build internal paths, such as paths to onion services.
tor_1               | Apr 25 18:22:15.000 [notice] Bootstrapped 45% (requesting_descriptors): Asking for relay descriptors
tor_1               | Apr 25 18:22:15.000 [notice] I learned some more directory information, but not enough to build a circuit: We need more microdescriptors: we have 0/6906, and can only build 0% of likely paths. (We have 0% of guards bw, 0% of midpoint bw, and 0% of end bw (no exits in consensus, using mid) = 0% of path bw.)
tor_1               | Apr 25 18:22:16.000 [notice] Bootstrapped 50% (loading_descriptors): Loading relay descriptors
tor_1               | Apr 25 18:22:17.000 [notice] The current consensus contains exit nodes. Tor can build exit and internal paths.
tor_1               | Apr 25 18:22:18.000 [notice] Bootstrapped 55% (loading_descriptors): Loading relay descriptors
tor_1               | Apr 25 18:22:19.000 [notice] Bootstrapped 61% (loading_descriptors): Loading relay descriptors
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 69% (loading_descriptors): Loading relay descriptors
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 75% (enough_dirinfo): Loaded enough directory info to build circuits
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 80% (ap_conn): Connecting to a relay to build circuits
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 85% (ap_conn_done): Connected to a relay to build circuits
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 89% (ap_handshake): Finishing handshake with a relay to build circuits
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 90% (ap_handshake_done): Handshake finished with a relay to build circuits
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 95% (circuit_create): Establishing a Tor circuit
tor_1               | Apr 25 18:22:21.000 [notice] Bootstrapped 100% (done): Done
tor_1               | Apr 25 18:23:17.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:24:10.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:24:25.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:25:31.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:26:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:26:22.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:26:42.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:28:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:30:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:32:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:34:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:34:36.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:35:25.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
tor_1               | Apr 25 18:36:11.000 [notice] Application asked to connect to port 0. Refusing.
tor_1               | Apr 25 18:36:30.000 [notice] Closed 1 streams for service [scrubbed].onion for reason resolve failed. Fetch status: No more HSDir available to query.
```

As you can see, there is an incoming control connection: `New control connection opened from 192.168.32.4.` (that's my bitcoind container)

Then: `Application asked to connect to port 0. Refusing.`

bitcoind.log, just in case:

```
bitcoind_1          | /entrypoint.sh: assuming arguments for bitcoind
bitcoind_1          | /entrypoint.sh: setting data directory to /home/bitcoin/.bitcoin
bitcoind_1          | 
bitcoind_1          | Bitcoin Core version v23.0.0 (release build)
bitcoind_1          | Signet derived magic (message start): 0a03cf40
bitcoind_1          | Assuming ancestors of block 00000112852484b5fe3451572368f93cfd2723279af3464e478aee35115256ef have valid signatures.
bitcoind_1          | Setting nMinimumChainWork=000000000000000000000000000000000000000000000000000000de26b0e471
bitcoind_1          | Using the 'x86_shani(1way,2way)' SHA256 implementation
bitcoind_1          | Using RdSeed as additional entropy source
bitcoind_1          | Using RdRand as an additional entropy source
bitcoind_1          | Startup time: 2022-04-25T18:22:12Z
bitcoind_1          | Default data directory /home/bitcoin/.bitcoin
bitcoind_1          | Using data directory /home/bitcoin/.bitcoin/signet
bitcoind_1          | Config file: /home/bitcoin/.bitcoin/bitcoin.conf (not found, skipping)
bitcoind_1          | Command-line arg: blockfilterindex=""basic""
bitcoind_1          | Command-line arg: datadir=""/home/bitcoin/.bitcoin""
bitcoind_1          | Command-line arg: disablewallet=""1""
bitcoind_1          | Command-line arg: discover=""1""
bitcoind_1          | Command-line arg: dnsseed=""0""
bitcoind_1          | Command-line arg: i2pacceptincoming=""1""
bitcoind_1          | Command-line arg: i2psam=""i2pd:7656""
bitcoind_1          | Command-line arg: listen=""1""
bitcoind_1          | Command-line arg: logtimestamps=""0""
bitcoind_1          | Command-line arg: maxconnections=""32""
bitcoind_1          | Command-line arg: onion=""tor:9050""
bitcoind_1          | Command-line arg: onlynet=""i2p""
bitcoind_1          | Command-line arg: onlynet=""onion""
bitcoind_1          | Command-line arg: par=""2""
bitcoind_1          | Command-line arg: peerblockfilters=""1""
bitcoind_1          | Command-line arg: peerbloomfilters=""1""
bitcoind_1          | Command-line arg: rpcallowip=""0.0.0.0/0""
bitcoind_1          | Command-line arg: rpcauth=****
bitcoind_1          | Command-line arg: rpcbind=****
bitcoind_1          | Command-line arg: rpcthreads=""2""
bitcoind_1          | Command-line arg: signet=""1""
bitcoind_1          | Command-line arg: torcontrol=""tor:9051""
bitcoind_1          | Command-line arg: torpassword=****
bitcoind_1          | Command-line arg: txindex=""1""
bitcoind_1          | Using at most 32 automatic connections (1048576 file descriptors available)
bitcoind_1          | Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
bitcoind_1          | Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
bitcoind_1          | Script verification uses 1 additional threads
bitcoind_1          | Wallet disabled!
bitcoind_1          | scheduler thread start
bitcoind_1          | WARNING: the RPC server is not safe to expose to untrusted networks such as the public internet
bitcoind_1          | HTTP: creating work queue of depth 16
bitcoind_1          | Using random cookie authentication.
bitcoind_1          | Generated RPC authentication cookie /home/bitcoin/.bitcoin/signet/.cookie
bitcoind_1          | Using rpcauth authentication.
bitcoind_1          | HTTP: starting 2 worker threads
bitcoind_1          | Using /16 prefix for IP bucketing
bitcoind_1          | init message: Loading P2P addresses…
bitcoind_1          | Loaded 59 addresses from peers.dat  1ms
bitcoind_1          | init message: Loading banlist…
bitcoind_1          | SetNetworkActive: true
bitcoind_1          | Cache configuration:
bitcoind_1          | * Using 2.0 MiB for block index database
bitcoind_1          | * Using 56.0 MiB for transaction index database
bitcoind_1          | * Using 49.0 MiB for basic block filter index database
bitcoind_1          | * Using 8.0 MiB for chain state database
bitcoind_1          | * Using 335.0 MiB for in-memory UTXO set (plus up to 286.1 MiB of unused mempool space)
bitcoind_1          | init message: Loading block index…
bitcoind_1          | Switching active chainstate to Chainstate [ibd] @ height -1 (null)
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/signet/blocks/index
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/signet/blocks/index: 0000000000000000
bitcoind_1          | LoadBlockIndexDB: last block file = 3
bitcoind_1          | LoadBlockIndexDB: last block file info: CBlockFileInfo(blocks=2325, size=13230151, heights=85238...87562, time=2022-04-10...2022-04-25)
bitcoind_1          | Checking all blk files are present...
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/signet/chainstate
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/signet/chainstate: 5f5400e5be1d375a
bitcoind_1          | Loaded best chain: hashBestChain=00000158896b9a56d815300f485b244d183936eb787c88eab0ba1cf3e2d8fcaf height=87562 date=2022-04-25T18:01:20Z progress=0.999890
bitcoind_1          | init message: Verifying blocks…
bitcoind_1          | Verifying last 6 blocks at level 3
bitcoind_1          | [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
bitcoind_1          | No coin database inconsistencies in last 6 blocks (139 transactions)
bitcoind_1          |  block index            1038ms
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/signet/indexes/txindex
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/signet/indexes/txindex: 0000000000000000
bitcoind_1          | txindex thread start
bitcoind_1          | Opening LevelDB in /home/bitcoin/.bitcoin/signet/indexes/blockfilter/basic/db
bitcoind_1          | txindex is enabled at height 87562
bitcoind_1          | txindex thread exit
bitcoind_1          | Opened LevelDB successfully
bitcoind_1          | Using obfuscation key for /home/bitcoin/.bitcoin/signet/indexes/blockfilter/basic/db: 0000000000000000
bitcoind_1          | basic block filter index thread start
bitcoind_1          | basic block filter index is enabled at height 87562
bitcoind_1          | basic block filter index thread exit
bitcoind_1          | loadblk thread start
bitcoind_1          | block tree size = 87568
bitcoind_1          | nBestHeight = 87562
bitcoind_1          | torcontrol thread start
bitcoind_1          | Leaving InitialBlockDownload (latching to false)
bitcoind_1          | Bound to 127.0.0.1:38334
bitcoind_1          | Bound to [::]:38333
bitcoind_1          | Bound to 0.0.0.0:38333
bitcoind_1          | Loaded 0 addresses from ""anchors.dat""
bitcoind_1          | 0 block-relay-only anchors will be tried for connections.
bitcoind_1          | init message: Starting network threads…
bitcoind_1          | DNS seeding disabled
bitcoind_1          | net thread start
bitcoind_1          | msghand thread start
bitcoind_1          | i2paccept thread start
bitcoind_1          | opencon thread start
bitcoind_1          | addcon thread start
bitcoind_1          | init message: Done loading
bitcoind_1          | Imported mempool transactions from disk: 47 succeeded, 0 failed, 0 expired, 0 already there, 0 waiting for initial broadcast
bitcoind_1          | loadblk thread exit
bitcoind_1          | tor: Got service ID xxx, advertising service xxx.onion:38333
bitcoind_1          | AddLocal(xxx.onion:38333,4)
bitcoind_1          | I2P: SAM session created: session id=eeb28ebf5c, my address=xxx.b32.i2p:0
bitcoind_1          | AddLocal(xxx.b32.i2p:0,4)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=1 (outbound-full-relay)
bitcoind_1          | UpdateTip: new best=000000dd9afeb8d7a6e63dbf99aac137e38dc73d9acd6f58de0a0366c388681d height=87563 version=0x20000000 log2_work=39.945971 tx=1584892 date='2022-04-25T18:22:58Z' progress=0.999999 cache=0.0MiB(115txo)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=2 (outbound-full-relay)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=3 (outbound-full-relay)
bitcoind_1          | Socks5() connect to m42cdyruu4g4mcrbku4q5faklcr6wa3yq2syu4f6ga634nc2uc5w5vad.onion:38333 failed: host unreachable
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=4 (outbound-full-relay)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=5 (outbound-full-relay)
bitcoind_1          | Socks5() connect to cr423k6sqllxbvtd4nvriy5dvqijly2eaxewjjvxxqjzjba2p2ggqdid.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to qscbjfqbglaoge2juooutgliw7uipjoyrs2puy5key23vcpl3j3eirqd.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to qb77j7stj2sq3ags3nn4gmfs3ts6k7xwyc5sxows7k5ugn36a3cljyyd.onion:38333 failed: host unreachable
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=7 (outbound-full-relay)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=8 (outbound-full-relay)
bitcoind_1          | Socks5() connect to d27tvdmi6wjjlffjyhm5mogwsw3433jxkaiwhti32eagvljdzseilqqd.onion:38333 failed: host unreachable
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=9 (outbound-full-relay)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=10 (block-relay-only)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87563, peer=11 (block-relay-only)
bitcoind_1          | Socks5() connect to okdqiz5nlhsoofaorxgjbrmqnywkxvlr3eswytjdtyx5ktljnfwc3lid.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to tm7eh4voltdm25wexfsyh6hzvdvmhgjjj4jbdr7lzaihb5ejivkcakqd.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to 3ysg6mt5toaa4pfg2onva2pmoyo7swxkziv6i3bjuejp7qg5etz6m5id.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to m42cdyruu4g4mcrbku4q5faklcr6wa3yq2syu4f6ga634nc2uc5w5vad.onion:38333 failed: host unreachable
bitcoind_1          | UpdateTip: new best=0000015583b391d8b8017d50356ad231fd60d97d5b6974a1a4a42bcdb386d6bc height=87564 version=0x20000000 log2_work=39.945988 tx=1584908 date='2022-04-25T18:37:20Z' progress=1.000000 cache=0.0MiB(131txo)
bitcoind_1          | New outbound peer connected: version: 70016, blocks=87564, peer=12 (block-relay-only)
bitcoind_1          | Socks5() connect to cr423k6sqllxbvtd4nvriy5dvqijly2eaxewjjvxxqjzjba2p2ggqdid.onion:38333 failed: host unreachable
bitcoind_1          | Socks5() connect to okdqiz5nlhsoofaorxgjbrmqnywkxvlr3eswytjdtyx5ktljnfwc3lid.onion:38333 failed: host unreachable
```"
bitcoin/bitcoin,2022-04-22 19:07:10,bug,Permissions corrupted in gen-sdk macosx headers,"I think #24534 broke the MacOS SDK:
```
guest@gnu /tmp/Xcode-12.2-12B45b-extracted-SDK-with-libcxx-headers$ ls -al
total 32
drwxr-xr-x 4 guest users 4096 Apr 22 18:58 ./
drwxrwxrwt 4 root  root  4096 Apr 22 18:58 ../
---x---r-- 1 guest users  127 Jan  1  1970 Entitlements.plist
---x---r-- 1 guest users 4307 Jan  1  1970 SDKSettings.json
---x---r-- 1 guest users 3597 Jan  1  1970 SDKSettings.plist
drwxr-xr-x 4 guest users 4096 Jan  1  1970 System/
drwxr-xr-x 7 guest users 4096 Jan  1  1970 usr/
```
Causing the guix build to fail with:
```
checking for x86_64-apple-darwin-gcc... env -u C_INCLUDE_PATH -u CPLUS_INCLUDE_PATH -u OBJC_INCLUDE_PATH -u OBJCPLUS_INCLUDE_PATH -u CPATH -u LIBRARY_PATH /home/kvaciral/.guix-profile/bin/clang --target=x86_64-apple-darwin -mmacosx-version-min=10.15 -B/bitcoin/depends/x86_64-apple-darwin/native/bin -mlinker-version=609 -isysroot/bitcoin/depends/SDKs/Xcode-12.2-12B45b-extracted-SDK-with-libcxx-headers -Xclang -internal-externc-isystem/gnu/store/v770rvqs8q21mbzwb3gkihr2glgn80am-clang-10.0.1/lib/clang/10.0.1/include -Xclang -internal-externc-isystem/bitcoin/depends/SDKs/Xcode-12.2-12B45b-extracted-SDK-with-libcxx-headers/usr/include
checking whether the C compiler works... no
configure: error: in `/bitcoin/depends/work/build/x86_64-apple-darwin/libevent/2.1.12-stable-6395cab7fb6':
configure: error: C compiler cannot create executables
See `config.log' for more details
make: *** [[funcs.mk:283](http://funcs.mk:283/): /bitcoin/depends/work/build/x86_64-apple-darwin/libevent/2.1.12-stable-6395cab7fb6/./.stamp_configured] Error 77
make: Leaving directory '/bitcoin/depends'
kvaciral@penumbra:~/projects/bitcoin$
```

Reported by @kvaciral."
bitcoin/bitcoin,2022-04-11 18:59:21,bug,Intermittent failure in feature_fee_estimation.py,"Observed in #24827 (https://cirrus-ci.com/task/6466255107391488) but very likely unrelated.
Looks to me that a testmempoolaccept RPC call issued by the mini-wallet timed out - so it seems that it occurs after the #24817 rework but has a root cause in bitcoind?

```
 node1 2022-04-11T17:04:58.456117Z [httpworker.3] [rpc/request.cpp:179] [parse] ThreadRPCServer method=testmempoolaccept user=__cookie__ 
 node1 2022-04-11T17:05:25.635870Z [msghand] [net.cpp:3041] [PushMessage] sending ping (8 bytes) peer=0 
 node1 2022-04-11T17:06:50.669591Z [scheduler] [random.cpp:520] [SeedPeriodic] Feeding 32422 bytes of dynamic environment data into RNG 
 node0 2022-04-11T17:06:50.669699Z [net] [net.cpp:1651] [SocketHandlerConnected] socket closed for peer=1 
 node0 2022-04-11T17:06:50.671075Z [net] [net.cpp:567] [CloseSocketDisconnect] disconnecting peer=1 
```"
bitcoin/bitcoin,2022-04-08 18:34:41,bug,Crash when creating wallet via bitcoin-cli,"**Description**
I expect to be able to create and load a wallet with Bitcoin CLI using the command ""bitcoin-cli createwallet mywallet.dat"" but it fails and Bitcoin Core crashes. The wallet appears to be partially created and the folder does exist.

I also have that wallet listed as one to load via the config. The crash then causes bitcoin to restart via the systemd service and when it restarts it exits with an error indicating the wallet cannot be read.

**Expected Behavior**
I expected the wallet to be created. This works fine on Bitcoin Core 21.2, but fails on 22.0.

**Actual Behavior**
Bitcoin crashes and a corrupt wallet is made. This causes Bitcoin to reboot, attempt to read the new wallet, and exit again leading to a cyclic restart.

**Reliability**
I can reliably re-create it, but only on one device. On another very similar device, the issue is not present.

**Version**
v22.0 from the website.

**Device Info**
Debian, amd64.

Linux mynode 4.19.0-20-amd64 #1 SMP Debian 4.19.235-1 (2022-03-17) x86_64 GNU/Linux

**GUI**
Not GUI related.

**CLI output when creating wallet**
```
admin@mynode:~$ bitcoin-cli createwallet break2
error: timeout on transient error: Could not connect to the server 127.0.0.1:8332 (error code 1 - ""EOF reached"")

Make sure the bitcoind server is running and that you are connecting to the correct RPC port.
admin@mynode:~$ 
```

**Log when creating wallet** (read bottom to top)
```
...
2022-04-08T18:28:12Z Bitcoin Core version v22.0.0 (release build)

<crash>

2022-04-08T18:27:41Z BerkeleyEnvironment::Open: LogDir=/home/bitcoin/.bitcoin/break2/database ErrorFile=/home/bitcoin/.bitcoin/break2/db.log
2022-04-08T18:27:41Z Using wallet /home/bitcoin/.bitcoin/break2/wallet.dat
2022-04-08T18:27:41Z Using BerkeleyDB version Berkeley DB 4.8.30: (April  9, 2010)
2022-04-08T18:27:41Z UpdateTip: new best=00000000000000000011ce88f1e73e85f7463f002b093b635e6fbdfb937937c5 height=552252 version=0x20000000 log2_work=90.084721 tx=361369223 date='2018-12-02T09:47:52Z' progress=0.507781 cache=622.6MiB(4620436txo)
2022-04-08T18:27:41Z UpdateTip: new best=00000000000000000026cd6e6d1ffad655908877051cc7b7b778d7236ae64e14 height=552251 version=0x20c00000 log2_work=90.084690 tx=361366404 date='2018-12-02T09:25:26Z' progress=0.507777 cache=622.4MiB(4618903txo)
```

"
bitcoin/bitcoin,2022-04-03 02:08:25,bug,Build breaks with ./configure --enable-debug on macosx (apple M1) with clang 13,"This is only broken when I enable the debug flag. 

$ gcc --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 13.0.0 (clang-1300.0.29.30)
Target: arm64-apple-darwin21.3.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

Error reported below:

In file included from util/system.cpp:14:
In file included from /opt/homebrew//include/boost/process.hpp:24:
In file included from /opt/homebrew//include/boost/process/async_system.hpp:22:
In file included from /opt/homebrew//include/boost/process/child.hpp:22:
In file included from /opt/homebrew//include/boost/process/detail/execute_impl.hpp:24:
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:156:36: error: non-constant-expression cannot be narrowed from type 'unsigned long' to 'int' in initializer list [-Wc++11-narrowing]
        int data[2] = {ec.value(), len + 1};
                                   ^~~~~~~
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:175:13: note: in instantiation of member function 'boost::process::detail::posix::executor<boost::fusion::joint_view<boost::fusion::tuple<boost::process::detail::posix::exe_cmd_init<char>>, boost::fusion::filter_view<const boost::fusion::tuple<const std::string &, boost::process::detail::posix::pipe_out<1, -1> &, boost::process::detail::posix::pipe_out<2, -1> &, boost::process::detail::posix::pipe_in &>, boost::process::detail::is_initializer<mpl_::arg<-1>>>>>::write_error' requested here
            write_error(ec, msg);
            ^
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:324:9: note: in instantiation of member function 'boost::process::detail::posix::executor<boost::fusion::joint_view<boost::fusion::tuple<boost::process::detail::posix::exe_cmd_init<char>>, boost::fusion::filter_view<const boost::fusion::tuple<const std::string &, boost::process::detail::posix::pipe_out<1, -1> &, boost::process::detail::posix::pipe_out<2, -1> &, boost::process::detail::posix::pipe_in &>, boost::process::detail::is_initializer<mpl_::arg<-1>>>>>::internal_error_handle' requested here
        internal_error_handle(ec, msg, has_error_handler(), has_ignore_error(), shall_use_vfork());
        ^
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:382:13: note: in instantiation of member function 'boost::process::detail::posix::executor<boost::fusion::joint_view<boost::fusion::tuple<boost::process::detail::posix::exe_cmd_init<char>>, boost::fusion::filter_view<const boost::fusion::tuple<const std::string &, boost::process::detail::posix::pipe_out<1, -1> &, boost::process::detail::posix::pipe_out<2, -1> &, boost::process::detail::posix::pipe_in &>, boost::process::detail::is_initializer<mpl_::arg<-1>>>>>::set_error' requested here
            set_error(::boost::process::detail::get_last_error(), ""pipe(2) failed"");
            ^
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:308:16: note: in instantiation of member function 'boost::process::detail::posix::executor<boost::fusion::joint_view<boost::fusion::tuple<boost::process::detail::posix::exe_cmd_init<char>>, boost::fusion::filter_view<const boost::fusion::tuple<const std::string &, boost::process::detail::posix::pipe_out<1, -1> &, boost::process::detail::posix::pipe_out<2, -1> &, boost::process::detail::posix::pipe_in &>, boost::process::detail::is_initializer<mpl_::arg<-1>>>>>::invoke' requested here
        return invoke(has_ignore_error(), shall_use_vfork());
               ^
/opt/homebrew//include/boost/process/detail/execute_impl.hpp:267:12: note: in instantiation of member function 'boost::process::detail::posix::executor<boost::fusion::joint_view<boost::fusion::tuple<boost::process::detail::posix::exe_cmd_init<char>>, boost::fusion::filter_view<const boost::fusion::tuple<const std::string &, boost::process::detail::posix::pipe_out<1, -1> &, boost::process::detail::posix::pipe_out<2, -1> &, boost::process::detail::posix::pipe_in &>, boost::process::detail::is_initializer<mpl_::arg<-1>>>>>::operator()' requested here
    return exec();
           ^
/opt/homebrew//include/boost/process/detail/execute_impl.hpp:275:12: note: in instantiation of function template specialization 'boost::process::detail::basic_execute_impl<char, const std::string &, boost::process::detail::posix::pipe_out<1, -1>, boost::process::detail::posix::pipe_out<2, -1>, boost::process::detail::posix::pipe_in>' requested here
    return basic_execute_impl<req_char_type>(
           ^
/opt/homebrew//include/boost/process/child.hpp:35:39: note: in instantiation of function template specialization 'boost::process::detail::execute_impl<const std::string &, boost::process::detail::posix::pipe_out<1, -1>, boost::process::detail::posix::pipe_out<2, -1>, boost::process::detail::posix::pipe_in>' requested here
    : child(::boost::process::detail::execute_impl(std::forward<Args>(args)...)) {}
                                      ^
util/system.cpp:1263:15: note: in instantiation of function template specialization 'boost::process::child::child<const std::string &, boost::process::detail::posix::pipe_out<1, -1>, boost::process::detail::posix::pipe_out<2, -1>, boost::process::detail::posix::pipe_in>' requested here
    bp::child c(
              ^
/opt/homebrew//include/boost/process/detail/posix/executor.hpp:156:36: note: insert an explicit cast to silence this issue
        int data[2] = {ec.value(), len + 1};
                                   ^~~~~~~
                                   static_cast<int>( )
22 warnings and 2 errors generated.
make[2]: *** [util/libbitcoin_util_a-system.o] Error 1
make[1]: *** [all-recursive] Error 1
make: *** [all-recursive] Error 1
"
bitcoin/bitcoin,2022-03-31 18:18:33,bug,Cross-compiled `bitcoind -signet` silently fails on Windows,"The issue was [discovered](https://github.com/bitcoin/bitcoin/issues/24501#issuecomment-1079738588) during 23.0rc2 testing.

MSVC build works flawlessly.

`bitcoind -testnet` also works flawlessly."
bitcoin/bitcoin,2022-03-23 06:49:45,bug,Build fails on Ubuntu 22.04 with GCC 8.5,"Building master (f05cf59d91eb03857dd9bdcc77607764da0349d2) on Ubuntu 22.04 with GCC 8.5:
```
$ ./autogen.sh
$ ./configure --with-incompatible-bdb CC=gcc-8 CXX=g++-8
$ make clean
$ make
...
  CXX      libbitcoin_node_a-validation.o
during RTL pass: reload
validation.cpp: In member function ‘bool CChainState::FlushStateToDisk(BlockValidationState&, FlushStateMode, int)’:
validation.cpp:2386:1: internal compiler error: Max. number of generated reload insns per insn is achieved (90)

 }
 ^
unrecognized DWARF version in .debug_info at 6
unrecognized DWARF version in .debug_info at 6
unrecognized DWARF version in .debug_info at 6
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-8/README.Bugs> for instructions.
make[2]: *** [Makefile:10721: libbitcoin_node_a-validation.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[2]: Leaving directory '/home/hebasto/GitHub/bitcoin/src'
make[1]: *** [Makefile:19149: all-recursive] Error 1
make[1]: Leaving directory '/home/hebasto/GitHub/bitcoin/src'
make: *** [Makefile:816: all-recursive] Error 1
```

Interesting that the upstream [bug](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=91105) has been fixed and [backported](https://changelogs.ubuntu.com/changelogs/pool/universe/g/gcc-8/gcc-8_8.5.0-0ubuntu4/changelog):
```
gcc-8 (8.3.0-25) unstable; urgency=medium

  * Update to SVN 20191126 (r278718) from the gcc-8-branch.
    - Fix PR c/91401, PR tree-optimization/91355, PR middle-end/90840,
      PR target/90867 (x86), PR c/90898, PR middle-end/91450,
      PR c++/92384, PR tree-optimization/92056, PR tree-optimization/91665,
      PR middle-end/91001, PR middle-end/91105, PR middle-end/91106,
      PR middle-end/91623, PR other/92090, PR target/92389 (x86),
      PR target/87833 (x86), PR target/92095 (SPARC), PR fortran/92113,
      PR tree-optimization/85887, PR c++/92201, PR c++/91974,
      PR preprocessor/92296, PR fortran/92569, PR ada/92575.

 -- Matthias Klose <doko@debian.org>  Tue, 26 Nov 2019 08:34:48 +0100

```"
bitcoin/bitcoin,2022-03-18 22:24:42,bug,ci: failure wallet_send.py --legacy-wallet  ,"https://cirrus-ci.com/task/4929921716846592?logs=functional_tests#L2737

```
 test  2022-03-18T20:16:34.251000Z TestFramework (ERROR): Assertion failed 
                                   Traceback (most recent call last):
                                     File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\test_framework\\test_framework.py"", line 132, in main
                                       self.run_test()
                                     File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\wallet_send.py"", line 560, in run_test
                                       assert_fee_amount(testres[""fees""][""base""], testres[""vsize""], Decimal(0.0001))
                                     File ""C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\\cirrus-ci-build\\test\\functional\\test_framework\\util.py"", line 46, in assert_fee_amount
                                       raise AssertionError(""Fee of %s BTC too high! (Should be %s BTC)"" % (str(fee), str(target_fee)))
                                   AssertionError: Fee of 0.00003160 BTC too high! (Should be 0.0000313 BTC)
```"
bitcoin/bitcoin,2022-03-16 18:44:23,bug,qa: Intermittent failure in feature_segwit.py --descriptors,"https://api.cirrus-ci.com/v1/task/5763159330914304/logs/ci.log
```
5/238 - feature_segwit.py --descriptors failed, Duration: 11 s

stdout:
2022-03-16T18:38:33.242000Z TestFramework (INFO): Initializing test directory /tmp/cirrus-ci-build/ci/scratch/test_runner/test_runner_₿_🏃_20220316_183814/feature_segwit_225
2022-03-16T18:38:39.177000Z TestFramework (INFO): Verify sigops are counted in GBT with pre-BIP141 rules before the fork
2022-03-16T18:38:43.168000Z TestFramework (INFO): Verify witness txs cannot be mined before the fork
2022-03-16T18:38:43.200000Z TestFramework (INFO): Verify unsigned p2sh witness txs without a redeem script are invalid
2022-03-16T18:38:43.630000Z TestFramework (ERROR): JSONRPC error
Traceback (most recent call last):
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 132, in main
    self.run_test()
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/feature_segwit.py"", line 210, in run_test
    self.generate(self.nodes[0], 4)  # blocks 428-431
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_framework.py"", line 639, in generate
    blocks = generator.generate(*args, invalid_call=False, **kwargs)
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_node.py"", line 303, in generate
    return self.generatetoaddress(nblocks=nblocks, address=self.get_deterministic_priv_key().address, maxtries=maxtries, **kwargs)
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/test_node.py"", line 311, in generatetoaddress
    return self.__getattr__('generatetoaddress')(*args, **kwargs)
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/coverage.py"", line 49, in __call__
    return_val = self.auth_service_proxy_instance.__call__(*args, **kwargs)
  File ""/tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/test_framework/authproxy.py"", line 144, in __call__
    raise JSONRPCException(response['error'], status)
test_framework.authproxy.JSONRPCException: CreateNewBlock: TestBlockValidity failed: unexpected-witness, ContextualCheckBlock : unexpected witness data found (-1)
2022-03-16T18:38:43.682000Z TestFramework (INFO): Stopping nodes
2022-03-16T18:38:43.856000Z TestFramework (WARNING): Not cleaning up dir /tmp/cirrus-ci-build/ci/scratch/test_runner/test_runner_₿_🏃_20220316_183814/feature_segwit_225
2022-03-16T18:38:43.856000Z TestFramework (ERROR): Test failed. Test logging available at /tmp/cirrus-ci-build/ci/scratch/test_runner/test_runner_₿_🏃_20220316_183814/feature_segwit_225/test_framework.log
2022-03-16T18:38:43.866000Z TestFramework (ERROR): 
2022-03-16T18:38:43.866000Z TestFramework (ERROR): Hint: Call /tmp/cirrus-ci-build/ci/scratch/build/bitcoin-i686-pc-linux-gnu/test/functional/combine_logs.py '/tmp/cirrus-ci-build/ci/scratch/test_runner/test_runner_₿_🏃_20220316_183814/feature_segwit_225' to consolidate all logs
2022-03-16T18:38:43.866000Z TestFramework (ERROR): 
2022-03-16T18:38:43.866000Z TestFramework (ERROR): If this failure happened unexpectedly or intermittently, please file a bug and provide a link or upload of the combined log.
2022-03-16T18:38:43.867000Z TestFramework (ERROR): https://github.com/bitcoin/bitcoin/issues
2022-03-16T18:38:43.867000Z TestFramework (ERROR): 
```"
bitcoin/bitcoin,2022-03-11 16:49:15,bug,Syscall Sandbox Termination,"I was testing v23.0rc2 compiled.

`./configure --without-miniupnpc --without-natpmp --enable-hardening --without-bdb --with-qrencode --with-zmq --with-sqlite=yes --disable-external-signer`

bitcoin.conf:

> assumevalid=0
> blockfilterindex=1
> coinstatsindex=1
> dbcache=1000
> prune=0
> sandbox=log-and-abort
> txindex=1
> cjdnsreachable=1
> discover=1
> i2psam=127.0.0.1:7656
> listen=1
> listenonion=1
> networkactive=1
> peerblockfilters=1
> proxy=127.0.0.1:9050
> addresstype=bech32m
> avoidpartialspends=1
> changetype=bech32m
> maxapsfee=0.001
> walletrbf=1
> shrinkdebugfile=1

I deleted my data directory (/home/user/.bitcoin) to start a fresh download of the blockchain to test any errors.  Ran out of space, and after deleting more and trying to start bitcoin-qt again:

> ERROR: The syscall ""linkat"" (syscall number 265) is not allowed by the syscall sandbox in thread ""main"". Please report.
> terminate called without an active exception
> Aborted

and in debug.log:

> 2022-03-11T13:00:02Z init message: Verifying blocks…
> 2022-03-11T13:00:02Z Verifying last 6 blocks at level 3
> 2022-03-11T13:00:02Z [0%]...[16%]...[33%]...[50%]...[66%]...[83%]...[99%]...[DONE].
> 2022-03-11T13:00:02Z No coin database inconsistencies in last 6 blocks (2938 transactions)
> 2022-03-11T13:00:02Z  block index            4963ms
> 2022-03-11T13:00:02Z Opening LevelDB in /home/user/.bitcoin/indexes/txindex
> 2022-03-11T13:00:02Z Opened LevelDB successfully
> 2022-03-11T13:00:02Z Using obfuscation key for /home/user/.bitcoin/indexes/txindex: 0000000000000000
> 2022-03-11T13:00:02Z Opening LevelDB in /home/user/.bitcoin/indexes/blockfilter/basic/db
> 2022-03-11T13:00:02Z txindex thread start
> 2022-03-11T13:00:02Z txindex is enabled at height 326934
> 2022-03-11T13:00:02Z txindex thread exit
> 2022-03-11T13:00:02Z Opened LevelDB successfully
> 2022-03-11T13:00:02Z Using obfuscation key for /home/user/.bitcoin/indexes/blockfilter/basic/db: 0000000000000000
> 2022-03-11T13:00:02Z Opening LevelDB in /home/user/.bitcoin/indexes/coinstats/db
> 2022-03-11T13:00:02Z basic block filter index thread start
> 2022-03-11T13:00:02Z basic block filter index is enabled at height 326934
> 2022-03-11T13:00:02Z basic block filter index thread exit
> 2022-03-11T13:00:02Z Opened LevelDB successfully
> 2022-03-11T13:00:02Z Using obfuscation key for /home/user/.bitcoin/indexes/coinstats/db: 0000000000000000
> 2022-03-11T13:00:02Z ERROR: Init: Cannot read current coinstatsindex state; index may be corrupted
> 2022-03-11T13:00:02Z Shutdown: In progress...
> 2022-03-11T13:00:02Z scheduler thread exit
> 2022-03-11T13:00:02Z Shutdown: done
> 2022-03-11T13:00:02Z ERROR: The syscall ""linkat"" (syscall number 265) is not allowed by the syscall sandbox in thread ""main"". Please report.

This was done on a Qubes 4.0 machine in a Debian 11 VM."
bitcoin/bitcoin,2022-03-06 19:53:52,bug,Automatic wallet rescan skipped after abort,"I ran bitcoin-qt with an old wallet, causing it to rescan, then stopped it with a SIGINT signal, resulting in the following log. Nothing unexpected, so far:
```
2022-03-05T16:01:03Z init message: Loading wallet…
2022-03-05T16:01:03Z BerkeleyEnvironment::Open: LogDir=…/.bitcoin/database ErrorFile=…/.bitcoin/db.log
2022-03-05T15:57:18Z [walletname.dat] Wallet File Version = 159900
2022-03-05T15:57:18Z [walletname.dat] Keys: 2397 plaintext, 0 encrypted, 2397 w/ metadata, 2397 total. Unknown wallet records: 0
2022-03-05T15:57:18Z [walletname.dat] Wallet completed loading in              65ms
2022-03-05T15:57:18Z init message: Rescanning…
2022-03-05T15:57:18Z [walletname.dat] Rescanning last 134072 blocks (from block 591711)...
2022-03-05T15:57:18Z [walletname.dat] Rescan started from block 000000000000000000075fcc1872da61f45bcfd0e0bb36b7c8cb45e7a328ad41...
2022-03-05T15:58:18Z [walletname.dat] Still rescanning. At block 593552. Progress=0.632716
2022-03-05T15:59:18Z [walletname.dat] Still rescanning. At block 595458. Progress=0.638084
2022-03-05T15:59:37Z [walletname.dat] Rescan interrupted by shutdown request at block 596080. Progress=0.639932
2022-03-05T15:59:37Z Error: Failed to rescan the wallet during initialization
2022-03-05T15:59:41Z GUI: initializeResult : Initialization result:  false
2022-03-05T15:59:41Z GUI: requestShutdown : Requesting shutdown
2022-03-05T15:59:41Z GUI: Running Shutdown in thread
2022-03-05T15:59:41Z Shutdown: In progress...
2022-03-05T15:59:41Z scheduler thread exit
2022-03-05T15:59:42Z Shutdown: done
2022-03-05T15:59:42Z GUI: Shutdown finished
2022-03-05T15:59:42Z GUI: ~InitExecutor : Stopping thread
2022-03-05T15:59:42Z GUI: ~InitExecutor : Stopped thread
```

However, the next time it was launched, the rescan didn't continue, nor did it start over. It seems to have been skipped?
```
2022-03-05T16:01:03Z init message: Loading wallet…
2022-03-05T16:01:03Z BerkeleyEnvironment::Open: LogDir=…/.bitcoin/database ErrorFile=…/.bitcoin/db.log
2022-03-05T16:01:03Z [walletname.dat] Wallet File Version = 159900
2022-03-05T16:01:03Z [walletname.dat] Keys: 2397 plaintext, 0 encrypted, 2397 w/ metadata, 2397 total. Unknown wallet records: 0
2022-03-05T16:01:03Z [walletname.dat] Wallet completed loading in              91ms
2022-03-05T16:01:03Z [walletname.dat] setKeyPool.size() = 2000
2022-03-05T16:01:03Z [walletname.dat] mapWallet.size() = 378
2022-03-05T16:01:03Z [walletname.dat] m_address_book.size() = 183
2022-03-05T16:01:03Z Loaded 2 addresses from ""anchors.dat""
```

I didn't investigate deeply, but could it be that despite the failed rescan, the new tip block was stored in the wallet? if so, this would be confusing."
bitcoin/bitcoin,2022-03-02 04:42:57,bug,Application Aborted,"I receive this error when trying to open `bitcoin-qt` when compiling from `master`:

> terminate called after throwing an instance of 'std::runtime_error'
>   what():  JSON value is not a string as expected
> Aborted

No debug info is generated since the application isn't started.

I am running Qubes 4.0 fully updated, in a Debian-11 VM with normal networking. Intel x86_64"
bitcoin/bitcoin,2022-02-19 12:46:26,bug,clang-14 ubsan,"New thread to continue the discussion from https://github.com/bitcoin/bitcoin/pull/24347#issuecomment-1045207127

"
bitcoin/bitcoin,2022-02-19 10:26:21,bug,build: Fail to build `libmultiprocess` for the `x86_64-w64-mingw32` target,"On master (https://github.com/bitcoin/bitcoin/commit/a6c3da131c855650e8888a9403776cfda0d0ee2e), building of `libmultiprocess` fails for the `x86_64-w64-mingw32` target:
```
$ make -C depends libmultiprocess MULTIPROCESS=1 HOST=x86_64-w64-mingw32
...
Configuring libmultiprocess...
CMake Warning:
  No source or binary directory provided.  Both will be assumed to be the
  same as the current working directory, but note that this warning will
  become a fatal error in future CMake releases.


-- The CXX compiler identification is GNU 9.3.0
-- Check for working CXX compiler: /usr/bin/x86_64-w64-mingw32-g++-posix
-- Check for working CXX compiler: /usr/bin/x86_64-w64-mingw32-g++-posix -- broken
CMake Error at /usr/share/cmake-3.16/Modules/CMakeTestCXXCompiler.cmake:53 (message):
  The C++ compiler

    ""/usr/bin/x86_64-w64-mingw32-g++-posix""

  is not able to compile a simple test program.

  It fails with the following output:

    Change Dir: /home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp
    
    Run Build Command(s):/usr/bin/make cmTC_14f5f/fast && make[1]: Entering directory '/home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp'
    /usr/bin/make -f CMakeFiles/cmTC_14f5f.dir/build.make CMakeFiles/cmTC_14f5f.dir/build
    make[2]: Entering directory '/home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp'
    Building CXX object CMakeFiles/cmTC_14f5f.dir/testCXXCompiler.cxx.o
    /usr/bin/x86_64-w64-mingw32-g++-posix    -I/home/hebasto/GitHub/bitcoin/depends/x86_64-w64-mingw32/include     -pipe -O2    -o CMakeFiles/cmTC_14f5f.dir/testCXXCompiler.cxx.o -c /home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp/testCXXCompiler.cxx
    Linking CXX executable cmTC_14f5f
    /usr/bin/cmake -E cmake_link_script CMakeFiles/cmTC_14f5f.dir/link.txt --verbose=1
    /usr/bin/x86_64-w64-mingw32-g++-posix  -I/home/hebasto/GitHub/bitcoin/depends/x86_64-w64-mingw32/include     -pipe -O2   -L/home/hebasto/GitHub/bitcoin/depends/x86_64-w64-mingw32/lib  -rdynamic CMakeFiles/cmTC_14f5f.dir/testCXXCompiler.cxx.o  -o cmTC_14f5f 
    x86_64-w64-mingw32-g++-posix: error: unrecognized command line option ‘-rdynamic’
    make[2]: *** [CMakeFiles/cmTC_14f5f.dir/build.make:87: cmTC_14f5f] Error 1
    make[2]: Leaving directory '/home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp'
    make[1]: *** [Makefile:121: cmTC_14f5f/fast] Error 2
    make[1]: Leaving directory '/home/hebasto/GitHub/bitcoin/depends/work/build/x86_64-w64-mingw32/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-7e18b85317d/CMakeFiles/CMakeTmp'
    
    

  

  CMake will not be able to correctly generate this project.
Call Stack (most recent call first):
  CMakeLists.txt:6 (project)


-- Configuring incomplete, errors occurred!
```"
bitcoin/bitcoin,2022-02-19 09:36:58,bug,build: Fail to build `libmultiprocess` for the `arm-linux-gnueabihf` target,"On master (a6c3da131c855650e8888a9403776cfda0d0ee2e), building of `libmultiprocess` fails for the `arm-linux-gnueabihf` target:
```
$ make -C depends libmultiprocess MULTIPROCESS=1 HOST=arm-linux-gnueabihf
make: Entering directory '/home/hebasto/GitHub/bitcoin/depends'
Building libmultiprocess...
make[1]: Entering directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
make[2]: Entering directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
make[3]: Entering directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
[ 14%] Compiling Cap'n Proto schema include/mp/proxy.capnp
/lib/ld-linux-armhf.so.3: No such file or directory
make[3]: *** [CMakeFiles/multiprocess.dir/build.make:62: include/mp/proxy.capnp.c++] Error 255
make[3]: Leaving directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
make[2]: *** [CMakeFiles/Makefile2:153: CMakeFiles/multiprocess.dir/all] Error 2
make[2]: Leaving directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
make[1]: *** [Makefile:130: all] Error 2
make[1]: Leaving directory '/home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d'
make: *** [funcs.mk:282: /home/hebasto/GitHub/bitcoin/depends/work/build/arm-linux-gnueabihf/libmultiprocess/d576d975debdc9090bd2582f83f49c76c0061698-e5741e5a04d/./.stamp_built] Error 2
make: Leaving directory '/home/hebasto/GitHub/bitcoin/depends'
```"
bitcoin/bitcoin,2022-02-17 13:49:19,bug,Syscall sandbox fails on Ubuntu 22.04,"It doesn't even specify which ""invalid syscall"".
```
$ src/bitcoind -sandbox=log-and-abort
2022-02-17T13:45:57Z Bitcoin Core version v22.99.0-b223c3c21e89 (release build)
2022-02-17T13:45:57Z Assuming ancestors of block 00000000000000000008a89e854d57e5667df88f1cdef6fde2fbca1de5b639ad have valid signatures.
2022-02-17T13:45:57Z Setting nMinimumChainWork=00000000000000000000000000000000000000001fa4663bbbe19f82de910280
2022-02-17T13:45:57Z Experimental syscall sandbox enabled (-sandbox=log-and-abort): bitcoind will terminate if an unexpected (not allowlisted) syscall is invoked.
2022-02-17T13:45:57Z Using the 'sse4(1way),sse41(4way),avx2(8way)' SHA256 implementation
2022-02-17T13:45:57Z Default data directory /…/.bitcoin
2022-02-17T13:45:57Z Using data directory /…/.bitcoin
2022-02-17T13:45:57Z Config file: /…/.bitcoin/bitcoin.conf (not found, skipping)
2022-02-17T13:45:57Z Command-line arg: sandbox=""log-and-abort""
2022-02-17T13:45:57Z Using at most 125 automatic connections (1024 file descriptors available)
2022-02-17T13:45:57Z Using 16 MiB out of 32/2 requested for signature cache, able to store 524288 elements
2022-02-17T13:45:57Z Using 16 MiB out of 32/2 requested for script execution cache, able to store 524288 elements
2022-02-17T13:45:57Z Script verification uses 1 additional threads
Bad system call (core dumped)
```
Nothing in `dmesg` either."
bitcoin/bitcoin,2022-02-14 23:18:12,bug,"Add descriptor_tests covering tr(), and fix minor bugs","This fixes two bugs in the current logic for `tr()` descriptors:
* ToPrivateString does not always work, because the provided private key may mismatch the parity of the x-only public key.
* The descriptors inferred for `pk()` inside `tr()` have the wrong x-only flag, leading to such descriptors generating the wrong scriptPubKey (roundtripping through ToString does fix it however, so this seems unobservable in the current code).

These were discovered while adding unit tests to descriptor_tests that cover various aspects of `tr()` descriptors, which are now also added here."
bitcoin/bitcoin,2022-02-13 05:51:29,bug,"Release process: ""does not appear to be a git repository""","The `release-process.md` docs erroneously omit the remote name from `git fetch`.

**Expected behavior**

`git fetch` should complete without errors.

**Actual behavior**

~~~
fatal: 'v22.0' does not appear to be a git repository
fatal: Could not read from remote repository.
~~~

**To reproduce**

Follow steps in `release-process.md` through this line:

`git fetch ""v${VERSION}""`

**System information**

Bitcoin Core b6b7815ddcff53177bb1f6318b39a4134cf42cb1, self-compiled.

Whonix 16 inside Qubes 4.1, Intel Haswell, HDD.

git v2.30.2.

Replacing the offending line with `git fetch origin ""v${VERSION}""` works fine.
"
bitcoin/bitcoin,2022-02-10 20:45:29,bug,Bionic with system libs doesn't link,"Steps to reproduce on a fresh install of Ubuntu Bionic:

```
export DEBIAN_FRONTEND=noninteractive && apt update && apt install curl wget htop git vim ccache -y && git clone https://github.com/bitcoin/bitcoin.git ./bitcoin-core && cd bitcoin-core && apt install build-essential libtool autotools-dev automake pkg-config bsdmainutils python3-zmq     libevent-dev libboost-system-dev libboost-filesystem-dev libboost-test-dev libboost-thread-dev  libsqlite3-dev libdb++-dev  gcc-8 g++-8 -y && ./autogen.sh && CXX=g++-8 CC=gcc-8 ./configure  --without-incompatible-bdb  && make -j $(nproc)
```

Output:

```
  CXXLD    bitcoind
libbitcoin_util.a(libbitcoin_util_a-system.o): In function `boost::system::error_category::std_category::equivalent(std::error_code const&, int) const':
/usr/include/boost/system/error_code.hpp:686: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:689: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:701: undefined reference to `boost::system::generic_category()'
libbitcoin_util.a(libbitcoin_util_a-system.o): In function `boost::system::error_category::std_category::equivalent(int, std::error_condition const&) const':
/usr/include/boost/system/error_code.hpp:656: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:659: undefined reference to `boost::system::generic_category()'
libbitcoin_util.a(libbitcoin_util_a-system.o):/usr/include/boost/system/error_code.hpp:206: more undefined references to `boost::system::generic_category()' follow
libbitcoin_util.a(libbitcoin_util_a-system.o): In function `__static_initialization_and_destruction_0':
/usr/include/boost/system/error_code.hpp:210: undefined reference to `boost::system::system_category()'
libbitcoin_util.a(libbitcoin_util_a-system.o): In function `_GLOBAL__sub_I_BITCOIN_CONF_FILENAME':
/usr/include/boost/asio/error.hpp:230: undefined reference to `boost::system::system_category()'
collect2: error: ld returned 1 exit status
Makefile:5708: recipe for target 'bitcoind' failed
make[2]: *** [bitcoind] Error 1
make[2]: Leaving directory '/bitcoin-core/src'
Makefile:16758: recipe for target 'all-recursive' failed
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory '/bitcoin-core/src'
Makefile:814: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1
```


Initial report by @hebasto in https://github.com/bitcoin/bitcoin/pull/20744#issuecomment-1029406510"
bitcoin/bitcoin,2022-02-04 13:56:29,bug,Cirrus CI / Win64 doesn't print assert errors from test_bitcoin.exe,"Cirrus CI / Win64 build does not seem to print errors from test_bitcoin.exe if an `assert` statement fails. I saw this recently in https://cirrus-ci.com/task/6752891435220992?logs=ci#L3597 from  #24251, and reproduced it locally running `FILE_ENV=./ci/test/00_setup_env_win64.sh ./ci/test_run_all.sh`.

If I run `test_bitcoin.exe` manually with `docker exec` I can see:

```
test/util_tests.cpp(54): Entering test case ""util_datadir""
Assertion failed: fs::equivalent(result, path), file util/system.cpp, line 256
```

But the default docker / cirrus output just shows the test_bitcoin.exe process exiting without any information about a failing assert.

I don't know if this is a known limitation, but it would be helpful if windows CI builds were able to print why they are failing in cases like this."
bitcoin/bitcoin,2022-02-03 21:17:25,bug,`getblocktemplate` returns a standard P2PKH with 0 sigops (testnet),"<!-- This issue tracker is only for technical issues related to Bitcoin Core.

General bitcoin questions and/or support requests are best directed to the Bitcoin StackExchange at https://bitcoin.stackexchange.com.

For reporting security issues, please read instructions at https://bitcoincore.org/en/contact/.

If the node is ""stuck"" during sync or giving ""block checksum mismatch"" errors, please ensure your hardware is stable by running memtest and observe CPU temperature with a load-test tool such as linpack before creating an issue! -->
i noticed a standard 0 sigops transaction when calling getblocktemplate, but that transaction turned out to be P2PKH of course **with** `OP_CHECKSIG`
<!-- Describe the issue -->
**Expected behavior**

`""sigops"": 36,` in block template json

<!--- What behavior did you expect? -->

**Actual behavior**

```
      ...
      ""txid"": ""54eee291ec73fcc3faf496cf90ffc9b6faa57940234802bdae58c51540016853"",
      ""hash"": ""54eee291ec73fcc3faf496cf90ffc9b6faa57940234802bdae58c51540016853"",
      ""depends"": [
      ],
      ""fee"": 1404,
      ""sigops"": 0,   <-- here
      ""weight"": 5472
      ...
```
<!--- What was the actual behavior (provide screenshots if the issue is GUI-related)? -->

**To reproduce**
<!--- How reliably can you reproduce the issue, what are the steps to do so? -->

somehow get the sigop count of `54eee291ec73fcc3faf496cf90ffc9b6faa57940234802bdae58c51540016853` (testnet)

**System information**

bitcoin.org binary v22.0.0

<!-- What version of Bitcoin Core are you using, where did you get it (website, self-compiled, etc)? -->

<!-- What type of machine are you observing the error on (OS/CPU and disk type)? -->

<!-- GUI-related issue? What is your operating system and its version? If Linux, what is your desktop environment and graphical shell? -->

<!-- Any extra information that might be useful in the debugging process. -->
<!--- This is normally the contents of a `debug.log` or `config.log` file. Raw text or a link to a pastebin type site are preferred. -->
"
bitcoin/bitcoin,2022-02-02 00:29:37,bug,Invalid SHA256SUMS.asc,"https://bitcoincore.org/bin/bitcoin-core-22.0/SHA256SUMS.asc doesn't seem valid (no hashes or files included).

```
~$ curl -sLO https://bitcoincore.org/bin/bitcoin-core-22.0/SHA256SUMS.asc
~$ cat SHA256SUMS.asc 
-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEDMuq/Xai7OLM0xQd4v/VsdiMqX0FAmE7QY0ACgkQ4v/VsdiM
...
n+msw3Q5hoYG/c9VlV/Am4NM8lxEM9BWz/6XbPvOFBKICQ8D3A0=
=MC+w
-----END PGP SIGNATURE-----
-----BEGIN PGP SIGNATURE-----
...
```

Compared to 0.21 and prior.

```
~$ curl -sLO https://bitcoin.org/bin/bitcoin-core-0.21.0/SHA256SUMS.asc
~$ cat SHA256SUMS.asc 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

43416854330914992bbba2d0e9adf2a6fff4130be9af8ae2ef1186e743d9a3fe  bitcoin-0.21.0-aarch64-linux-gnu.tar.gz
..
da7766775e3f9c98d7a9145429f2be8297c2672fe5b118fd3dc2411fb48e0032  bitcoin-0.21.0-x86_64-linux-gnu.tar.gz
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)

iQIcBAEBCAAGBQJgADqTAAoJEJDIAZ42wulkjtkQAJwlSTDinKsxZIMky3MeVhwB
...
ka8Z9KLt/N0ziabBexAw
=bi4p
-----END PGP SIGNATURE-----
```

Maybe related (or not), it seems to verify the v22 file above, you need laanwj.asc and not laanwj-releases.asc (not that it matters because I think the SHA256SUMS.asc is incorrect).

"
opencv/opencv,2023-09-28 07:19:24,bug,ci: tests on Ubuntu2004-x64 are automatically cancelled for some reasons after the Ubuntu2004-x64-OpenVINO is added,"### System Information

N/A

### Detailed description

See this with Ubuntu2004-x64-OpenVINO: https://github.com/opencv/opencv/actions/runs/6334391424/job/17205417534?pr=24334

See this without Ubuntu2004-x64-OpenVINO: https://github.com/opencv/opencv/pull/24335

Maybe the jobs is capped due to too many are assgined on a single machine (or it exceeds the number of workers that we set)?

### Steps to reproduce

Push new commits and trigger Ubuntu2004-x64-OpenVINO, then jobs of Ubuntu2004-x6 are cancelled instantly.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-24 11:13:24,bug,DNN: OpenCL FP16 tests are failed with Einsum layer (2023-09-22),"OpenCL: Intel iGPU

relates #24037

- Windows: https://pullrequest.opencv.org/buildbot/builders/4_x-win64-vc14/builds/100269
- Linux (similar problem): https://pullrequest.opencv.org/buildbot/builders/4_x-lin64/builds/100257

```
[  FAILED  ] 7 tests, listed below:
[  FAILED  ] Test_ONNX_conformance.Layer_Test/test_einsum_batch_matmul_OCV_OCL_FP16, where GetParam() = (test_einsum_batch_matmul, OCV/OCL_FP16)
[  FAILED  ] Test_ONNX_conformance.Layer_Test/test_einsum_sum_OCV_OCL_FP16, where GetParam() = (test_einsum_sum, OCV/OCL_FP16)
[  FAILED  ] Test_ONNX_layers.Einsum_2D/1, where GetParam() = OCV/OCL_FP16
[  FAILED  ] Test_ONNX_layers.Einsum_3D/1, where GetParam() = OCV/OCL_FP16
[  FAILED  ] Test_ONNX_layers.Einsum_4D/1, where GetParam() = OCV/OCL_FP16
[  FAILED  ] Test_ONNX_layers.Einsum_5D/1, where GetParam() = OCV/OCL_FP16
[  FAILED  ] Test_ONNX_layers.Einsum_Sum/1, where GetParam() = OCV/OCL_FP16
```"
opencv/opencv,2023-09-20 03:58:09,bug,dnn onnx: invalid shape dimension and wrong results in test case Test_ONNX_layers.OpenAI_CLIP_head,"### System Information

latest opencv and lastest onnxruntime

### Detailed description

Related PR: https://github.com/opencv/opencv/pull/23419 and https://github.com/opencv/opencv_extra/pull/1049.

In the above PR, several fixes are indtroduced for the clip head. Those fixes are fine, but the manually generated onnx model named ""clip-vit-base-head.onnx"" is invalid and **cannot be run by onnxruntime**:

1. In the Expand operator, `shape` input should be of type `int64` instead of `int32`. Please use `onnx.checker.check_model(model)` (it checks operator definitions and so on) before saving a manually generated onnx model.
2. Having that fixed, onnxruntime still complains invalid shape in the Expand operator. Changing the shape value from [1, 1, -1] to [1, 1, 1] fixes the problem. Please run the manually generated onnx model with onnxruntime to check whether it is all valid.

cc @dkurt 

### Steps to reproduce

Install onnxruntime, then

```
import numpy as np
from onnxruntime import InferenceSession


net = InferenceSession(""./clip-vit-base-head.onnx"")
out = net.run([], {""input"": np.random.randn(1, 1, 3).astype(np.float32)})
print(out[0].shape)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-14 07:04:32,bug,FindOpenBLAS doesn't find OpenBLAS due to incorrect filename,"### System Information

OpenCV Version 4.7.0 (commit 725e440d278aca07d35a5e8963ef990572b07316), but same behaviour in latest version
Windows 10
cmake 3.26.4

### Detailed description

https://github.com/opencv/opencv/blob/4790a3732e725b102f6c27858e7b43d78aee2c3e/cmake/OpenCVFindOpenBLAS.cmake#L76C1-L76C95

contains the line 

    FIND_LIBRARY(OpenBLAS_LIB NAMES openblas PATHS ${Open_BLAS_LIB_SEARCH_PATHS}  NO_DEFAULT_PATH)

which looks for a library called openblas.lib on windows. The windows release of openBLAS (https://github.com/xianyi/OpenBLAS/releases/download/v0.3.24/OpenBLAS-0.3.24-x64.zip) however contains `libopenblas` which is not recognized by cmake.

Changing the line to

    FIND_LIBRARY(OpenBLAS_LIB NAMES openblas libopenblas PATHS ${Open_BLAS_LIB_SEARCH_PATHS}  NO_DEFAULT_PATH)

makes sure that the windows release of openblas can also be found

### Steps to reproduce

```
set OpenBLAS_HOME=%CURRENT_DIR%\\dependencies\\OpenBLAS-0.3.24-x64
git clone https://github.com/opencv/opencv
git clone https://github.com/opencv/opencv_contrib
cd opencv
mkdir build
cd build

cd opencv
mkdir build_binary_openblas
cd build_binary_openblas

#Build OpenCV in Debug config
rm -r * 
""C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Auxiliary\\Build\\vcvars64.bat""
cmake -G ""Visual Studio 16 2019"" -DCMAKE_BUILD_TYPE=DEBUG ^
    -DPYTHON3_EXECUTABLE=%py_three% ^
    -DPYTHON2_EXECUTABLE=%py_two% ^
    -DWITH_OPENCL=True ^
    -DWITH_QT=False ^
    -DWITH_FFMPEG=False ^
    -DWITH_GSTREAMER=False ^
    -DWITH_DSHOW=False ^
    -DBUILD_opencv_dnn=OFF ^
    -DBUILD_opencv_video=ON ^
    -DBUILD_opencv_videoio=ON ^
    -DBUILD_opencv_python2=OFF ^
    -DBUILD_opencv_python3=OFF ^
    -DWITH_PROTOBUF=OFF ^
    -DCMAKE_INSTALL_PREFIX=..\\..\\build_binary_openblas ^
    -DOPENCV_EXTRA_MODULES_PATH=..\\..\\opencv_contrib\\modules ^
    -DBUILD_opencv_=OFF ^
    -DBUILD_opencv_alphamat=OFF ^
    -DBUILD_opencv_aruco=OFF ^
    -DBUILD_opencv_bgsegm=OFF ^
    -DBUILD_opencv_bioinspired=OFF ^
    -DBUILD_opencv_ccalib=OFF ^
    -DBUILD_opencv_cnn_3dobj=OFF ^
    -DBUILD_opencv_cvv=ON ^
    -DBUILD_opencv_datasets=OFF ^
    -DBUILD_opencv_dnn_objdetect=OFF ^
    -DBUILD_opencv_dnn_superres=OFF ^
    -DBUILD_opencv_dnns_easily_fooled=OFF ^
    -DBUILD_opencv_dpm=OFF ^
    -DBUILD_opencv_face=OFF ^
    -DBUILD_opencv_freetype=OFF ^
    -DBUILD_opencv_fuzzy=OFF ^
    -DBUILD_opencv_hdf=OFF ^
    -DBUILD_opencv_hfs=OFF ^
    -DBUILD_opencv_img_hash=OFF ^
    -DBUILD_opencv_intensity_transform=OFF ^
    -DBUILD_opencv_julia=OFF ^
    -DBUILD_opencv_line_descriptor=OFF ^
    -DBUILD_opencv_matlab=OFF ^
    -DBUILD_opencv_mcc=OFF ^
    -DBUILD_opencv_optflow=OFF ^
    -DBUILD_opencv_ovis=OFF ^
    -DBUILD_opencv_phase_unwrapping=OFF ^
    -DBUILD_opencv_plot=OFF ^
    -DBUILD_opencv_quality=OFF ^
    -DBUILD_opencv_rapid=OFF ^
    -DBUILD_opencv_reg=OFF ^
    -DBUILD_opencv_rgbd=OFF ^
    -DBUILD_opencv_saliency=OFF ^
    -DBUILD_opencv_sfm=OFF ^
    -DBUILD_opencv_shape=OFF ^
    -DBUILD_opencv_stereo=OFF ^
    -DBUILD_opencv_structured_light=OFF ^
    -DBUILD_opencv_superres=OFF ^
    -DBUILD_opencv_surface_matching=OFF ^
    -DBUILD_opencv_text=OFF ^
    -DBUILD_opencv_tracking=OFF ^
    -DBUILD_opencv_videostab=OFF ^
    -DBUILD_opencv_viz=OFF ^
    -DBUILD_opencv_world=ON ^
    -DBUILD_opencv_wechat_qrcode=OFF ^
    -DBUILD_opencv_xfeatures2d=OFF ^
    -DBUILD_opencv_ximgproc=ON ^
    -DBUILD_opencv_xobjdetect=OFF ^
    -DBUILD_opencv_xphoto=OFF ..


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-11 19:02:36,bug,DNN: OpenCL FP16 test is broken (Test_Caffe_nets.FasterRCNN_vgg16) (2023-09-05),"OpenCL device: Intel iGPU

Nightly builds:

- Linux: http://pullrequest.opencv.org/buildbot/builders/4_x-lin64/builds/100240

```
[ RUN      ] Test_Caffe_nets.FasterRCNN_vgg16/1, where GetParam() = OCV/OCL_FP16
Unmatched prediction: class 7 score 0.998047 box [241.5 x 83.6875 from (481.5, 92.3125)]
Highest IoU: 0
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:145: Failure
Value of: matched
  Actual: false
Expected: true
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched prediction: class 12 score 0.994141 box [217.75 x 374 from (133.25, 189.5)]
Highest IoU: 0
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:145: Failure
Value of: matched
  Actual: false
Expected: true
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched reference: class 7 score 0.997022 box [240.844 x 83.6312 from (481.841, 92.3218)] IoU diff: 1
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:157: Failure
Expected: (refScores[i]) <= (confThreshold), actual: 0.997022 vs 0.8
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched reference: class 12 score 0.993028 box [217.773 x 373.789 from (133.221, 189.377)] IoU diff: 1
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:157: Failure
Expected: (refScores[i]) <= (confThreshold), actual: 0.993028 vs 0.8
model name: VGG16_faster_rcnn_final.caffemodel
[ INFO:0@72.149] global ts.cpp:857 testTearDown Memory_usage (OpenCL): 937353526 (base=1200  current=0)
[  FAILED  ] Test_Caffe_nets.FasterRCNN_vgg16/1, where GetParam() = OCV/OCL_FP16 (12935 ms)
```

- Windows: http://pullrequest.opencv.org/buildbot/builders/4_x-win64-vc14/builds/100252

```
[ RUN      ] Test_Caffe_nets.FasterRCNN_vgg16/1, where GetParam() = OCV/OCL_FP16
Unmatched prediction: class 7 score 0.998047 box [241.5 x 83.6875 from (481.5, 92.3125)]
Highest IoU: 0
C:\\build\\precommit_opencl\\4.x\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(145): error: Value of: matched
  Actual: false
Expected: true
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched prediction: class 12 score 0.994141 box [217.75 x 374 from (133.25, 189.5)]
Highest IoU: 0
C:\\build\\precommit_opencl\\4.x\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(145): error: Value of: matched
  Actual: false
Expected: true
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched reference: class 7 score 0.997022 box [240.844 x 83.6312 from (481.841, 92.3218)] IoU diff: 1
C:\\build\\precommit_opencl\\4.x\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(157): error: Expected: (refScores[i]) <= (confThreshold), actual: 0.997022 vs 0.8
model name: VGG16_faster_rcnn_final.caffemodel
Unmatched reference: class 12 score 0.993028 box [217.773 x 373.789 from (133.221, 189.377)] IoU diff: 1
C:\\build\\precommit_opencl\\4.x\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(157): error: Expected: (refScores[i]) <= (confThreshold), actual: 0.993028 vs 0.8
model name: VGG16_faster_rcnn_final.caffemodel
[ INFO:0@171.036] global ts.cpp:857 cvtest::testTearDown Memory_usage (OpenCL): 937353526 (base=1200  current=0)
[  FAILED  ] Test_Caffe_nets.FasterRCNN_vgg16/1, where GetParam() = OCV/OCL_FP16 (25139 ms)
```"
opencv/opencv,2023-09-09 14:33:44,bug,error C2819: type 'ov::Output<ov::Node>' does not have an overloaded member 'operator ->',"### System Information

```
-- Detected processor: AMD64
-- Found ZLIB: optimized;C:/install/zlib/lib/zlib.lib;debug;C:/install/zlib/lib/zlibd.lib (found suitable version ""1.2.13"", minimum required is ""1.2.3"")
-- libjpeg-turbo: VERSION = 2.1.3, BUILD = opencv-4.8.0-dev-libjpeg-turbo
-- libjpeg-turbo(SIMD): SIMD extensions disabled: could not find NASM compiler.  Performance will suffer.
-- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources
-- OpenJPEG: VERSION = 2.5.0, BUILD = opencv-4.8.0-dev-openjp2-2.5.0
-- OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.5.0"")
-- Found ZLIB: optimized;C:/install/zlib/lib/zlib.lib;debug;C:/install/zlib/lib/zlibd.lib (found version ""1.2.13"")
-- Found TBB (cmake): C:/install/openvino/runtime/3rdparty/tbb/bin/tbb.dll
-- found Intel IPP (ICV version): 2021.8.0 [2021.8]
-- at: C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/icv
-- found Intel IPP Integration Wrappers sources: 2021.8.0
-- at: C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/iw
-- CUDA detected: 12.1
-- CUDA: Using CUDA_ARCH_BIN=8.6
-- CUDA NVCC target flags: -gencode;arch=compute_86,code=sm_86;-D_FORCE_INLINES
-- CUDA: MSVS generator is detected. Disabling CMake re-run checks (CMAKE_SUPPRESS_REGENERATION=ON). You need to run CMake manually if updates are required.
-- Found MKL 2023.0.0 at: C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0
-- LAPACK(MKL): LAPACK_LIBRARIES: C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0/lib/intel64/mkl_intel_lp64.lib;C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0/lib/intel64/mkl_tbb_thread.lib;C:/Prograore.lib;tbb
-- LAPACK(MKL): Can't build LAPACK check code. This LAPACK version is not supported.
-- Could not find OpenBLAS include. Turning OpenBLAS_FOUND off
-- Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off
-- Could NOT find BLAS (missing: BLAS_LIBRARIES)
-- Could NOT find LAPACK (missing: LAPACK_LIBRARIES)
    Reason given by package: LAPACK could not be found because dependency BLAS could not be found.

-- Found apache ant: C:/apache-ant-1.10.13/bin/ant.bat (1.10.13)
-- OpenVINO FOUND: 2022.3.0
-- Found VTK 9.2.5
-- freetype2:   NO
-- harfbuzz:    NO
-- Julia not found. Not compiling Julia Bindings.
-- Module opencv_ovis disabled because OGRE3D was not found
-- CERES support is disabled. Ceres Solver for reconstruction API is required.
-- Tesseract:   YES (ver 5.3.0-35-gae3bf)
-- Allocator metrics storage type: 'long long'
-- Excluding from source files list: modules/imgproc/src/imgwarp.lasx.cpp
-- Excluding from source files list: modules/imgproc/src/resize.lasx.cpp
-- Registering hook 'INIT_MODULE_SOURCES_opencv_dnn': C:/lib/opencv/modules/dnn/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake
-- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.rvv.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.lasx.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/int8layers/layers_common.lasx.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/cpu_kernels/conv_depthwise.rvv.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/cpu_kernels/conv_depthwise.lasx.cpp
-- imgcodecs: OpenEXR codec is disabled in runtime. Details: https://github.com/opencv/opencv/issues/21326
-- highgui: using builtin backend: WIN32UI
-- CERES support is disabled. Ceres Solver for reconstruction API is required.
-- Building with NVIDIA Optical Flow API 2.0
-- Found 'misc' Python modules from C:/lib/opencv/modules/python/package/extra_modules
-- Found 'mat_wrapper;utils' Python modules from C:/lib/opencv/modules/core/misc/python/package
-- Found 'gapi' Python modules from C:/lib/opencv/modules/gapi/misc/python/package
-- Found 'misc' Python modules from C:/lib/opencv/modules/python/package/extra_modules
-- Found 'mat_wrapper;utils' Python modules from C:/lib/opencv/modules/core/misc/python/package
-- Found 'gapi' Python modules from C:/lib/opencv/modules/gapi/misc/python/package
-- DNNL_CONFIGURATION: cpu_dpcpp_gpu_dpcpp
-- SYCL/OpenCL samples are skipped: SYCL SDK is required
--    - check configuration of SYCL_DIR/SYCL_ROOT/CMAKE_MODULE_PATH
--    - ensure that right compiler is selected from SYCL SDK (e.g, clang++): CMAKE_CXX_COMPILER=C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe
-- Registered 'check_pylint' target: using C:/Users/laurent/AppData/Roaming/Python/Python310/Scripts/pylint.exe (ver: 2.16.2), checks: 193
-- Registered 'check_flake8' target: using C:/Users/laurent/AppData/Roaming/Python/Python310/Scripts/flake8.exe (ver: 6.0.0)
CMake Warning at cmake/OpenCVGenSetupVars.cmake:54 (message):
  CONFIGURATION IS NOT SUPPORTED: validate setupvars script in install
  directory
Call Stack (most recent call first):
  CMakeLists.txt:1073 (include)


--
-- General configuration for OpenCV 4.8.0-dev =====================================
--   Version control:               4.8.0-215-g1a8d37d19e
--
--   Extra modules:
--     Location (extra):            C:/lib/opencv_contrib/modules
--     Version control (extra):     4.8.0-14-g9e134699
--
--   Platform:
--     Timestamp:                   2023-09-09T13:26:40Z
--     Host:                        Windows 10.0.22621 AMD64
--     CMake:                       3.26.1
--     CMake generator:             Visual Studio 17 2022
--     CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe
--     MSVC:                        1935
--     Configuration:               Debug Release
--
--   CPU/HW features:
--     Baseline:                    SSE SSE2 SSE3
--       requested:                 SSE3
--     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX
--       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
--       SSE4_1 (18 files):         + SSSE3 SSE4_1
--       SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
--       FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
--       AVX (8 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
--       AVX2 (37 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
--       AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX
--
--   C/C++:
--     Built as dynamic libs?:      YES
--     C++ standard:                11
--     C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe  (ver 19.35.32215.0)
--     C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 //DNDEBUG
--     C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 / /Od /RTC1
--     C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe
--     C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MD /O2 /Ob2 /DNDEBU
--     C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MDd /Zi /Ob0 /Od /RTC
--     Linker flags (Release):      /machine:x64  /INCREMENTAL:NO
--     Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL
--     ccache:                      NO
--     Precompiled headers:         YES
--     Extra dependencies:          cudart_static.lib nppc.lib nppial.lib nppicc.lib nppidei.lib nppif.lib nppig.lib nppim.lib nppist.lib nppisu.lib nppitc.lib npps.lib cublas.lib cudnn.lib cufft.lib -LIv12.1/lib/x64
--     3rdparty dependencies:
--
--   OpenCV modules:
--     To be built:                 alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudasperres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg  superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
--     Disabled:                    world
--     Disabled by dependency:      -
--     Unavailable:                 cvv freetype hdf julia matlab ovis python2
--     Applications:                tests perf_tests examples apps
--     Documentation:               doxygen python javadoc
--     Non-free algorithms:         YES
--
--   Windows RT support:            NO
--
--   GUI:                           WIN32UI
--     Win32 UI:                    YES
--     OpenGL support:              YES (opengl32 glu32)
--     VTK support:                 YES (ver 9.2.5)
--
--   Media I/O:
--     ZLib:                        optimized C:/install/zlib/lib/zlib.lib debug C:/install/zlib/lib/zlibd.lib (ver 1.2.13)
--     JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)
--       SIMD Support Request:      YES
--       SIMD Support:              NO
--     WEBP:                        build (ver encoder: 0x020f)
--     PNG:                         optimized C:/install/libpng/lib/libpng16.lib debug C:/install/libpng/lib/libpng16d.lib (ver 1.6.40)
--     TIFF:                        build (ver 42 - 4.2.0)
--     JPEG 2000:                   build (ver 2.5.0)
--     OpenEXR:                     build (ver 2.3.0)
--     HDR:                         YES
--     SUNRASTER:                   YES
--     PXM:                         YES
--     PFM:                         YES
--
--   Video I/O:
--     DC1394:                      NO
--     FFMPEG:                      YES (prebuilt binaries)
--       avcodec:                   YES (58.134.100)
--       avformat:                  YES (58.76.100)
--       avutil:                    YES (56.70.100)
--       swscale:                   YES (5.9.100)
--       avresample:                YES (4.0.0)
--     GStreamer:                   NO
--     DirectShow:                  YES
--     Media Foundation:            YES
--       DXVA:                      YES
--
--   Parallel framework:            TBB (ver 2020.3 interface 11103)
--
--   Other third-party libraries:
--     Intel IPP:                   2021.8 [2021.8.0]
--            at:                   C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/icv
--     Intel IPP IW:                sources (2021.8.0)
--               at:                C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/iw
--     Lapack:                      NO
--     OpenVINO:                    YES (2022.3.0)
--     Default DNN backend:         DNN_BACKEND_OPENCV
--     Eigen:                       YES (ver ..)
--     Custom HAL:                  NO
--     Protobuf:                    build (3.19.1)
--     Flatbuffers:                 builtin/3rdparty (23.5.9)
--
--   NVIDIA CUDA:                   YES (ver 12.1, CUFFT CUBLAS)
--     NVIDIA GPU arch:             86
--     NVIDIA PTX archs:
--
--   cuDNN:                         YES (ver 8.8.0)
--
--   OpenCL:                        YES (NVD3D11)
--     Include path:                C:/lib/opencv/3rdparty/include/opencl/1.2
--     Link libraries:              Dynamic load
--
--   Python 3:
--     Interpreter:                 C:/Program Files/Python310/python.exe (ver 3.10.10)
--     Libraries:                   optimized C:/Program Files/Python310/libs/python310.lib debug C:/Program Files/Python310/libs/python310_d.lib (ver 3.10.10)
--     numpy:                       C:/Users/laurent/AppData/Roaming/Python/Python310/site-packages/numpy/core/include (ver 1.23.5)
--     install path:                C:/Users/laurent/AppData/Roaming/Python/Python310/site-packages/cv2/python-3.10
--
--   Python (for build):            C:/Program Files/Python310/python.exe
--
--   Java:
--     ant:                         C:/apache-ant-1.10.13/bin/ant.bat (ver 1.10.13)
--     Java:                        NO
--     JNI:                         C:/Program Files/Java/jdk-17/include C:/Program Files/Java/jdk-17/include/win32 C:/Program Files/Java/jdk-17/include
--     Java wrappers:               YES (ANT)
--     Java tests:                  YES
--
--   Install to:                    C:/install/opencv
-- -----------------------------------------------------------------
--
-- Configuring done (10.2s)
-- Generating done (22.2s)
-- Build files have been written to: C:/lib/build/opencv



```
### Detailed description

I cannot build opencv dnn module. 

```
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): error C2819: type 'ov::Output<ov::Node>' does not have an overloaded member 'operator ->'
1>C:\\install\\openvino\\runtime\\include\\openvino/core/node_output.hpp(30,33): message : see declaration of 'ov::Output<ov::Node>'
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): message : did you intend to use '.' instead?
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): error C2039: 'get_friendly_name': is not a member of 'ov::Output<ov::Node>'
1>C:\\install\\openvino\\runtime\\include\\openvino/core/node_output.hpp(30,33): message : see declaration of 'ov::Output<ov::Node>'

```

Full build
Build started...
```
1>------ Build started: Project: opencv_dnn, Configuration: Debug x64 ------
1>Processing OpenCL kernels (dnn)
1>-- C:/lib/build/opencv/modules/dnn/opencl_kernels_dnn.hpp contains the same content
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_activation_eltwise.cu.obj
1>activation_eltwise.cu
1>activation_eltwise.cu
1>tmpxft_00004d60_00000000-10_activation_eltwise.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_activations.cu.obj
1>activations.cu
1>activations.cu
1>tmpxft_0000286c_00000000-10_activations.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_bias_activation.cu.obj
1>bias_activation.cu
1>bias_activation.cu
1>tmpxft_000008e8_00000000-10_bias_activation.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_bias_activation_eltwise.cu.obj
1>bias_activation_eltwise.cu
1>bias_activation_eltwise.cu
1>tmpxft_00004584_00000000-10_bias_activation_eltwise.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_bias_eltwise_activation.cu.obj
1>bias_eltwise_activation.cu
1>bias_eltwise_activation.cu
1>tmpxft_00001984_00000000-10_bias_eltwise_activation.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_concat.cu.obj
1>concat.cu
1>concat.cu
1>tmpxft_00003f44_00000000-10_concat.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_crop_and_resize.cu.obj
1>crop_and_resize.cu
1>crop_and_resize.cu
1>tmpxft_00000b5c_00000000-10_crop_and_resize.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_detection_output.cu.obj
1>detection_output.cu
1>detection_output.cu
1>tmpxft_0000619c_00000000-10_detection_output.cudafe1.cpp
1>C:\\lib\\opencv\\modules\\dnn\\src\\cuda\\detection_output.cu(707): warning C4805: '|': unsafe mix of type 'int' and type 'bool' in operation
1>C:\\lib\\opencv\\modules\\dnn\\src\\cuda\\detection_output.cu(711): note: see reference to function template instantiation 'void cv::dnn::cuda4dnn::kernels::decode_bboxes<__half>(const cv::dnn::cuda4dnn::csl::Stream &,cv::dnn::cuda4dnn::csl::Span<__half>,cv::dnn::cuda4dnn::csl::Span<const __half>,cv::dnn::cuda4dnn::csl::Span<const __half>,size_t,bool,size_t,bool,bool,bool,bool,bool,float,float)' being compiled
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_eltwise_activation.cu.obj
1>eltwise_activation.cu
1>eltwise_activation.cu
1>tmpxft_00003ed0_00000000-10_eltwise_activation.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_eltwise_ops.cu.obj
1>eltwise_ops.cu
1>eltwise_ops.cu
1>tmpxft_00004e1c_00000000-10_eltwise_ops.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_fill_copy.cu.obj
1>fill_copy.cu
1>fill_copy.cu
1>tmpxft_00001d8c_00000000-10_fill_copy.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_fp_conversion.cu.obj
1>fp_conversion.cu
1>fp_conversion.cu
1>tmpxft_000035fc_00000000-10_fp_conversion.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_grid_nms.cu.obj
1>grid_nms.cu
1>grid_nms.cu
1>tmpxft_00003cc8_00000000-10_grid_nms.cudafe1.cpp
1>C:\\lib\\opencv\\modules\\dnn\\src\\cuda\\grid_nms.cu(451): warning C4189: 'ITEMS_PER_THREAD': local variable is initialized but not referenced
1>C:\\lib\\opencv\\modules\\dnn\\src\\cuda\\grid_nms.cu(464): note: see reference to function template instantiation 'void cv::dnn::cuda4dnn::kernels::grid_nms<__half>(const cv::dnn::cuda4dnn::csl::Stream &,cv::dnn::cuda4dnn::csl::Span<unsigned int>,cv::dnn::cuda4dnn::csl::TensorSpan<int>,cv::dnn::cuda4dnn::csl::TensorSpan<int>,cv::dnn::cuda4dnn::csl::TensorView<__half>,int,bool,float)' being compiled
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_max_unpooling.cu.obj
1>max_unpooling.cu
1>max_unpooling.cu
1>tmpxft_000060ec_00000000-10_max_unpooling.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_mvn.cu.obj
1>mvn.cu
1>mvn.cu
1>tmpxft_00004f48_00000000-10_mvn.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_normalize.cu.obj
1>normalize.cu
1>normalize.cu
1>tmpxft_00003c28_00000000-10_normalize.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_padding.cu.obj
1>padding.cu
1>padding.cu
1>tmpxft_00005010_00000000-10_padding.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_permute.cu.obj
1>permute.cu
1>permute.cu
1>tmpxft_000067c8_00000000-10_permute.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_prior_box.cu.obj
1>prior_box.cu
1>prior_box.cu
1>tmpxft_0000293c_00000000-10_prior_box.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_region.cu.obj
1>region.cu
1>region.cu
1>tmpxft_00003b00_00000000-10_region.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_resize.cu.obj
1>resize.cu
1>resize.cu
1>tmpxft_00006588_00000000-10_resize.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_roi_pooling.cu.obj
1>roi_pooling.cu
1>roi_pooling.cu
1>tmpxft_00005d70_00000000-10_roi_pooling.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_scale_shift.cu.obj
1>scale_shift.cu
1>scale_shift.cu
1>tmpxft_00000bb0_00000000-10_scale_shift.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_shortcut.cu.obj
1>shortcut.cu
1>shortcut.cu
1>tmpxft_00003830_00000000-10_shortcut.cudafe1.cpp
1>Building NVCC (Device) object modules/dnn/CMakeFiles/cuda_compile_1.dir/src/cuda/Debug/cuda_compile_1_generated_slice.cu.obj
1>slice.cu
1>slice.cu
1>tmpxft_00000c34_00000000-10_slice.cudafe1.cpp
1>cmake_pch.cxx
1>opencv-caffe.pb.cc
1>opencv-onnx.pb.cc
1>attr_value.pb.cc
1>function.pb.cc
1>graph.pb.cc
1>op_def.pb.cc
1>tensor.pb.cc
1>tensor_shape.pb.cc
1>types.pb.cc
1>versions.pb.cc
1>caffe_importer.cpp
1>caffe_io.cpp
1>caffe_shrinker.cpp
1>darknet_importer.cpp
1>darknet_io.cpp
1>debug_utils.cpp
1>dnn.cpp
1>dnn_read.cpp
1>dnn_utils.cpp
1>graph_simplifier.cpp
1>halide_scheduler.cpp
1>ie_ngraph.cpp
1>init.cpp
1>quantization_utils.cpp
1>layer.cpp
1>layer_factory.cpp
1>accum_layer.cpp
1>arg_layer.cpp
1>blank_layer.cpp
1>concat_layer.cpp
1>const_layer.cpp
1>correlation_layer.cpp
1>conv_depthwise.cpp
1>conv_winograd_f63.cpp
1>convolution.cpp
1>crop_and_resize_layer.cpp
1>cumsum_layer.cpp
1>detection_output_layer.cpp
1>flatten_layer.cpp
1>flow_warp_layer.cpp
1>gather_layer.cpp
1>layer_norm.cpp
1>layers_common.cpp
1>lrn_layer.cpp
1>max_unpooling_layer.cpp
1>mvn_layer.cpp
1>nary_eltwise_layers.cpp
1>normalize_bbox_layer.cpp
1>not_implemented_layer.cpp
1>padding_layer.cpp
1>permute_layer.cpp
1>prior_box_layer.cpp
1>proposal_layer.cpp
1>recurrent_layers.cpp
1>reduce_layer.cpp
1>region_layer.cpp
1>reorg_layer.cpp
1>reshape_layer.cpp
1>resize_layer.cpp
1>scatterND_layer.cpp
1>scatter_layer.cpp
1>shuffle_channel_layer.cpp
1>slice_layer.cpp
1>split_layer.cpp
1>tile_layer.cpp
1>legacy_backend.cpp
1>model.cpp
1>net.cpp
1>net_cann.cpp
1>net_impl.cpp
1>net_impl_backend.cpp
1>net_impl_fuse.cpp
1>net_openvino.cpp
1>net_quantization.cpp
1>nms.cpp
1>common.cpp
1>math_functions.cpp
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): error C2819: type 'ov::Output<ov::Node>' does not have an overloaded member 'operator ->'
1>C:\\install\\openvino\\runtime\\include\\openvino/core/node_output.hpp(30,33): message : see declaration of 'ov::Output<ov::Node>'
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): message : did you intend to use '.' instead?
1>C:\\lib\\opencv\\modules\\dnn\\src\\net_openvino.cpp(255,17): error C2039: 'get_friendly_name': is not a member of 'ov::Output<ov::Node>'
1>C:\\install\\openvino\\runtime\\include\\openvino/core/node_output.hpp(30,33): message : see declaration of 'ov::Output<ov::Node>'
1>ocl4dnn_conv_spatial.cpp
1>ocl4dnn_inner_product.cpp
1>ocl4dnn_lrn.cpp
1>ocl4dnn_pool.cpp
1>ocl4dnn_softmax.cpp
1>onnx_graph_simplifier.cpp
1>onnx_importer.cpp
1>op_cann.cpp
1>op_cuda.cpp
1>op_halide.cpp
1>op_inf_engine.cpp
1>op_timvx.cpp
1>op_vkcom.cpp
1>op_webnn.cpp
1>registry.cpp
1>tf_graph_simplifier.cpp
1>tf_importer.cpp
1>tf_io.cpp
1>tflite_importer.cpp
1>THDiskFile.cpp
1>THFile.cpp
1>THGeneral.cpp
1>torch_importer.cpp
1>conv_1x1_fast_spv.cpp
1>conv_depthwise_3x3_spv.cpp
1>conv_depthwise_spv.cpp
1>conv_implicit_gemm_spv.cpp
1>gemm_spv.cpp
1>spv_shader.cpp
1>buffer.cpp
1>command.cpp
1>context.cpp
1>fence.cpp
1>internal.cpp
1>op_base.cpp
1>op_conv.cpp
1>op_matmul.cpp
1>pipeline.cpp
1>tensor.cpp
1>vk_functions.cpp
1>vk_loader.cpp
1>opencl_kernels_dnn.cpp
1>opencv_dnn_main.cpp
1>Done building project ""opencv_dnn.vcxproj"" -- FAILED.
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========

```========== Build started at 4:23 PM and took 01:44,367 minutes ==========

### Steps to reproduce

No revelvant

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-08 09:37:51,bug,Check that cv::merge input matrices are not empty,"Resolves #24242

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-09-05 22:17:59,bug,fix extendDictionary,"Fixes https://github.com/opencv/opencv/issues/24222

Now, if the size of base Dictionary is higher than markers, only the first markers in base Dictionary are taken and no new marker is added.


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-09-05 10:57:18,bug,Incorrect dictionary size returned from cv::aruco::extendDictionary,"### System Information

OpenCV version: 4.8.0
Operating System / Platform: Ubuntu 22.04
Compiler & compiler version: GCC 11.4

### Detailed description

According to the documentation for extendDictionary, if the size of baseDictionary is higher than nMarkers, only the first nMarkers in baseDictionary are taken and no new marker is added. However, if nMarkers is smaller than the size of baseDictionary, the complete baseDictionary will be returned.

```
/** @brief Extend base dictionary by new nMarkers
  *
  * @param nMarkers number of markers in the dictionary
  * @param markerSize number of bits per dimension of each markers
  * @param baseDictionary Include the markers in this dictionary at the beginning (optional)
  * @param randomSeed a user supplied seed for theRNG()
  *
  * This function creates a new dictionary composed by nMarkers markers and each markers composed
  * by markerSize x markerSize bits. If baseDictionary is provided, its markers are directly
  * included and the rest are generated based on them. If the size of baseDictionary is higher
  * than nMarkers, only the first nMarkers in baseDictionary are taken and no new marker is added.
  */
CV_EXPORTS_W Dictionary extendDictionary(int nMarkers, int markerSize, const Dictionary &baseDictionary = Dictionary(),
                                         int randomSeed=0);
```

### Steps to reproduce

```
cv::aruco::Dictionary base_dictionary = cv::aruco::getPredefinedDictionary(cv::aruco::DICT_4X4_250);
cv::aruco::Dictionary custom_dictionary= cv::aruco::extendDictionary(150, 4, base_dictionary);
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-01 12:46:25,bug,distanceTransform for inputs with large step and height,"### Pull Request Readiness Checklist

resolves https://github.com/opencv/opencv/issues/23895

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-08-31 19:00:06,bug,"Fix ""use after free"" issue in `essential_solver.cpp`","The address sanitizer highlighted this issue in our code base. It
looks like the code is currently grabbing a pointer to a temporary
object and then performing operations on it.

I printed some information right before the asan crash:

    eigensolver address: 0x7f0ad95032f0
    eigensolver size: 4528
    eig_vecs_ ptr: 0x7f0ad95045e0
    eig_vecs_ offset: 4848

This shows that `eig_vecs_` points past the end of `eigensolver`. In
other words, it points at the temporary object created by the
`eigensolver.eigenvectors()` call.

Compare the docs for `.eigenvalues()`:
https://eigen.tuxfamily.org/dox/classEigen_1_1EigenSolver.html#a0f507ad7ab14797882f474ca8f2773e7
to the docs for `.eigenvectors()`:
https://eigen.tuxfamily.org/dox/classEigen_1_1EigenSolver.html#a66288022802172e3ee059283b26201d7

The difference in return types is interesting. `.eigenvalues()`
returns a reference. But `.eigenvectors()` returns a matrix.

This patch here fixes the problem by saving the temporary object and
then grabbing a pointer into it.

This is a curated snippet of the original asan failure:

    ==12==ERROR: AddressSanitizer: stack-use-after-scope on address 0x7fc633704640 at pc 0x7fc64f7f1593 bp 0x7ffe8875fc90 sp 0x7ffe8875fc88
    READ of size 8 at 0x7fc633704640 thread T0
        #0 0x7fc64f7f1592 in cv::usac::EssentialMinimalSolverStewenius5ptsImpl::estimate(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<cv::Mat, std::__1::allocator<cv::Mat> >&) const /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/essential_solver.cpp:181:48
        #1 0x7fc64f915d92 in cv::usac::EssentialEstimatorImpl::estimateModels(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<cv::Mat, std::__1::allocator<cv::Mat> >&) const /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/estimator.cpp:110:46
        #2 0x7fc64fa74fb0 in cv::usac::Ransac::run(cv::Ptr<cv::usac::RansacOutput>&) /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/ransac_solvers.cpp:152:58
        #3 0x7fc64fa6cd8e in cv::usac::run(cv::Ptr<cv::usac::Model const> const&, cv::_InputArray const&, cv::_InputArray const&, int, cv::Ptr<cv::usac::RansacOutput>&, cv::_InputArray const&, cv::_InputArray const&, cv::_InputArray const&, cv::_InputArray const&) /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/ransac_solvers.cpp:1010:16
        #4 0x7fc64fa6fb46 in cv::usac::findEssentialMat(cv::_InputArray const&, cv::_InputArray const&, cv::_InputArray const&, int, double, double, cv::_OutputArray const&) /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/ransac_solvers.cpp:527:9
        #5 0x7fc64f3b5522 in cv::findEssentialMat(cv::_InputArray const&, cv::_InputArray const&, cv::_InputArray const&, int, double, double, int, cv::_OutputArray const&) /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/five-point.cpp:437:16
        #6 0x7fc64f3b7e00 in cv::findEssentialMat(cv::_InputArray const&, cv::_InputArray const&, cv::_InputArray const&, int, double, double, cv::_OutputArray const&) /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/five-point.cpp:486:12
        ...

    Address 0x7fc633704640 is located in stack of thread T0 at offset 17984 in frame
        #0 0x7fc64f7ed4ff in cv::usac::EssentialMinimalSolverStewenius5ptsImpl::estimate(std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<cv::Mat, std::__1::allocator<cv::Mat> >&) const /proc/self/cwd/external/com_github_opencv_opencv/modules/calib3d/src/usac/essential_solver.cpp:36

      This frame has 63 object(s):
        [32, 56) 'coefficients' (line 38)
        [96, 384) 'ee' (line 55)
        ...
        [13040, 17568) 'eigensolver' (line 142)
        [17824, 17840) 'ref.tmp518' (line 143)
        [17856, 17872) 'ref.tmp523' (line 144)
        [17888, 19488) 'ref.tmp524' (line 144) <== Memory access at offset 17984 is inside this variable
        [19616, 19640) 'ref.tmp532' (line 169)
        ...

The crash report says that we're accessing a temporary object from
line 144 when we shouldn't be. Line 144 looks like this:
https://github.com/opencv/opencv/blob/4.6.0/modules/calib3d/src/usac/essential_solver.cpp#L144

    const auto * const eig_vecs_ = (double *) eigensolver.eigenvectors().real().data();

We are using version 4.6.0 for this, but the problem is present on the
4.x branch.

Note that I am dropping the `.real()` call here. I think that is safe because
of the code further down (line 277 in the most recent version):
```cpp
    const int eig_i = 20 * i + 12; // eigen stores imaginary values too
```
The code appears to expect to have to skip doubles for the imaginary parts
of the complex numbers.

Admittedly, I couldn't find a test case that exercised this code path to 
validate correctness."
opencv/opencv,2023-08-30 07:35:15,bug,cv::max function in-place operation gets wrong result,"### System Information

OpenCV version: 4.8.0
Operating System / Platform: Windows 11
Compiler & compiler version: MSVC 19

### Detailed description

Given cv::Mat A and a B(B is a 1x1x1 mat), calling function cv::max(A, B, A) and cv::max(A, B, B) gets different result. It seems that the former calling is right. The latter calling results in B is set to the same size with A, however, every pixel of B will be set to 205(0xCD).

### Steps to reproduce

```
cv::Mat A = cv::Mat::ones(100, 100, CV_8UC1) * 150;
cv::Mat B(1, 1, CV_8UC1);
B.setTo(160);
cv::max(A, B, A); 
cv::max(A, B, B);//wrong
```

I checked source code, seems to cause by line 238 in [https://github.com/opencv/opencv/blob/4.x/modules/core/src/arithm.cpp](url)

Wondering it is count for a bug?

### Issue submission checklist

- [X] I report the issue, it's not a question
- [x] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-28 03:22:52,bug,Fix compilation on arm64 with FP16 when disabled,"If building with -mcpu=native or any other setting which implies the current CPU has FP16 but with intrinsics disabled, we mistakenly try to use it even though convolution.hpp conditionally defines it correctly based on whether we should *use it*. convolution.cpp on the other hand was mismatched and trying to use it if the CPU supported it, even if not enabled in the build system.

Make the guards match.

Bug: https://bugs.gentoo.org/913031

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-08-24 13:54:05,bug,Fix compilation when forcing later C++.,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
"
opencv/opencv,2023-08-19 11:12:13,bug,Fixed the channels when capturing yuv422 with v4l2 backend,"example to reproduce the problem
```cpp
#include <iostream>

#include <opencv2/core.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/videoio.hpp>

using namespace cv;
using namespace std;

void help_func(VideoCapture& cap) {
  int height      = cap.get(cv::CAP_PROP_FRAME_HEIGHT);
  int width       = cap.get(cv::CAP_PROP_FRAME_WIDTH);
  int pixel_type  = cap.get(cv::CAP_PROP_FORMAT);
  int channels    = CV_MAT_CN(pixel_type);
  int pixel_bytes = CV_ELEM_SIZE(pixel_type);
  bool to_bgr     = static_cast<bool>(cap.get(cv::CAP_PROP_CONVERT_RGB));

  std::cout << ""backend: "" << cap.getBackendName() << std::endl;
  std::cout << std::hex << ""fourcc: "" << static_cast<int>(cap.get(cv::CAP_PROP_FOURCC)) << std::endl;
  std::cout << std::boolalpha << ""to_bgr: "" << to_bgr << std::endl;
  std::cout << std::dec << ""height: "" << height << "" width: "" << width << "" channels: "" << channels
            << "" pixel_bytes: "" << pixel_bytes << std::endl;

  std::cout << ""-----------------------------------------"" << std::endl;
}

int main(int, char**) {

  VideoCapture cap;
  cap.open(""/dev/video0"");
  if (!cap.isOpened()) {
    cerr << ""ERROR! Unable to open camera\\n"";
    return -1;
  }

  {
    help_func(cap);
  }

  {
    cap.set(cv::CAP_PROP_FRAME_HEIGHT, 1080);
    cap.set(cv::CAP_PROP_FRAME_WIDTH, 1920);
    cap.set(cv::CAP_PROP_CONVERT_RGB, 0);
    help_func(cap);
  }

  // {
  //   cap.set(cv::CAP_PROP_CONVERT_RGB, 0);
  //   cap.set(cv::CAP_PROP_FRAME_HEIGHT, 1080);
  //   cap.set(cv::CAP_PROP_FRAME_WIDTH, 1920);
  //   help_func(cap);
  // }

  Mat frame;
  int frame_idx = 0;
  while (cap.read(frame)) {
    std::cout << ""frame index: "" << frame_idx++ << std::endl;
    help_func(cap);
    if (frame.empty()) {
      cerr << ""ERROR! blank frame grabbed\\n"";
      break;
    }
    Mat bgr;
    if (cap.get(cv::CAP_PROP_CONVERT_RGB)) {
      bgr = frame;
    } else {
      cv::cvtColor(frame, bgr, cv::COLOR_YUV2BGR_YUYV);
    }

    imshow(""frame"", bgr);
    if (waitKey(5) >= 0) {
      break;
    }
  }

  return 0;
}
```
The above code will get the wrong channels. By changing lines 41-45 like below, can get the correct channels.
<img width=""747"" alt=""code"" src=""https://github.com/opencv/opencv/assets/16932438/55f44463-8465-4dba-a979-e71a50d58008"">
This is because `cap.set(cv::CAP_PROP_FRAME_HEIGHT, 1080);` and `cap.set(cv::CAP_PROP_FRAME_WIDTH, 1920);` reinitialize the `frame`, but `cap.set(cv::CAP_PROP_CONVERT_RGB, 0);` not.
Log info.
<img width=""691"" alt=""log"" src=""https://github.com/opencv/opencv/assets/16932438/236e3b26-f5b2-447a-b202-bcd607c71af6"">
We can also observe that we get the correct channels in the while loop. This is because:
https://github.com/opencv/opencv/blob/ca0bd70cde431b1dd211254011dd9bcf965f582f/modules/videoio/src/cap_v4l.cpp#L2309-L2310
reinitialize the `frame`.
"
opencv/opencv,2023-08-17 08:40:22,bug,cv::VideoWriter with CAP_OPENCV_MJPEG api creates broken (or just unfinished) video file.,"### System Information

OpenCV version: 4.8.0
Windows 11
Microsoft Visual Studio 2022 64bit

### Detailed description

If I create a certain video using cv::VideoWriter with CAP_OPENCV_MJPEG api, I can load the video correctly using cv::VideoCapture, but I get an error message during loading.
```
[mjpeg @ 0000015a20f1df00] error count: 268435456
[mjpeg @ 0000015a20f1df00] error y=0 x=0
```
Also I can watch the created video using Windows Media Player without any errors.
If I create the same video using CAP_FFMPEG api, I don't get error messages.
The error reproduces only on a certain video content, so I can create a different video with the same size without error messages.

### Steps to reproduce

```
#include <opencv2/highgui.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/imgproc.hpp>

#include <iostream>

int w = 16;
int h = 16;
int num_frames = 1;

void saveVideo(std::string filename, cv::VideoCaptureAPIs api, long base_color) {
    cv::VideoWriter writer(filename, api, cv::VideoWriter::fourcc('M', 'J', 'P', 'G'), 30, cv::Size(w, h), true);
    for (long i = 0; i < num_frames; ++i) {
        cv::Mat frame1(h, w, CV_8UC3);
        for (long x = 0; x < w; x++) {
            for (long y = 0; y < h; y++) {
                frame1.at<cv::Vec3b>(y, x) = cv::Vec3b((base_color * x) % 256, base_color + 2, base_color + 3);
            }
        }
        writer.write(frame1);
    }
    writer.release();
}

void loadVideo(std::string filename) {
    std::cout << ""===== loading "" << filename << std::endl << std::flush;
    cv::VideoCapture cap(filename, cv::CAP_FFMPEG);
    if (!cap.isOpened()) {
        std::cout << ""No video stream detected"" << std::endl;
        return;
    }
    cv::Mat frame;
    uint32_t frame_cnt = 0;
    while (true) {
        cap >> frame;
        if (frame.empty()) {
            break;
        }

    }
    cap.release();
    std::cout << ""===== loaded "" << filename << std::endl << std::flush;
}

int main(int argc, char* argv[]) {
    saveVideo(""out1.avi"", cv::CAP_FFMPEG, 147);
    saveVideo(""out2.avi"", cv::CAP_FFMPEG, 146);
    saveVideo(""out3.avi"", cv::CAP_OPENCV_MJPEG, 147);
    saveVideo(""out4.avi"", cv::CAP_OPENCV_MJPEG, 146);

    loadVideo(""out1.avi"");
    loadVideo(""out2.avi"");
    loadVideo(""out3.avi"");
    loadVideo(""out4.avi""); // prints error message
}
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-16 19:48:37,bug,add missing include,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work (https://github.com/microsoft/vcpkg/issues/33200)
"
opencv/opencv,2023-08-12 20:31:30,bug,Error in python but not in C++,"### System Information

opencv 4.8 even with branch 4.x
python binding build using this branch



I changed source code of void Net::setInput(InputArray blob, const String& name, double scalefactor, const Scalar& mean) to whatch data

New source code is 

```
void Net::setInput(InputArray blob, const String& name, double scalefactor, const Scalar& mean)
{
    CV_TRACE_FUNCTION();
    CV_TRACE_ARG_VALUE(name, ""name"", name.c_str());
    CV_Assert(impl);
    Mat blobx = blob.getMat();
    std::cout << name << "" layer\\n"";
    std::cout << blobx.channels() << "" channel\\n"";
    for (int i = 0; i < blobx.dims; i++)
    {
        std::cout << ""dim  "" << i << "" : "" << blobx.size[i] << ""\\t"";
    }
    std::cout << ""\\n"";

    return impl->setInput(blob, name, scalefactor, mean);
}

```

### Detailed description

When I run my python code I have got an error : 

```
image_embeddings layer
1 channel
dim  0 : 1      dim  1 : 256    dim  2 : 64     dim  3 : 64
coord shape :  (1, 5, 2)
point_coords layer
2 channel
dim  0 : 1      dim  1 : 5**
point_labels layer
1 channel
dim  0 : 1      dim  1 : 2
mask_input layer
1 channel
dim  0 : 1      dim  1 : 1      dim  2 : 256    dim  3 : 256
1 channel
dim  0 : 1      dim  1 : 1
[ERROR:0@873.187] global net_impl.cpp:1169 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively OPENCV/DNN: [InnerProduct]:(onnx_node!/MatMul): getMemoryShapes() throws exception. inputs=1 outputs=0/1 blobs=1
[ERROR:0@873.187] global net_impl.cpp:1172 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively     input[0] = [ 1 5 ]
[ERROR:0@873.187] global net_impl.cpp:1180 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively     blobs[0] = CV_32FC1 [ 128 2 ]
[ERROR:0@873.187] global net_impl.cpp:1182 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively Exception message: OpenCV(4.8.0-dev) C:\\lib\\opencv\\modules\\dnn\\include\\opencv2/dnn/shape_utils.hpp:243: error: (-2:Unspecified error) in function 'int __cdecl cv::dnn::dnn4_v20230620::normalize_axis(int,int)'
> :
>     'axis >= -dims && axis < dims'
> where
>     'axis' is 2

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 22, in <module>
cv2.error: OpenCV(4.8.0-dev) C:\\lib\\opencv\\modules\\dnn\\include\\opencv2/dnn/shape_utils.hpp:243: error: (-2:Unspecified error) in function 'int __cdecl cv::dnn::dnn4_v20230620::normalize_axis(int,int)'
> :
>     'axis >= -dims && axis < dims'
> where
>     'axis' is 2
```

<s>I think problem is 
```
coord shape :  (1, 5, 2)
point_coords layer
2 channel
dim  0 : 1      dim  1 : 5

```
Why dim size  is not 1, 5, 2</s>



When I run same code in c++



result is 
```
image_embeddings layer
1 channel
dim  0 : 1      dim  1 : 256    dim  2 : 64     dim  3 : 64
point_coords layer
1 channel
dim  0 : 1      dim  1 : 5      dim  2 : 2
point_labels layer
1 channel
dim  0 : 1      dim  1 : 5
mask_input layer
1 channel
dim  0 : 1      dim  1 : 1      dim  2 : 256    dim  3 : 256
has_mask_input layer
1 channel
dim  0 : 1      dim  1 : 1


```


dim size for point_coords is
point_coords layer
1 channel
dim  0 : 1      dim  1 : 5      dim  2 : 2
That's good in C++


### Steps to reproduce

Python code

```
import numpy as np
import cv2 as cv

input_point = np.array([[605, 205]])
input_label = np.array([1]) 

coord = np.concatenate([input_point, np.zeros((4,2))], axis=0)[None, :, :].astype(np.float32)
label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)
mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)
has_mask_input = np.zeros(1, dtype=np.float32)

decoder_onnx_ocv__name =  ""sam_vit_b.fixed.nopost.sim.onnx""

net_decoder = cv.dnn.readNet(decoder_onnx_ocv__name)
net_decoder.setInput(np.zeros((1, 256, 64, 64), dtype=np.float32), ""image_embeddings"")
print(""coord shape : "",coord.shape)
net_decoder.setInput(coord, ""point_coords"")
net_decoder.setInput(label, ""point_labels"")
net_decoder.setInput(mask_input, ""mask_input"")
net_decoder.setInput(has_mask_input, ""has_mask_input"")
net_decoder.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)
net_decoder.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)
output = net_decoder.forward([""iou_predictions"", ""low_res_masks""])

```

C++ code
```
         Net netMask = readNet(""sam_vit_b.fixed.nopost.sim.onnx"");
        Mat blob(vector<int>{1, 256, 64, 64}, CV_32FC1, Scalar::all(0));
        netMask.setInput(blob, ""image_embeddings"");
        Mat inputPoint = (Mat_<float>(5, 2) << 590, 620, 0, 0, 0, 0, 0, 0, 0, 0);
        vector<int> dimPoint{ 1, 5, 2 };
        Mat blobPoint = inputPoint.reshape(1, dimPoint);
        netMask.setInput(blobPoint, ""point_coords"");
        Mat blobLabel(1, 5, CV_32FC1, Scalar(-1));
        blobLabel.at<float>(0, 0) = 1;
        netMask.setInput(blobLabel, ""point_labels"");
        Mat maskInput(vector<int> {1, 1, 256, 256}, CV_32FC1);
        netMask.setInput(maskInput, ""mask_input"");
        Mat hasMaskInputMat(vector<int> {1}, CV_32FC1, Scalar::all(0));
        netMask.setInput(hasMaskInputMat, ""has_mask_input"");
        vector<Mat> blobOut(2);
        const vector<string> outputName{ ""iou_predictions"", ""low_res_masks"" };
        netMask.setPreferableBackend(DNN_BACKEND_OPENCV);
        netMask.setPreferableTarget(DNN_TARGET_CPU);
        netMask.forward(blobOut, outputName);   
        return 0; 

```

model is [here ](https://drive.usercontent.google.com/download?id=1rY5zarZO89JIIQKu1pcGrIP4ikrtvX0Q&export=download&confirm=t&uuid=373c383b-7f5a-45b8-aa39-3c923f8db481)

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-12 10:03:04,bug,how to use ipp_minMaxIdx?,"### System Information

OpenCV version: 4.1.0
Operating System / Platform: CentOS 7.9
Compiler & compiler version: GCC 4.8.5



### Detailed description

minMaxIdx function takes a long time in OpenCV 4.1.0，but OpenCV 3.4.3 very fast




### Steps to reproduce

I noticed that in modules/core/src/minmax.cpp head

```cpp
#undef HAVE_IPP
#undef CV_IPP_RUN_FAST
#define CV_IPP_RUN_FAST(f, ...)
#undef CV_IPP_RUN
#define CV_IPP_RUN(c, f, ...)
```
what problem to fix? When can we push it

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-11 14:02:38,bug,Fixed buffer overrun; removed the last two uses of sprintf,"Prefer snprintf, which can never overflow.

In one case I cheated and used strcpy, because I cannot figure out the buffer size at that point in the code.
"
opencv/opencv,2023-08-11 08:14:32,bug,PYTHON3_PACKAGES_PATH converted to absolute path on first CMake run,"### System Information

OpenCV version: 4.8.0
Operation System / Patform: Windows 11
Compiler & compiler version: Visual Studio 2022

### Detailed description

The CMake scripts to configure the `PYTHON3_PACKAGES_PATH` generally support both, absolute and relative paths for the latter. In particular, the generated `config.py` of the cv2 module will contain the correct path pointing to the OpenCV libraries in both cases, either as an absolute or relative path, respectively, depending on an absolute or relative prefix.

However, running CMake with a user-defined, relative `PYTHON3_PACKAGES_PATH` always replaces it with an absolute path during the first run:
```
git clone https://github.com/opencv/opencv.git OpenCV
cmake -S . -B build -D ""PYTHON3_PACKAGES_PATH=python""
```
This generates the following build configuration:
```
--   Python 3:
--     Interpreter:                 C:/Program Files/Python/python.exe (ver 3.10.11)
--     Libraries:                   C:/Program Files/Python/libs/python310.lib (ver 3.10.11)
--     numpy:                       C:/Program Files/Python/lib/site-packages/numpy/core/include (ver 1.23.5)
--     install path:                C:/OpenCV/python/cv2/python-3.10
```
Note that the install path defined by `PYTHON3_PACKAGES_PATH` contains the **source directoy** as a prefix, `C:/OpenCV` here. After re-running cmake with the same arguments, the relative path is set correctly:
```
cmake -S . -B build -D ""PYTHON3_PACKAGES_PATH=python""
```
```
--   Python 3:
--     Interpreter:                 C:/Program Files/Python/python.exe (ver 3.10.11)
--     Libraries:                   C:/Program Files/Python/libs/python310.lib (ver 3.10.11)
--     numpy:                       C:/Program Files/Python/lib/site-packages/numpy/core/include (ver 1.23.5)
--     install path:                python/cv2/python-3.10
```
We investigated the issue already. Here, `PYTHON3_PACKAGES_PATH` is set by `find_python()` inside `OpenCVDetectPython.cmake` and it seems that this function does not properly handle absolute and relative prefixes. Still, we wanted to report this bug first. We think that something needs to be fixed here, running CMake twice is a rather unsatisfying workaround. Or does somebody know a different solution for this issue?

### Steps to reproduce

1. Clone the repository
2. Run CMake first with `PYTHON3_PACKAGES_PATH` set to a relative path converts the path to an absolute path.
3. Re-run CMake with the same commands and the path correctly remains relative.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [x] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-10 08:44:03,bug,videoio: fix camera opening with GStreamer plugin,"related #24133, #23056, #23937

Steps to reproduce:
* build OpenCV with GStreamer backend as plugin (`-DVIDEOIO_PLUGIN_LIST=gstreamer`)
* open camera by index using GStreamer backend (`VideoCapture cap(0, CAP_GSTREAMER)`)

Result: failure, capture is not opened, when using GStreamer backend as built-in camera can be opened

This was happening due to bug in plugin wrapper."
opencv/opencv,2023-08-08 12:18:35,bug,Fix bug at blobFromImagesWithParams,"## `blobFromImagesWithParams`(image-s) apply swap scalefactor for every images when swapRB is true. This patch moves the swap scalefactor code out of the loop, that fix the above issue.

We would like to improve the recently introduced function `blobFromImagesWithParams` with respect to three minor issues that we noticed:

- Currently, `mean` and `scalefactor` are recreated for every image, even though they should be reused. Maybe the compiler is smart enough to optimize this, still we think the variable should be created / swapped before the main loop.
- Next, we would like to improve the function's robustness with respect to inconsistent arguments, i.e. if a single-channel image is passed but `swapRB` is true, as explained below.
- Finally, we made some minor modifications to follow the coding guidelines (`if - else if - else` instead of nested else).

Function wise, there are two improvements from this:

1. Currently, if the user passes a single-channel image in combination with `mean` and `scalefactor` values, but unintentionally sets `swapRB` to true, the whole result can be silently zeroed. The `mean` and `scalefactor` arrays will only contain a single value (whereas the other indices' values remain 0), but `swapRB` will swap out the true mean / scalefactor values and swap in zeros for these instead. There is no warning or assertion, only the whole image will be zeroed by the following multiplication. Therefore, we suggest to only swap these values if the image is not a single-channel image, i.e. `mean` and `scalefactor` contain correct values for all channels, therefore add `if (nch > 2)`.
2. Next, a related issue exists in combination with `DNN_LAYOUT_NHWC`. If the user passes this in combination with a single-channel image and `swapRB` is true, the function produces the following error:
```
OpenCV\\modules\\core\\src\\copy.cpp:320: error: (-215:Assertion failed) channels() == CV_MAT_CN(dtype) in function 'cv::Mat::copyTo'
```
This results from first converting the image to RGB (even though it is grayscale instead of the expected BGR layout) and, thereafter, copying the resulting three-channel image into the single-channel blob. This is avoided by checking `nch > 2` here, too, which is also consistent to the function's implementation of the `DNN_LAYOUT_NCHW` case.

Both changes can be summarized such that `swapRB` only has an effect if the image has at least three channels. Most importantly, nothing changes at all if the arguments are consistent.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-08-08 11:20:13,bug,fix charuco checkBoard,"Fixes #23905

checkBoard() is an internal function that checks that the found board has the correct structure

1. fixed a bug with checking extraneous markers:
```
                if (find(boardIds.begin(), boardIds.end(), idMaker) == boardIds.end())
                    continue;
```
2. fixed an indexing bug:
```
                const int nearestMarkerId1 = boardIds[nearestMarkerIdx[chId][0]];
                const int nearestMarkerId2 = boardIds[nearestMarkerIdx[chId][1]];
```
3. updated docs for getNearestMarkerIdx()/getNearestMarkerCorners()
4. added `checkBoardStructure` flag, this flag allows you to return the old behavior or use your custom board structure check.
### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-08-07 20:09:16,bug,OCL_FP16 MatMul with large batch,"### Pull Request Readiness Checklist

* resolves https://github.com/opencv/opencv/issues/24111
* Updates required for actual models from https://github.com/opencv/opencv_extra/pull/1080

**Merge with extra**: https://github.com/opencv/opencv_extra/pull/1080

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake

```
force_builders=Linux OpenCL,Win64 OpenCL
buildworker:Linux OpenCL=linux-1
```"
opencv/opencv,2023-08-05 09:35:10,bug,DNN: OpenCL FP16 tests are broken (Test_ONNX_layers.MatMul_init_bcast) (2023-08-03),"OpenCL device: Intel iGPU

Nightly builds:
- Linux: https://pullrequest.opencv.org/buildbot/builders/4_x-lin64/builds/100209

```
[ RUN      ] Test_ONNX_layers.MatMul_init_bcast/1, where GetParam() = OCV/OCL_FP16
[ INFO:0@226.817] global onnx_importer.cpp:827 populateNet DNN/ONNX: loading ONNX v8 model produced by 'matmul_init_bcast'. Number of nodes = 1, initializers = 1, inputs = 2, outputs = 1
[ INFO:0@226.817] global onnx_importer.cpp:728 parseOperatorSet DNN/ONNX: ONNX opset version = 17
[ INFO:0@226.817] global onnx_importer.cpp:1001 handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [MatMul]:(onnx_node_output_0!output) from domain='ai.onnx'
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:76: Failure
Expected: (normL1) <= (l1), actual: 1.22411 vs 0.004
matmul_init_bcast  |ref| = 6.9979562759399414
/build/4_x-lin64/opencv/modules/dnn/test/test_common.impl.hpp:79: Failure
Expected: (normInf) <= (lInf), actual: 6.99796 vs 0.02
matmul_init_bcast  |ref| = 6.9979562759399414
[ INFO:0@226.817] global ts.cpp:857 testTearDown Memory_usage (OpenCL): 1488 (base=0  current=0)
[  FAILED  ] Test_ONNX_layers.MatMul_init_bcast/1, where GetParam() = OCV/OCL_FP16 (1 ms)
```

- Windows: https://pullrequest.opencv.org/buildbot/builders/4_x-win64-vc16/builds/100211

```
[ RUN      ] Test_ONNX_layers.MatMul_init_bcast/1, where GetParam() = OCV/OCL_FP16
[ INFO:0@495.097] global onnx_importer.cpp:835 cv::dnn::dnn4_v20230620::ONNXImporter::populateNet DNN/ONNX: loading ONNX v8 model produced by 'matmul_init_bcast'. Number of nodes = 1, initializers = 1, inputs = 2, outputs = 1
[ INFO:0@495.097] global onnx_importer.cpp:728 cv::dnn::dnn4_v20230620::ONNXImporter::parseOperatorSet DNN/ONNX: ONNX opset version = 17
[ INFO:0@495.097] global onnx_importer.cpp:1006 cv::dnn::dnn4_v20230620::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [MatMul]:(onnx_node_output_0!output) from domain='ai.onnx'
C:\\build\\4_x-win64-vc16\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(76): error: Expected: (normL1) <= (l1), actual: 1.22411 vs 0.004
matmul_init_bcast  |ref| = 6.9979562759399414
C:\\build\\4_x-win64-vc16\\opencv\\modules\\dnn\\test\\test_common.impl.hpp(79): error: Expected: (normInf) <= (lInf), actual: 6.99796 vs 0.02
matmul_init_bcast  |ref| = 6.9979562759399414
[ INFO:0@495.098] global ts.cpp:857 cvtest::testTearDown Memory_usage (OpenCL): 1488 (base=0  current=0)
[  FAILED  ] Test_ONNX_layers.MatMul_init_bcast/1, where GetParam() = OCV/OCL_FP16 (2 ms)
```"
opencv/opencv,2023-08-03 15:28:54,bug,Add a test for backends on retrieving intermediate blobs in any order,"### System Information

 OpenCV version: 4.8.0

### Detailed description

Observed with OpenVINO backend that it cannot feed output tensors properly in case of wrong order during the execution.

### Steps to reproduce

related: https://github.com/opencv/opencv/pull/24039 (see `Test_TFLite.max_unpooling`)

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-08-02 14:31:48,bug,macOS Fullscreen broken,"##### System information (version)
- OpenCV => 4.8 (python)
- Operating System / Platform => macOS Ventura (13.3)


##### Details
The details can be found here:
https://github.com/opencv/opencv-python/issues/804

##### Fix
I'm working on a fix in [0xMihir/opencv](https://github.com/0xMihir/opencv), and I'm going to open a draft PR, but I noticed that whenever I create a window, and set it to fullscreen, I have to call waitKey for a few seconds until the title bar hides.

"
opencv/opencv,2023-08-01 08:35:39,bug,DNN module does not use OpenVINO backend by default,"### System Information

OpenCV version: 4.8.0
Operating System / Platform: Windows 11
Compiler & compiler version: VS 2022 (17.6.3)
Python version: 3.10.11

### Detailed description

OpenCV supports OpenVINO as a backend to speedup DNNs at inference time, which requires to compile with OpenVINO support.   Excerpt from the build config:
```
General configuration for OpenCV 4.8.0 =====================================
  ...
  Parallel framework:            TBB (ver 2021.9 interface 12090)
  ...
  Other third-party libraries:
    OpenVINO:                    YES (2022.3.0)
```
So OpenVINO is available. Thereafter, the documentation states
```
If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT
means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV.
```
To us, this reads as that by default OpenVINO / DNN_BACKEND_INFERENCE_ENGINE should be used by default in this build configuration. However, performing some tests using a ResNet model from the model zoo shows a different image:
```
# test script:
import os
import time
import numpy as np
import cv2 as cv

# download link: https://github.com/opencv/opencv_zoo/tree/main/models/image_classification_ppresnet
model = cv.dnn.readNet(os.path.join(os.path.dirname(__file__), 'image_classification_ppresnet50_2022jan.onnx'))

# optionally select a backend
#model.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)
#model.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)
#model.setPreferableBackend(cv.dnn.DNN_BACKEND_DEFAULT)

# test prediction time
model.setInput(cv.dnn.blobFromImage(np.ones((224,224,3), dtype=np.float32)))
for i in range(5): # warmup
    out = model.forward()
# test 100 predictions
t1 = time.time()
for i in range(100):
    out = model.forward()

t2 = time.time()
print(t2-t1)
input()
```
Running this script this way (i.e. no explicit backend) or setting either `cv.dnn.DNN_BACKEND_OPENCV` or `cv.dnn.DNN_BACKEND_DEFAULT` requires about 15-16 seconds on the testing machine (i7-10510U CPU) but only about 9 seconds with setting `cv.dnn.DNN_BACKEND_INFERENCE_ENGINE`. Thus, our conclusion is that OpenVINO is actually *not* used by default and requires an explicit setting of the respective backend.

Furthermore, checking the integer values of the backends shows
```
>>> cv.dnn.DNN_BACKEND_DEFAULT
0
>>> cv.dnn.DNN_BACKEND_OPENCV
3
>>> cv.dnn.DNN_BACKEND_INFERENCE_ENGINE
2
```
which means that the default values do not align with the ones explained in documentation.

### Steps to reproduce

Running the provided code with enabling OpenVINO, i.e. adding the line
```
model.setPreferableBackend(cv.dnn.DNN_BACKEND_INFERENCE_ENGINE)
```
is significantly faster than any of the other options.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-29 19:36:04,bug,Inconsistency when using OpenCL,"### System Information

OpenCV version: 4.8.0 and latest master
Operating System / Platform: Arch Linux
Compiler & compiler version: gcc (GCC) 13.1.1 20230714

### Detailed description

Using the attached code for image translation led to two different results:

1. When not using OpenCL, the translation is correct.
2. However, when using `opencl-nvidia`, the translation degrades the image if the output and input are the same `UMat` in `warpAffine`. 
![registered_screenshot_29 07 2023](https://github.com/opencv/opencv/assets/23381534/f52eaddb-59b1-4ab3-add7-e3959aaac6bc)




### Steps to reproduce

```cpp
#include <opencv2/highgui.hpp>
#include <opencv2/imgproc.hpp>

using namespace cv;

int main() {
  UMat frame, out;
  imread(""astro.jpg"", IMREAD_GRAYSCALE).copyTo(frame);

  Mat H = (Mat_<float>(2, 3) << 1.0, 0.0, 20, 0.0, 1.0, 20);
  warpAffine(frame, frame, H, frame.size());

  imshow(""frame"", frame);

  // This will fix the problem:
  /*warpAffine(frame, out, H, frame.size());
  imshow(""frame"", out);*/
  waitKey();
}
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-26 22:41:07,bug,Python typing refinement for dnn_registerLayer/dnn_unregisterLayer functions,"This patch introduces typings generation for `dnn_registerLayer`/`dnn_unregisterLayer` manually defined in [`cv2/modules/dnn/misc/python/pyopencv_dnn.hpp`](https://github.com/opencv/opencv/blob/4.x/modules/dnn/misc/python/pyopencv_dnn.hpp)

Updates:

- Add `LayerProtocol` to `cv2/dnn/__init__.pyi`:

    ```python
    class LayerProtocol(Protocol):
        def __init__(
            self, params: dict[str, DictValue],
            blobs: typing.Sequence[cv2.typing.MatLike]
        ) -> None: ...

        def getMemoryShapes(
            self, inputs: typing.Sequence[typing.Sequence[int]]
        ) -> typing.Sequence[typing.Sequence[int]]: ...

        def forward(
            self, inputs: typing.Sequence[cv2.typing.MatLike]
        ) -> typing.Sequence[cv2.typing.MatLike]: ...
    ```

- Add `dnn_registerLayer` function to `cv2/__init__.pyi`:

    ```python
    def dnn_registerLayer(layerTypeName: str,
                          layerClass: typing.Type[LayerProtocol]) -> None: ...
    ```

- Add `dnn_unregisterLayer` function to `cv2/__init__.pyi`:

    ```python
    def dnn_unregisterLayer(layerTypeName: str) -> None: ...
    ```
### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-26 13:52:03,bug,feat: add typing stub for redirectError,"`redirectError` is defined in `cv2_util.cpp` and manually exported in `cv2.cpp`

Python interface for `redirectError`:

```python
def redirectError(
    onError: Callable[[int, str, str, str, int], None] | None
) -> None: ...
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-25 08:43:12,bug,[Build] popcnt is not supported on windows ARM,"### System Information

OpenCV version: 4.8.0
Operating system: Windows 11 ARM64
Compiler: MSVC v143 - VS 2022

### Detailed description

Error building opencv on windows ARM
`libopenjpg2.lib(ht_dec.obj): error LNK2019: unresolved external symbol __popcnt referenced in function opt_t1_ht_decode_cblk`



### Steps to reproduce

To reproduce install:
```
1. MSVC v143 - VS 2022 C++ ARM64/ARM64EC build tools (latest)
2. MSVC v143 - VS 2022 C++ ARM64/ARM64EC Spectre-mitigated libs (Latest)
3. Windows 11 SDK (10.0.22621.0)
4. C++ ATL for latest v143 build tools with SPectre Mitigations (ARM64/ARM64EC)
5. C++ ATL for latest v143 build tools (ARM64/ARM64EC)
```
Then run:
`python setup.py bdist_wheel`


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-21 13:14:22,bug,inference with onnx and opencv gives different results,"### System Information

```
General configuration for OpenCV 4.8.0-dev =====================================
  Version control:               4.8.0-64-g1f7025f028-dirty

  Extra modules:
    Location (extra):            C:/lib/opencv_contrib/modules
    Version control (extra):     4.8.0-4-gd89b2b9b

  Platform:
    Timestamp:                   2023-06-29T16:39:13Z
    Host:                        Windows 10.0.22621 AMD64
    CMake:                       3.26.1
    CMake generator:             Visual Studio 17 2022
    CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe
    MSVC:                        1935
    Configuration:               Debug Release

  CPU/HW features:
    Baseline:                    SSE SSE2 SSE3
      requested:                 SSE3
    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX
      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
      SSE4_1 (18 files):         + SSSE3 SSE4_1
      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
      AVX (8 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
      AVX2 (37 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
      AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX

  C/C++:
    Built as dynamic libs?:      YES
    C++ standard:                11
    C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe  (ver 19.35.32215.0)
    C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MD /O2 /Ob2 /DNDEBUG
    C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MDd /Zi /Ob0 /Od /RTC1
    C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.35.32215/bin/Hostx64/x64/cl.exe
    C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MD /O2 /Ob2 /DNDEBUG
    C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MDd /Zi /Ob0 /Od /RTC1
    Linker flags (Release):      /machine:x64  /INCREMENTAL:NO
    Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL
    ccache:                      NO
    Precompiled headers:         YES
    Extra dependencies:          cudart_static.lib nppc.lib nppial.lib nppicc.lib nppidei.lib nppif.lib nppig.lib nppim.lib nppist.lib nppisu.lib nppitc.lib npps.lib cublas.lib cudnn.lib cufft.lib -LIBPATH:C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/lib/x64
    3rdparty dependencies:

  OpenCV modules:
    To be built:                 alphamat aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform java line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency sfm shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab viz wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
    Disabled:                    world
    Disabled by dependency:      -
    Unavailable:                 cvv freetype hdf julia matlab ovis python2
    Applications:                tests perf_tests examples apps
    Documentation:               doxygen python javadoc
    Non-free algorithms:         YES

  Windows RT support:            NO

  GUI:                           WIN32UI
    Win32 UI:                    YES
    OpenGL support:              YES (opengl32 glu32)
    VTK support:                 YES (ver 9.2.5)

  Media I/O:
    ZLib:                        optimized C:/install/zlib/lib/zlib.lib debug C:/install/zlib/lib/zlibd.lib (ver 1.2.13)
    JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)
      SIMD Support Request:      YES
      SIMD Support:              NO
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         optimized C:/install/libpng/lib/libpng16.lib debug C:/install/libpng/lib/libpng16d.lib (ver 1.6.40)
    TIFF:                        build (ver 42 - 4.2.0)
    JPEG 2000:                   build (ver 2.5.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES

  Video I/O:
    DC1394:                      NO
    FFMPEG:                      YES (prebuilt binaries)
      avcodec:                   YES (58.134.100)
      avformat:                  YES (58.76.100)
      avutil:                    YES (56.70.100)
      swscale:                   YES (5.9.100)
      avresample:                YES (4.0.0)
    GStreamer:                   NO
    DirectShow:                  YES
    Media Foundation:            YES
      DXVA:                      YES

  Parallel framework:            Concurrency

  Other third-party libraries:
    Intel IPP:                   2021.8 [2021.8.0]
           at:                   C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/icv
    Intel IPP IW:                sources (2021.8.0)
              at:                C:/lib/build/opencv/3rdparty/ippicv/ippicv_win/iw
    Lapack:                      YES (C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0/lib/intel64/mkl_intel_lp64.lib C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0/lib/intel64/mkl_sequential.lib C:/Program Files (x86)/Intel/oneAPI/mkl/2023.0.0/lib/intel64/mkl_core.lib)
    OpenVINO:                    YES (2022.3.0)
    Eigen:                       YES (ver ..)
    Custom HAL:                  NO
    Protobuf:                    build (3.19.1)
    Flatbuffers:                 builtin/3rdparty (23.5.9)
  NVIDIA CUDA:                   YES (ver 12.1, CUFFT CUBLAS)
    NVIDIA GPU arch:             86
    NVIDIA PTX archs:

  cuDNN:                         YES (ver 8.8.0)

  OpenCL:                        YES (NVD3D11)
    Include path:                C:/lib/opencv/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load

  Python 3:
    Interpreter:                 C:/Program Files/Python310/python.exe (ver 3.10.10)
    Libraries:                   optimized C:/Program Files/Python310/libs/python310.lib debug C:/Program Files/Python310/libs/python310_d.lib (ver 3.10.10)
    numpy:                       C:/Users/laurent/AppData/Roaming/Python/Python310/site-packages/numpy/core/include (ver 1.23.5)
    install path:                C:/Users/laurent/AppData/Roaming/Python/Python310/site-packages/cv2/python-3.10

  Python (for build):            C:/Program Files/Python310/python.exe

  Java:
    ant:                         C:/apache-ant-1.10.13/bin/ant.bat (ver 1.10.13)
    Java:                        NO
    JNI:                         C:/Program Files/Java/jdk-19/include C:/Program Files/Java/jdk-19/include/win32 C:/Program Files/Java/jdk-19/include
    Java wrappers:               YES (ANT)
    Java tests:                  YES

  Install to:                    C:/install/opencv
-----------------------------------------------------------------
```

### Detailed description

[pytorch and onnx gives same results](https://github.com/pytorch/pytorch/issues/105661)
opencv does not give good results

Results 


ONNX RESULT
[[4.446731  4.4490666 4.46463   4.4546375 4.4510665 4.456948 ]
 [4.4421244 4.4491835 4.4703193 4.460532  4.4576974 4.462741 ]
 [4.440228  4.4505563 4.4774194 4.4691973 4.4677935 4.470848 ]
 [4.435331  4.4522853 4.484742  4.480783  4.480365  4.4803877]
 [4.4323177 4.4522853 4.4912124 4.4923916 4.4934096 4.4903293]]
OPENCV
[[5.1145616 5.1145616 5.1145616 5.1145616 5.1145616 5.1145616]
 [5.115294  5.115294  5.115294  5.115294  5.115294  5.115294 ]
 [5.116197  5.116197  5.116197  5.116197  5.116197  5.116197 ]
 [5.1162567 5.1162567 5.1162567 5.1162567 5.1162567 5.1162567]
 [5.115356  5.115356  5.115356  5.115356  5.115356  5.115356 ]]
0.84839463
7.712089

### Steps to reproduce

simplified model can be loaded [here](http://www.traimaocv.fr/CoursTF/AdaBins_kitti_sim.onnx)
image is [here](https://github.com/shariqfarooq123/AdaBins/blob/main/test_imgs/classroom__rgb_00283.jpg)

```
import onnx
import onnxruntime as rt
import numpy as np
import cv2 as cv

onnx_name = ""AdaBins_kitti_sim.onnx""

image = cv.imread(cv.samples.findFile(""classroom__rgb_00283.jpg""))
# DATA for ONNX and OPENCV 
blob = np.transpose(image.astype(np.float32), [2, 0, 1])/255
blob = blob.reshape((1,3, 480, 640))

# ONNX inference
sess = rt.InferenceSession(onnx_name) 
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name
predonnx = sess.run([sess.get_outputs()[0].name, 
                 sess.get_outputs()[1].name,
               ], 
                 {input_name: blob})
print(""ONNX RESULT"")
disparity_onnx = predonnx[1][0, 0, : ,:]
bins_onnx = predonnx[0]
print(disparity_onnx[100:105,25:43:3])

# OPENCV inference
net = cv.dnn.readNet(onnx_name)
net.setInput(blob)
pred_opencv = net.forward([sess.get_outputs()[0].name, 
                 sess.get_outputs()[1].name,
               ])
print(""OPENCV"")
disparity_opencv = pred_opencv[1][0, 0, : ,:]
bins_opencv = pred_opencv[0]
print(disparity_opencv[100:105,25:43:3])
print(np.mean((disparity_onnx-disparity_opencv)**2))
print(np.max((disparity_onnx-disparity_opencv)**2))
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-21 12:08:52,bug,Open CV will not infere more than 2 models when cv::utils::logging::setLogLevel(cv::utils::logging::LOG_LEVEL_VERBOSE); is set,"### System Information

OpenCV 4.7.0 and 4.8.0 compiled from source
Operating system: various Linux versions
Compiler: gcc5, gcc7 and gcc11

### Detailed description

Took me a whole week to investigate https://github.com/opencv/opencv/issues/23982 and found a very bizzare behaviour that nobody will easily expect.

When infering DNNs everything is ok, as long as I infere the same model several times (I tried up to 3). However, the problem start when infering different models. With standard logging, still, everything works fine.

However, when ```cv::utils::logging::setLogLevel(cv::utils::logging::LOG_LEVEL_VERBOSE);``` is used (I use that to ensure my build of OpenCV is actually multi-threading on various platforms), the third inference stops working reproducibly . The provided code as 4 net and commenting inferences shows that the concrete model does not seem to matter.

### Steps to reproduce

The problem is reproducible and enforceable.

The code used:
```
#include ""opencv2/opencv.hpp""
#include ""opencv2/core/utils/logger.hpp""

        cv::utils::logging::setLogLevel(cv::utils::logging::LOG_LEVEL_VERBOSE);
        std::string alexnetModelConfiguration = R""(/build/uk/source/ovtest/deploy_alexnet_places365.prototxt)"";
        std::string alexnetModelWeights = R""(/build/uk/source/ovtest/alexnet_places365.caffemodel)"";

        // Load the network
        cv::dnn::Net alexnetNet = cv::dnn::readNetFromCaffe(alexnetModelConfiguration, alexnetModelWeights);
        alexnetNet.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);
        alexnetNet.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);

        std::string googlenetModelConfiguration = R""(/build/uk/source/ovtest/deploy_googlenet_places365.prototxt)"";
        std::string googlenetModelWeights = R""(/build/uk/source/ovtest/googlenet_places365.caffemodel)"";

        // Load the network
        cv::dnn::Net googlenetNet = cv::dnn::readNetFromCaffe(googlenetModelConfiguration, googlenetModelWeights);
        googlenetNet.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);
        googlenetNet.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);

        std::string resnet152ModelConfiguration = R""(/build/uk/source/ovtest/deploy_resnet152_places365.prototxt)"";
        std::string resnet152ModelWeights = R""(/build/uk/source/ovtest/resnet152_places365.caffemodel)"";

        // Load the network
        cv::dnn::Net resnet152Net = cv::dnn::readNetFromCaffe(resnet152ModelConfiguration, resnet152ModelWeights);
        resnet152Net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);
        resnet152Net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);

        std::string vgg16ModelConfiguration = R""(/build/uk/source/ovtest/deploy_vgg16_places365.prototxt)"";
        std::string vgg16ModelWeights = R""(/build/uk/source/ovtest/vgg16_places365.caffemodel)"";

        // Load the network
        cv::dnn::Net vgg16Net = cv::dnn::readNetFromCaffe(vgg16ModelConfiguration, vgg16ModelWeights);
        vgg16Net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);
        vgg16Net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);

        std::string inputFile = R""(/build/uk/source/ovtest/Beach.png)"";

        cv::Mat image = cv::imread(inputFile, cv::IMREAD_UNCHANGED);

        cv::Mat blob227 =
            cv::dnn::blobFromImage(image, 1.0, cv::Size(227, 227), cv::Scalar(103.94, 116.78, 123.68), true, false);
        cv::Mat blob224 =
            cv::dnn::blobFromImage(image, 1.0, cv::Size(224, 224), cv::Scalar(103.94, 116.78, 123.68), true, false);

        std::vector<cv::Mat> ret;
        alexnetNet.setInput(blob227);
        alexnetNet.forward(ret);
        std::cout << ret.size() << ""  "" << ret[0].rows << ""  "" << ret[0].cols << std::endl;

        googlenetNet.setInput(blob227);
        googlenetNet.forward(ret);
        std::cout << ret.size() << ""  "" << ret[0].rows << ""  "" << ret[0].cols << std::endl;

        resnet152Net.setInput(blob224);
        resnet152Net.forward(ret);
        std::cout << ret.size() << ""  "" << ret[0].rows << ""  "" << ret[0].cols << std::endl;

        vgg16Net.setInput(blob224);
        vgg16Net.forward(ret);
        std::cout << ret.size() << ""  "" << ret[0].rows << ""  "" << ret[0].cols << std::endl;
```

Error message:
```
OpenCV(4.8.0) Error: Requested object was not found (Required argument ""operation"" not found into dictionary) in get, file /build/uk/source/4.8.0_trial/source/lib/modules/dnn/include/opencv2/dnn/dnn.inl.hpp, line 350
[DEBUG:0@3.570] global system.cpp:2881 restoreFPDenormalsState core: restore FP mxcsr flags = 0x00001fa0
ERROR: caught an exception
```

I used the following files for the nets:

- https://drive.google.com/file/d/1Q6rOuY3XW0EB1QIFb2xpcsXLhjir22rF/view?usp=drive_link
- https://drive.google.com/file/d/1-f_S2o76-NjH0_Ke32JqgetX7z-xrPKr/view?usp=drive_link
- https://drive.google.com/file/d/1ZkXRvD8rEhxXMxLpfPvukd-OPxUsdCwK/view?usp=sharing
- https://drive.google.com/file/d/1fi7ETFWeZEif-nh6za0sxLXvudtfR5Ib/view?usp=sharing
- https://drive.google.com/file/d/1yZwED-J3--lxwMgUGJfa5-7ONtLqTyP4/view?usp=sharing
- https://drive.google.com/file/d/1ACj0_BKF4dENsV1W3Jpb2bOXZhYp1d74/view?usp=sharing
- https://drive.google.com/file/d/12DRxFjHI8CD1Mmd7fyQrGUW8GnV_kw58/view?usp=sharing
- https://drive.google.com/file/d/1A7QhfnTn9RJK0--EOS0SMGroZR4gXJfG/view?usp=sharing

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-20 22:13:56,bug,Programs that link OpenCV wont start,"### System Information

OpenCV version: latest 3; latest 4
OS: macOS
compiler: clang

### Detailed description

Programs that link OpenCV wont execute. Running them results in no output; the main function never gets called, the program hangs indefinitely.

### Steps to reproduce

Install OpenCV with brew and compile anything that links both it and protobuf.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-20 10:00:54,bug,Inconsistent shape for ConcatLayer in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes',"### System Information

Python 3.10.10
General configuration for OpenCV 4.8.0-dev =====================================
  Version control:               4.8.0-64-g1f7025f028-dirty

  Extra modules:
    Location (extra):            C:/lib/opencv_contrib/modules
    Version control (extra):     4.8.0-4-gd89b2b9b

  Platform:
    Timestamp:                   2023-06-29T16:39:13Z
    Host:                        Windows 10.0.22621 AMD64
    CMake:                       3.26.1
    CMake generator:             Visual Studio 17 2022
    CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe
    MSVC:                        1935
    Configuration:               Debug Release


### Detailed description

this issue follow this [one](https://github.com/opencv/opencv/issues/23886)
simplified model can be loaded [here](http://www.traimaocv.fr/CoursTF/AdaBins_kitti_sim.onnx)

I downloaded model and make inference with onnx no problem

I donwloaded model with opencv and I have got an error : 


```
[ERROR:0@2.111] global net_impl.cpp:1169 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively OPENCV/DNN: [Concat]:(onnx_node!/decoder/up1/Concat_2): getMemoryShapes() throws exception. inputs=2 outputs=1/1 blobs=0
[ERROR:0@2.111] global net_impl.cpp:1172 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively     input[0] = [ 1 2048 30 40 ]
[ERROR:0@2.111] global net_impl.cpp:1172 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively     input[1] = [ 1 176 40 30 ]
[ERROR:0@2.111] global net_impl.cpp:1176 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively     output[0] = [ 1 2048 30 40 ]
[ERROR:0@2.111] global net_impl.cpp:1182 cv::dnn::dnn4_v20230620::Net::Impl::getLayerShapesRecursively Exception message: OpenCV(4.8.0-dev) C:\\lib\\opencv\\modules\\dnn\\src\\layers\\concat_layer.cpp:109: error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes'

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
cv2.error: OpenCV(4.8.0-dev) C:\\lib\\opencv\\modules\\dnn\\src\\layers\\concat_layer.cpp:109: error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer in function 'cv::dnn::ConcatLayerImpl::getMemoryShapes'

```

### Steps to reproduce

simplified model can be loaded [here](http://www.traimaocv.fr/CoursTF/AdaBins_kitti_sim.onnx)

```
import onnx
import onnxscript
import onnxruntime as rt
import numpy as np
import cv2 as cv



onnx_name = ""C:/Users/laurent/Desktop/adabins_kitty/AdaBins_kitti_sim.onnx""

image = cv.imread(cv.samples.findFile(""right.jpg""))
img = cv.resize(image, (640, 480))
img = cv.cvtColor(img, cv.COLOR_BGR2RGB)
img = img.astype(np.float32)/255
blob = np.transpose(img, [2, 0, 1])

sess = rt.InferenceSession(onnx_name) 
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name
pred = sess.run([sess.get_outputs()[0].name, 
                 sess.get_outputs()[1].name,
               ], 
                 {input_name: [blob]})

net = cv.dnn.readNet(onnx_name)
paramAdabins = cv.dnn.Image2BlobParams()
paramAdabins.datalayout = cv.dnn.DNN_LAYOUT_NCHW;
paramAdabins.ddepth = cv.CV_32F;
paramAdabins.mean = (0,0,0);
paramAdabins.scalefactor = (1 / 128., 1 / 128., 1 / 128.);
paramAdabins.size = (480, 640);
paramAdabins.swapRB = True;
paramAdabins.paddingmode = cv.dnn.DNN_PMODE_NULL;
blob = cv.dnn.blobFromImageWithParams(	image, paramAdabins) 
net.setInput(blob)
net.forward()

```


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-19 16:48:38,bug,Fix FLANN python bindings,"As a side-effect this patch improves reporting errors by FLANN `get_param`.

resolves #21642

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-19 13:43:39,bug,Python typing magic constants,"This patch adds typing stubs generation for `__all__` and `__version__` constants.

Introduced `__all__` is intentionally empty for all generated modules stubs. 

Type hints won't work for star imports

resolves #23950

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-19 08:40:33,bug,Test_Caffe_nets.FasterRCNN_zf fails with ENABLE_FAST_MATH option,"### System Information

OpenCV: current 4.x (4.8.0-dev)
Platform: Ubuntu 18.04, GCC 7.5
Hardware: Core i5 2500k (no avx2 and no avx512)

### Detailed description

Net inference accuracy issue is raised.

### Steps to reproduce

```
cmake -DENABLE_FAST_MATH=1 ../opencv
make -j4
./bin/opencv_test_dnn --gtest_filter=""Test_Caffe_nets.FasterRCNN_zf/*""
```

Log:
```
[ RUN      ] Test_Caffe_nets.FasterRCNN_zf/2, where GetParam() = OCV/CPU
Unmatched prediction: class 7 score 0.986261 box [249.607 x 105.463 from (468.582, 78.9142)]
Highest IoU: 0
/home/alexander/Projects/OpenCV/opencv-master/modules/dnn/test/test_common.impl.hpp:145: Failure
Value of: matched
  Actual: false
Expected: true
model name: ZF_faster_rcnn_final.caffemodel
Unmatched reference: class 7 score 0.988779 box [248.791 x 111.586 from (469.849, 75.1756)] IoU diff: 1
/home/alexander/Projects/OpenCV/opencv-master/modules/dnn/test/test_common.impl.hpp:157: Failure
Expected: (refScores[i]) <= (confThreshold), actual: 0.988779 vs 0.8
model name: ZF_faster_rcnn_final.caffemodel
[  FAILED  ] Test_Caffe_nets.FasterRCNN_zf/2, where GetParam() = OCV/CPU (2352 ms)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-19 07:10:24,bug,"Pods/OpenCV2_iOS/opencv2.framework/opencv2(opencl_kernels_calib3d.o), building for iOS Simulator, but linking in object file built for iOS","### System Information

Pods/OpenCV2_iOS/opencv2.framework/opencv2(opencl_kernels_calib3d.o), building for iOS Simulator, but linking in object file built for iOS

### Detailed description

I am getting when I use Openc Depencdency.


### Steps to reproduce

Create a new project  add openCV through pod and build the project.
.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-17 15:11:01,bug,`SkipTestException` is not handled in CUDA tests,"### System Information

OpenCV version: 4.8.0

### Detailed description

As observed in https://github.com/opencv/opencv_contrib/pull/3477 when a `SkipTestException` is thrown the test is incorrectly marked as failed.

It look like all that is required to fix this is updating the definition of [GTEST_TEST_CLASS_NAME_(test_case_name, test_name)::TestBody()](https://github.com/opencv/opencv/blob/192099352577d18b46840cdaf3cbf365e4c6e663/modules/ts/include/opencv2/ts/cuda_test.hpp#L199) to handle this type of exception in the same way as [`CV__TEST_BODY_IMPL(name)`](https://github.com/opencv/opencv/blob/192099352577d18b46840cdaf3cbf365e4c6e663/modules/ts/include/opencv2/ts/ts_ext.hpp#L36) does.

### Steps to reproduce

Throw a `SkipTestException` anywhere in a CUDA test.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [x] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-14 12:07:43,bug,Issue loading TFLite net,"
##### System information (version)

- OpenCV => 4.8.0
- Operating System / Platform => Windows 10 64 Bit
- Compiler => Visual Studio 2022



##### Detailed description

So, I saw that tflite support has been added, but when I try readNetFromTFLite(detect.tflite);, I get:
```
[ERROR:0@0.395] global tflite_importer.cpp:243 cv::dnn::dnn4_v20230620::TFLiteImporter::populateNet DNN/TFLite: Problem during import of operator [PACK]:(ssd_mobile_net_v2_fpn_keras_feature_extractor/FeatureMaps/top_down/nearest_neighbor_upsampling/nearest_neighbor_upsampling/w_stack) (65/157). Exception: OpenCV(4.8.0) G:\\opencv\\opencv-4.8.0\\modules\\dnn\\src\\tflite\\tflite_importer.cpp:235: error: (-213:The function/feature is not implemented) Unsupported operator type PACK in function 'cv::dnn::dnn4_v20230620::TFLiteImporter::populateNet'

OpenCV(4.8.0) G:\\opencv\\opencv-4.8.0\\modules\\dnn\\src\\tflite\\tflite_importer.cpp:235: error: (-213:The function/feature is not implemented) Unsupported operator type PACK in function 'cv::dnn::dnn4_v20230620::TFLiteImporter::populateNet'
```
The model was trained and converted by following [this tutorial](https://colab.research.google.com/github/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Train_TFLite2_Object_Detction_Model.ipynb):"
opencv/opencv,2023-07-12 13:54:52,bug,C++ version of OpenCV 4.8 won't load caffe model,"### System Information

OpenCV 4.8.0
Operating System: CentOS 
Compiler: GCC11

opencv-python-rolling-4.8.0.20230624
Windows 10

### Detailed description

I am loading the same model with OpenCV Python ok and fail to load the model with a compiled version of OpenCV 4.8.

This used to work fine for OpenCV 4.5.2 and fails for 4.7 and 4.8. The compile machine is offline and I provided the correct version of ADE by hand.

Other DNN models load and work fine.

Find the model here https://drive.google.com/file/d/1Q6rOuY3XW0EB1QIFb2xpcsXLhjir22rF/view?usp=sharing and https://drive.google.com/file/d/1-f_S2o76-NjH0_Ke32JqgetX7z-xrPKr/view?usp=sharing .

### Steps to reproduce

Compile OpenCV 4.8 offline using the following configuartion:
```
cmake3 -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=/build/uk/Frameworks/opencv-4.8.0-ov-AVX2-default-debug-install VERBOSE=1 -DOPENCV_EXTRA_MODULES_PATH=~/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/ -DCPU_BASELINE=AVX2 -DENABLE_OMIT_FRAME_POINTER=OFF -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_EXAMPLES=OFF -DBUILD_opencv_apps=OFF  ~/local/Frameworks/opencv/4.8.0_trial/source/lib
-- ocv_init_download: OpenCV source tree is not fetched as git repository. 3rdparty resources will be downloaded from github.com by default.
-- Detected processor: x86_64
Python 2.7.5
-- Looking for ccache - not found
Cleaning INTERNAL cached variable: ZLIB_LIBRARY
Cleaning INTERNAL cached variable: ZLIB_INCLUDE_DIR
-- Could NOT find ZLIB (missing: ZLIB_LIBRARY ZLIB_INCLUDE_DIR) (Required is at least version ""1.2.3"")
Cleaning INTERNAL cached variable: JPEG_LIBRARY
Cleaning INTERNAL cached variable: JPEG_INCLUDE_DIR
-- Could NOT find JPEG (missing: JPEG_LIBRARY JPEG_INCLUDE_DIR)
-- libjpeg-turbo: VERSION = 2.1.3, BUILD = opencv-4.8.0-libjpeg-turbo-debug
Cleaning INTERNAL cached variable: TIFF_LIBRARY
Cleaning INTERNAL cached variable: TIFF_INCLUDE_DIR
-- Could NOT find TIFF (missing: TIFF_LIBRARY TIFF_INCLUDE_DIR)
Cleaning INTERNAL cached variable: WEBP_LIBRARY
Cleaning INTERNAL cached variable: WEBP_INCLUDE_DIR
-- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources
-- OpenJPEG: VERSION = 2.5.0, BUILD = opencv-4.8.0-openjp2-2.5.0-debug
-- OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.5.0"")
Cleaning INTERNAL cached variable: PNG_LIBRARY
Cleaning INTERNAL cached variable: PNG_INCLUDE_DIR
-- Could NOT find PNG (missing: PNG_LIBRARY PNG_PNG_INCLUDE_DIR)
-- IPPICV: Downloading ippicv_2021.8_lnx_intel64_20230330_general.tgz from https://raw.githubusercontent.com/opencv/opencv_3rdparty/1224f78da6684df04397ac0f40c961ed37f79ccb/ippicv/ippicv_2021.8_lnx_intel64_20230330_general.tgz
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at cmake/OpenCVDownload.cmake:248 (message):
  IPPICV: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  3rdparty/ippicv/ippicv.cmake:37 (ocv_download)
  cmake/OpenCVFindIPP.cmake:259 (download_ippicv)
  cmake/OpenCVFindLibsPerf.cmake:12 (include)
  CMakeLists.txt:756 (include)


-- Could not find OpenBLAS include. Turning OpenBLAS_FOUND off
-- Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off
-- Could NOT find Atlas (missing: Atlas_CBLAS_INCLUDE_DIR Atlas_CLAPACK_INCLUDE_DIR Atlas_CBLAS_LIBRARY Atlas_BLAS_LIBRARY Atlas_LAPACK_LIBRARY)
-- Could NOT find BLAS (missing: BLAS_LIBRARIES)
-- LAPACK requires BLAS
-- A library with LAPACK API not found. Please specify library location.
-- Could NOT find Java (missing: Java_JAR_EXECUTABLE Java_JAVAC_EXECUTABLE Java_JAVAH_EXECUTABLE Java_JAVADOC_EXECUTABLE) (found version ""1.8.0_372"")
-- Could NOT find JNI (missing: JAVA_INCLUDE_PATH JAVA_INCLUDE_PATH2 JAVA_AWT_INCLUDE_PATH)
-- VTK is not found. Please set -DVTK_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file
-- Checking for module 'gtk+-3.0'
--   No package 'gtk+-3.0' found
-- Checking for module 'gtk+-2.0'
--   No package 'gtk+-2.0' found
-- Checking for module 'gthread-2.0>=2.32'
--   No package 'gthread-2.0' found
-- Checking for modules 'libavcodec;libavformat;libavutil;libswscale'
--   No package 'libavcodec' found
--   No package 'libavformat' found
--   No package 'libavutil' found
--   No package 'libswscale' found
-- FFMPEG is disabled. Required libraries: libavcodec;libavformat;libavutil;libswscale. Missing libraries: libavcodec;libavformat;libavutil;libswscale
-- Checking for module 'gstreamer-base-1.0'
--   No package 'gstreamer-base-1.0' found
-- Checking for module 'gstreamer-app-1.0'
--   No package 'gstreamer-app-1.0' found
-- Checking for module 'gstreamer-riff-1.0'
--   No package 'gstreamer-riff-1.0' found
-- Checking for module 'gstreamer-pbutils-1.0'
--   No package 'gstreamer-pbutils-1.0' found
-- Checking for module 'gstreamer-video-1.0'
--   No package 'gstreamer-video-1.0' found
-- Checking for module 'gstreamer-audio-1.0'
--   No package 'gstreamer-audio-1.0' found
-- Checking for module 'libdc1394-2'
--   No package 'libdc1394-2' found
-- Module opencv_alphamat disabled because the following dependencies are not found: Eigen
-- Checking for module 'freetype2'
--   No package 'freetype2' found
-- Checking for module 'harfbuzz'
--   No package 'harfbuzz' found
-- freetype2:   NO
-- harfbuzz:    NO
-- Could NOT find HDF5 (missing: HDF5_LIBRARIES HDF5_INCLUDE_DIRS) (found version """")
-- Julia not found. Not compiling Julia Bindings.
-- Module opencv_ovis disabled because OGRE3D was not found
-- No preference for use of exported gflags CMake configuration set, and no hints for include/library directories provided. Defaulting to preferring an installed/exported gflags CMake configuration if available.
-- Failed to find installed gflags CMake configuration, searching for gflags build directories exported with CMake.
-- Failed to find gflags - Failed to find an installed/exported CMake configuration for gflags, will perform search for installed gflags components.
-- Failed to find gflags - Could not find gflags include directory, set GFLAGS_INCLUDE_DIR to directory containing gflags/gflags.h
-- Failed to find glog - Could not find glog include directory, set GLOG_INCLUDE_DIR to directory containing glog/logging.h
-- Module opencv_sfm disabled because the following dependencies are not found: Eigen Glog/Gflags
-- Checking for module 'tesseract'
--   No package 'tesseract' found
-- Tesseract:   NO
-- Allocator metrics storage type: 'long long'
-- Excluding from source files list: modules/imgproc/src/imgwarp.lasx.cpp
-- Excluding from source files list: modules/imgproc/src/resize.lasx.cpp
-- Registering hook 'INIT_MODULE_SOURCES_opencv_dnn': /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/modules/dnn/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake
-- opencv_dnn: filter out cuda4dnn source code
-- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.rvv.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.lasx.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/int8layers/layers_common.lasx.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/cpu_kernels/conv_depthwise.rvv.cpp
-- Excluding from source files list: <BUILD>/modules/dnn/layers/cpu_kernels/conv_depthwise.lasx.cpp
-- imgcodecs: OpenEXR codec is disabled in runtime. Details: https://github.com/opencv/opencv/issues/21326
-- highgui: using builtin backend: NONE
-- rgbd: Eigen support is disabled. Eigen is Required for Posegraph optimization
-- wechat_qrcode: Downloading detect.caffemodel from https://raw.githubusercontent.com/WeChatCV/opencv_3rdparty/a8b69ccc738421293254aec5ddb38bd523503252/detect.caffemodel
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  wechat_qrcode: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:26 (ocv_download)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:37 (message):
  WeChatQRCode: Can't get detect caffemodel file for wechat qrcode.


-- wechat_qrcode: Downloading detect.prototxt from https://raw.githubusercontent.com/WeChatCV/opencv_3rdparty/a8b69ccc738421293254aec5ddb38bd523503252/detect.prototxt
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  wechat_qrcode: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:26 (ocv_download)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:37 (message):
  WeChatQRCode: Can't get detect prototxt file for wechat qrcode.


-- wechat_qrcode: Downloading sr.caffemodel from https://raw.githubusercontent.com/WeChatCV/opencv_3rdparty/a8b69ccc738421293254aec5ddb38bd523503252/sr.caffemodel
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  wechat_qrcode: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:26 (ocv_download)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:37 (message):
  WeChatQRCode: Can't get sr caffemodel file for wechat qrcode.


-- wechat_qrcode: Downloading sr.prototxt from https://raw.githubusercontent.com/WeChatCV/opencv_3rdparty/a8b69ccc738421293254aec5ddb38bd523503252/sr.prototxt
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  wechat_qrcode: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:26 (ocv_download)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/wechat_qrcode/CMakeLists.txt:37 (message):
  WeChatQRCode: Can't get sr prototxt file for wechat qrcode.


-- xfeatures2d/boostdesc: Downloading boostdesc_bgm.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_bgm.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_bgm_bi.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_bgm_bi.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_bgm_hd.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_bgm_hd.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_binboost_064.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_binboost_064.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_binboost_128.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_binboost_128.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_binboost_256.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_binboost_256.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/boostdesc: Downloading boostdesc_lbgm.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/34e4206aef44d50e6bbcd0ab06354b52e7466d26/boostdesc_lbgm.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/boostdesc: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_boostdesc.cmake:22 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:12 (download_boost_descriptors)


-- xfeatures2d/vgg: Downloading vgg_generated_48.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/fccf7cd6a4b12079f73bbfb21745f9babcd4eb1d/vgg_generated_48.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/vgg: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_vgg.cmake:16 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:13 (download_vgg_descriptors)


-- xfeatures2d/vgg: Downloading vgg_generated_64.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/fccf7cd6a4b12079f73bbfb21745f9babcd4eb1d/vgg_generated_64.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/vgg: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_vgg.cmake:16 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:13 (download_vgg_descriptors)


-- xfeatures2d/vgg: Downloading vgg_generated_80.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/fccf7cd6a4b12079f73bbfb21745f9babcd4eb1d/vgg_generated_80.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/vgg: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_vgg.cmake:16 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:13 (download_vgg_descriptors)


-- xfeatures2d/vgg: Downloading vgg_generated_120.i from https://raw.githubusercontent.com/opencv/opencv_3rdparty/fccf7cd6a4b12079f73bbfb21745f9babcd4eb1d/vgg_generated_120.i
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  xfeatures2d/vgg: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/cmake/download_vgg.cmake:16 (ocv_download)
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:13 (download_vgg_descriptors)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:17 (message):
  features2d: Boost descriptor implementation is not available due to missing
  data (download failed:
  https://github.com/opencv/opencv_contrib/issues/1301)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/xfeatures2d/CMakeLists.txt:22 (message):
  features2d: VGG descriptor implementation is not available due to missing
  data (download failed:
  https://github.com/opencv/opencv_contrib/issues/1301)


-- data: Downloading face_landmark_model.dat from https://raw.githubusercontent.com/opencv/opencv_3rdparty/8afa57abc8229d611c4937165d20e2a2d9fc5a12/face_landmark_model.dat
-- Try 1 failed
--
=======================================================================
  Couldn't download files from the Internet.
  Please check the Internet access on this host.
=======================================================================

CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/cmake/OpenCVDownload.cmake:248 (message):
  data: Download failed: 6;""Couldn't resolve host name""

  For details please refer to the download log file:


  /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug/CMakeDownloadLog.txt


Call Stack (most recent call first):
  /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/face/CMakeLists.txt:13 (ocv_download)


CMake Warning at /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules/face/CMakeLists.txt:26 (message):
  Face: Can't get model file for face alignment.


-- Found 'misc' Python modules from /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/modules/python/package/extra_modules
-- Found 'mat_wrapper;utils' Python modules from /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/modules/core/misc/python/package
-- Found 'gapi' Python modules from /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/modules/gapi/misc/python/package
--
-- General configuration for OpenCV 4.8.0 =====================================
--   Version control:               unknown
--
--   Extra modules:
--     Location (extra):            /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/contrib/modules
--     Version control (extra):     unknown
--
--   Platform:
--     Timestamp:                   2023-07-12T11:50:51Z
--     Host:                        Linux 5.4.248-1.el7.elrepo.x86_64 x86_64
--     CMake:                       3.17.5
--     CMake generator:             Unix Makefiles
--     CMake build tool:            /usr/bin/gmake
--     Configuration:               Debug
--
--   CPU/HW features:
--     Baseline:                    SSE SSE2 SSE3 SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
--       requested:                 AVX2
--     Dispatched code generation:  AVX512_SKX
--       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
--       AVX512_SKX (5 files):      + AVX_512F AVX512_COMMON AVX512_SKX
--
--   C/C++:
--     Built as dynamic libs?:      YES
--     C++ standard:                11
--     C++ Compiler:                /ovde_plugins/gcc11/linux/bin/g++  (ver 11.3.0)
--     C++ flags (Release):         -fsigned-char -W -Wall -Wreturn-type -Wnon-virtual-dtor -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fno-omit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mf16c -mfma -mavx -mavx2 -fvisibility=hidden -fvisibility-inlines-hidden -O2 -DNDEBUG  -DNDEBUG
--     C++ flags (Debug):           -fsigned-char -W -Wall -Wreturn-type -Wnon-virtual-dtor -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fno-omit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mf16c -mfma -mavx -mavx2 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG
--     C Compiler:                  /ovde_plugins/gcc11/linux/bin/gcc
--     C flags (Release):           -fsigned-char -W -Wall -Wreturn-type -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fno-omit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mf16c -mfma -mavx -mavx2 -fvisibility=hidden -O2 -DNDEBUG  -DNDEBUG
--     C flags (Debug):             -fsigned-char -W -Wall -Wreturn-type -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fno-omit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mf16c -mfma -mavx -mavx2 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG
--     Linker flags (Release):      -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined
--     Linker flags (Debug):        -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined
--     ccache:                      NO
--     Precompiled headers:         NO
--     Extra dependencies:          dl m pthread rt
--     3rdparty dependencies:
--
--   OpenCV modules:
--     To be built:                 aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
--     Disabled:                    world
--     Disabled by dependency:      -
--     Unavailable:                 alphamat cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv freetype hdf java julia matlab ovis python2 python3 sfm ts viz
--     Applications:                -
--     Documentation:               NO
--     Non-free algorithms:         NO
--
--   GUI:                           NONE
--     GTK+:                        NO
--     VTK support:                 NO
--
--   Media I/O:
--     ZLib:                        zlib (ver 1.2.13)
--     JPEG:                        libjpeg-turbo (ver 2.1.3-62)
--     WEBP:                        build (ver encoder: 0x020f)
--     PNG:                         build (ver 1.6.37)
--     TIFF:                        build (ver 42 - 4.2.0)
--     JPEG 2000:                   build (ver 2.5.0)
--     OpenEXR:                     build (ver 2.3.0)
--     HDR:                         YES
--     SUNRASTER:                   YES
--     PXM:                         YES
--     PFM:                         YES
--
--   Video I/O:
--     DC1394:                      NO
--     FFMPEG:                      NO
--       avcodec:                   NO
--       avformat:                  NO
--       avutil:                    NO
--       swscale:                   NO
--       avresample:                NO
--     GStreamer:                   NO
--     v4l/v4l2:                    YES (linux/videodev2.h)
--
--   Parallel framework:            pthreads
--
--   Trace:                         YES (with Intel ITT)
--
--   Other third-party libraries:
--     VA:                          YES
--     Lapack:                      NO
--     Eigen:                       NO
--     Custom HAL:                  NO
--     Protobuf:                    build (3.19.1)
--     Flatbuffers:                 builtin/3rdparty (23.5.9)
--
--   OpenCL:                        YES (INTELVA)
--     Include path:                /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/3rdparty/include/opencl/1.2
--     Link libraries:              Dynamic load
--
--   Python (for build):            /usr/bin/python2.7
--
--   Java:
--     ant:                         NO
--     Java:                        NO
--     JNI:                         NO
--     Java wrappers:               NO
--     Java tests:                  NO
--
--   Install to:                    /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-default-debug-install
-- -----------------------------------------------------------------
--
-- Configuring done
-- Generating done
-- Build files have been written to: /build/uk/Frameworks/opencv-4.8.0-ov-AVX2-PThreads-OpenMP-debug
```

Compile the test using the following configuration:
```
cmake3 -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=/build/uk/Frameworks/opencv-4.8.0-ov-AVX2-default-debug-install VERBOSE=1 ~/local/Frameworks/opencv/4.8.0_trial/source/ovtest
```

This is the code used to load the model:
```
std::string googlenetModelConfiguration = R""(deploy_googlenet_places365.prototxt)"";
std::string googlenetModelWeights = R""(googlenet_places365.caffemodel)"";

// Load the network
cv::dnn::Net googlenetNet = cv::dnn::readNetFromCaffe(googlenetModelConfiguration, googlenetModelWeights);
googlenetNet.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);
googlenetNet.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);
```

The error message when running the code (same for OpenCV 4.7.0)
```
      Starting subtest: Exception free test execution
opencv_unittests[15228]: ERROR: PrintAndLog.cpp:102 logInfo: Failed: Last chance exception handler! Stopping test.   Caught the following std::exception cv::Exception:
      Failed: Last chance exception handler! Stopping test.   Caught the following std::exception cv::Exception:
      OpenCV(4.8.0) /Net/subnet-homes/Development/User/ukoehler/local/Frameworks/opencv/4.8.0_trial/source/lib/modules/dnn/include/opencv2/dnn/dnn.inl.hpp:350: error: (-204:Requested object was not found) Required argument ""operation"" not found into dictionary in function 'get'
```



### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-11 19:14:09,bug,RISC-V: fix unaligned loads and stores,"We experienced Segmentation Fault errors in _core_ `Flip` and _imgproc_ `Bayer` tests. Debugging has shown that issues are caused by an unaligned memory access in RVV code. This PR fixes them.

Below are quick performance comparison:

<details>
<summary>x86_64 performance results</summary>

**Core i5-11600**

|Name of Test|before|after|(x-factor)|
|---|:-:|:-:|:-:|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_BOTH)|0.008|0.009|0.99|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_COLS)|0.009|0.009|1.00|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_ROWS)|0.008|0.008|1.05|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_BOTH)|0.033|0.033|1.00|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_COLS)|0.035|0.035|1.00|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_ROWS)|0.035|0.035|1.00|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_BOTH)|0.033|0.033|1.00|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_COLS)|0.035|0.035|1.00|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_ROWS)|0.037|0.036|1.00|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_BOTH)|0.151|0.152|0.99|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_COLS)|0.155|0.156|0.99|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_ROWS)|0.155|0.155|1.00|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_BOTH)|0.026|0.026|1.00|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_COLS)|0.026|0.026|1.00|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_ROWS)|0.027|0.027|1.00|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_BOTH)|0.100|0.101|1.00|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_COLS)|0.103|0.103|1.00|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_ROWS)|0.104|0.107|0.97|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_BOTH)|0.100|0.101|1.00|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_COLS)|0.103|0.104|1.00|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_ROWS)|0.106|0.107|1.00|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_BOTH)|0.600|0.608|0.99|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_COLS)|0.602|0.609|0.99|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_ROWS)|1.001|1.004|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_BOTH)|0.057|0.057|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_COLS)|0.056|0.056|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_ROWS)|0.061|0.061|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_BOTH)|0.214|0.215|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_COLS)|0.214|0.216|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_ROWS)|0.398|0.394|1.01|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_BOTH)|0.215|0.216|0.99|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_COLS)|0.214|0.216|0.99|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_ROWS)|0.276|0.278|0.99|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_BOTH)|1.517|1.519|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_COLS)|1.521|1.514|1.00|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_ROWS)|1.534|1.542|0.99|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_BOTH)|0.260|0.254|1.02|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_COLS)|0.257|0.250|1.03|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_ROWS)|0.403|0.412|0.98|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_BOTH)|1.450|1.459|0.99|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_COLS)|1.475|1.477|1.00|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_ROWS)|2.373|2.363|1.00|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_BOTH)|1.461|1.462|1.00|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_COLS)|1.481|1.473|1.01|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_ROWS)|1.551|1.546|1.00|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_BOTH)|6.487|6.449|1.01|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_COLS)|6.474|6.464|1.00|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_ROWS)|6.528|6.518|1.00|

|Name of Test|before|after|(x-factor)|
|---|:-:|:-:|:-:|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGR)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGRA)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGR_VNG)|0.038|0.038|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2GRAY)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGR)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGRA)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGR_VNG)|0.038|0.038|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2GRAY)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGR)|0.003|0.003|1.01|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGRA)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGR_VNG)|0.037|0.038|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2GRAY)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGR)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGRA)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGR_VNG)|0.038|0.037|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2GRAY)|0.003|0.003|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGR)|0.042|0.043|0.97|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGRA)|0.039|0.039|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGR_VNG)|1.534|1.542|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2GRAY)|0.031|0.031|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGR)|0.039|0.042|0.92|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGRA)|0.035|0.038|0.93|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGR_VNG)|1.534|1.542|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2GRAY)|0.028|0.032|0.87|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGR)|0.038|0.043|0.89|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGRA)|0.036|0.039|0.92|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGR_VNG)|1.535|1.546|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2GRAY)|0.028|0.031|0.90|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGR)|0.038|0.042|0.92|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGRA)|0.035|0.038|0.93|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGR_VNG)|1.535|1.544|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2GRAY)|0.029|0.028|1.02|
</details>

<details>
<summary>AArch64 performance results</summary>

**RK3588**

|Name of Test|before|after|(x-factor)|
|---|:-:|:-:|:-:|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_BOTH)|0.040|0.036|1.12|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_COLS)|0.018|0.018|1.01|
|Flip::OCL_FlipFixture::(640x480, 8UC1, FLIP_ROWS)|0.011|0.011|1.01|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_BOTH)|0.239|0.261|0.92|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_COLS)|0.174|0.139|1.25|
|Flip::OCL_FlipFixture::(640x480, 32FC1, FLIP_ROWS)|0.132|0.115|1.14|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_BOTH)|0.152|0.185|0.82|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_COLS)|0.086|0.110|0.78|
|Flip::OCL_FlipFixture::(640x480, 8UC4, FLIP_ROWS)|0.067|0.089|0.75|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_BOTH)|1.120|1.277|0.88|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_COLS)|0.675|0.759|0.89|
|Flip::OCL_FlipFixture::(640x480, 32FC4, FLIP_ROWS)|0.416|0.474|0.88|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_BOTH)|0.110|0.119|0.92|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_COLS)|0.064|0.057|1.13|
|Flip::OCL_FlipFixture::(1280x720, 8UC1, FLIP_ROWS)|0.043|0.044|0.97|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_BOTH)|0.837|0.851|0.98|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_COLS)|0.487|0.519|0.94|
|Flip::OCL_FlipFixture::(1280x720, 32FC1, FLIP_ROWS)|0.348|0.371|0.94|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_BOTH)|0.783|0.840|0.93|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_COLS)|0.484|0.502|0.96|
|Flip::OCL_FlipFixture::(1280x720, 8UC4, FLIP_ROWS)|0.351|0.377|0.93|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_BOTH)|3.199|3.580|0.89|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_COLS)|1.763|2.033|0.87|
|Flip::OCL_FlipFixture::(1280x720, 32FC4, FLIP_ROWS)|1.246|1.455|0.86|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_BOTH)|0.276|0.296|0.93|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_COLS)|0.201|0.206|0.98|
|Flip::OCL_FlipFixture::(1920x1080, 8UC1, FLIP_ROWS)|0.164|0.165|0.99|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_BOTH)|1.841|2.070|0.89|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_COLS)|1.118|1.192|0.94|
|Flip::OCL_FlipFixture::(1920x1080, 32FC1, FLIP_ROWS)|0.740|0.851|0.87|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_BOTH)|1.847|2.077|0.89|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_COLS)|1.105|1.211|0.91|
|Flip::OCL_FlipFixture::(1920x1080, 8UC4, FLIP_ROWS)|0.738|0.851|0.87|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_BOTH)|7.048|8.085|0.87|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_COLS)|3.593|4.216|0.85|
|Flip::OCL_FlipFixture::(1920x1080, 32FC4, FLIP_ROWS)|2.835|3.362|0.84|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_BOTH)|1.891|2.145|0.88|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_COLS)|0.991|1.146|0.86|
|Flip::OCL_FlipFixture::(3840x2160, 8UC1, FLIP_ROWS)|0.796|0.937|0.85|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_BOTH)|9.008|10.067|0.89|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_COLS)|4.972|5.634|0.88|
|Flip::OCL_FlipFixture::(3840x2160, 32FC1, FLIP_ROWS)|3.041|3.505|0.87|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_BOTH)|9.044|10.042|0.90|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_COLS)|4.970|5.625|0.88|
|Flip::OCL_FlipFixture::(3840x2160, 8UC4, FLIP_ROWS)|3.122|3.506|0.89|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_BOTH)|28.244|30.723|0.92|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_COLS)|13.721|13.630|1.01|
|Flip::OCL_FlipFixture::(3840x2160, 32FC4, FLIP_ROWS)|12.112|12.300|0.98|

|Name of Test|before|after|(x-factor)|
|---|:-:|:-:|:-:|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGR)|0.009|0.009|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGRA)|0.010|0.010|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2BGR_VNG)|0.095|0.096|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerBG2GRAY)|0.008|0.008|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGR)|0.009|0.009|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGRA)|0.010|0.010|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2BGR_VNG)|0.095|0.096|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGB2GRAY)|0.008|0.008|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGR)|0.009|0.009|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGRA)|0.010|0.010|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2BGR_VNG)|0.095|0.096|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerGR2GRAY)|0.008|0.008|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGR)|0.009|0.009|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGRA)|0.010|0.010|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2BGR_VNG)|0.095|0.095|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(127x61, COLOR_BayerRG2GRAY)|0.008|0.008|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGR)|0.162|0.163|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGRA)|0.173|0.174|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2BGR_VNG)|4.003|4.031|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerBG2GRAY)|0.174|0.147|1.19|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGR)|0.162|0.163|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGRA)|0.173|0.174|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2BGR_VNG)|3.990|4.011|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGB2GRAY)|0.151|0.171|0.88|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGR)|0.165|0.163|1.01|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGRA)|0.173|0.174|0.99|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2BGR_VNG)|4.006|4.014|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerGR2GRAY)|0.154|0.171|0.90|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGR)|0.163|0.163|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGRA)|0.173|0.174|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2BGR_VNG)|3.992|4.007|1.00|
|cvtColorBayer8u::Size_CvtMode_Bayer::(640x480, COLOR_BayerRG2GRAY)|0.164|0.160|1.03|

</details>"
opencv/opencv,2023-07-10 11:43:12,bug,fix: recursively re-export nested submodules in typing stubs,"Addresses the same issue as #23809 , but also handles nested modules case

e.g. adds the following section to `cv2/gapi/__init__.pyi`:

```python
from cv2.gapi import core as core
from cv2.gapi import ie as ie
from cv2.gapi import imgproc as imgproc
from cv2.gapi import oak as oak
from cv2.gapi import onnx as onnx
from cv2.gapi import ov as ov
from cv2.gapi import own as own
from cv2.gapi import render as render
from cv2.gapi import streaming as streaming
from cv2.gapi import video as video
from cv2.gapi import wip as wip

```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [] There is a reference to the original bug report and related work
- [] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-08 19:41:31,bug,Python type stubs: `__all__` and `__version__` dunders are missing,"### System Information

OpenCV python: opencv-python-headless==4.8.0.74
Operating System / Platform: 10.0.19045 Build 19045
Python version: 3.9.13

### Detailed description

`cv2.__all__` and `cv2.__version__` are missing from the stubs.

`cv2.__version__` is useful for compatibility checks.
`cv2.__all__` is *important* for type checkers to know which symbols are.

Without `__all__` specified:
![image](https://github.com/opencv/opencv/assets/1350584/b4554180-5e88-4955-86e5-8704a2d17fa8)

With `__all__ = []` specified:
![image](https://github.com/opencv/opencv/assets/1350584/abab2789-9072-4f05-9b0a-f3dd75d83267)

Runtime:
```py
>>> from cv2 import *
>>> SORT_EVERY_ROW
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'SORT_EVERY_ROW' is not defined
```

### Steps to reproduce

```py
>>> import cv2
>>> [x for x in dir(cv2) if x.startswith(""__"") and x.endswith(""__"")]
['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__']
```

```py
import cv2
cv2.__all__  # ""__all__"" is not a known member of module ""cv2""
cv2.__builtins__
cv2.__cached__
cv2.__doc__
cv2.__file__
cv2.__loader__
cv2.__name__
cv2.__package__
cv2.__path__
cv2.__spec__
cv2.__version__  # ""__version__"" is not a known member of module ""cv2""
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-07 16:17:23,bug,Python type stubs generator fails on contrib/cudaoptflow module,"### System Information

Platform: CUDA
Python: 3.6+

### Detailed description

```
typing_stubs_generation.nodes.type_node.TypeResolutionError: Failed to resolve ""cv2"" namespace against ""None"". Errors: 
[
'Failed to resolve ""cv2.cuda"" namespace against ""cv2"". 
Errors: [\\'Failed to resolve ""cv2.cuda.NvidiaOpticalFlow_1_0"" class against ""cv2"". 
Errors: [\\\\\\'Failed to resolve ""cv2.cuda.NvidiaOpticalFlow_1_0.create"" function against ""cv2"". 
Errors: [0]: Failed to resolve ""perfPreset"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_1_0_NVIDIA_OF_PERF_LEVEL"" exposed as ""cuda_NvidiaOpticalFlow_1_0_NVIDIA_OF_PERF_LEVEL""\\\\\\']\\', 

\\'Failed to resolve ""cv2.cuda.NvidiaOpticalFlow_2_0"" class against ""cv2"". 
Errors: [\\\\\\'Failed to resolve ""cv2.cuda.NvidiaOpticalFlow_2_0.create"" function against ""cv2"". 
Errors: [0]: Failed to resolve ""perfPreset"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_PERF_LEVEL"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_PERF_LEVEL"", 
[1]: Failed to resolve ""outputGridSize"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_OUTPUT_VECTOR_GRID_SIZE"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_OUTPUT_VECTOR_GRID_SIZE"", 
[2]: Failed to resolve ""hintGridSize"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_HINT_VECTOR_GRID_SIZE"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_HINT_VECTOR_GRID_SIZE"", 
[3]: Failed to resolve ""perfPreset"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_PERF_LEVEL"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_PERF_LEVEL"", 
[4]: Failed to resolve ""outputGridSize"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_OUTPUT_VECTOR_GRID_SIZE"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_OUTPUT_VECTOR_GRID_SIZE"", 
[5]: Failed to resolve ""hintGridSize"" argument: Failed to resolve ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_HINT_VECTOR_GRID_SIZE"" exposed as ""cuda_NvidiaOpticalFlow_2_0_NVIDIA_OF_HINT_VECTOR_GRID_SIZE""\\\\\\']\\']',
```

### Steps to reproduce

`cmake -DBUILD_opencv_cudacodec=OFF  -DPYTHON3_EXECUTABLE=`which python3.8` -DPYTHON_DEFAULT_EXECUTABLE=`which python3.8` -DWITH_CUDA=ON -DCUDA_ARCH_BIN=61 -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules/ ../opencv-master`

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-07 13:44:03,bug,Fix checkSignature not thread safe for AVIF.,"A common decoder cannot be shared with checkSignature which is used like a static function (on a static ist of decoders).

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-06 18:09:49,bug,videoio: fix CAP_IMAGES with non-numbered file,"resolves #23935

Diff looks slightly complex because I moved out common part (`grabFrame` call) and realigned the code. Actual fix is added `return false` to the `if (!haveImageReader(...))` block. Also unified log messages."
opencv/opencv,2023-07-05 14:27:00,bug,videoio_dynamic.basic_write sporadically fails on CI,"### System Information

Platform: Ubuntu 20.04 ARM64
OpenCV version: 4.8.0-dev


### Detailed description

```
[ RUN      ] videoio_dynamic.basic_write

(opencv_test_videoio:14940): GStreamer-CRITICAL **: 13:37:41.417: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed
[ERROR:0@2.430] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.0-dev) /home/ci/opencv/modules/videoio/src/cap_images.cpp:431: error: (-215:Assertion failed) !filename_pattern.empty() in function 'open'



(opencv_test_videoio:14940): GStreamer-CRITICAL **: 13:37:42.594: gst_query_set_position: assertion 'format == g_value_get_enum (gst_structure_id_get_value (s, GST_QUARK (FORMAT)))' failed
[ WARN:0@3.144] global cap_gstreamer.cpp:1728 open OpenCV | GStreamer warning: Cannot query video position: status=1, value=-1, duration=120
[ WARN:0@3.635] global cap.cpp:204 open VIDEOIO(V4L2): backend is generally available but can't be used to capture by name
/home/ci/opencv/modules/videoio/test/test_dynamic.cpp:73: Failure
Expected equality of these values:
  count
    Which is: 0
  FRAME_COUNT
    Which is: 120
Corrupt JPEG data: premature end of data segment
Corrupt JPEG data: premature end of data segment
Corrupt JPEG data: premature end of data segment
Corrupt JPEG data: premature end of data segment
Corrupt JPEG data: premature end of data segment
[  FAILED  ] videoio_dynamic.basic_write (3466 ms)
```

### Steps to reproduce

CI example: https://github.com/opencv/opencv/actions/runs/5464664040/jobs/9947005647?pr=23881

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-05 11:10:18,bug,Fixed possible out-of-bound access in circles drawing,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-03 09:46:59,bug,Python typing stub missing matrix type constants like CV_32F,"### System Information

OpenCV python version:4.8.0.74
OS: Windows 11 22H2
Python version: 3.11.4

### Detailed description

Where and how should we report issues with the typing? It seems all `CV_...` constants are missing.

_Originally posted by @damonmaria in https://github.com/opencv/opencv/issues/20370#issuecomment-1617305546_


The commit in question is missing the `CV_...` constants as stated by the comment I quoted, causing cannot find reference errors in PyCharm and failing to autocorrect.

### Steps to reproduce

Open any python file in PyCharm or create a new one.

Type the following to the top of the file:
```
import cv2
test = cv2.CV_32F
```

Observe the yellow squiggly line below the CV_32F part. Hover the mouse cursor over that part to observe the warning: 
```Cannot find reference 'CV_32F' in '__init__.pyi' ```

### Issue submission checklist

- [x] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-01 14:46:26,bug,Problem of install openCV to openVINO,"### System Information

// example for python user
OpenCV python version: 4.2
Operating System / Platform: Ubuntu 20.04
Python version: 3.9.6



### Detailed description

Since the exmaple model on **openVINO** rquire me to install **openCV**, and  i follow the intstrucion of this links: `https://github.com/opencv/opencv/wiki/BuildOpenCV4OpenVINO`, but it shows the below error message. I don't know how to fix it, it seems like a bug? Or what version of openCV i should install to solve this problem?

```In file included from /home/chen/opencv/modules/objdetect/test/test_qrcode.cpp:6:
/home/chen/opencv/modules/objdetect/test/test_qr_utils.hpp: In function ‘void opencv_test::check_qr(const string&, const string&, const string&, const std::vector<cv::Point_<int> >&, const std::vector<std::__cxx11::basic_string<char> >&, int, bool)’:
/home/chen/opencv/modules/objdetect/test/test_qr_utils.hpp:11:44: warning: unused parameter ‘decoded_info’ [-Wunused-parameter]
   11 |                 const std::vector<string>& decoded_info, const int max_pixel_error,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
/home/chen/opencv/modules/objdetect/test/test_qrcode.cpp: In member function ‘virtual void opencv_test::{anonymous}::Objdetect_QRCode_Multi_regression_Test::Body()’:
/home/chen/opencv/modules/objdetect/test/test_qrcode.cpp:389:68: error: ‘decoded_info’ was not declared in this scope
  389 |     check_qr(root, name_current_image, ""multiple_images"", corners, decoded_info, pixels_error, true);
      |                                                                    ^~~~~~~~~~~~
/home/chen/opencv/modules/objdetect/test/test_qrcode.cpp: In member function ‘virtual void opencv_test::{anonymous}::Objdetect_QRCode_detect_flipped_regression_23249_Test::Body()’:
/home/chen/opencv/modules/objdetect/test/test_qrcode.cpp:541:28: warning: unused variable ‘expect_msg’ [-Wunused-variable]
  541 |         const std::string &expect_msg = flipped_image.second;
      |                            ^~~~~~~~~~
[ 81%] Building CXX object modules/video/CMakeFiles/opencv_perf_video.dir/perf/opencl/perf_dis_optflow.cpp.o
make[2]: *** [modules/objdetect/CMakeFiles/opencv_test_objdetect.dir/build.make:180: modules/objdetect/CMakeFiles/opencv_test_objdetect.dir/test/test_qrcode.cpp.o] Error 1```


### Steps to reproduce

``````cpp
cmake \\
-D BUILD_INFO_SKIP_EXTRA_MODULES=ON \\
-D BUILD_EXAMPLES=OFF \\
-D BUILD_JASPER=OFF \\
-D BUILD_JAVA=OFF \\
-D BUILD_JPEG=ON \\
-D BUILD_APPS_LIST=version \\
-D BUILD_opencv_apps=ON \\
-D BUILD_opencv_java=OFF \\
-D BUILD_OPENEXR=OFF \\
-D BUILD_PNG=ON \\
-D BUILD_TBB=OFF \\
-D BUILD_WEBP=OFF \\
-D BUILD_ZLIB=ON \\
-D WITH_1394=OFF \\
-D WITH_CUDA=OFF \\
-D WITH_EIGEN=OFF \\
-D WITH_GPHOTO2=OFF \\
-D WITH_GSTREAMER=ON \\
-D OPENCV_GAPI_GSTREAMER=OFF \\
-D WITH_GTK_2_X=OFF \\
-D WITH_IPP=ON \\
-D WITH_JASPER=OFF \\
-D WITH_LAPACK=OFF \\
-D WITH_MATLAB=OFF \\
-D WITH_MFX=ON \\
-D WITH_OPENCLAMDBLAS=OFF \\
-D WITH_OPENCLAMDFFT=OFF \\
-D WITH_OPENEXR=OFF \\
-D WITH_OPENJPEG=OFF \\
-D WITH_QUIRC=OFF \\
-D WITH_TBB=OFF \\
-D WITH_TIFF=OFF \\
-D WITH_VTK=OFF \\
-D WITH_WEBP=OFF \\
-D CMAKE_USE_RELATIVE_PATHS=ON \\
-D CMAKE_SKIP_INSTALL_RPATH=ON \\
-D ENABLE_BUILD_HARDENING=ON \\
-D ENABLE_CONFIG_VERIFICATION=ON \\
-D ENABLE_PRECOMPILED_HEADERS=OFF \\
-D ENABLE_CXX11=ON \\
-D INSTALL_PDB=ON \\
-D INSTALL_TESTS=ON \\
-D INSTALL_C_EXAMPLES=ON \\
-D INSTALL_PYTHON_EXAMPLES=ON \\
-D CMAKE_INSTALL_PREFIX=install \\
-D OPENCV_SKIP_PKGCONFIG_GENERATION=ON \\
-D OPENCV_SKIP_PYTHON_LOADER=OFF \\
-D OPENCV_SKIP_CMAKE_ROOT_CONFIG=ON \\
-D OPENCV_GENERATE_SETUPVARS=OFF \\
-D OPENCV_BIN_INSTALL_PATH=bin \\
-D OPENCV_INCLUDE_INSTALL_PATH=include \\
-D OPENCV_LIB_INSTALL_PATH=lib \\
-D OPENCV_CONFIG_INSTALL_PATH=cmake \\
-D OPENCV_3P_LIB_INSTALL_PATH=3rdparty \\
-D OPENCV_SAMPLES_SRC_INSTALL_PATH=samples \\
-D OPENCV_DOC_INSTALL_PATH=doc \\
-D OPENCV_OTHER_INSTALL_PATH=etc \\
-D OPENCV_LICENSES_INSTALL_PATH=etc/licenses \\
-D OPENCV_INSTALL_FFMPEG_DOWNLOAD_SCRIPT=ON \\
-D BUILD_opencv_world=OFF \\
-D BUILD_opencv_python2=OFF \\
-D BUILD_opencv_python3=ON \\
-D PYTHON3_PACKAGES_PATH=install/python/python3 \\
-D PYTHON3_LIMITED_API=ON \\
-D HIGHGUI_PLUGIN_LIST=all \\
-D OPENCV_PYTHON_INSTALL_PATH=python \\
-D CPU_BASELINE=SSE4_2 \\
-D OPENCV_IPP_GAUSSIAN_BLUR=ON \\
-D WITH_OPENVINO=ON \\
-D VIDEOIO_PLUGIN_LIST=ffmpeg,gstreamer,mfx \\
-D CMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined \\
-D CMAKE_BUILD_TYPE=Release \\
-S ~./opencv \\
-B ~/build-opencv && \\
cmake --build ~/build-opencv --parallel $(nproc) &&
cmake --install ~/build-opencv
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-28 13:57:29,bug,Possible memory leak in cv::resize in multithread environment,"### Expected behaviour

memory usage to remain constant as the program runs

### Actual behaviour

memory usage steadily increases over time around ~ 200 Kb/sec (seen in windows task manager)

### Steps to reproduce

I'm running this in python 3.7 with opencv-python-4.7.0.72 on windows, I've also seen this also seems to happen on ubuntu machines


this piece of code reproduces the issue, it is not exclusive to resize, it seems that any function that returns data from cv2 causes leaked memory 
```
import threading
import cv2
import numpy as np
import gc

class Memleak_test:
    def __init__(self):
        self.thread = None

    def test(self):
        frame = np.zeros((10, 10, 3)).astype(np.uint8)
        frame = cv2.resize(frame, (20, 20))

    def run_threaded(self):
        self.thread = threading.Thread(target=self.test)
        self.thread.start()

    def test_memleak(self):
        if self.thread is not None:
            self.thread.join()
        self.run_threaded() # leaks memory
        return None

m = Memleak_test()
while True:

    m.test_memleak()
    gc.collect()
```
    
I've seen some mention of similar issues being related to TLS (thread local state) but have supposedly been fixed a long time ago  https://github.com/opencv/opencv/issues/9745
"
opencv/opencv,2023-06-28 09:31:58,bug,G-API: Fix async inference for OpenVINO backend,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-23 13:24:38,bug,dnn: fix overflow in sigmoid layer for 3.4,"Fixes #23850 

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-22 18:54:50,bug,"Float overflow and NaNs in SigmoidLayer (YOLOv5n detection, CPU)","### System Information

OpenCV versions: 4.5.5, 4.7.0, 4.7.0_dev
OS Ubuntu 20.04
g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0


### Detailed description

Greetings, colleagues!
I use the YOLOv5n neural network in my project. After training and exporting to onnx, I upload it using cv::dnn::readNetFromONNX(path to .onnx).
If I use dnnBackend CUDA I have detections, when switching to dnnBackend default and dnnTarget CPU detections disappear.
I started comparing the conclusions from the layers of the model. In the output of the last layer, when using the CPU, there were a lot of NaN (more than half of the values). When using CUDA, there were the usual floats. The first NaN appeared after the SigmoidLayer in the first layers of the model. I created a c++ script with a sigmoid formula and tried to get NaN in it. In the model, the input values are  -92.xxx ... -96. xxxx, but the script gave out 0.0f all the time. I didn't understand why it turned out to be NaN, and negative (""-nan""). But the solution to my problem was the modification of the SigmoidLayer implementation in
modules/dnn/src/layers/elementwise_layers.cpp (lines 1101 - 1104)
Instead of 
```
inline float calculate(float x) const
{
    return 1.f / (1.f + exp(-x));
}
```
I did
```
inline float calculate(float x) const
{
    double dx = double(x);
    double sigm = 1.0 / (1.0 + exp(-dx));
    return static_cast<float>(sigm);
}
```
NaN disappeared, detections appeared.

### Steps to reproduce
```
#include <iostream>
#include <cmath>
#include <typeinfo>

using namespace std;

inline long double sigmoid(float x)
{
    return 1. / (1. + exp(-x));
}


int main()
{
    float x = -97.978683;
    double dx = double(x);

    double sig = 1.0 / (1.0 + exp(-dx));
    std::cout << ""double sigmoid = "" << sig << '\\n';
    std::cout << typeid(sig).name() << '\\n';

    float sigm = static_cast\\<float\\>(sig);
    std::cout << ""float sigmoid = "" << sigm << '\\n';
    std::cout << typeid(sigm).name() << '\\n';

    std::cout << ""\\n\\n"";
    double* p_x = &dx;
    sig = 1.0 / (1.0 + exp(-(*p_x)));
    double* p_sig = &sig;
    cout << ""\\nsigmoid("" << *p_x << "") = "" << *p_sig << ""\\n\\n"";


//    double dSQRTValue = sqrt( -1.00 ); // An image processing algorithm may eventually invoke sqrt() with -1 as its input.
//    double dResult = -dSQRTValue; // An image processing algorithm may involve taking the negative of another value.
//
//    std::cout << dResult << '\\n';
//
//    float s;
//    if (dResult == dResult)
//        s = sig;
//    else
//        s = 123.0f;
//    std::cout << ""else "" << s << '\\n';

    double tmp1 = exp(-(*p_x));
    double* p_tmp1 = &tmp1;
    cout << ""float(exp(x)) = "" << *p_tmp1 << '\\n';

    double tmp2 = 1 + *p_tmp1;
    double* p_tmp2 = &tmp2;
    cout << ""(1 + exp(x)) = "" << *p_tmp2 << '\\n';

    double tmp3 = 1 / *p_tmp2;
    double* p_tmp3 = &tmp3;
    cout << ""(1 / (1 + exp(-x))) = "" << *p_tmp3 << '\\n';

    cout << sigmoid(x) << '\\n';

    return 0;
}
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-22 12:17:42,bug,Fix detect diamonds api,"`detectDiamonds` cannot be called from python, reproducer:

```
import numpy as np
import cv2 as cv

detector = cv.aruco.CharucoDetector(
    cv.aruco.CharucoBoard(
        (3, 3), 200.0, 100.0,
        cv.aruco.getPredefinedDictionary(cv.aruco.DICT_4X4_250)
    )
)
image = np.zeros((640, 480, 1), dtype=np.uint8)
res = detector.detectDiamonds(image)
print(res)
```

The error in `detectDiamonds` API fixed by replacing `InputOutputArrayOfArrays markerIds` with `InputOutputArray markerIds`.


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-21 14:20:23,bug,G-API: fix static build with OpenVINO,"Need to register external dependency if dnn module is not used:

```
-DWITH_OPENVINO=ON -DBUILD_opencv_dnn=OFF -DBUILD_SHARED_LIBS=OFF
```

Similar to https://github.com/opencv/opencv/blame/1db1422fbdc1a03d1523b50ad9b05d72cbd39616/modules/dnn/CMakeLists.txt#L241"
opencv/opencv,2023-06-16 20:41:19,bug,Linux docker SIMD build fails,"### System Information

opencv-4.7.0
emscripten/emsdk:3.1.41
Docker version 20.10.24, build 297e128

![image](https://github.com/opencv/opencv/assets/27722274/66baa84c-05d6-4789-a159-7ade0fdc3cde)


### Detailed description

want to build opencv 4.7.0 using docker with build.js and --simd flag.

**end of error log:**
![image](https://github.com/opencv/opencv/assets/27722274/ace8aae6-9f37-4ebd-9d6d-7d7672ae11c8)


### errors log (43672 lines)
[errors.txt](https://github.com/opencv/opencv/files/11774785/errors.txt)

### build log (278 lines)
[build.txt](https://github.com/opencv/opencv/files/11774786/build.txt)


### Steps to reproduce

cd into opencv-4.7.0 folder and:

```bash
docker run --rm -v $(pwd):/src -u $(id -u):$(id -g) -e EMSCRIPTEN=/emsdk/upstream/emscripten emscripten/emsdk:3.1.41 python3 ./platforms/js/build_js.py build_simd --build_wasm --simd
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-16 18:52:02,bug,Force mat_wrapper import to satisfy dependencies for MatLike alias,"Solves:
```
Python 3.9.5 (default, Nov 23 2021, 15:27:38) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import cv2
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ksenia/Projects/opencv-build-3.9/install/lib/python3.9/site-packages/cv2/__init__.py"", line 181, in <module>
    bootstrap()
  File ""/home/ksenia/Projects/opencv-build-3.9/install/lib/python3.9/site-packages/cv2/__init__.py"", line 175, in bootstrap
    if __load_extra_py_code_for_module(""cv2"", submodule, DEBUG):
  File ""/home/ksenia/Projects/opencv-build-3.9/install/lib/python3.9/site-packages/cv2/__init__.py"", line 28, in __load_extra_py_code_for_module
    py_module = importlib.import_module(module_name)
  File ""/usr/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/ksenia/Projects/opencv-build-3.9/install/lib/python3.9/site-packages/cv2/typing/__init__.py"", line 73, in <module>
    MatLike = typing.Union[cv2.mat_wrapper.Mat, numpy.ndarray[typing.Any, numpy.dtype[numpy.generic]]]
AttributeError: partially initialized module 'cv2' has no attribute 'mat_wrapper' (most likely due to a circular import)
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-15 17:28:09,bug,fix: typing module enums references,"Enum names exist only during type checking.
During runtime they should be denoted as named integral types

This patch fixes circular import issue introduced by #23798.

Before patch `TermCriteria` was defined as follows:
```python
TermCriteria = typing.Tuple[cv2.TermCriteria_Type, int, float]
""""""Any type providing sequence protocol is supported""""""
```
which works fine during type checking, but introduces runtime failure:
```python
>>> import cv2
...
    TermCriteria = tuple[cv2.TermCriteria_Type, int, float]  # Any type providing sequence protocol is supported
                         ^^^^^^^^^^^^^^^^^^^^^
AttributeError: partially initialized module 'cv2' has no attribute 'TermCriteria_Type' (most likely due to a circular import)
```

Generated `__init__.py` after patch will hide `cv2.TermCriteria_Type` enumeration by `typing.TYPE_CHECKNIG` guard.

```python
if typing.TYPE_CHECKING:
    TermCriteria_Type = cv2.TermCriteria_Type
else:
    TermCriteria_Type = int

TermCriteria = typing.Tuple[TermCriteria_Type, int, float]
""""""Any type providing sequence protocol is supported""""""
```

So everything can be imported without any issues
```python
>>> import cv2
>>> import cv2.typing
>>> cv2.typing.TermCriteria
typing.Tuple[int, int, float]
>>> 
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-13 07:46:04,bug,Change Scalar assignment in Python from single value,"### Pull Request Readiness Checklist

resolves https://github.com/opencv/opencv/issues/23764

I don't think that `scalefactor` as a `Scalar` is a good idea. Is there models with different scales for channels?

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-10 20:16:56,bug,Type stubs: Missing the `cv2.error` class,"### System Information

OpenCV python: opencv_python_headless-4.7.0+772599a-cp37-abi3-win_amd64.whl
Operating System / Platform: 10.0.19045 Build 19045
Python version: 3.9.13

### Detailed description

The `cv2.error` class is missing from the generated stubs, it's an important one to catch cv2 errors.

### Steps to reproduce

```py
import cv2

try:
    cv2.VideoCapture().read()
# ""error"" is not a known member of module ""cv2"" Pylance(reportGeneralTypeIssues)
# Type of ""error"" is unknown Pylance(reportUnknownVariableType)
except cv2.error as error:
  pass
```

Runtime:
```py
>>> import cv2
>>> from cv2 import error
>>> cv2.error
<class 'cv2.error'>
>>> error
<class 'cv2.error'>
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-10 19:27:06,bug,Type stubs: `Optional` parameters incorrectly marked as not-None in `cv2.calcHist` and `cv2.resize`,"### System Information

OpenCV python: opencv_python_headless-4.7.0+772599a-cp37-abi3-win_amd64.whl
Operating System / Platform: 10.0.19045 Build 19045
Python version: 3.9.13

### Detailed description

`cv2.calcHist`: the `mask` parameter should accept `None`
`cv2.resize`: the `dsize` parameter should accept `None`

### Steps to reproduce

```py
import cv2

image = cv2.imread(""some_path"", cv2.IMREAD_UNCHANGED)

# Argument of type ""None"" cannot be assigned to parameter ""dsize"" of type ""Size"" in function ""resize""
#  Type ""None"" cannot be assigned to type ""Size""PylancereportGeneralTypeIssues
cv2.resize(image, dsize=None)

# Argument of type ""None"" cannot be assigned to parameter ""mask"" of type ""UMat"" in function ""calcHist""
#  Type ""None"" cannot be assigned to type ""UMat""PylancereportGeneralTypeIssues
cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-10 18:37:50,bug,Type stubs: cv2.Error is missing ALL_CAPS constants,"### System Information

OpenCV python: opencv_python_headless-4.7.0+772599a-cp37-abi3-win_amd64.whl
Operating System / Platform: 10.0.19045 Build 19045
Python version: 3.9.13

### Detailed description

cv2.Error is missing ALL_CAPS constants, which is the norm for python constants/Final, and **do** exists at runtime. See steps below for an example.

It's likely other modules are also missing their ALL_CAPS variants, I've only noticed `cv2.Error` because I'm using it.

### Steps to reproduce

```py
import cv2.Error

cv2.Error.STS_ERROR # ""STS_ERROR"" is not a known member of module ""cv2.Error"" Pylance(reportGeneralTypeIssues)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-09 09:49:16,bug,build(ios): disable workaround for CMake 3.25.1+,fixes #23156
opencv/opencv,2023-06-08 00:35:35,bug,DNN: fix bug for X86 winograd,"Address https://github.com/opencv/opencv/issues/23760
The patch aims to add a runtime check for X86 platform without AVX(2).

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-07 12:39:02,bug,Winograd convolution fails with assertion on CPU without AVX,"### System Information

OpenCV: 4.x (before 4.8.0)
OS: Ubuntu Linux 22.04
CPU: Core 2 Duo 6600

### Detailed description

CPU Features:
```
cat /proc/cpuinfo 
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 15
model name      : Intel(R) Core(TM)2 CPU          6600  @ 2.40GHz
stepping        : 6
microcode       : 0xd0
cpu MHz         : 1600.000
cache size      : 4096 KB
physical id     : 0
siblings        : 2
core id         : 0
cpu cores       : 2
apicid          : 0
initial apicid  : 0
fpu             : yes
fpu_exception   : yes
cpuid level     : 10
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm lahf_lm pti tpr_shadow dtherm
vmx flags       : tsc_offset vtpr
bugs            : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit mmio_unknown
bogomips        : 4799.70
clflush size    : 64
cache_alignment : 64
address sizes   : 36 bits physical, 48 bits virtual
power management:
```

OpenCV Optimizations:
```
""  CPU/HW features:\\n""
""    Baseline:                    SSE SSE2 SSE3\\n""
""      requested:                 SSE3\\n""
""    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX\\n""
""      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\\n""
""      SSE4_1 (18 files):         + SSSE3 SSE4_1\\n""
""      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\\n""
""      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\\n""
""      AVX (8 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\\n""
""      AVX2 (36 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\\n""
""      AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX\\n""
```

### Steps to reproduce

Tested with Core 2 Duo 6600 and Ubuntu 22.04 Found some issue with latest convolution optimizations in DNN:
```
./bin/opencv_test_dnn 
CTEST_FULL_OUTPUT
OpenCV version: 4.7.0-dev
OpenCV VCS version: 4.7.0-396-gd3e7968927
Build type: Release
Compiler: /usr/bin/c++  (ver 11.3.0)
Parallel framework: pthreads (nthreads=2)
CPU features: SSE SSE2 SSE3 *SSE4.1? *SSE4.2? *FP16? *AVX? *AVX2? *AVX512-SKX?
Intel(R) IPP version: disabled
OpenCL is disabled
TEST: Skip tests with tags: 'mem_6gb', 'verylong', 'dnn_skip_opencv_backend', 'dnn_skip_cpu', 'dnn_skip_cpu_fp16', 'dnn_skip_ocl', 'dnn_skip_ocl_fp16', 'dnn_skip_onnx_conformance', 'dnn_skip_parser'
....
[----------] 20 tests from DNNTestNetwork
[ RUN      ] DNNTestNetwork.AlexNet/0, where GetParam() = OCV/CPU
unknown file: Failure
C++ exception with description ""OpenCV(4.7.0-dev) /home/arina/Projects/opencv/modules/dnn/src/layers/cpu_kernels/conv_winograd_f63.cpp:401: error: (-215:Assertion failed) CONV_WINO_IBLOCK == 3 && CONV_WINO_KBLOCK == 4 && CONV_WINO_ATOM_F32 == 4 in function 'winofunc_BtXB_8x8_f32'
"" thrown in the test body.
[  FAILED  ] DNNTestNetwork.AlexNet/0, where GetParam() = OCV/CPU (1216 ms)
[ RUN      ] DNNTestNetwork.ResNet_50/0, where GetParam() = OCV/CPU
[     SKIP ] OpenCV tests: Can't find data file: dnn/ResNet-50-model.caffemodel
[       OK ] DNNTestNetwork.ResNet_50/0 (1 ms)
[ RUN      ] DNNTestNetwork.SqueezeNet_v1_1/0, where GetParam() = OCV/CPU
unknown file: Failure
C++ exception with description ""OpenCV(4.7.0-dev) /home/arina/Projects/opencv/modules/dnn/src/layers/cpu_kernels/conv_winograd_f63.cpp:401: error: (-215:Assertion failed) CONV_WINO_IBLOCK == 3 && CONV_WINO_KBLOCK == 4 && CONV_WINO_ATOM_F32 == 4 in function 'winofunc_BtXB_8x8_f32'
"" thrown in the test body.
[  FAILED  ] DNNTestNetwork.SqueezeNet_v1_1/0, where GetParam() = OCV/CPU (27 ms)
[ RUN      ] DNNTestNetwork.GoogLeNet/0, where GetParam() = OCV/CPU
unknown file: Failure
C++ exception with description ""OpenCV(4.7.0-dev) /home/arina/Projects/opencv/modules/dnn/src/layers/cpu_kernels/conv_winograd_f63.cpp:401: error: (-215:Assertion failed) CONV_WINO_IBLOCK == 3 && CONV_WINO_KBLOCK == 4 && CONV_WINO_ATOM_F32 == 4 in function 'winofunc_BtXB_8x8_f32'
"" thrown in the test body.
[  FAILED  ] DNNTestNetwork.GoogLeNet/0, where GetParam() = OCV/CPU (127 ms)
[ RUN      ] DNNTestNetwork.Inception_5h/0, where GetParam() = OCV/CPU
unknown file: Failure
C++ exception with description ""OpenCV(4.7.0-dev) /home/arina/Projects/opencv/modules/dnn/src/layers/cpu_kernels/conv_winograd_f63.cpp:401: error: (-215:Assertion failed) CONV_WINO_IBLOCK == 3 && CONV_WINO_KBLOCK == 4 && CONV_WINO_ATOM_F32 == 4 in function 'winofunc_BtXB_8x8_f32'
"" thrown in the test body.
[  FAILED  ] DNNTestNetwork.Inception_5h/0, where GetParam() = OCV/CPU (258 ms)
[ RUN      ] DNNTestNetwork.ENet/0, where GetParam() = OCV/CPU
double free or corruption (!prev)
Segmentation fault (core dumped)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-06 07:33:20,bug,Please do not hard-code CMAKE_CXX_STANDARD,"### System Information

OpenCV version: 4.7.0
For redistribution across several platforms in [conda-forge](https://github.com/conda-forge/opencv-feedstock)

### Detailed description

Hi, I'm a volunteer in conda-forge, the volunteer-driven packaging effort for conda/mamba (which I see is [mentioned](https://github.com/opencv/opencv/blob/4.x/doc/tutorials/introduction/general_install/general_install.markdown#third-party-packages--tutorial_general_install_prebuilt_thirdparty) here already).

We've been shipping opencv for about 7 years through the so-called [feedstock](https://github.com/conda-forge/opencv-feedstock), and packages can be installed like:
```
conda install -c conda-forge opencv # or mamba instead of conda
```
We have builds for linux-{64, aarch64, ppc64le}, osx-{64, arm64}, win-64, multiplied by (currently) CPython 3.8-3.11 as well as PyPy 3.8 & 3.9, multiplied by (currently) ffmpeg 5 & 6, multiplied by protobuf 3.21 & 4.23 [<- in progress].

Now to the actual issue:
-------

There are various [places](https://github.com/search?q=repo%3Aopencv%2Fopencv+CMAKE_CXX_STANDARD&type=code) where opencv hard-codes the C++ standard, rather than respecting a user input like:
```
cmake [options] -DCMAKE_CXX_STANDARD=17 ..
```

This is problematic because abseil (which opencv now depends on through protobuf, as of their 22.x series) has an ABI that's sensitive to the standard version and does not support any mixing of artefacts compiled with different versions. For distribution purposes, we therefore absolutely need to ensure that everything gets compiled consistently, and that currently means patching out these hard-coded `11`s and replacing them with `17`s. It would be much nicer (and not just for us, I'd argue), to be able to set `CMAKE_CXX_STANDARD` once.

That would likely involve some CMake if-conditions depending on whether `CMAKE_CXX_STANDARD` is set, and/or erroring out if it's set lower than what you require as a minimum.

### Steps to reproduce

Use the following code [search](https://github.com/search?q=repo%3Aopencv%2Fopencv+CMAKE_CXX_STANDARD&type=code)

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-05 12:47:03,bug,TopK layer not implemented,"### System Information

System: Ubuntu 20.04 
Complier: GCC 7.5.0
OpenCV version: 4.7.0 (build from source)
Torch version: 2.0.0+cpu
Torchvision version: 0.15.1+cpu
Onnx version: 1.11.0

### Detailed description

Following assertion in raised when TopK layer is being parsed. It seems that TopK layer in C++ is absent.

```console 
[ INFO:0@0.967] global onnx_importer.cpp:1006 handleNode DNN/ONNX: processing node with 2 inputs and 2 outputs: [TopK]:(onnx_node!/TopK) from domain='ai.onnx'
[ INFO:0@0.968] global onnx_importer.cpp:3333 parseCustomLayer DNN/ONNX: unknown node type, try using custom handler for node with 2 inputs and 2 outputs: [TopK]:(onnx_node!/TopK)
OpenCV(4.7.0-dev) Error: Unspecified error (Can't create layer ""onnx_node!/TopK"" of type ""TopK"") in getLayerInstance, file opencv/modules/dnn/src/net_impl.hpp, line 108
[ERROR:0@0.968] global onnx_importer.cpp:1064 handleNode DNN/ONNX: ERROR during processing node with 2 inputs and 2 outputs: [TopK]:(onnx_node!/TopK) from domain='ai.onnx'
[ INFO:0@0.968] global onnx_importer.cpp:1068 handleNode     Input[0] = 'input'
[ INFO:0@0.968] global onnx_importer.cpp:1068 handleNode     Input[1] = '/Constant_output_0'
[ INFO:0@0.968] global onnx_importer.cpp:1072 handleNode     Output[0] = 'output'
[ INFO:0@0.968] global onnx_importer.cpp:1072 handleNode     Output[1] = '5'
OpenCV(4.7.0-dev) Error: Unspecified error (> Node [TopK@ai.onnx]:(onnx_node!/TopK) parse error: OpenCV(4.7.0-dev) opencv/modules/dnn/src/net_impl.hpp:108: error: (-2:Unspecified error) Can't create layer ""onnx_node!/TopK"" of type ""TopK"" in function 'getLayerInstance'
> ) in handleNode, file opencv/modules/dnn/src/onnx/onnx_importer.cpp, line 1083
[DEBUG:0@0.968] global system.cpp:2881 restoreFPDenormalsState core: restore FP mxcsr flags = 0x00001fa3
Traceback (most recent call last):
  File ""topk.py"", line 43, in <module>
    opencv_net = cv2.dnn.readNetFromONNX(full_model_path)
cv2.error: OpenCV(4.7.0-dev) opencv/modules/dnn/src/onnx/onnx_importer.cpp:1083: error: (-2:Unspecified error) in function 'handleNode'
```

### Steps to reproduce

```python 


import cv2
import onnx, os
import torch
import torch.nn as nn
from onnx import shape_inference
from onnxsim import simplify

class TopK(nn.Module):

    def forward(self, x):
        return torch.topk(x, 3)

if __name__ == ""__main__"":

    onnx_model_path = ""models""
    onnx_model_name = ""topk.onnx""
    full_model_path = os.path.join(onnx_model_path, onnx_model_name)

    topk = TopK()
    x = torch.arange(1., 6.)
    v, i = topk(x)

    torch.onnx.export(
        topk,
        x,
        full_model_path,
        verbose=False,
        input_names=[""input""],
        output_names=[""output""],
        opset_version=11
    )
    
    onnx_model = onnx.load(full_model_path)
    onnx.checker.check_model(onnx_model)

    onnx_model = shape_inference.infer_shapes(onnx_model)
    onnx.save(onnx_model, full_model_path)

    opencv_net = cv2.dnn.readNetFromONNX(full_model_path)
   ```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-04 11:02:26,bug,Unable to compile OpenCV 4.7.0 on Ubuntu 22.04,"### System Information

OS: Ubuntu 22.04
OpenCV version 4.7.0
Compiler version: cmake version 3.22.1, gcc version 11.3.0 (Ubuntu 11.3.0-1ubuntu1~22.04.1)

### Detailed description

I'm trying to compile OpenCV for use with C++ and Python with CUDA enabled. For this, I cloned the OpenCV as well as the opencv-contrib repositories and tried to compile. I get no errors or warnings when running cmake, but the compilation breaks at around 50% with the following last output lines:
```
…
[ 47%] Building CXX object modules/dnn/CMakeFiles/opencv_dnn.dir/int8layers/layers_common.avx2.cpp.o
[ 47%] Building CXX object modules/dnn/CMakeFiles/opencv_dnn.dir/layers/layers_common.avx512_skx.cpp.o
[ 47%] Building CXX object modules/dnn/CMakeFiles/opencv_dnn.dir/int8layers/layers_common.avx512_skx.cpp.o
[ 47%] Linking CXX shared library ../../lib/libopencv_dnn.so
[ 47%] Linking CXX executable ../../bin/opencv_test_imgproc
[ 47%] Built target opencv_dnn
[ 47%] Built target opencv_test_imgproc
[ 48%] Building CXX object modules/cudev/test/CMakeFiles/opencv_test_cudev.dir/test_main.cpp.o
[ 48%] Linking CXX executable ../../../bin/opencv_test_cudev
[ 48%] Built target opencv_test_cudev
[ 48%] Linking CXX executable ../../bin/opencv_test_core
[ 48%] Built target opencv_test_core
make: *** [Makefile:166: all] Error 2
```

### Steps to reproduce

I ran the following commands:
```
git clone https://github.com/opencv/opencv.git
git clone https://github.com/opencv/opencv_contrib.git
cd opencv
mkdir build
cd build
cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_CUDA=ON -D WITH_CUDNN=ON -D WITH_CUBLAS=ON -D WITH_TBB=ON -D OPENCV_DNN_CUDA=ON -D OPENCV_ENABLE_NONFREE=ON -D CUDA_ARCH_BIN=8.9 -D OPENCV_EXTRA_MODULES_PATH=/home/luiz/Downloads/opencv_contrib/modules -D BUILD_EXAMPLES=OFF -D HAVE_opencv_python3=ON -D ENABLE_FAST_MATH=1 -D CUDA_FAST_MATH=1 -D WITH_CUBLAS=1 -D WITH_FFMPEG=1 ..
make -j 24
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-02 13:48:02,bug,Fix missuse of try_gpu in stitching/FeatherBlender,"Removed passing try_gpu parameter to FeatherBlender constructor because it only has sharpness parameter and does not support GPU branch,

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-02 12:06:13,bug,Torch interpolate's onnx graph fails while parsing,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: Ubuntu 20.04
Compiler & compiler version: GCC 7.5.0
Python version: 3.8.8


### Detailed description

Trying to read `.onnx` file created using `torch.nn.functional.interpolate`. I get this error

```console
opencv_net = cv2.dnn.readNetFromONNX(full_model_path)
cv2.error: OpenCV(4.7.0-dev) 
/opencv/modules/dnn/src/onnx/onnx_importer.cpp:1083: error: (-2:Unspecified error) in function 'handleNode'
> Node [Floor@ai.onnx]:(onnx_node!/Floor) parse error: OpenCV(4.7.0-dev) /opencv/modules/dnn/src/layers/elementwise_layers.cpp:261: error: (-215:Assertion failed) src.size == dst.size && src.type() == dst.type() && src.isContinuous() && dst.isContinuous() && src.type() == CV_32F in function 'forward'

```

### Steps to reproduce

```python


import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import onnx
import cv2
from onnx import shape_inference

class Resizer(nn.Module):

    def forward(self, image : torch.tensor):
        image = F.interpolate(
                    image,
                    size=None,
                    scale_factor=2,
                    mode=""bilinear"",
                    recompute_scale_factor=2,
                    align_corners=False,
                )
        return image



if __name__ == ""__main__"":

    resize = Resizer()
    x = torch.ones((1, 3, 608, 608))
    print(f""x: {x.shape}"")
    x_r = resize(x)
    print(f""x_r: {x_r.shape}"")

    onnx_model_path = ""models""

    # define the name of further converted model
    onnx_model_name = ""resize.onnx""

    os.makedirs(onnx_model_path, exist_ok=True)
    full_model_path = os.path.join(onnx_model_path, onnx_model_name)

    print(""\\t\\t ---> EXPORTING <---"")
    # model export into ONNX format
    torch.onnx.export(
        resize,
        x,
        full_model_path,
        verbose=False,
        input_names=[""input""],
        output_names=[""output""],
        opset_version=11
    )

    print(""\\t\\t ---> READING ONNX FILE IN ONNX <---"")
    onnx_model = onnx.load(full_model_path)
    onnx.checker.check_model(onnx_model)

    onnx_model = shape_inference.infer_shapes(onnx_model)
    onnx.save(onnx_model, full_model_path)


    print(""\\t\\t ---> READING ONNX in OPENCV FILE <---"")
    opencv_net = cv2.dnn.readNetFromONNX(full_model_path)

```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-30 17:14:33,bug,Build regression after OpenJPEG upgrade,"relates #23682

CI logs:
- https://pullrequest.opencv.org/buildbot/builders/4_x_noOCL-lin32/builds/100397
- https://pullrequest.opencv.org/buildbot/builders/4_x_noOCL_noICV-lin32/builds/100133
- https://pullrequest.opencv.org/buildbot/builders/precommit_linux32/builds/100198"
opencv/opencv,2023-05-27 09:24:11,bug,Update setup.py,"Fix error:
UnboundLocalError: local variable 'typing_stub_files' referenced before assignment

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-25 19:33:02,bug,Image must be cloned in text spotting,"### System Information

Same problem in scene_text_spotting than in https://github.com/opencv/opencv/issues/23683

### Detailed description

Image where result are printed must be a clone when --rgb is set to 1
https://github.com/opencv/opencv/blob/87331ca1a0ab3655ff3118e9bb8e44958e19b73a/samples/dnn/scene_text_spotting.cpp#L113


https://github.com/opencv/opencv/blob/87331ca1a0ab3655ff3118e9bb8e44958e19b73a/samples/dnn/scene_text_spotting.cpp#L134

### Steps to reproduce

scene_text_spotting --rgb=1 -i=""C:\\lib\\opencv_zoo\\models\\text_detection_db\\examples\\gsoc.jpg"" --dmp=C:\\dnn_models\\DB_IC15_resnet50.onnx --rmp=C:\\dnn_models\\crnn_cs.onnx
0: 'program'
**1: 'broar'**
2: 'view'
3: 'announced'
4: 'program'
5: '2022'
6: 'code'
7: 'summer'
8: 'google'

Result with clone
0: 'program'
**1: 'page'**
2: 'view'
3: 'announced'
4: 'program'
5: '2022'
6: 'code'
7: 'summer'
8: 'google'

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-24 13:18:06,bug,Fixed FPS computation on some videos for FFmpeg backend,"Address #https://github.com/opencv/opencv/issues/21006
Introduced in https://github.com/opencv/opencv/pull/7213. Most probably a typo.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-22 12:30:43,bug,Added check that YUYV input of cvtColor has even width.,"Resolves https://github.com/opencv/opencv/issues/21035

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-20 20:31:40,bug,Update matchTemplate with mask docstring,"* Update the documentation
* Minor optimizations to mask2 calculation

resolves https://github.com/opencv/opencv/issues/23585 alternatively to https://github.com/opencv/opencv/pull/23615

docs preview: https://pullrequest.opencv.org/buildbot/export/pr/23651/docs/df/dfb/group__imgproc__object.html#ga586ebfb0a7fb604b35a23d85391329be

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-18 22:43:07,bug,add charuco board check,"Added charuco board checking to avoid detection of incorrect board.
Fixes #23517


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-17 21:20:31,bug,Proposed solution for issue #23633,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
[https://github.com/opencv/opencv/issues/23633](url)
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-17 08:10:21,bug,opencv-python Memory leak caused by failed cvtColor() calls,"### System Information

OpenCV python version: 4.7.0.72
Operating System: Ubuntu 18.04
Python version: 3.8.0

### Detailed description

In my application frames are received from multiple sensors and processed as fast as possible. The processing step is designed in a way, that some exceptions can occur in small number and are ignored in order to keep the system running.

Multiple mistakes resulted that following case:
Some frames got passed to process using wrong `dtype` and caused an error at `cvtColor()` function. The exception got logged but ignored, the frame was dropped and the process kept running. And I noticed unusual memory usage in the long run.
 

### Steps to reproduce

### Sample code to present the failed conversion:
```python
import cv2
import numpy as np

if __name__ == ""__main__"":
    frame1 = np.zeros((480, 640, 3), dtype=np.uint8)
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    print(""Frame1 succesfully converted\\n"")

    frame2 = np.zeros((480, 640, 3), dtype=np.float64)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
    print(""Frame2 succesfully converted\\n"")
```
Produces the following output:
```
Frame1 succesfully converted

Traceback (most recent call last):
  File ""cv_memory_leak_test.py"", line 10, in <module>
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
cv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'
> Unsupported depth of input image:
>     'VDepth::contains(depth)'
> where
>     'depth' is 6 (CV_64F)
```

### Sample code to reproduce noticeable memory usage:
This code simulates what would happen if the conversion would fail a lot of times. In this code memory usage is measured by using a terminal command, if you are not using Ubuntu, you might want to use a different method to track memory usage.

Note that the memory **does get** cleaned up, but just after the program is terminated.

```python
import os

import cv2
import numpy as np

if __name__ == ""__main__"":
    # frame = np.zeros((480, 640, 3), dtype=np.uint8)   # No memory leaks, successful conversion
    frame = np.zeros((480, 640, 3), dtype=np.float64)   # Memory leak, failed conversion
    N = 1_000_000   # N = 1_000_000 will result (~1.3 GB reserved memory)
    for i in range(N):
        # Measure memory usage
        if (i % (N//100) == 0):
            memory_usage = os.popen('free -h').readlines()[1].split()
            print((""i = %06d | Memory used %s from %s"")%(i, memory_usage[2], memory_usage[1]))
        # Attempt conversion
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        except  Exception as e:
            pass    # For the sake of simplicity I pass here, the exception did get logged in the actual application
```

Possible output:
```
i = 000000 | Memory used 4,7G from 15G
i = 100000 | Memory used 4,8G from 15G
i = 200000 | Memory used 5,0G from 15G
i = 300000 | Memory used 5,1G from 15G
i = 400000 | Memory used 5,3G from 15G
i = 500000 | Memory used 5,4G from 15G
i = 600000 | Memory used 5,5G from 15G
i = 700000 | Memory used 5,7G from 15G
i = 800000 | Memory used 5,8G from 15G
i = 900000 | Memory used 6,0G from 15G
```

I checked some memory leak related issues in the repo, but not all of them. Please tag, if some issues relate.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-15 17:02:38,bug,iOS 4.7.0 release zip file layout changed without version bump,"### System Information

OpenCV version: 4.7.0
Operating System / Platform: macOS 13.3.1
Compiler & compiler version: Apple clang version 14.0.3 (clang-1403.0.22.14.1)

### Detailed description

Not sure if this is the best place to surface this issue, happy to move the discussion elsewhere with the maintainers.

We noticed that for the 4.7.0, the zipfile layout for the iOS framework has changed without version bump on May 15, 2023.

It's quite annoying that our CI and all of our developers' laptop were just mysterious broken this morning. 

In general, we would really appreciate it any changes to the release is associated with a different minor version bump, instead of directly editing the same release. 

Before 

```
$ unzip -l opencv-4.7.0-ios-framework.zip                                                              [12:54:49]
Archive:  opencv-4.7.0-ios-framework.zip
  Length      Date    Time    Name
---------  ---------- -----   ----
        0  12-28-2022 14:18   opencv2.framework/
       26  12-28-2022 14:18   opencv2.framework/Resources
        0  12-28-2022 14:18   opencv2.framework/Versions/
        0  12-28-2022 14:18   opencv2.framework/Versions/A/
        0  12-28-2022 14:18   opencv2.framework/Versions/A/Resources/
      559  12-28-2022 14:18   opencv2.framework/Versions/A/Resources/Info.plist
        0  12-28-2022 14:05   opencv2.framework/Versions/A/Headers/
   247939  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/imgproc.hpp
        0  12-28-2022 14:05   opencv2.framework/Versions/A/Headers/video/
    32477  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/video/tracking.hpp
    11476  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/video/tracking_c.h
    13726  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/video/background_segm.hpp
     2367  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/video/video.hpp
     2325  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/world.hpp
        0  12-28-2022 14:05   opencv2.framework/Versions/A/Headers/shape/
     3232  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/shape/emdL1.hpp
    10624  12-28-2022 14:03   opencv2.framework/Versions/A/Headers/shape/shape_distance.hpp
.... (more)
```

After 

```
$ unzip -l opencv-4.7.0-ios-framework.zip                                                                   [12:49:15]
Archive:  opencv-4.7.0-ios-framework.zip
  Length      Date    Time    Name
---------  ---------- -----   ----
200805626  05-15-2023 10:32   opencv-4.x-ios-framework.zip
---------                     -------
200805626                     1 file
```

### Steps to reproduce

Compare the `opencv-4.7.0-ios-framework.zip` prior to May 15th, 2023 (md5 checksum 92750cd0089e03b9991bb6cbaf113c49) to `opencv-4.7.0-ios-framework.zip` after May 15th, 2023 (md5 checksum 4855d3d7ad34e6ec6761a3df56999c58)

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-15 15:52:43,bug,remove unused var,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work https://github.com/opencv/opencv/issues/23617
- [X] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [X] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-14 20:21:53,bug,QRCodeDetector: don't floodFill with outside-of-image seedPoint,"Naive change for the #21532 issue.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-10 22:33:42,bug,LSTM layer with `batch_firs=True` fails,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: Ubuntu 20.04
Compiler & compiler version: GCC 7.5.0
Python version: 3.8.10

### Detailed description

When I convert LSTM torch model with `batch_first=True` and run it in OpenCV it fails with the following error: 

```bash
OpenCV(4.7.0-dev) Error: Assertion failed ((int)_numAxes == inputs[0].size()) in getMemoryShapes, file /home/abduragim/projects/opencv_proj/opencv/modules/dnn/src/layers/permute_layer.cpp, line 163
[ERROR:0@0.080] global net_impl.cpp:1164 getLayerShapesRecursively OPENCV/DNN: [Permute]:(onnx_node!/rnns/Transpose): getMemoryShapes() throws exception. inputs=1 outputs=0/1 blobs=0
[ERROR:0@0.080] global net_impl.cpp:1167 getLayerShapesRecursively     input[0] = [ 5 2 3 1 ]
[ERROR:0@0.080] global net_impl.cpp:1177 getLayerShapesRecursively Exception message: OpenCV(4.7.0-dev) /home/abduragim/projects/opencv_proj/opencv/modules/dnn/src/layers/permute_layer.cpp:163: error: (-215:Assertion failed) (int)_numAxes == inputs[0].size() in function 'getMemoryShapes'

Traceback (most recent call last):
  File ""test_lstm.py"", line 190, in <module>
    out_opencv = net.forward()
cv2.error: OpenCV(4.7.0-dev) /home/abduragim/projects/opencv_proj/opencv/modules/dnn/src/layers/permute_layer.cpp:163: error: (-215:Assertion failed) (int)_numAxes == inputs[0].size() in function 'getMemoryShapes'
```

### Steps to reproduce

```python

import torch
import torch.nn as nn
import onnxruntime
import numpy as np
import cv2
import random

# Set random seeds
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.set_printoptions(precision=15)


class LayerLSTM(nn.Module):
    def __init__(self, features, hidden_size, layers=1, batch_first=False):
        super(LayerLSTM, self).__init__()
        self.rnns = nn.LSTM(
            input_size=features,
            hidden_size=hidden_size,
            num_layers=layers,
            batch_first=batch_first
        )
        self.set_weights_to_ones()

    def forward(self, x, hx, cx):
        x, (hx, cx) = self.rnns(x, (hx,cx))
        return x, (hx, cx)

    def set_weights_to_ones(self):
        with torch.no_grad():
            for name, param in self.rnns.named_parameters():
                if 'weight' in name or 'bias' in name:
                    param.fill_(1.0)

if __name__ == ""__main__"":

    features    = 3
    hidden_size = 7
    batch_size  = 5
    seq_len     = 2
    layout      = True

    model = LayerLSTM(features, hidden_size, batch_first=layout)

    with torch.no_grad():
        if layout:
            x  = torch.ones(batch_size, seq_len, features)
        else:
            x  = torch.ones(seq_len, batch_size, features)

        hx = torch.ones(1, batch_size, hidden_size)
        cx = torch.ones(1, batch_size, hidden_size)

        print(x.shape, hx.shape, cx.shape, sep='\\n')
        out, (hn, cn) = model(x, hx, cx)
        
    torch.onnx.export(
       model,
       (x, hx, cx),
       './sample_lstm.onnx',
       verbose=True,
       input_names=['x', 'hx', 'cx'],
       output_names=['output']
    )
    
    net = cv2.dnn.readNetFromONNX('./sample_lstm.onnx')

    x_opencv  = x.numpy()[..., None]
    hx_opencv = hx.numpy()[..., None]
    cx_opencv = cx.numpy()[..., None]

    # Set input data
    net.setInput(x_opencv)
    net.setInput(hx_opencv, name=""hx"")
    net.setInput(cx_opencv, name=""cx"")
    
    out_opencv = net.forward()
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-08 16:58:42,bug,Crash in net.forward(),"### System Information

OpenCV version: 4.7.0
Operating System / Platform: windows10 x64
Compiler & compiler version: Code::Blocks 20.03 gcc 8.1.0

### Detailed description

software crash in net.forward() function and show this :
Process returned -1073741819 (0xC0000005)   execution time : 24.258 s

### Steps to reproduce

```
    net = cv::dnn::readNetFromCaffe(caffeConfigFile, caffeWeightFile);
    net.setPreferableBackend(DNN_TARGET_CPU);
    source.read(frame);
    inputBlob = cv::dnn::blobFromImage(frame, inScaleFactor, cv::Size(inWidth, inHeight), meanVal, false, false);
    net.setInput(inputBlob, ""data"");
    cv::Mat detection = net.forward(""detection_out"");
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [x] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-01 22:31:37,bug,Reformulated some pointer arithmetic to avoid (unsigned) overflow,"Although unsigned overflow is well-defined by the C++ standard, it is often unintentional or confusing and so UBSan has an option to warn about it.

It warned here:

`Addition of unsigned offset to 0x00017fd31b97 overflowed to 0x00017fd30c97`

In my own use of OpenCV at least, this is the only case of unsigned overflow.

I reformunated the pointer arithmetic to either add or subtract based on the sign of y. It's easier to understand this way vs always adding and wrapping around to an address *smaller* than the base address.
"
opencv/opencv,2023-04-28 09:35:07,bug,DNN/CUDA: Solve the bug of same shape broadcast with CUDA,"Merged after https://github.com/opencv/opencv/pull/23557

Fix yolo regression error mentioned in https://github.com/opencv/opencv/commit/e3e1f704a4c59a38747102d586925e05de1729aa#commitcomment-110475963

After fix this bug, the brute force test mentioned in https://github.com/opencv/opencv/issues/23556 will all passed with CUDA. (different shape broadcast will fallback to cpu)

Test is added in PR https://github.com/opencv/opencv/pull/23557


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-28 06:22:31,bug,fix nary elementwise bug in cpu,"After fix this bug, the brute force test mentioned in https://github.com/opencv/opencv/issues/23556 will all passed in CPU.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-28 00:34:05,bug,don't ignore documentation for cv::format in doxygen,"See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work issue https://github.com/opencv/opencv/issues/23553
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-25 05:27:28,bug,`VideoCapture`: fix opencv/contrib/#3474,"Fix [opencv/contrib/#3474](https://github.com/opencv/opencv_contrib/issues/3474) by allowing the bitstream filter to be applied to all h264/5 raw streams.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-20 02:58:57,bug,Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask',"### System Information

OpenCV version: 4.x
Operating System / Platform: Windows 11
Python version: 3.9.12

### Detailed description
Related issue: https://github.com/opencv/opencv/issues/23278
Reading [this onnx model](https://github.com/opencv/opencv/files/10826659/model3.zip) is correct, this model can be forwarded on CPU.

However, it cannot be forwarded on CUDA. Seems like reshape layer have a bug.
```shell
[ INFO:0@3.303] global onnx_importer.cpp:831 cv::dnn::dnn4_v20221220::ONNXImporter::populateNet DNN/ONNX: loading ONNX v7 model produced by 'tf2onnx':1.13.0 2c1db5. Number of nodes = 95, initializers = 22, inputs = 1, outputs = 1
[ INFO:0@3.304] global onnx_importer.cpp:725 cv::dnn::dnn4_v20221220::ONNXImporter::parseOperatorSet DNN/ONNX: ONNX opset version = 13
[ INFO:0@3.304] global onnx_importer.cpp:740 cv::dnn::dnn4_v20221220::ONNXImporter::parseOperatorSet DNN/ONNX: unknown domain='ai.onnx.ml' version=13. No dispatch map, you may need to register 'custom' layers.
[ INFO:0@3.423] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/0_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.426] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/0_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.427] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/0_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.427] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/0_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.428] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/0_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.429] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/0_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.430] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack1_block1_shortcut_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.430] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack1_block1_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.432] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack1_block1_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.433] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack1_block1_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.434] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack1_block1_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.434] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack1_block1_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.435] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack1_block1_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.435] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack1_block1_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.436] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack1_block1_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.436] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack1_block1_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.437] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack1_block1_add/add) from domain='ai.onnx'
[ INFO:0@3.438] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack1_block2_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.438] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack1_block2_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.439] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack1_block2_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.440] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack1_block2_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.440] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack1_block2_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.441] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack1_block2_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.441] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack1_block2_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.442] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack1_block2_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.442] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack1_block2_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.443] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack1_block2_add/add) from domain='ai.onnx'
[ INFO:0@3.444] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack2_block1_shortcut_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.444] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack2_block1_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.445] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack2_block1_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.446] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack2_block1_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.446] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack2_block1_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.447] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack2_block1_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.447] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack2_block1_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.447] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack2_block1_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.448] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack2_block1_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.448] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack2_block1_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.449] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack2_block1_add/add) from domain='ai.onnx'
[ INFO:0@3.450] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack2_block2_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.450] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack2_block2_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.451] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack2_block2_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.451] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack2_block2_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.452] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack2_block2_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.452] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack2_block2_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.452] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack2_block2_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.453] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack2_block2_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.454] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack2_block2_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.454] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack2_block2_add/add) from domain='ai.onnx'
[ INFO:0@3.455] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack3_block1_shortcut_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.455] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack3_block1_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.455] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack3_block1_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.456] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack3_block1_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.456] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack3_block1_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.456] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack3_block1_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.457] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack3_block1_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.457] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack3_block1_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.457] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack3_block1_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.458] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack3_block1_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.458] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack3_block1_add/add) from domain='ai.onnx'
[ INFO:0@3.458] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack3_block2_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.459] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack3_block2_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.459] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack3_block2_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.460] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack3_block2_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.460] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack3_block2_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.461] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack3_block2_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.462] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack3_block2_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.463] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack3_block2_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.463] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack3_block2_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.464] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack3_block2_add/add) from domain='ai.onnx'
[ INFO:0@3.466] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack4_block1_shortcut_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.466] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack4_block1_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.467] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack4_block1_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.468] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack4_block1_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.469] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack4_block1_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.469] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack4_block1_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.470] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack4_block1_2_PReLU/Relu_1) from domain='ai.onnx'
[ INFO:0@3.470] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack4_block1_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.471] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack4_block1_2_PReLU/add) from domain='ai.onnx'
[ INFO:0@3.471] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack4_block1_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.472] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack4_block1_add/add) from domain='ai.onnx'
[ INFO:0@3.472] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack4_block2_1_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.473] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack4_block2_1_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.474] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/stack4_block2_2_bn/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.474] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack4_block2_2_PReLU/Relu) from domain='ai.onnx'
[ INFO:0@3.475] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Neg]:(onnx_node!ResNet18/stack4_block2_2_PReLU/Neg_1) from domain='ai.onnx'
[ INFO:0@3.475] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Relu]:(onnx_node!ResNet18/stack4_block2_2_PReLU/Relu_1) from domain='ai.onnx'
[ERROR:0@6.071] global net_impl.cpp:1164 cv::dnn::dnn4_v20221220::Net::Impl::getLayerShapesRecursively OPENCV/DNN: [Reshape]:(onnx_node!ResNet18/E_flatten/Reshape): getMemoryShapes() throws exception. inputs=1 outputs=1/1 blobs=0
[ INFO:0@3.476] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/stack4_block2_2_PReLU/mul) from domain='ai.onnx'
[ INFO:0@3.476] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack4_block2_2_PReLU/add) from domain='ai.onnx'
[ERROR:0@6.072] global net_impl.cpp:1167 cv::dnn::dnn4_v20221220::Net::Impl::getLayerShapesRecursively     input[0] = [ 1 1 1 512 ]
[ INFO:0@3.477] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 3 inputs and 1 outputs: [Conv]:(onnx_node!ResNet18/stack4_block2_2_conv/Conv2D) from domain='ai.onnx'
[ INFO:0@3.477] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Add]:(onnx_node!ResNet18/stack4_block2_add/add) from domain='ai.onnx'
[ INFO:0@3.478] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 5 inputs and 1 outputs: [BatchNormalization]:(onnx_node!ResNet18/E_batchnorm/FusedBatchNormV3) from domain='ai.onnx'
[ INFO:0@3.478] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 1 inputs and 1 outputs: [Transpose]:(onnx_node!ResNet18/E_batchnorm/FusedBatchNormV3__210) from domain='ai.onnx'
[ INFO:0@3.479] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Reshape]:(onnx_node!ResNet18/E_flatten/Reshape) from domain='ai.onnx'
[ INFO:0@3.479] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [MatMul]:(onnx_node!ResNet18/E_dense/MatMul) from domain='ai.onnx'
[ERROR:0@6.072] global net_impl.cpp:1171 cv::dnn::dnn4_v20221220::Net::Impl::getLayerShapesRecursively     output[0] = [ 1 25088 ]
[ERROR:0@6.072] global net_impl.cpp:1177 cv::dnn::dnn4_v20221220::Net::Impl::getLayerShapesRecursively Exception message: OpenCV(4.7.0-dev) C:\\Users\\Zoom\\Desktop\\OpenCV_China\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:150: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'

[ INFO:0@6.017] global onnx_importer.cpp:1002 cv::dnn::dnn4_v20221220::ONNXImporter::handleNode DNN/ONNX: processing node with 2 inputs and 1 outputs: [Mul]:(onnx_node!ResNet18/pre_embedding/batchnorm/mul_1) from domain='ai.onnx'
unknown file: error: C++ exception with description ""OpenCV(4.7.0-dev) C:\\Users\\Zoom\\Desktop\\OpenCV_China\\opencv\\modules\\dnn\\src\\layers\\reshape_layer.cpp:150: error: (-1:Backtrace) Can't infer a dim denoted by -1 in function 'cv::dnn::computeShapeByReshapeMask'
"" thrown in the test body.


Process finished with exit code 1

```

### Steps to reproduce

```cpp
auto imageToTest = ""path/to/img"";
auto modelToTest = ""path/to/model"";
int modelInputWidth = 112;
int modelInputHeight = 112;

cv::Size currSize = cv::Size(modelInputWidth, modelInputHeight);
std::string modelToTestOnnx = modelToTest;
std::string imagefilename = imageToTest;
unsigned int num_inferences = 100;

cv::dnn::Net net = cv::dnn::readNetFromONNX(modelToTestOnnx);

net.setPreferableBackend(cv::dnn::DNN_BACKEND_CUDA);
net.setPreferableTarget(cv::dnn::DNN_TARGET_CUDA);

cv::Mat img = cv::imread(imagefilename, cv::IMREAD_ANYCOLOR);
cv::Mat resized;
cv::resize(img, resized, currSize);

std::vector<cv::Mat> imgBatch = { resized };
bool swaprbchannels = false;
cv::Mat blob = cv::dnn::blobFromImages(imgBatch, 1.0f / 255.0f, cv::Size(), cv::Scalar(), swaprbchannels, false, CV_32F);

net.setInput(blob);

std::vector<cv::String> unconnectedOutLayerNames = net.getUnconnectedOutLayersNames();

std::vector<cv::Mat> outputs;
outputs.clear();

auto timeLoadModelPlusInference1 = std::chrono::high_resolution_clock::now();

net.forward(outputs, unconnectedOutLayerNames);
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-04-19 16:01:17,bug,CharucoDetector::detectBoard returns garbadge in case of board parameters mismatch,"### System Information

OpenCV version: 4.7.0+

### Detailed description

Issue origin: https://github.com/opencv/opencv/pull/23486#issuecomment-1513117982

In case if board width and height are swapped the method detects board, but result is inadequate. Marker IDs and corners coordinates looks  realistic , but does not match actual board layout.

### Steps to reproduce

Board: from PR: https://github.com/opencv/opencv/pull/23363

How to reproduce:
- With interactive calibration tool (copy interactive-calibration/defaultConfig.xml to current dir and replace charuco_dict value to 5): ./bin/opencv_interactive-calibration -w=6 -h=4 -t=charuco
- With calibration.cpp (PR https://github.com/opencv/opencv/pull/23486): `./example_cpp_calibration -w=6 -h=4 -pt=charucoboard -ad=5` 

Proposed behaviour:
- Return empty list for ids and coordinates
- Add return value and return false

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-04-19 12:04:18,bug,DNN: try to fix the sporadic crashes in perf_dnn,"Address https://github.com/opencv/opencv/issues/23465
Merge with test: https://github.com/opencv/opencv_extra/pull/1058

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-17 18:56:11,bug,more sprintf removals,"This first 3 commits are from https://github.com/opencv/opencv/pull/23055 (I'll rebase this after that is merged).

The last commit is an incomplete proposal to deprecated `convertTypeStr` and make a new variant using `std::string`.  If it's acceptable, I'll complete the PR to use the new method everywhere.

```
force_builders=Linux OpenCL,Win64 OpenCL
```"
opencv/opencv,2023-04-14 12:52:32,bug,cv2.aruco.calibrateCameraCharuco does not exist,"### System Information

OpenCV python version: 4.7.0
Operating System / Platform: Ubuntu 22.04
Python version: 3.10

### Detailed description

I am working on calibrating a camera with a Charuco board. I ported my pre-OpenCV 4.5 code to 4.7 now, but I am stuck at actually calibrating the camera using the detected points since a necessary function is apparently not included in the generated python bindings:

```
Traceback (most recent call last):
  File ""/home/fe/korjakow/Projects/intrinsic_calibrator/install/camera_calibration/lib/python3.10/site-packages/camera_calibration/camera_calibrator.py"", line 278, in on_mouse
    self.c.do_calibration()
  File ""/home/fe/korjakow/Projects/intrinsic_calibrator/install/camera_calibration/lib/python3.10/site-packages/camera_calibration/calibrator.py"", line 1017, in do_calibration
    self.cal_fromcorners(self.good_corners)
  File ""/home/fe/korjakow/Projects/intrinsic_calibrator/install/camera_calibration/lib/python3.10/site-packages/camera_calibration/calibrator.py"", line 774, in cal_fromcorners
    reproj_err, self.intrinsics, self.distortion, rvecs, tvecs = cv2.aruco.calibrateCameraCharuco(
AttributeError: module 'cv2.aruco' has no attribute 'calibrateCameraCharuco'
```

According to the docs the function should exist though:
https://docs.opencv.org/4.7.0/d4/d17/namespacecv_1_1aruco.html

Would it be possible to add it back in?

### Steps to reproduce

```python
import cv2
cv2.aruco.calibrateCameraCharuco(None, None, None, None, None, None)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-04-09 12:35:59,bug,Fix ONNX parser for single-layer LSTM hidden and cell states,"### Fix ONNX parser for single-layer LSTM hidden and cell states

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake


This PR addresses #21118 [issue](https://github.com/opencv/opencv/issues/21118). The problem is that the ONNX parser is unable to read the hidden state and cell state for single-layer LSTMs. This PR fixes the issue by updating the parser to correctly read hidden and cell states.
"
opencv/opencv,2023-09-23 14:01:14,question,Allow find_package(OpenCV) in CMake to return paths without versions on Linux,"### Describe the feature and motivation

Allow applications finding OpenCV by CMake to specify, whether they want to link to the path containing OpenCV version, e.g. `/usr/lib/libopencv_imgproc.so.408` or to version-less file, such as `/usr/lib/libopencv_imgproc.so`.

This could be done by setting a CMake variable before calling `find_package`, like so:
```cmake
set(OPENCV_USE_PATHS_WITH_VERSIONS 0)
find_package(OpenCV)
```

To keep compatibility, the default value should be `1`.

NOTE: I'd be happy to prepare a PR and implement this, but I'd need a confirmation from someone on the team, that this is desired, so I don't waste my time :)

### Additional context

On my system (Arch Linux) the path to actual `.so` is assigned to OpenCV CMake targets in `/usr/lib/cmake/opencv4/OpenCVModules-release.cmake` file:
```cmake
...

# Import target ""opencv_imgproc"" for configuration ""Release""
set_property(TARGET opencv_imgproc APPEND PROPERTY IMPORTED_CONFIGURATIONS RELEASE)
set_target_properties(opencv_imgproc PROPERTIES
  IMPORTED_LOCATION_RELEASE ""${_IMPORT_PREFIX}/lib/libopencv_imgproc.so.4.8.0""
  IMPORTED_SONAME_RELEASE ""libopencv_imgproc.so.408""
  )

...
```

Because the full path is being hardcoded here, applications link to a very specific library version. When OpenCV is upgraded, the applications can no longer find the version and fail to load. An example of this happening often is a `nomacs` package: https://aur.archlinux.org/packages/nomacs (see comments). I encounter this problem regularly and have to recompile `nomacs` each time OpenCV is upgraded on my machine."
opencv/opencv,2023-09-19 02:03:14,question,Installing opencv by conda makes pytorch CUDA not available,"### System Information

OpenCV python version: 4.6.0
Operating System / Platform: Ubuntu 16.04
Python version (conda env): 3.11.5
GPU: NVIDIA GeForce RTX 3090
GPU Driver Version: 465.19.01

### Detailed description

After installing opencv by this command though conda, pytorch cuda is not available anymore:

```
conda install -c conda-forge opencv
```

This command replaced the CUDA-enabled pytorch with CPU-enabled-only pytroch:

<img width=""1034"" alt=""image"" src=""https://github.com/opencv/opencv/assets/93981430/a0a6cd56-a9ff-4e3c-a487-ab6932c64401"">


results in:

```
(mouse) $ python -c ""import torch; print(torch.cuda.is_available())""
False
```

### Steps to reproduce

1. Install pytorch following the official instruction: `conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia`
3. Install opencv: `conda install -c conda-forge opencv`


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-18 09:55:52,question,"bug, the video is being displayed accelerated","### System Information

windows 10
python 3.11.4
opencv version = Name: opencv-python
Version: 4.8.0.76
Summary: Wrapper package for OpenCV python bindings.
Home-page: https://github.com/opencv/opencv-python
Author:
Author-email:
License: Apache 2.0
Location: C:\\Users\\Administrator\\Desktop\\ALL-IN-1-VAREJO\\venv\\Lib\\site-packages
Requires: numpy, numpy, numpy, numpy, numpy
Required-by: cvzone, openvino-dev, ultralytics

### Detailed description

###THE VIDEO SHOWN IS BEING ACCELERATED

import cv2

cap = cv2.VideoCapture('contador_pessoas/pessoas1.mp4')

while(True):
    ret, frame = cap.read()
    if not ret:
        print(""Leitura não bem sucedida. Saindo do loop."")
        break
    cv2.imshow('frame', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

https://github.com/opencv/opencv/assets/76708662/88af912c-75d4-490d-864d-1e5170ac9364




### Steps to reproduce

###THE VIDEO SHOWN IS BEING ACCELERATED

import cv2

cap = cv2.VideoCapture('contador_pessoas/pessoas1.mp4')

while(True):
    ret, frame = cap.read()
    if not ret:
        print(""Leitura não bem sucedida. Saindo do loop."")
        break
    cv2.imshow('frame', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

https://github.com/opencv/opencv/assets/76708662/88af912c-75d4-490d-864d-1e5170ac9364




### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-11 09:50:20,question,resize() function interface ,"### System Information

// for c++ user
OpenCV version: 4.8.0
Operating System / Platform: windows10
Compiler & compiler version: VS2017

### Detailed description

我在使用resize时，遇到问题，我不确定这算不算bug.
对同一幅图片中的相同ROI区域：
当背景区域像素值为0、前景区域像素值为1时，我进行resize后得到结果
当背景区域像素值为0、前景区域像素值为255时，我进行resize后得到结果
这两种结果有时差别非常大。我想请问：这是什么原因造成的？为什么？
我在resize时，使用的默认的双线性插值(INTER_LINEAR)



### Steps to reproduce

eg:
cv::mat src0;  // src0 is a image which pixel value of foreground is 1, and pixel value of background is 0.
cv::dst0;
cv::resize(src0, dst0, cv::Size(width, height));

cv::mat src1; // src1 is a image which pixel value of foreground is 255, and pixel value of background is 0.
cv::dst1;
cv::resize(src1, dst1, cv::Size(width, height));

wherein, src0 and src1 are the same image with only different pixel value of foreground at the same roi region.
But, there is a big difference between dst0 and dst1, why?


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-09-06 06:59:31,question,About installing Mediapipe,"I want to use the compiled opencv with cuda (including contrib) to install Mediapipe, but it cannot be installed.

It can only be successfully installed through pip's opencv python installation. How should we solve it."
opencv/opencv,2023-08-27 21:14:53,question,"Undefined symbols for architecture arm64:   ""cv::DISOpticalFlow::create(int)"", referenced from: ...","Hi, i'm on mac m2 pro and try to build using opencv and got this result, is this means DISOpticalFLow is not available for arm64 ?

Or I make some mistakes ?

Here's what my compile command :

```
g++ disflow.cpp -o disflow -O2 -std=c++11 -I/usr/local/include/opencv4/ || { echo ""Compilation failed""; exit 1; }
```

TIA"
opencv/opencv,2023-07-19 07:58:04,question,"Pods/OpenCV2_iOS/opencv2.framework/opencv2(opencl_kernels_calib3d.o), building for iOS Simulator, but linking in object file built for iOS","### System Information

OpenCV version: 4.3.0
Operating System / Platform: Apple M1 Pro / iOS
Xcode Version - 14.3.1

### Detailed description

After adding opencv through pod I am getting this issue.

### Steps to reproduce

Create new project and add opencv dependency.
pod 'OpenCV2'

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-11 01:20:51,question,Segmentation fault in cv::imwrite,"### System Information

OpenCV version: 4.5.5
Ubuntu 20.04
gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### Detailed description

Here is the core dump info:

Program terminated with signal SIGSEGV, Segmentation fault.
#0  0x000000100000000f in ?? ()
[Current thread is 1 (Thread 0x7f7c5719b700 (LWP 15608))]
(gdb) bt
#0  0x000000100000000f in ?? ()
#1  0x00007f7c7821329a in findEncoder ()
    at /opt/vcpkg/buildtrees/opencv4/src/4.5.5-923325adf5.clean/modules/imgcodecs/src/loadsave.cpp:303
#2  0x00007f7c78213f1a in imwrite_ ()
    at /opt/vcpkg/buildtrees/opencv4/src/4.5.5-923325adf5.clean/modules/imgcodecs/src/loadsave.cpp:728
#3  cv::imwrite () at /opt/vcpkg/buildtrees/opencv4/src/4.5.5-923325adf5.clean/modules/imgcodecs/src/loadsave.cpp:810



### Steps to reproduce

It's not easy to reproduce.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-07-04 20:12:34,question,"Unknown CMake command ""ocv_add_application""","### System Information

OpenCV version: 4.8.0
Operating System / Platform: Ubuntu 20.04
Compiler & compiler version: GCC 9.3.0

### Detailed description

haadi@haadi:~/opencv/apps/interactive-calibration$ cmake .
CMake Warning (dev) in CMakeLists.txt:
  No project() command is present.  The top-level CMakeLists.txt file must
  contain a literal, direct call to the project() command.  Add a line of
  code such as

    project(ProjectName)

  near the top of the file, but after cmake_minimum_required().

  CMake is pretending there is a ""project(Project)"" command on the first
  line.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Error at CMakeLists.txt:3 (ocv_add_application):
  Unknown CMake command ""ocv_add_application"".


CMake Warning (dev) in CMakeLists.txt:
  No cmake_minimum_required command is present.  A line of code such as

    cmake_minimum_required(VERSION 3.16)

  should be added at the top of the file.  The version specified may be lower
  if you wish to support older CMake versions for this project.  For more
  information run ""cmake --help-policy CMP0000"".
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Configuring incomplete, errors occurred!
See also ""/home/haadi/opencv/apps/interactive-calibration/CMakeFiles/CMakeOutput.log"".

### Steps to reproduce

I am trying to use the interactive camera calibration app provided by the opencv. But when I run the cmake . inside the interactive_camera_calibration, it's showing the above error. Please, can someone help me asap, I need to submit the work and this is an integeral part of it. I would highly appreciate any kind of help,

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-06-24 14:58:36,question,"How to feed mat of 6*224*224,CV_32FC(6) into cvdnn?","How to feed mat of 6x224x224,CV_32FC(6) into cvdnn? My deep learning model input size is 1x6x224x224
Using the cv::merge method to merge 6 mats of type CV_32FC1, the size is 224x224x6, how to quickly change the data dimension to 6x224x224

"
opencv/opencv,2023-06-22 11:09:28,question,CV2 videowriter generated video fail to display in html code and broswer,"```
### System Information


video = cv2.VideoCapture(""./ATPSingle34YoshihitoNishioka.mp4"")     
ret, frame = video.read()
fps, w, h = 30, frame.shape[1], frame.shape[0]                     
result = cv2.VideoWriter('filename.mp4',cv2.VideoWriter_fourcc(*'mp4v'),fps, (w,h))
        
while(True):
    ret, frame = video.read()
    if ret == True:
        result.write(frame)
        cv2.imshow('Frame', frame)
        if cv2.waitKey(1) & 0xFF == ord('s'):
            break

    # Break the loop
    else:
        break

# When everything done, release
# the video capture and video
# write objects
video.release()
result.release()
    
# Closes all the frames
cv2.destroyAllWindows()

print(""The video was successfully saved"")
```

### Detailed description

video is https://drive.google.com/file/d/1fqRjGBLW5F7maZOqc6n1PIDeF2rwNZVl/view?usp=sharing

You can download an try to put it in the browser

I also try to put it in Firefox, the error is No Video With Supported Format And MIME Type Found

### Steps to reproduce

```
video = cv2.VideoCapture(""./ATPSingle34YoshihitoNishioka.mp4"")     
ret, frame = video.read()
fps, w, h = 30, frame.shape[1], frame.shape[0]                     
result = cv2.VideoWriter('filename.mp4',cv2.VideoWriter_fourcc(*'mp4v'),fps, (w,h))
        
while(True):
    ret, frame = video.read()
    if ret == True:
        result.write(frame)
        cv2.imshow('Frame', frame)
        if cv2.waitKey(1) & 0xFF == ord('s'):
            break

    # Break the loop
    else:
        break

# When everything done, release
# the video capture and video
# write objects
video.release()
result.release()
    
# Closes all the frames
cv2.destroyAllWindows()

print(""The video was successfully saved"")
```"
opencv/opencv,2023-06-09 15:11:17,question,RotatedRect.GetVertices() sometimes gives wrong vertices,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: Windows 10

### Detailed description

If you have two RotatedRects, one sloping left and the other sloping right, GetVertices function returns the wrong order of vertices for one of them. In addition to the order of the vertices being wrong, the function jumbles up x and y coordinates of the actual vertices (joins x coordinate of a vertex with y coordinate of another vertex, and says that's one vertex)

### Steps to reproduce

If you have two RotatedRects, one sloping left and the other sloping right, GetVertices function returns the wrong order of vertices for one of them. In addition to the order of the vertices being wrong, the function jumbles up x and y coordinates of the actual vertices (joins x coordinate of a vertex with y coordinate of another vertex, and says that's one vertex)

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-31 08:45:54,question,"Rebuild the library with Windows, GTK+ 2.x or Cocoa support","### System Information

OpenCV version: 4.7.0
Operating System / Platform: Ubuntu 18.04
Compiler & compiler version: GCC 7.5.0
pkg-config gtk+-2.0 --modversion （2.24.32）
pkg-config gtk+-3.0 --modversion   （3.22.30）
pkg-config gthread-2.0 --modversion  （2.56.4）

### Detailed description

terminate called after throwing an instance of 'cv::Exception'
  what():  OpenCV(4.7.0) /usr/src/opencv-4.7.0/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'


### Steps to reproduce

in opencv-4.7.0 dir:
sudo mkdir build
cd build
sudo cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D OPENCV_EXTRA_MODULES_PATH=/usr/src/opencv-4.7.0/opencv_contrib-4.7.0/modules -D WITH_CUDA=ON -D ENABLE_FAST_MATH=1 -D CUDA_FAST_MATH=1 -D WITH_CUBLAS=1 -D WITH_CUDNN=1 -DWITH_GTK=ON ..
sudo make -j32
sudo make install

I have turned on the support for GPU and DNN modules in my compilation options, and I can compile successfully, but when I use cv::imshow() in the code, it will throw a display error, just like the error error message I provided , could you please help me to see how I can fix this problem? thank you!


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-23 10:03:44,question,stitching images ,"### Describe the feature and motivation

- Images not stitched sequentially.
- How to stitch sequentially.
- first image in right site and last image in left site stitch.
- I am using this code [stitching_detailed.cpp](https://docs.opencv.org/4.x/d9/dd8/samples_2cpp_2stitching_detailed_8cpp-example.html)
### Additional context

_No response_"
opencv/opencv,2023-05-21 19:39:31,question,Mat pointsMat = Mat(points).reshape(1);And pointsMat.u == nullptr,"### System Information

// example for c++ user
OpenCV version: 4.7.0
Operating System / Platform: 
版本	Windows 10 专业版
版本号	22H2
安装日期	‎2022/‎1/‎29
操作系统内部版本	19045.2846
序列号	5CD1430KTF
体验	Windows Feature Experience Pack 120.2212.4190.0

Compiler & compiler version: qt6.5 mingw1120_64



### Detailed description

Compile reference：https://www.cnblogs.com/zdt168/articles/16663379.html


bug is pointsMat.u == nullptr

the code from https://github.com/opencv/opencv/tags/4.7.0/samples/cpp
void MainWindow::on_mat_operations_clicked()
{
    QString fileName = "":/data/lena.jpg"";
    QMessageBox::information(this,""图像读取操作"",""按任意键继续..."");
    {
        Mat src = loadFromQrc(fileName);
        imshow( ""src"", src );
        cv::waitKey();
    }
    {
        Mat src = loadFromQrc(fileName,IMREAD_GRAYSCALE);
        imshow( ""src"", src );
        cv::waitKey();
    }
    {
        QMessageBox::information(this,""图像保存操作"",""按任意键继续..."");
        Mat img(4,4,CV_8U);
        //! [Save image]
        QString filepath= QFileDialog::getSaveFileName(this,tr(""保存文件""),""."",tr("" (*.png)""));
        if(!filepath.isEmpty()){
            imwrite(filepath.toStdString(), img);
            //! [Save image]
        }
    }
    // 访问像素值
    {
        QMessageBox::information(this,""图像像素访问"",""按任意键继续..."");
        Mat img(4,4,CV_8U);//未初始化赋值
        int y = 1, x = 0;
        {
            Scalar intensity = img.at<uchar>(y, x);
            cout<<""(""<<y<<"",""<<x<<"")""<<""Point: ""<<intensity<<endl;
        }
        {
            Scalar intensity = img.at<uchar>(Point(x, y));
            cout<<""(""<<x<<"",""<<y<<"")""<<""Point: ""<<intensity<<endl;
        }
        {
            Vec3b intensity = img.at<Vec3b>(y, x);
            uchar blue = intensity.val[0];
            uchar green = intensity.val[1];
            uchar red = intensity.val[2];
            /**
             * 由于uchar是无符号整数类型，因此我们需要使用int()函数将其转换为整型，以便正确打印
             */
            cout<<""(""<<y<<"",""<<x<<"")""<<""Point: ""<<intensity<<"", blue = ""<<int(blue)<<"", green = ""<<int(green)<<"", red = ""<<int(red)<<endl;
        }
        {
            Vec3f intensity = img.at<Vec3f>(y, x);
            float blue = intensity.val[0];
            float green = intensity.val[1];
            float red = intensity.val[2];
            cout<<""float: (""<<y<<"",""<<x<<"")""<<""Point: ""<<intensity<<"", blue = ""<<blue<<"", green = ""<<green<<"", red = ""<<red<<endl;
        }
        {
            img.at<uchar>(y, x) = 128;
            cout<<""img.at(): (""<<y<<"",""<<x<<"")""<<""Point: ""<<(int)img.at<uchar>(y, x)<<endl;
        }
        {
            int i = 0;
            //点向量集合构造Mat
            std::vector<Point2f> points;//2维点坐标
            //... fill the array
            points.push_back(cv::Point2f(10.0f, 20.0f));
            points.push_back(cv::Point2f(30.0f, 40.0f));
            points.push_back(cv::Point2f(50.0f, 60.0f));
            Mat pointsMat = Mat(points);

            //! [Point access]
            Point2f point = pointsMat.at<Point2f>(i, 0);
            //! [Point access]
            cout<<""Point: ""<<point<<endl;
        }
    }
    // 内存管理和引用计数
    {
        QMessageBox::information(this,""内存管理和引用计数"",""按任意键继续..."");
        //! [Reference counting 1]
        std::vector<Point3f> points;
        // .. fill the array
        points.push_back(cv::Point3f(10.0f, 20.0f,25.0f));
        points.push_back(cv::Point3f(30.0f, 40.0f, 45.0f));
        points.push_back(cv::Point3f(50.0f, 60.0f, 65.0f));
        Mat pointsMat = Mat(points).reshape(1);
        std::cout << ""pointsMat = "" << std::endl << pointsMat << std::endl;
        cout << ""pointsMat size: "" << pointsMat.size() << endl;
        UMatData* u = pointsMat.u;
        if (u != nullptr) {
            cout << ""refcount: "" << u->refcount << endl;
            cv::waitKey();
        }else{
            cout << ""u == nullptr ""<< endl;//u == nullptr
        }
    }
    {
        //! [Reference counting 2]
        Mat img = loadFromQrc(fileName);
        UMatData* u = img.u;
        if (u != nullptr) {
            cout << ""imgInit: refcount: "" << u->refcount << endl;
            cv::waitKey();
        }else{
            cout << ""imgInit: u == nullptr ""<< endl;//u == nullptr
        }
        Mat img1 = img.clone();
        //! [Reference counting 2]
        u = img.u;
        if (u != nullptr) {
            cout << ""img: refcount: "" << u->refcount << endl;
            cv::waitKey();
        }else{
            cout << ""img: u == nullptr ""<< endl;//u == nullptr
        }
        u = img1.u;
        if (u != nullptr) {
            cout << ""img1: refcount: "" << u->refcount << endl;
            cv::waitKey();
        }else{
            cout << ""img1: u == nullptr ""<< endl;//u == nullptr
        }
        cout<<""img refcount: ""<<img.u->refcount<<endl;
        cout<<""img1 refcount: ""<<img1.u->refcount<<endl;
        cv::waitKey();
    }
    return;
    {
        //! [Reference counting 3]
        Mat img = imread(""image.jpg"");
        Mat sobelx;
        /**
         * Sobel算子边缘检测的函数
         * @brief Sobel
         */
        Sobel(img, sobelx, CV_32F, 1, 0);
        cout<<""img refcount: ""<<img.u->refcount<<endl;
        cout<<""img1 refcount: ""<<sobelx.u->refcount<<endl;
        cv::waitKey();
    }
    // 基本操作
    {
        QMessageBox::information(this,""基本操作"",""按任意键继续..."");
        Mat img;
        {
            //! [Set image to black]
            img = Scalar(0);
            imshow( ""img"", img );
            //! [Set image to black]
            cout<<""blackImag""<<endl;
            cv::waitKey();
        }
        {
            //! [Select ROI]
            Rect r(10, 10, 100, 100);
            Mat smallImg = img(r);
            //! [Select ROI]
            cout<<""Select ROI""<<endl;
            imshow( ""smallImg"", smallImg);
            cv::waitKey();
        }
    }
    {
        QMessageBox::information(this,""BGR to Gray"",""按任意键继续..."");
        //! [BGR to Gray]
//        Mat img = imread(""image.jpg""); // loading a 8UC3 image
        Mat img = loadFromQrc(fileName); // loading a 8UC3 image
        imshow( ""src"", img);
        Mat grey;
        cvtColor(img, grey, COLOR_BGR2GRAY);
        imshow( ""grey"", img);
        //! [BGR to Gray]
        cv::waitKey();
    }
    {
        QMessageBox::information(this,""Convert to CV_32F"",""按任意键继续..."");
        Mat dst, src;
        src = loadFromQrc(fileName);
        //! [Convert to CV_32F]
        src.convertTo(dst, CV_32F);
        imshow( ""src"", src);
        imshow( ""dst"", dst);
        //! [Convert to CV_32F]
        cv::waitKey();
    }
    // 可视化图像
    {
        QMessageBox::information(this,""可视化图像"",""按任意键继续..."");
        //! [imshow 1]
        Mat img = loadFromQrc(fileName);
        namedWindow(""image"", WINDOW_AUTOSIZE);
        imshow(""image"", img);
        waitKey();
        //! [imshow 1]
    }
    {
        //! [imshow 2]
        Mat img = loadFromQrc(fileName);
        Mat grey;
        cvtColor(img, grey, COLOR_BGR2GRAY);
        Mat sobelx;
        Sobel(grey, sobelx, CV_32F, 1, 0);
        double minVal, maxVal;
        minMaxLoc(sobelx, &minVal, &maxVal); //find minimum and maximum intensities
        Mat draw;
        sobelx.convertTo(draw, CV_8U, 255.0/(maxVal - minVal), -minVal * 255.0/(maxVal - minVal));
        namedWindow(""image"", WINDOW_AUTOSIZE);
        imshow(""image"", draw);
        waitKey();
        //! [imshow 2]
    }
}



//there are debug info
(1,0)Point: [0, 0, 0, 0]
(0,1)Point: [0, 0, 0, 0]
(1,0)Point: [0, 0, 0], blue = 0, green = 0, red = 0
float: (1,0)Point: [0, 0, 0], blue = 0, green = 0, red = 0
img.at(): (1,0)Point: 128
Point: [10, 20]
pointsMat = 
[10, 20, 25;
 30, 40, 45;
 50, 60, 65]
pointsMat size: [3 x 3]
u == nullptr 
imgInit: refcount: 1
img: refcount: 1
img1: refcount: 1
img refcount: 1
img1 refcount: 1

### Steps to reproduce

I think that's enough information
![image](https://github.com/opencv/opencv/assets/74864828/c1a73296-7493-473e-9e5f-c3b12fa6ea32)


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-18 09:45:53,question,ANN_MLP - large dataset results in stack overflow and other errors,"### System Information

OpenCV version: 4.5.5
Operating system: Windows 11
Compiler: latest Visual Studio 2017 

### Detailed description

Increasing input dataset size past certain point results in stack overflow.
Using provided code everything works fine with `datasetSize` (line 35) up until 632 past which it results in stack overflow on `train` call.

### Steps to reproduce

```cpp
#include <opencv2/core.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/ml/ml.hpp>

#include <iostream>
#include <iomanip>

using namespace cv;
using namespace ml;
using namespace std;

void print(Mat& mat, int prec)
{
	for (int i = 0; i < mat.size().height; i++)
	{
		cout << ""["";
		for (int j = 0; j < mat.size().width; j++)
		{
			cout << fixed << setw(2) << setprecision(prec) << mat.at<float>(i, j);
			if (j != mat.size().width - 1)
				cout << "", "";
			else
				cout << ""]"" << endl;
		}
	}
}

int main()
{
	try{
	const int inputSize = 20 * 20;
	const int hiddenLayerSize = inputSize * inputSize;
	const int datasetSize = 637;

	float inputTrainingDataArray[datasetSize][inputSize];
	float outputTrainingDataArray[datasetSize][2];

	std::cout << ""init\\n"";
	for (int i = 0; i < datasetSize; i++)
	{
		int value = i < datasetSize / 2 ? 0 : 1;
		for (int j = 0; j < inputSize; j++)
			inputTrainingDataArray[i][j] = value;
		if (value)
		{
			outputTrainingDataArray[i][0] = 0;
			outputTrainingDataArray[i][1] = 1;
		}
		else
		{
			outputTrainingDataArray[i][0] = 1;
			outputTrainingDataArray[i][1] = 0;
		}
	}
	std::cout << ""loop passed\\n"";

	Mat inputTrainingData = Mat(datasetSize, inputSize, CV_32F, inputTrainingDataArray);
	Mat outputTrainingData = Mat(datasetSize, 2, CV_32F, outputTrainingDataArray);
	std::cout << ""Mat\\n"";

	Ptr<ANN_MLP> mlp = ANN_MLP::create();

	Mat layersSize = Mat(3, 1, CV_16U);
	layersSize.row(0) = Scalar(inputTrainingData.cols);
	layersSize.row(1) = Scalar(hiddenLayerSize);
	layersSize.row(2) = Scalar(outputTrainingData.cols);
	mlp->setLayerSizes(layersSize);

	mlp->setActivationFunction(ANN_MLP::ActivationFunctions::SIGMOID_SYM);

	TermCriteria termCrit = TermCriteria(
		//TermCriteria::Type::COUNT + TermCriteria::Type::EPS,
		TermCriteria::Type::MAX_ITER + TermCriteria::Type::EPS,
		500,
		0.0001
	);
	mlp->setTermCriteria(termCrit);

	mlp->setTrainMethod(ANN_MLP::TrainingMethods::BACKPROP);

	Ptr<TrainData> trainingData = TrainData::create(
		inputTrainingData,
		SampleTypes::ROW_SAMPLE,
		outputTrainingData
	);

	auto start = std::chrono::high_resolution_clock::now();
	mlp->train(trainingData);
	auto stop = std::chrono::high_resolution_clock::now();
	cout << ""Elapsed: "" << static_cast<double>(std::chrono::duration_cast<std::chrono::microseconds>(stop - start).count()) / 1000 << "" ms"" << endl;

	for (int i = 0; i < inputTrainingData.rows; i++) {
		Mat sample = Mat(1, inputTrainingData.cols, CV_32F, inputTrainingDataArray[i]);
		Mat result;
		mlp->predict(sample, result);
		if (i == 0 || i == inputTrainingData.rows - 1)
		{
			cout << inputTrainingDataArray[i][0] << "" -> "";// << result << endl;
			print(result, 0);
			cout << endl;
		}
	}
	}
	catch (const std::runtime_error& runtimeError) {
		std::cerr << ""Runtime error: "" << runtimeError.what() << std::endl;
	} catch (const std::exception& exception) {
		std::cerr << ""Error occurred: "" << exception.what() << std::endl;
	} catch (...) {
		std::cerr << ""Unknown failure occurred. Possible memory corruption"" << std::endl;
	}
	return 0;
}
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-05-05 08:20:58,question,Error LNK2001 unresolved external symbol ,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 3.4.1
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

![d79b809b247345e62efcd346762576e](https://user-images.githubusercontent.com/121961555/236408998-ccb8e5ea-47c4-42a9-ae78-917b607d4048.png)

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
''' '''
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2023-04-20 08:15:20,question,undefined reference to `pow@GLIBC_2.29' at arm64,"### System Information

OpenCV version: 4.5.4
Distribution: Ubuntu 20.04 focal (aarch64)
Compiler & compiler version: GCC 9.4.0

### Detailed description
```
/usr/bin/ld: /usr/local/opencv-4.5.4/lib/libopencv_imgproc.so: undefined reference to `pow@GLIBC_2.29'
/usr/bin/ld: /usr/local/opencv-4.5.4/lib/libopencv_core.so: undefined reference to `exp@GLIBC_2.29'
/usr/bin/ld: /usr/local/opencv-4.5.4/lib/libopencv_core.so: undefined reference to `log@GLIBC_2.29'
collect2: error: ld returned 1 exit status
```
### Steps to reproduce

1.compile opencv-4.5.4 source and install 
2. add its'include and library to CMakelist.txt
2. compile my project with cmake and make command
3. try add `lm` to ""target_compile_options"" in CMakeLists.txt but still.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-04-05 08:34:59,question,OpenCV build - cmake don't recognize GTK2 or GTK3,"##### System information (version)

- OpenCV => 4.7.0
- Operating System / Platform => Ubuntu 20.04
- Compiler => g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
- C++ Standard: 11

##### Detailed description

I am trying to create OpenCV with the GTK GUI. I have `GTK2` and `GTK3` installed on the system, but neither version is recognised by cmake. 

![image](https://user-images.githubusercontent.com/62354721/230026534-66053f4e-67d3-45cd-93c2-4902365400c7.png)
![image](https://user-images.githubusercontent.com/62354721/230026727-07d64f8c-c453-4048-9d23-3608b1c94663.png)


The output after executing cmake remains 
**GUI: NONE** 
**GTK+:NO**

I execute the cmake command as follows:

`cmake -DPYTHON_DEFAULT_EXECUTABLE=$(which python3) -DWITH_GTK=ON ../opencv`

or

`cmake -DPYTHON_DEFAULT_EXECUTABLE=$(which python3) -DWITH_GTK=ON -DWITH_GTK_2_X=ON ../opencv`

##### Steps to reproduce

git clone https://github.com/opencv/opencv.git
mkdir -p build && cd build
cmake ../opencv (default, custom look above)

Thank you for your help
"
opencv/opencv,2023-03-17 17:40:15,question,Can't import cv2 : DLL is missing and so on,"### System Information

Hello,

as many people, i tried to compile opencv with some options (like Cuda, GStreamer, etc.).

My system : 
Windows 11 laptop
Python 3.11.2
Visual studio 2022

I successfully compiled opencv 4.7.0 and 4.5.5 with Cuda and GStreamer.

BUT, when i try to import cv2, for sure, i get an issue : DLL is missing or things like that. I saw topics about that kind of errors. It seems they are quite common. As for me, i give up, after trying many things with always the same result : epic fail.

I would like to say that it is really boring to spend so many time waiting for compilation result and seeing the issue when importing cv2.

I DO like opencv but it is really strange to have to compile opencv for very common extension like Cuda or GSttreamer. And of course, it is also more strange to spend hour in order to always get an issue. Really boring.

So, i have uninstalled everything (so i won't be able to reproduce the issue) and i use very standard opencv version without GPU acceleration and with crappy video encode. Too bad.

Please, try to make something more usable and more user friendly.

Many thanks in advance.

Alain

### Detailed description

Can't import cv2

### Steps to reproduce

Just compile opencv with Cuda & GStreamer and to to import cv2 with Python 3.11

You will get the issue.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-03-08 13:50:04,question,Need help to find the correct parameters for AMD Optimizing CPU Libraries,"##### System information (version)
- OpenCV => 4.7
- Operating System / Platform => Windows 11 64 Bit
- Compiler => Visual Studio 2022
- Cuda 12
- Nvidia RTX 3070
- AMD Ryzen 7

##### Detailed description
Hello to all,
I'm new to OpenCv and all related stuff like TBB, BLAS, LAPACK etc. I have been forced to build it from scratch in order to take advantage of all the capabilities of my CPU/GPU.  Although I managed to get over around for the GPU side, I'm completely stuck for the CPU side, mainly because I can't find any documentation for AMD cpus. I have read that for AMD are recommended the [AMD Optimizing CPU Libraries (AOCL)](https://www.amd.com/en/developer/aocl.html), but I have no idea even on how to start. I can't find even a single option variable in CMake that refers to such kind of configuration.

I would really appreciate if someone could offer some help and let me know the right configuration (cmake parameters, dependencies etc).
Thanks
"
opencv/opencv,2023-03-02 06:15:04,question,How to cross compile the harmonyos opencv？,"### Descripe the feature and motivation

How to cross compile the harmonyos opencv, run on harmonyos .app/.hap

### Additional context

_No response_"
opencv/opencv,2023-02-11 10:09:14,question,error: ‘phase_unwrapping’ in namespace ‘cv’ does not name a type,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

<!-- your description -->

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2023-02-04 18:20:42,question,OpenCV 4.7 cuda 12.0 VS 22 lnk2019 error,"### System Information

OpenCV version: 4.7
Operation System: Windows 11
Cuda: 12
Visual Studio: 22

### Detailed description

Getting Lnk errors when using opencv with cuda:

I have no issue with cuda, but when I try to add OpenCV with cuda I encounterd the following problem. Is there something missing in my CMakeList to link it correctly? 

Error: LNK2019	unresolved external symbol ""void __cdecl cv::cuda::printCudaDeviceInfo(int)"" (?printCudaDeviceInfo@cuda@cv@@YAXH@Z) referenced in function main	

Thanks!

### Steps to reproduce

Follow this setup guide for OpenCV 

https://medium.com/@batuhanhangun/opencv454-gpu-support-cpp-bef2cc145090

with version 4.7 VS22 and without dnn

main.cpp

```
#include <iostream>
#include <opencv2/dnn.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/core.hpp>
#include <opencv2/core/cuda.hpp>
#include <opencv2/cudaimgproc.hpp>


using namespace std;
using namespace cv;
using namespace cuda;


int main(int, char**) {


    printCudaDeviceInfo(0);

    cout << ""Hello World!\\n"";
}
```

CMakeLists.txt

```
cmake_minimum_required(VERSION 3.20)

set(CMAKE_CONFIGURATION_TYPES Debug Release CACHE TYPE INTERNAL FORCE)

project(opencvTest LANGUAGES C CXX CUDA)

## CUDA
find_package(CUDAToolkit REQUIRED)

# OpenCV
# Optional: Set OpenCV_DIR if you want to use a custom version of OpenCV
SET(""OpenCV_DIR"" ""C:/Development/Libs/opencv-4.7.0/Build"")
find_package(OpenCV 4.0 REQUIRED COMPONENTS core highgui)

include_directories(${OpenCV_INCLUDE_DIRS})

# ------------------------------------------------

add_executable(opencvTest src/main.cpp)

target_link_libraries(opencvTest $(OpenCV_LIBS))
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-01-31 14:38:46,question,cv2 returns black image,"### System Information

```
python -c 'import cv2 as cv; print(cv.getBuildInformation())'

General configuration for OpenCV 4.7.0 =====================================
  Version control:               4.7.0-dirty

  Platform:
    Timestamp:                   2022-12-29T19:13:29Z
    Host:                        Linux 5.3.0-28-generic aarch64
    CMake:                       3.25.0
    CMake generator:             Unix Makefiles
    CMake build tool:            /bin/gmake
    Configuration:               Release

  CPU/HW features:
    Baseline:                    NEON FP16

  C/C++:
    Built as dynamic libs?:      NO
    C++ standard:                11
    C++ Compiler:                /opt/rh/devtoolset-10/root/usr/bin/c++  (ver 10.2.1)
    C++ flags (Release):         -Wl,-strip-all   -fsigned-char -W -Wall -Wreturn-type -Wnon-virtual-dtor -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG
    C++ flags (Debug):           -Wl,-strip-all   -fsigned-char -W -Wall -Wreturn-type -Wnon-virtual-dtor -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections    -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG
    C Compiler:                  /opt/rh/devtoolset-10/root/usr/bin/cc
    C flags (Release):           -Wl,-strip-all   -fsigned-char -W -Wall -Wreturn-type -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections    -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG
    C flags (Debug):             -Wl,-strip-all   -fsigned-char -W -Wall -Wreturn-type -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections    -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG
    Linker flags (Release):      -L/ffmpeg_build/lib  -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined  
    Linker flags (Debug):        -L/ffmpeg_build/lib  -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined  
    ccache:                      YES
    Precompiled headers:         NO
    Extra dependencies:          /lib64/libopenblas.so Qt5::Core Qt5::Gui Qt5::Widgets Qt5::Test Qt5::Concurrent /usr/local/lib/libpng.so /usr/local/lib/libz.so dl m pthread rt
    3rdparty dependencies:       libprotobuf ade ittnotify libjpeg-turbo libwebp libtiff libopenjp2 IlmImf quirc tegra_hal

  OpenCV modules:
    To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo python3 stitching video videoio
    Disabled:                    world
    Disabled by dependency:      -
    Unavailable:                 java python2 ts
    Applications:                -
    Documentation:               NO
    Non-free algorithms:         NO

  GUI:                           QT5
    QT:                          YES (ver 5.15.0 )
      QT OpenGL support:         NO
    GTK+:                        NO
    VTK support:                 NO

  Media I/O: 
    ZLib:                        /usr/local/lib/libz.so (ver 1.2.13)
    JPEG:                        libjpeg-turbo (ver 2.1.3-62)
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         /usr/local/lib/libpng.so (ver 1.6.37)
    TIFF:                        build (ver 42 - 4.2.0)
    JPEG 2000:                   build (ver 2.4.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES

  Video I/O:
    DC1394:                      NO
    FFMPEG:                      YES
      avcodec:                   YES (59.37.100)
      avformat:                  YES (59.27.100)
      avutil:                    YES (57.28.100)
      swscale:                   YES (6.7.100)
      avresample:                NO
    GStreamer:                   NO
    v4l/v4l2:                    YES (linux/videodev2.h)

  Parallel framework:            pthreads

  Trace:                         YES (with Intel ITT)

  Other third-party libraries:
    Lapack:                      YES (/lib64/libopenblas.so)
    Eigen:                       NO
    Custom HAL:                  YES (carotene (ver 0.0.1))
    Protobuf:                    build (3.19.1)

  OpenCL:                        YES (no extra features)
    Include path:                /io/opencv/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load

  Python 3:
    Interpreter:                 /opt/python/cp37-cp37m/bin/python3.7 (ver 3.7.16)
    Libraries:                   libpython3.7m.a (ver 3.7.16)
    numpy:                       /home/ci/.local/lib/python3.7/site-packages/numpy/core/include (ver 1.19.3)
    install path:                python/cv2/python-3

  Python (for build):            /bin/python2.7

  Java:                          
    ant:                         NO
    JNI:                         NO
    Java wrappers:               NO
    Java tests:                  NO

  Install to:                    /io/_skbuild/linux-aarch64-3.7/cmake-install
-----------------------------------------------------------------
```

### Detailed description

The following piece of code returns a black image on my raspi.
Attached is a usb webcam.

```
ffmpeg -f video4linux2 -s 640x480 -i /dev/video0 -ss 500ms -frames 1 ffmpeg.png
```
creates the correct picture though

### Steps to reproduce

```python
#!/bin/python

import cv2
import sys
import time
cam_port = 0 #/dev/video0
cam = cv2.VideoCapture(cam_port)
time.sleep(0.5)
outpath = sys.argv[1] if len(sys.argv)>1 else ""out.png""
result, image = cam.read()
if result: cv2.imwrite(outpath, image)
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-01-26 19:19:37,question,OpenCv erosion-function affecting not only edges,"I run the cv2.erode function on the left image below, and get the right image as a result. As expected, the edges of the object in the right image has been eroded compared to the left. But what is NOT expected is that areas within the object - far from the edges - also is affected. The most obvious part is the dark vertical area in the bottom left in the image. As seen, this dark area has grown bigger in the right image, even though erosion is only supposed to affect the edges of the object. Why is this happening?

```
kernel_size = 3
kernel = np.ones((kernel_size, kernel_size), np.uint8)
im_eroded = cv2.erode(im, kernel, iterations=1)
```

![image](https://user-images.githubusercontent.com/11705343/214929377-e21f910f-75f0-495a-8272-f318f4bb27f1.png)
"
opencv/opencv,2023-01-20 07:46:58,question,Frequent use of cmake_install_prefix and cmake_binary_dir will cause very serious errors ,"### System Information

OpenCV version：4.6.0
use aarach64-musl


### Detailed description

Frequent use of cmake_install_prefix and cmake_binary_dir will cause very serious errors when compiling multiple files. If you want to modify it, where should you start?

### Steps to reproduce

...

### Issue submission checklist

- [X] I report the issue, it's not a question
- [x] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-01-06 08:09:40,question,error: ‘GaussianBlur’ is not a member of ‘cv::cuda’,"### System Information

OpenCV version: 3.2.0
Operating System / Platform: Ubuntu 18.04
Compiler & compiler version: GCC 7.5.0



### Detailed description

Does GaussianBlur() support running on Gpu？
An error was reported when I compiled (cv::cuda::GaussianBlur(src,dst,cv::Size(0,0),10.0);)

error: ‘GaussianBlur’ is not a member of ‘cv::cuda’

### Steps to reproduce

This is my codel：
cv::cuda::GpuMat dst, src; 
src.upload(NewDensity); 
cv::cuda::GaussianBlur(src,dst,cv::Size(0,0),10.0);
dst.download(densitymap); 

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2023-01-03 14:31:21,question,YoloV7 produces different results for opencv-python==4.6.0 and onnxruntime-gpu==1.10.0,"### System Information

opencv-python==4.6.0
onnxruntime-gpu==1.10.0
Operating System / Platform: Windows 10


### Detailed description

Hey guys,

I was trying to implement YoloV7 using OpenCVDNN in Python, based on this sample: https://github.com/PINTO0309/PINTO_model_zoo/tree/main/307_YOLOv7/demo

After running one inference using the provided onnxruntime python script:
https://github.com/ibaiGorordo/ONNX-YOLOv7-Object-Detection

And one inference using a custom opencvdnn python script (using the same onnx model):
[main_onnx_opencvdnn460.zip](https://github.com/opencv/opencv/files/10337201/main_onnx_opencvdnn460.zip)

Scores and box coordinates (output array) are different from one script to another. You can see down bellow the first 10 float values from each script inference:

First 10 float values (onnxruntime python script):

4.1796083
2.7915716
9.457193
6.8277607
6.0796738e-06
0.25375873
0.0018338561
0.027819633
0.0014881492
0.0025510192

First 10 float values (opencvdnn python script):

3.3212
3.11392
7.16211
6.72352
1.89161e-06
0.193173
0.00234643
0.0284113
0.00213374
0.00480309

I know that using two different frameworks can generate slightly different results, but in this case, this change in results generates significant changes in the confidence of the boxes generated by each model.
Do you have any hint on why this is happening?

Thanks, César.

### Steps to reproduce

Script python onnxrutime: https://github.com/ibaiGorordo/ONNX-YOLOv7-Object-Detection
Script python opencvdnn (using ReadFromONNX): [main_onnx_opencvdnn460.zip](https://github.com/opencv/opencv/files/10337201/main_onnx_opencvdnn460.zip)
YoloV7 model used: 
[Uploading yolov7-tiny_256x320.zip…]()


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-12-30 10:01:21,question,Open the camera always hangs up,"### System Information

OpenCV python version: 4.6.0.66
Operating System / Platform: Windows 10
Python version: 3.8.15

### Detailed description

I turn on the camera and display the image through cv2, but there is no response：

![1672394204068](https://user-images.githubusercontent.com/18117854/210057804-44c51e1b-5585-4424-b6f5-24b1d64e4d3b.png)


### Steps to reproduce

requirements.txt
```
mediapipe==0.9.0.1
pyqt5-tools==5.15.4.3.2
opencv-python==4.6.0.66
```

example.py
```
# importing required libraries
import cv2
import numpy as np

# taking the input from webcam
vid = cv2.VideoCapture(0)

# running while loop just to make sure that
# our program keep running until we stop it
while True:

    # capturing the current frame
    _, frame = vid.read()

    # displaying the current frame
    cv2.imshow(""frame"", frame)

    # setting values for base colors
    b = frame[:, :, :1]
    g = frame[:, :, 1:2]
    r = frame[:, :, 2:]

    # computing the mean
    b_mean = np.mean(b)
    g_mean = np.mean(g)
    r_mean = np.mean(r)

    print('R {}, G {}, B {}'.format(r_mean, g_mean, b_mean))

    # displaying the most prominent color
    if b_mean > g_mean and b_mean > r_mean:
        print(""Blue"")
    if g_mean > r_mean and g_mean > b_mean:
        print(""Green"")
    else:
        print(""Red"")
```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-12-28 09:41:30,question,How to print login statment ,"### Descripe the feature and motivation

try to print some logs but not understand how to enable logs so help me print logs print 

### Additional context

_No response_"
opencv/opencv,2022-12-25 03:05:43,question,I build static opencv still get undefined errors,"### System Information

macOS

### Detailed description

```

Showing Recent Messages
  ""_gzclose"", referenced from:


```
<img width=""625"" alt=""image"" src=""https://user-images.githubusercontent.com/21303438/209455777-f77a1335-6a67-425e-b45a-9ec5a347d8eb.png"">


### Steps to reproduce

build static lib and link error got

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-12-22 10:24:20,question,"Can not save video format as "".mp4"".","
It seems that VideoWriter not support format mp4, i can only save the video as format “.avi”. Someone can give me some advice?"
opencv/opencv,2022-12-21 02:37:23,question,Null pointer (NULL window: 'Tracker') in cvGetModeWindow_W32？,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: win10
Compiler & compiler version: vc15



### Detailed description

When I have used so method to close window：


The bug happend: 

OpenCV: terminate handler is called! The last OpenCV error is:
OpenCV(4.6.0) Error: Null pointer (NULL window: 'Tracker') in cvGetModeWindow_W32, file c:\\build\\master_winpack-build-win64-vc15\\opencv\\modules\\highgui\\src\\window_w32.cpp, line 565

But the bug is no happend in opencv 4.5,  just in opencv4.6 !

### Steps to reproduce

namedWindow(""test"", WINDOW_NORMAL);

VideoCapture capture;

capture.open(""test.mp4"");

Mat frame;

while(true)
{
capture >> frame;

if (frame.empty())
	break;

// 判断是否点击窗口关闭按钮
if (cv::getWindowProperty(""test"", WINDOW_NORMAL) == -1)
	break;

cv::imshow(""test"", frame);
if (waitKey(20) == 27)
	break;
}

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-12-17 06:56:16,question,cv2 does not detect and decode QR-code,"### System Information

// example for python user
OpenCV python version: 4.6.0
Operating System / Platform: Windows 10 Enterprise
Python version: 3.9.5

### Detailed description

I try to create a QR code and decode that using cv2. The tutorials says the exact same below but I could not reproduce the code.  

The problem persists for both small and large data. I looked any tutorials but any does not work.  I have attached a small code for your information.

Thank you


### Steps to reproduce

import cv2
import qrcode
import matplotlib.pyplot as plt

# Create a QR code 

test = ""Hello World""

qr = qrcode.QRCode(
    version=None,
    error_correction=qrcode.constants.ERROR_CORRECT_L,
    box_size=5,
    border=1,
)

qr.add_data(test)
qr.make(fit=True)
img = qr.make_image(fill_color=""white"", back_color=""black"")
img.save(""test.png"")

# plot the generated QR code

im = plt.imread(""test.png"")
implot = plt.imshow(im)

# Read and Decode the QR code

img = cv2.imread(""test.png"")
qcd = cv2.QRCodeDetector()
retval, decoded_info, points, straight_qrcode = qcd.detectAndDecodeMulti(img)
print(retval)

Output:
False


### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-12-05 06:49:46,question,OpenCV C++ Sample Error,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: Ubuntu 20.04
Compiler & compiler version: RISCV-GNU-GCC

### Detailed description

I'm following this tutorial for executing c++ opencv googlenet code with RISCV but always get the constant error.

`root@ubuntu:/home/anil/Desktop/googlenet# riscv64-unknown-linux-gnu-g++ main.cpp -o sone -I /home/anil/Desktop/project/opencv/include/ -I /home/anil/Desktop/project/opencv/modules/core/include -I /home/anil/Desktop/project/opencv/build/install/include/opencv4 -I /home/anil/Desktop/project/opencv/modules/core/include/opencv2 -I /home/anil/Desktop/project/riscv-gnu-toolchain/gdb/zlib -I /home/anil/Desktop/deneme/gitdene/cnpy/ -I /usr/include/mkl -L/home/anil/Desktop/project/opencv/build/lib -lopencv_core -lopencv_imgcodecs -lopencv_imgproc --static -I /home/anil/Desktop/project/opencv/build/install/include/opencv4/opencv2/core/cuda -I/usr/local/cuda/include`

```
main.cpp: In function 'int main(int, char**)':
main.cpp:32:17: error: 'genPreprocArguments' was not declared in this scope
   32 |         keys += genPreprocArguments(modelName, zooFile);
      |                 ^~~~~~~~~~~~~~~~~~~
main.cpp:45:24: error: 'findFile' was not declared in this scope; did you mean 'cv::samples::findFile'?
   45 |         String model = findFile(parser.get<String>(""model""));
      |                        ^~~~~~~~
      |                        cv::samples::findFile
In file included from /home/anil/Desktop/project/opencv/include/opencv2/core.hpp:3350,
                 from /home/anil/Desktop/project/opencv/build/install/include/opencv4/opencv2/dnn/dnn.hpp:46,
                 from /home/anil/Desktop/project/opencv/build/install/include/opencv4/opencv2/dnn.hpp:76,
                 from main.cpp:3:
/home/anil/Desktop/project/opencv/modules/core/include/opencv2/core/utility.hpp:1181:25: note: 'cv::samples::findFile' declared here
 1181 | CV_EXPORTS_W cv::String findFile(const cv::String& relative_path, bool required = true, bool silentMode = false);
```
Is there a problem in the documentation?

### Steps to reproduce

The sample code is same with this link: https://docs.opencv.org/4.0.0/d5/de7/tutorial_dnn_googlenet.html

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-11-25 05:05:22,question,OpenCV 3.4.8 cannot build,"### System Information

// example for c++ user
OpenCV version: 3.4.8
Operating System / Platform: Debian 5.19.11
Compiler & compiler version: gcc (Debian 12.2.0-3) 12.2.0

### Detailed description

I was following this link to build OpenCV 3.4.8: https://docs.opencv.org/3.4.8/d7/d9f/tutorial_linux_install.html

When I ran ""make -j7"", I got this error message in the console:

.../lnli/opencv-3.4.8/modules/core/src/persistence_base64.cpp: In function ‘bool base64::base64_valid(const uint8_t*, size_t, size_t)’:
/usr/local/google/home/lnli/opencv-3.4.8/modules/core/src/persistence_base64.cpp:167:31: error: comparing the result of pointer addition ‘(src + ((sizetype)off))’ and NULL [-Werror=address]
  167 |     if (src == 0 || src + off == 0)
      |                     ~~~~~~~~~~^~~~

### Steps to reproduce

Described in the detailed description section.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-11-16 09:17:30,question,/opencv/modules/dnn/src/dnn.cpp (1447) : setUpNet DNN module was not built with CUDA backend; switching to CPU,"##### System information 
**- OpenCV = 4.5.4
- Operating System / Platform => NVIDIA JETSON Orin (Tegra)
- Compiler => Visual Studio 2019
CUDNN 8.6 and CUDA 11.4.**

##### 


I have configured the opencv with cmake-gui, enabling, 

**opencv_dnn_cuda, 

build_opencv_cuda, 

with_cuda, 

with_cudnn, 

with_cublas.**

i also linked ln -s /usr/local/lib/python3/dist-packages/cv2/python-3.8/[cv2.cpython-38-aarch64-linux-gnu.so](http://cv2.cpython-38-aarch64-linux-gnu.so/) cv2.so

The opencv test : import cv2 also works. 

I use this segment in my code.
cout << ""Using GPU device"" << endl;
net.setPreferableBackend(DNN_BACKEND_CUDA);
net.setPreferableTarget(DNN_TARGET_CUDA);

When run,
it switches to CPU and compiles giving the result in 400 milliseconds.
The issue is displayed as below:
**[ WARN:0] global /home/ubuntu/build_opencv/opencv/modules/dnn/src/dnn.cpp (1447) setUpNet DNN module was not built with CUDA backend; switching to CPU**

**My directory structure is ( i cloned the opencv into my tk_ws workspace)

tk_ws/ build/ bin
tk_ws/opencv-4.5.4
tk_ws/opencv_contrib-4.5.4

Also, in the dnn.cpp is in the path: tk_ws/opencv-4.5.4/modules/dnn/src**

while as per the instructions on https://gist.github.com/YashasSamaga/a84cf2826ab2dc755005321fe17cd15d

I can see the opencv_test_dnn in green

I could not figure out why I am repeatedly failing. 

I would be immensely grateful if you could help me, please.

Warm Regards,
Karishma"
opencv/opencv,2022-11-12 10:20:51,question,Cannot open BigTiff file,"### System Information

OpenCV version: 4.6.0
Operating System: Windows 10 x64

### Detailed description

I use opencv_java.jar and opencv_java.dll to work with images in java-project.
Error occuries when i try to open BigTiff images.
**Example 1**:
image size >4.5GB, resolution 94012x16837 pixels.
> CvException [org.opencv.core.CvException: cv::Exception: OpenCV(4.6.0) C:\\build\\master_winpack-bindings-win64-vc14-static\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:77: error: (-215:Assertion failed) pixels <= CV_IO_MAX_IMAGE_PIXELS in function 'cv::validateInputImageSize'

**Example 2**:
image size 2GB, resolution 62675x11224 pixels.
> imread_('D:\\temp\\testfiles\\sample_bigtif_small.tif'): can't read data: OpenCV(4.6.0) C:\\build\\master_winpack-bindings-win64-vc14-static\\opencv\\modules\\imgcodecs\\src\\grfmt_tiff.cpp:663: error: (-215:Assertion failed) ((uint64_t)tile_width0 * tile_height0 * ncn * std::max(1, (int)(bpp / bitsPerByte)) < MAX_TILE_SIZE) && ""TIFF tile size is too large: >= 1Gb"" in function 'cv::TiffDecoder::readData'

Images were created in Photoshop using http://bigtiff.org
Moreover I tried to load tiff files with big resolution (1000000x12000 px), created in paint.net. And the same error occurred.
Seems like not all assertions improved in #18717

### Steps to reproduce

1. Create simple java maven project.
2. Put opencv_java.jar into lib folder and set up as system-scope dependency
3. Put opencv_java.dll into src/main/resources/META-INF/native/windows64
4. Setup dependency for hawtjni-runtime:
`<dependency> 
    <groupId>org.fusesource.hawtjni</groupId> 
        <artifactId>hawtjni-runtime</artifactId> 
        <version>1.15</version>
</dependency>`
5. Try to load image with this code sample:

public static void main(String[] args) { 	

    Library library = new Library(""opencv_java"", ""4.6.0"", Loader.class.getClassLoader());
    library.load(); 
    String absolutePath = ""D:\\\\temp\\\\testfiles\\\\sample_bigtif.tif""; 
    Mat mat = Imgcodecs.imread(absolutePath); 
    if (mat.dataAddr() == 0) { 
        throw new RuntimeException(""OpenCV can't load image: "" + absolutePath); 
    }
}

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-11-09 02:52:26,question,resize source code,"

Hi,

I trained the neural network with resize image, and then I would like to run the same procedure in embedded MCU.
I tried to implement my own resize, however, the output of my resize is always slightly different from the resize of OpenCV.

Furthermore, the resize source code in Opencv is very tough to be run in embedded MCU as there is parallel and C++ feature in the code.


Is there any substitute resize code that outputs the same result as OpenCV?
"
opencv/opencv,2022-11-03 16:02:27,question,videowriter libraryLoad load opencv_videoio_ffmpeg460_64d.dll,"### Descripe the feature and motivation

Can I get opencv_videoio_ffmpeg460_64d.dll and opencv_videoio_ffmpeg460_64d.dll?(Shared libraries for debugging)Thank you!

### Additional context

I try to create video file by VideoWriter but failed,information like following:
backend_plugin.cpp (383) cv::impl::getPluginCandidates Found 3 plugin(s) for FFMPEG
opencv-4.6.0\\modules\\core\\src\\utils\\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load D:\\develop\\opencv-4.6.0\\vs2017_x64\\bin\\Debug\\opencv_videoio_ffmpeg460_64d.dll => FAILED
opencv-4.6.0\\modules\\core\\src\\utils\\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_videoio_ffmpeg460_64d.dll => FAILED
opencv-4.6.0\\modules\\core\\src\\utils\\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_videoio_ffmpeg460_64.dll => FAILED
opencv-4.6.0\\modules\\videoio\\src\\backend_plugin.cpp (383) cv::impl::getPluginCandidates Found 2 plugin(s) for INTEL_MFX
opencv-4.6.0\\modules\\core\\src\\utils\\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv-4.6.0\\vs2017_x64\\bin\\Debug\\opencv_videoio_intel_mfx460_64d.dll => FAILED
[ INFO:0@51.519] \\opencv-4.6.0\\modules\\core\\src\\utils\\plugin_loader.impl.hpp (67) cv::plugin::impl::DynamicLib::libraryLoad load opencv_videoio_intel_mfx460_64d.dll => FAILED"
opencv/opencv,2022-10-27 09:35:09,question,about like opencv_videoio_ffmpeg450_64.dll for ubuntu,"### Descripe the feature and motivation

hello! when I run a demo to read video in ubuntu, I find that the demo has many dependent libraries,such as libavcodec libswscale libx264 libavformat and so on. It is very hard to put this demo to another PC because of too many dependent libraries should be copy for it. Then whether has some ideas like ""opencv_videoio_ffmpeg450_64.dll"" for windows to reduce dependent libraries for ubuntu?  I use ""sudo apt-get install libavcodec libswscale libavformat ffmpeg"" to build opencv for reading video

### Additional context

_No response_"
opencv/opencv,2022-10-21 05:57:16,question,opencv DNN build error,"### System Information

OpenCV version: 4.6.0
Operating System / Platform: Ubuntu 20.04
Compiler & compiler version: GCC 9.4.0



### Detailed description

It seems the protocol buffer headers are not compatible with mine. It builds fine if I remove ""dnn"" from -DBUILD_LIST, but the ""libopencv_dnn.so"" was not built. How should I address the protocol buffer version difference? What is DNN using?

```
[ 67%] Building CXX object modules/dnn/CMakeFiles/opencv_dnn.dir/misc/tensorflow/function.pb.cc.o
In file included from /home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.cc:4:
/home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.h:17:2: error: #error This file was generated by an older version of protoc which is
   17 | #error This file was generated by an older version of protoc which is
      |  ^~~~~
/home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please
   18 | #error incompatible with your Protocol Buffer headers. Please
      |  ^~~~~
/home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
```
...
...
```
/home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/op_def.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.
   19 | #error regenerate this file with a newer version of protoc.
      |  ^~~~~
In file included from /home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.h:26,
                 from /home/wxin/Code/shared/temp/opencv/modules/dnn/misc/tensorflow/attr_value.pb.cc:4:
/opt/3p/include/google/protobuf/generated_message_table_driven.h:287:25: error: ‘MapEntryHelper’ does not name a type; did you mean ‘MapEntryLite’?
  287 |   bool operator()(const MapEntryHelper<T>& a,
      |                         ^~~~~~~~~~~~~~
      |                         MapEntryLite
/opt/3p/include/google/protobuf/generated_message_table_driven.h:287:39: error: expected ‘,’ or ‘...’ before ‘<’ token
  287 |   bool operator()(const MapEntryHelper<T>& a,
      |                                       ^
```


### Steps to reproduce


git clone git@github.com:opencv/opencv.git
mkdir build
cd build
/opt/3p/bin/cmake -DBUILD_LIST=""dnn"" -DCMAKE_PREFIX_PATH=/opt/3p -DCMAKE_INSTALL_PREFIX=/tmp/opencv -DCMAKE_BUILD_TYPE=Release ..
make -j4 install

### Issue submission checklist

- [X] I report the issue, it's not a question
- [ ] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [ ] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-10-19 18:09:04,question,Import module error !!!,"I am using Anaconda to install opencv. 
After success of installation in the Anaconda I go to the jupyter notebook in venv created in the Anaconda.
In the Anaconda terminal the importing is successful but in the jupyter notebook it is not successful. 
Please se the image attached for more information.
![image](https://user-images.githubusercontent.com/40560482/196770745-639a6589-2313-4f72-ac1c-9028e0694ba6.png)
"
opencv/opencv,2022-10-16 18:17:38,question,Linking issues with cv::,"### System Information

opencv 4.6.0
ubuntu 18.04
gcc +14


### Detailed description

I have built the latest stable opencv version 4.6.0. Now I am trying to build rtabmap with that but i am having linking issues
here how i built opencv4.6

```
cmake -D CMAKE_BUILD_TYPE=RELEASE \\
	-D CMAKE_INSTALL_PREFIX=/usr/local \\
	-D INSTALL_PYTHON_EXAMPLES=OFF \\
	-D INSTALL_C_EXAMPLES=OFF \\
	-D OPENCV_ENABLE_NONFREE=ON \\
	-D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-4.6.0/modules
```

### Steps to reproduce

git clone https://github.com/introlab/rtabmap.git
cd rtabamp
mkdir build
cmake -DWITH_DEPTHAI=ON ..
make -j4


```
CMakeFiles/imagesJoiner.dir/main.cpp.o: In function `main':
main.cpp:(.text.startup+0x3f5): undefined reference to `cv::Mat::Mat(cv::Size_<int>, int)'
main.cpp:(.text.startup+0x518): undefined reference to `cv::imwrite(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, cv::_InputArray const&, std::vector<int, std::allocator<int> > const&)'
main.cpp:(.text.startup+0xa3f): undefined reference to `cv::imread(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)'
main.cpp:(.text.startup+0xac4): undefined reference to `cv::imread(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)'
collect2: error: ld returned 1 exit status
tools/ImagesJoiner/CMakeFiles/imagesJoiner.dir/build.make:132: recipe for target 'bin/rtabmap-imagesJoiner' failed
make[2]: *** [bin/rtabmap-imagesJoiner] Error 1
CMakeFiles/Makefile2:570: recipe for target 'tools/ImagesJoiner/CMakeFiles/imagesJoiner.dir/all' failed
make[1]: *** [tools/ImagesJoiner/CMakeFiles/imagesJoiner.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....

```

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-10-06 07:50:43,question,cv2.error: OpenCV(4.6.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:816: error: (-215:Assertion failed) !buf.empty() in function 'imdecode_',"### System Information

**System Information**

OpenCV python version: 4.6.0.
Operating System / Platform: Ubuntu 18.04
Python version: 3.8.13 (anaconda env)
torch version: 1.12.1+cu113,
cuda version: 11.3

Also, I am using linux server GPUs of type Tesla V100-SXM2



### Detailed description

```
  File ""train.py"", line 625, in <module>                                                                            
  main(opt)                                                                                                     
  File ""train.py"", line 520, in main                                                                                
  train(opt.hyp, opt, device, callbacks)                                                                        
  File ""train.py"", line 282, in train                                                                               
  for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------                                                                                                              
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/tqdm/std.py"", line 1195, in __iter__             
  for obj in iterable:                                                                                          
  File ""/home/yolov5/utils/dataloaders.py"", line 167, in __iter__                                                
  yield next(self.iterator)                                                                                     
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 681, in __next__                                                                                                           
  data = self._next_data()                                                                                      
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1356, in _next_data                                                                                                        
  return self._process_data(data)                                                                               
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1402, in _process_data                                                                                                     
  data.reraise()                                                                                                
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/_utils.py"", line 461, in reraise           raise exception                                                                                             
  cv2.error: Caught error in DataLoader worker process 5.                                                         Original Traceback (most recent call last):                                                                       
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py"", line 302, in _worker_loop                                                                                                    
  data = fetcher.fetch(index)                                                                                   
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch                                         
  data = [self.dataset[idx] for idx in possibly_batched_index]                                                  
  File ""/home/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in <listcomp>      
  data = [self.dataset[idx] for idx in possibly_batched_index]                                                  
  File ""/home/yolov5/utils/dataloaders.py"", line 601, in __getitem__                                             
  img, labels = self.load_mosaic(index)                                                                         
  File ""/home/yolov5/utils/dataloaders.py"", line 700, in load_mosaic                                             
  img, _, (h, w) = self.load_image(index)                                                                       
  File ""/home/yolov5/utils/dataloaders.py"", line 675, in load_image                                              
  im = cv2.imread(f)  # BGR                                                                                     
  File ""/home/yolov5/utils/general.py"", line 1021, in imread                                                     
  return cv2.imdecode(np.fromfile(path, np.uint8), flags)                                                     
  cv2.error: OpenCV(4.6.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:816: error: (-215:Assertion failed) !buf.empty() in function 'imdecode_'
```

### Steps to reproduce

The below command is for training a custom dataset with a well-known [yolov5](https://github.com/ultralytics/yolov5)
`python -m torch.distributed.run --nproc_per_node 2 train.py --data data/data.yaml --weights yolov5l6.pt --img 1280 --batch-size 24 --device 0,1 --rect`

To my surprise, I used the same system environment yesterday and it finished the training successfully. This is happening like in 30% of my training attempts.

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [x] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-09-23 07:55:32,question,"face example error  error: undefined reference to `cv::FaceDetectorYN::create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, ","System information (version)

- OpenCV => 4.5.4
- Operating System / Platform => ubuntu 16.04+ros
- Compiler => QT 59


```
#include <opencv2/dnn.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/objdetect.hpp>
#include <opencv2/objdetect/face.hpp>
#include<opencv2/core/cvstd.hpp>

#include <iostream>
using namespace cv;
using namespace std;
static
void visualize(Mat& input, int frame, Mat& faces, double fps, int thickness = 2)
{
    std::string fpsString = cv::format(""FPS : %.2f"", (float)fps);
    if (frame >= 0)
        cout << ""Frame "" << frame << "", "";
    cout << ""FPS: "" << fpsString << endl;
    for (int i = 0; i < faces.rows; i++)
    {
        // Print results
        cout << ""Face "" << i
             << "", top-left coordinates: ("" << faces.at<float>(i, 0) << "", "" << faces.at<float>(i, 1) << ""), ""
             << ""box width: "" << faces.at<float>(i, 2)  << "", box height: "" << faces.at<float>(i, 3) << "", ""
             << ""score: "" << cv::format(""%.2f"", faces.at<float>(i, 14))
             << endl;
        // Draw bounding box
        rectangle(input, Rect2i(int(faces.at<float>(i, 0)), int(faces.at<float>(i, 1)), int(faces.at<float>(i, 2)), int(faces.at<float>(i, 3))), Scalar(0, 255, 0), thickness);
        // Draw landmarks
        circle(input, Point2i(int(faces.at<float>(i, 4)), int(faces.at<float>(i, 5))), 2, Scalar(255, 0, 0), thickness);
        circle(input, Point2i(int(faces.at<float>(i, 6)), int(faces.at<float>(i, 7))), 2, Scalar(0, 0, 255), thickness);
        circle(input, Point2i(int(faces.at<float>(i, 8)), int(faces.at<float>(i, 9))), 2, Scalar(0, 255, 0), thickness);
        circle(input, Point2i(int(faces.at<float>(i, 10)), int(faces.at<float>(i, 11))), 2, Scalar(255, 0, 255), thickness);
        circle(input, Point2i(int(faces.at<float>(i, 12)), int(faces.at<float>(i, 13))), 2, Scalar(0, 255, 255), thickness);
    }
    putText(input, fpsString, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 255, 0), 2);
}
int main(int argc, char** argv)
{
    CommandLineParser parser(argc, argv,
        ""{help  h           |            | Print this message}""
        ""{image1 i1         |            | Path to the input image1. Omit for detecting through VideoCapture}""
        ""{image2 i2         |            | Path to the input image2. When image1 and image2 parameters given then the program try to find a face on both images and runs face recognition algorithm}""
        ""{video v           | 0          | Path to the input video}""
        ""{scale sc          | 1.0        | Scale factor used to resize input video frames}""
        ""{fd_model fd       | face_detection_yunet_2021dec.onnx| Path to the model. Download yunet.onnx in https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet}""
        ""{fr_model fr       | face_recognition_sface_2021dec.onnx | Path to the face recognition model. Download the model at https://github.com/opencv/opencv_zoo/tree/master/models/face_recognition_sface}""
        ""{score_threshold   | 0.9        | Filter out faces of score < score_threshold}""
        ""{nms_threshold     | 0.3        | Suppress bounding boxes of iou >= nms_threshold}""
        ""{top_k             | 5000       | Keep top_k bounding boxes before NMS}""
        ""{save s            | false      | Set true to save results. This flag is invalid when using camera}""
    );
    if (parser.has(""help""))
    {
        parser.printMessage();
        return 0;
    }
    cv::String fd_modelPath = parser.get<cv::String>(""fd_model"");
    cv::String fr_modelPath = parser.get<cv::String>(""fr_model"");
    float scoreThreshold = parser.get<float>(""score_threshold"");
    float nmsThreshold = parser.get<float>(""nms_threshold"");
    int topK = parser.get<int>(""top_k"");
    bool save = parser.get<bool>(""save"");
    float scale = parser.get<float>(""scale"");
    double cosine_similar_thresh = 0.363;
    double l2norm_similar_thresh = 1.128;
    // Initialize FaceDetectorYN
    Ptr<FaceDetectorYN> detector = FaceDetectorYN::create(fd_modelPath, "" "", Size(320, 320), scoreThreshold, nmsThreshold, topK);
//    Ptr<FaceDetectorYN> detector;
    TickMeter tm;
    // If input is an image
    if (parser.has(""image1""))
    {
        String input1 = parser.get<String>(""image1"");
        Mat image1 = imread(samples::findFile(input1));
        if (image1.empty())
        {
            std::cerr << ""Cannot read image: "" << input1 << std::endl;
            return 2;
        }
        int imageWidth = int(image1.cols * scale);
        int imageHeight = int(image1.rows * scale);
        resize(image1, image1, Size(imageWidth, imageHeight));
        tm.start();
        // Set input size before inference
        detector->setInputSize(image1.size());
        Mat faces1;
        detector->detect(image1, faces1);
        if (faces1.rows < 1)
        {
            std::cerr << ""Cannot find a face in "" << input1 << std::endl;
            return 1;
        }
        tm.stop();
        // Draw results on the input image
        visualize(image1, -1, faces1, tm.getFPS());
        // Save results if save is true
        if (save)
        {
            cout << ""Saving result.jpg...\\n"";
            imwrite(""result.jpg"", image1);
        }
        // Visualize results
        imshow(""image1"", image1);
        pollKey();  // handle UI events to show content
        if (parser.has(""image2""))
        {
            String input2 = parser.get<String>(""image2"");
            Mat image2 = imread(samples::findFile(input2));
            if (image2.empty())
            {
                std::cerr << ""Cannot read image2: "" << input2 << std::endl;
                return 2;
            }
            tm.reset();
            tm.start();
            detector->setInputSize(image2.size());
            Mat faces2;
            detector->detect(image2, faces2);
            if (faces2.rows < 1)
            {
                std::cerr << ""Cannot find a face in "" << input2 << std::endl;
                return 1;
            }
            tm.stop();
            visualize(image2, -1, faces2, tm.getFPS());
            if (save)
            {
                cout << ""Saving result2.jpg...\\n"";
                imwrite(""result2.jpg"", image2);
            }
            imshow(""image2"", image2);
            pollKey();
            // Initialize FaceRecognizerSF
//            Ptr<FaceRecognizerSF> faceRecognizer = FaceRecognizerSF::create(fr_modelPath, """");
            Ptr<FaceRecognizerSF> faceRecognizer;
            // Aligning and cropping facial image through the first face of faces detected.
            Mat aligned_face1, aligned_face2;
            faceRecognizer->alignCrop(image1, faces1.row(0), aligned_face1);
            faceRecognizer->alignCrop(image2, faces2.row(0), aligned_face2);
            // Run feature extraction with given aligned_face
            Mat feature1, feature2;
            faceRecognizer->feature(aligned_face1, feature1);
            feature1 = feature1.clone();
            faceRecognizer->feature(aligned_face2, feature2);
            feature2 = feature2.clone();
            double cos_score = faceRecognizer->match(feature1, feature2, FaceRecognizerSF::DisType::FR_COSINE);
            double L2_score = faceRecognizer->match(feature1, feature2, FaceRecognizerSF::DisType::FR_NORM_L2);
            if (cos_score >= cosine_similar_thresh)
            {
                std::cout << ""They have the same identity;"";
            }
            else
            {
                std::cout << ""They have different identities;"";
            }
            std::cout << "" Cosine Similarity: "" << cos_score << "", threshold: "" << cosine_similar_thresh << "". (higher value means higher similarity, max 1.0)\\n"";
            if (L2_score <= l2norm_similar_thresh)
            {
                std::cout << ""They have the same identity;"";
            }
            else
            {
                std::cout << ""They have different identities."";
            }
            std::cout << "" NormL2 Distance: "" << L2_score << "", threshold: "" << l2norm_similar_thresh << "". (lower value means higher similarity, min 0.0)\\n"";
        }
        cout << ""Press any key to exit..."" << endl;
        waitKey(0);
    }
//    else
//    {
//        int frameWidth, frameHeight;
//        VideoCapture capture;
//        std::string video = parser.get<string>(""video"");
//        if (video.size() == 1 && isdigit(video[0]))
//            capture.open(parser.get<int>(""video""));
//        else
//            capture.open(samples::findFileOrKeep(video));  // keep GStreamer pipelines
//        if (capture.isOpened())
//        {
//            frameWidth = int(capture.get(CAP_PROP_FRAME_WIDTH) * scale);
//            frameHeight = int(capture.get(CAP_PROP_FRAME_HEIGHT) * scale);
//            cout << ""Video "" << video
//                << "": width="" << frameWidth
//                << "", height="" << frameHeight
//                << endl;
//        }
//        else
//        {
//            cout << ""Could not initialize video capturing: "" << video << ""\\n"";
//            return 1;
//        }
//        detector->setInputSize(Size(frameWidth, frameHeight));
//        cout << ""Press 'SPACE' to save frame, any other key to exit..."" << endl;
//        int nFrame = 0;
//        for (;;)
//        {
//            // Get frame
//            Mat frame;
//            if (!capture.read(frame))
//            {
//                cerr << ""Can't grab frame! Stop\\n"";
//                break;
//            }
//            resize(frame, frame, Size(frameWidth, frameHeight));
//            // Inference
//            Mat faces;
//            tm.start();
//            detector->detect(frame, faces);
//            tm.stop();
//            Mat result = frame.clone();
//            // Draw results on the input image
//            visualize(result, nFrame, faces, tm.getFPS());
//            // Visualize results
//            imshow(""Live"", result);
//            int key = waitKey(1);
//            bool saveFrame = save;
//            if (key == ' ')
//            {
//                saveFrame = true;
//                key = 0;  // handled
//            }
//            if (saveFrame)
//            {
//                std::string frame_name = cv::format(""frame_%05d.png"", nFrame);
//                std::string result_name = cv::format(""result_%05d.jpg"", nFrame);
//                cout << ""Saving '"" << frame_name << ""' and '"" << result_name << ""' ...\\n"";
//                imwrite(frame_name, frame);
//                imwrite(result_name, result);
//            }
//            ++nFrame;
//            if (key > 0)
//                break;
//        }
//        cout << ""Processed "" << nFrame << "" frames"" << endl;
//    }
    cout << ""Done."" << endl;
    return 0;
}
```



CMakeLists.txt

```
cmake_minimum_required(VERSION 2.8.3)

set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
project(dog_pose)

## Compile as C++11, supported in ROS Kinetic and newer
# add_compile_options(-std=c++11)

## Find catkin macros and libraries
## if COMPONENTS list like find_package(catkin REQUIRED COMPONENTS xyz)
## is used, also find other catkin packages
find_package(catkin REQUIRED COMPONENTS
  rospy
  roscpp
  sensor_msgs
  #std_msgs
  pcl_ros
  pcl_conversions
  cv_bridge
  sensor_msgs
  visualization_msgs
  tf
  cmake_modules
  geometry_msgs
  pcl_msgs
  kdl_parser

)



find_package(Boost REQUIRED)
find_package(Eigen REQUIRED)
find_package(octomap REQUIRED)

link_libraries(${OCTOMAP_LIBRARIES})


catkin_package(
    INCLUDE_DIRS
    CATKIN_DEPENDS roscpp rospy
    orocos_kdl
    kdl_parser
#    intera_core_msgs
    DEPENDS system_lib
    )
set(PCL_DIR ""/home/zs/pcl-1.8/share/pcl-1.8"")
find_package(PCL 1.8 REQUIRED)
#find_package(PCL REQUIRED)
 include_directories(
  ${PCL_INCLUDE_DIRS}
)
link_directories(
  ${PCL_LIBRARY_DIRS}
)

find_package(OpenCV PATHS /usr/local/opencv454/lib/cmake/opencv4)
include_directories(include ${catkin_INCLUDE_DIRS} /usr/local/opencv454/include/opencv4)
link_directories(/usr/local/opencv454/lib)
#find_package(OpenCV 4 REQUIRED)

#set(OpenCV_DIR /usr/local/opencv455/lib/cmake/OpenCV4)
#find_package(OpenCV REQUIRED)
#find_package(OpenCV 3 REQUIRED)
#include_directories(${OpenCV_INCLUDE_DIRS})
## System dependencies are found with CMake's conventions
# find_package(Boost REQUIRED COMPONENTS system)


## Uncomment this if the package has a setup.py. This macro ensures
## modules and global scripts declared therein get installed
## See http://ros.org/doc/api/catkin/html/user_guide/setup_dot_py.html
# catkin_python_setup()

################################################
## Declare ROS messages, services and actions ##
################################################

## To declare and build messages, services or actions from within this
## package, follow these steps:
## * Let MSG_DEP_SET be the set of packages whose message types you use in
##   your messages/services/actions (e.g. std_msgs, actionlib_msgs, ...).
## * In the file package.xml:
##   * add a build_depend tag for ""message_generation""
##   * add a build_depend and a run_depend tag for each package in MSG_DEP_SET
##   * If MSG_DEP_SET isn't empty the following dependency has been pulled in
##     but can be declared for certainty nonetheless:
##     * add a run_depend tag for ""message_runtime""
## * In this file (CMakeLists.txt):
##   * add ""message_generation"" and every package in MSG_DEP_SET to
##     find_package(catkin REQUIRED COMPONENTS ...)
##   * add ""message_runtime"" and every package in MSG_DEP_SET to
##     catkin_package(CATKIN_DEPENDS ...)
##   * uncomment the add_*_files sections below as needed
##     and list every .msg/.srv/.action file to be processed
##   * uncomment the generate_messages entry below
##   * add every package in MSG_DEP_SET to generate_messages(DEPENDENCIES ...)

## Generate messages in the 'msg' folder
# add_message_files(
#   FILES
#   Message1.msg
#   Message2.msg
# )

## Generate services in the 'srv' folder
# add_service_files(
#   FILES
#   Service1.srv
#   Service2.srv
# )

## Generate actions in the 'action' folder
# add_action_files(
#   FILES
#   Action1.action
#   Action2.action
# )

## Generate added messages and services with any dependencies listed here
# generate_messages(
#   DEPENDENCIES
#   std_msgs
# )

################################################
## Declare ROS dynamic reconfigure parameters ##
################################################

## To declare and build dynamic reconfigure parameters within this
## package, follow these steps:
## * In the file package.xml:
##   * add a build_depend and a run_depend tag for ""dynamic_reconfigure""
## * In this file (CMakeLists.txt):
##   * add ""dynamic_reconfigure"" to
##     find_package(catkin REQUIRED COMPONENTS ...)
##   * uncomment the ""generate_dynamic_reconfigure_options"" section below
##     and list every .cfg file to be processed

## Generate dynamic reconfigure parameters in the 'cfg' folder
# generate_dynamic_reconfigure_options(
#   cfg/DynReconf1.cfg
#   cfg/DynReconf2.cfg
# )

###################################
## catkin specific configuration ##
###################################
## The catkin_package macro generates cmake config files for your package
## Declare things to be passed to dependent projects
## INCLUDE_DIRS: uncomment this if you package contains header files
## LIBRARIES: libraries you create in this project that dependent projects also need
## CATKIN_DEPENDS: catkin_packages dependent projects also need
## DEPENDS: system dependencies of this project that dependent projects also need
catkin_package(
    CATKIN_DEPENDS pcl_conversions pcl_msgs pcl_ros sensor_msgs
  #  DEPENDS system_libCATKIN_DEPENDS
    DEPENDS
      Boost
      Eigen
      PCL
      orocos_kdl
      kdl_parser
      #kdl_parser
      DEPENDS system_lib
#  LIBRARIES project_1
#  CATKIN_DEPENDS rospy std_msgs
#  DEPENDS system_lib
)

###########
## Build ##
###########

## Specify additional locations of header files
## Your package locations should be listed before other locations
# include_directories(include)
include_directories(
  ${catkin_INCLUDE_DIRS}
#  /home/zs/pcl_ws/src/pcl_object_tracking/project_1/include
)

## Declare a C++ library
# add_library(project_1
#   src/${PROJECT_NAME}/project_1.cpp
# )

## Add cmake target dependencies of the library
## as an example, code may need to be generated before libraries
## either from message generation or dynamic reconfigure
# add_dependencies(project_1 ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})

## Declare a C++ executable
# add_executable(project_1_node src/project_1_node.cpp)

## Add cmake target dependencies of the executable
## same as for the library above
# add_dependencies(project_1_node ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})

## Specify libraries to link a library or executable target against
# target_link_libraries(project_1_node
#   ${catkin_LIBRARIES}
# )

#############
## Install ##
#############

# all install targets should use catkin DESTINATION variables
# See http://ros.org/doc/api/catkin/html/adv_user_guide/variables.html

## Mark executable scripts (Python etc.) for installation
## in contrast to setup.py, you can choose the destination
# install(PROGRAMS
#   scripts/my_python_script
#   DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
# )

## Mark executables and/or libraries for installation
# install(TARGETS project_1 project_1_node
#   ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
#   LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
#   RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
# )

## Mark cpp header files for installation
# install(DIRECTORY include/${PROJECT_NAME}/
#   DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}
#   FILES_MATCHING PATTERN ""*.h""
#   PATTERN "".svn"" EXCLUDE
# )

## Mark other files for installation (e.g. launch and bag files, etc.)
# install(FILES
#   # myfile1
#   # myfile2
#   DESTINATION ${CATKIN_PACKAGE_SHARE_DESTINATION}
# )

#############
## Testing ##
#############


#add_executable(face_3d_pose src/face_3d_pose.cpp)
#target_link_libraries(face_3d_pose ${catkin_LIBRARIES}   ${OpenCV_LIBRARIES})

add_executable(face_detection src/face_detection.cpp)
target_link_libraries(face_detection ${catkin_LIBRARIES}   ${OpenCV_LIBRARIES}
    /usr/local/opencv454/lib/libopencv_highgui.so
    /usr/local/opencv454/lib/libopencv_core.so
    /usr/local/opencv454/lib/libopencv_imgproc.so
    /usr/local/opencv454/lib/libopencv_imgcodecs.so
    /usr/local/opencv454/lib/libopencv_dnn.so
    /usr/local/opencv454/lib/libopencv_dnn.so)
```



Error output

face_detection.cpp:67: error: undefined reference to `cv::FaceDetectorYN::create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, cv::Size_<int> const&, float, float, int, int, int)'

"
opencv/opencv,2022-09-21 02:07:13,question,What are the requirements for the VideoCapture URL?,"What are the requirements for the VideoCapture URL?

1.https://gitee.com/laolaolulu/public/raw/master/lADPDgQ9rpdwHBXNArzNArw_700_700.jpg

2.https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg.jj20.com%2Fup%2Fallimg%2F4k%2Fs%2F02%2F2109242332225H9-0-lp.jpg&refer=http%3A%2F%2Fimg.jj20.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1666315958&t=32dbab240541a94343584ef5df52595d



Why 1 reads null"
opencv/opencv,2022-09-11 18:00:43,question,mAP score,"I am using [SSD MobileNet V2 FPNLite 320x320] to train my model. I have chest x-ray to detect Covid-19. There 1349 Normal chest x-rays and 3883 Covid-19 chest x-rays. I have used different Augmentations to increase my Normal chest-xray from 1349 to 2215. and pneumonia images from 3883 to 4032. 
I have trained my model 4000 training steps, and it has training loss of 0.21 and evaluation loss of 0.23. 

I am confused that I have gotten 0.837 mAP. Is it a good result? I need to compare my results with some other papers, but they have accuracy nearest 96 or 98. There result is in accuracy and mine is in mAP. How am I going to compare them??

![image](https://user-images.githubusercontent.com/43025113/189542174-62298b2e-f953-4a5d-8b0e-7c84986d9269.png)


Another question is that most of the Tensorflow object detection API has mAP between 0.20 to 0.50, but mine is 0.83. So is their some issue with my model or its fine? Because it is detecting and classifying most of the images accurately. 

Please please any expert guide me regarding all of the issues I have asked."
opencv/opencv,2022-09-08 19:37:59,question,Unable to build iOS framework for OpenCV 3.4.5,"##### System information (version)
- OpenCV => 3.4.5
- Operating System / Platform => Mac/iOS
- Compiler => Xcode

##### Detailed description
I checked out OpenCV (and contrib) version 3.4.5.
Trying to run python `opencv/platforms/ios/build_framework.py ios`

But getting following error:
```
Using IPHONEOS_DEPLOYMENT_TARGET=8.0
Using iPhoneOS ARCHS=['armv7', 'armv7s', 'arm64']
Using iPhoneSimulator ARCHS=['i386', 'x86_64']
============================================================
ERROR: cannot use a string pattern on a bytes-like object
============================================================
Traceback (most recent call last):
  File ""opencv/platforms/ios/build_framework.py"", line 117, in build
    self._build(outdir)
  File ""opencv/platforms/ios/build_framework.py"", line 85, in _build
    xcode_ver = getXCodeMajor()
  File ""opencv/platforms/ios/build_framework.py"", line 45, in getXCodeMajor
    m = re.match(r'Xcode\\s+(\\d+)\\..*', ret, flags=re.IGNORECASE)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/re.py"", line 191, in match
    return _compile(pattern, flags).match(string)
TypeError: cannot use a string pattern on a bytes-like object
```
"
opencv/opencv,2022-08-31 03:57:30,question,Problem with OpenCV and Jupyterhub,"
<Hi everyone,

I am new in Python and object Detection. I have followed the instruction about object detection and yolov4 [https://github.com/techzizou/yolov4-custom_Training](https://github.com/orgs/community/discussions/url). I applied everything same on Colab and it was working good. However, when I change the datasource and parameter to fit to my project. I have run on Jupyterhub server docker and got the problem with OpenCV and build darknet.


![image](https://user-images.githubusercontent.com/112531600/187589319-3372cfa2-dfb4-43bf-8ab9-c14de3bca71e.png)


OpenCV is already installed on Jupyterhub.

`Package Version

oauth2client 4.1.3
oauthlib 3.1.1
object-detection 0.1
opencv-python 4.5.4.60
opencv-python-headless 4.6.0.66
openpyxl 3.0.10
`

Have anyone know the solution for this?
Thank you so much.>"
opencv/opencv,2022-08-27 05:43:31,question,Why the method be finished?,"##### System information (version)
- OpenCV => 4.4
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
- 
The API: ""getWindowProperty"" no saved in opencv 4.4?

 while(1){

 namedWindow(""exit window"", WINDOW_AUTOSIZE);  //绘制窗口
 imshow(""exit window"",frame);    //显示当前帧
 waitKey(30); //延时30ms

 if( getWindowProperty(""exit window"", WND_PROP_AUTOSIZE) != 1)  //判断是否点击窗口关闭按钮
     break;
}

"
opencv/opencv,2022-08-23 07:25:00,question,Error: Assertion failed (elemSize() == sizeof(_Tp)) in cv::Mat::at ,"<pre>resized_img.convertTo(final_img, CV_32F, 1.f / 255);
    cout << ""Image pixels are rescaled."" << endl;
    int dims[3] = { channel, height, width };
    cv::Mat mat_nchw = cv::Mat(3, dims, CV_32F);
    for (int i = 0; i < height; i++) {
        for (int j = 0; j < width; j++) {
            for (int k = 0; k < channel; k++) {
                float temp = [final_img.at](http://final_img.at/)<float>(i, j, k);
                [mat_nchw.at](http://mat_nchw.at/)<float>(k, i, j) = temp;
            }
        }
    }</pre>

I am getting error message like this:
OpenCV(4.4.0) Error: Assertion failed (elemSize()iss== sizeof(_Tp)) in cv::Mat::at, file C:\\lib\\install\\opencv\\include\\opencv2/core/mat.inl.hpp, line 1227
Exception ocurs: Unknown exception 

I want to convert my matrix from h x w x c to c x h x w, at first I tried reshape() but it's not working and then I have tried to construct a new matrix by positioning each element at propoer position of new matrix. Although all of matrices are CV_32F in data type and I have also used float for type casting in at() but still I am getting some data type mismatching error. Can anyone help me with this issue? 
"
opencv/opencv,2022-08-16 19:31:10,question,Choosing `patternSize` parameter in `findChessboardCorners`,"Hello,

Quick question here on the `patternSize` parameter of `findChessboardCorners()` —

I need to know how to properly define the number of ""inner corner points"" along the rows, cols of a simple 2D chessboard calibration target. 

Here's one of the calibration target images I'm using:
![GOPR0059](https://user-images.githubusercontent.com/22329957/184963456-b3733efb-8f56-4ce6-be85-82dd442c5e80.jpg)

And how I assume the inner corner coordinates should be mapped:
![2022-08-10-Figure-3-Chessboard-Corners](https://user-images.githubusercontent.com/22329957/184963640-f245b16d-7967-489f-92e7-d761f92ba592.jpg)

Resulting in a `patternSize = (8,6)` which I assume is the correct inner point count (starting at 1, not zero). 

However, in the OpenCV [`py_calibration.markdown`](https://github.com/opencv/opencv/blob/3.4/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.markdown) tutorial, the calibration pattern of `patternSize = (7,6)` (but actually `(8,6)` by my logic) has the following detected inner points:
![calib_pattern](https://user-images.githubusercontent.com/22329957/184964441-91affe09-2140-420f-a8a9-32c081d00ea6.jpg)
which seem to only include the second-to-last corner of each square (shown as the lower corner point in the upper-most row of black squares, but *not* the upper corner points).

Can you help clarify this in the context of my calibration pattern? Thanks!
"
opencv/opencv,2022-08-15 07:58:47,question,Is opencv.js supports dft and dct  now?,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => 4.6.0
- Operating System / Platform => web(opencv.js)
- Compiler => 

##### Detailed description

<!-- your description -->
Is `opencv.js` supports `dft` and `dct` on web now?

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

I was using opencv.js to perform `dft` and `dct`, but it raise an exception. Here is my code
```javascript
            let src = cv.imread(imgElement);
            let dst = new cv.Mat();
            let dft = new cv.Mat();
            // To distinguish the input and output, we graying the image.
            // You can try different conversions.
            cv.cvtColor(src, dst, cv.COLOR_RGBA2GRAY);
            cv.dft(dst,dft)
            cv.imshow('canvasOutput', dft);
            src.delete();
            dst.delete();
            dft.delete();
```
![image](https://user-images.githubusercontent.com/33513462/184597882-52377ad4-7bec-43c5-93b1-20874a79987d.png)



##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-08-06 06:03:44,question,Input size not specified in function 'processFrame',"pencv-python/opencv-python/opencv/modules/dnn/src/model.cpp:98: error: (-201:Incorrect size of input array) Input size not specified in function 'processFrame'

##### System information (version)

- OpenCV => opencv-python   4.6.0.66
- Operating System / Platform => Mac OS 
- Compiler => Python 3.9.13

##### Detailed description
```
im = cv.imread(src123)
DB_TD500_resnet50 = ""./pre-trained_models/DB_TD500_resnet50.onnx""

detector = dnn.TextDetectionModel_DB(DB_TD500_resnet50)
detector.setBinaryThreshold(0.3)   
detector.setPolygonThreshold(0.5)  
detector.setUnclipRatio(2.0)      
detector.setMaxCandidates(200)    

detections = detector.detect(im)   # error: (-201:Incorrect size of input array) Input size not specified in function 'processFrame'
```
"
opencv/opencv,2022-07-29 03:07:27,question,How to understand the function FastX::calcFeatureMap in chessboard.cpp,"```
     for(float *pout=out.ptr<float>(0,0);pout != pout_end;++pout)
    {
        //reset values
        rating = 0.0; count1 = 0;
        noise = 255; signal = 0;

        //calc rating
        pend = pimages+channels;
        val1 = *(pend-1);                       // wrap around (last value)
        wrap_around = pimages++;                // store for wrap around (first value)
        val2 = *wrap_around;                    // first value
        for(;pimages != pend;++pimages)
        {
            val3 = *pimages;
            if(val1 <= val2)
            {
                if(val3 < val2) // maxima
                {
                    if(signal < val2)
                        signal = val2;
                    ++count1;
                }
            }
            else if(val1 > val2 && val3 >= val2) // minima
            {
                if(noise > val2)
                    noise = val2;
                ++count1;
            }
            val1 = val2;
            val2 = val3;
        }
        // wrap around
        if(val1 <= val2) // maxima
        {
            if(*wrap_around < val2)
            {
                if(signal < val2)
                    signal = val2;
                ++count1;
            }
        }
        else if(val1 > val2 && *wrap_around >= val2) // minima
        {
            if(noise > val2)
                noise = val2;
            ++count1;
        }

        // store rating
        if(count1 == parameters.branches)
        {
            rating = signal-noise;
            *pout = rating*rating; //store rating in the feature map
        }
    }
```
<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

<!-- your description -->

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-07-25 08:19:17,question,"Parse error.  Expected a command name, got unquoted argument with text   ""|"".","Hi, i have a question when i click the configure button on Cmake-3.13.4. the log is as following:
```
Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.19044.
The CXX compiler identification is MSVC 19.16.27048.0
The C compiler identification is MSVC 19.16.27048.0
Check for working CXX compiler: D:/vs2017_community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
Check for working CXX compiler: D:/vs2017_community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
Detecting CXX compiler ABI info
Detecting CXX compiler ABI info - done
Detecting CXX compile features
Detecting CXX compile features - done
Check for working C compiler: D:/vs2017_community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe
Check for working C compiler: D:/vs2017_community/VC/Tools/MSVC/14.16.27023/bin/Hostx86/x64/cl.exe -- works
Detecting C compiler ABI info
Detecting C compiler ABI info - done
Detecting C compile features
Detecting C compile features - done
Detected processor: AMD64
Performing Test HAVE_CXX11 (check file: cmake/checks/cxx11.cpp)
Performing Test HAVE_CXX11 - Success
Found PythonInterp: D:/anaconda/python.exe (found suitable version ""3.9.12"", minimum required is ""2.7"") 
CMake Warning at cmake/OpenCVDetectPython.cmake:81 (message):
  CMake's 'find_host_package(PythonInterp 2.7)' found wrong Python version:

  PYTHON_EXECUTABLE=D:/anaconda/python.exe

  PYTHON_VERSION_STRING=3.9.12

  Consider providing the 'PYTHON2_EXECUTABLE' variable via CMake command line
  or environment variables

Call Stack (most recent call first):
  cmake/OpenCVDetectPython.cmake:271 (find_python)
  CMakeLists.txt:622 (include)


Could NOT find Python2 (missing: Python2_EXECUTABLE Interpreter) 
Found PythonInterp: D:/anaconda/python.exe (found suitable version ""3.9.12"", minimum required is ""3.2"") 
Found PythonLibs: D:/anaconda/libs/python39.lib (found suitable exact version ""3.9.12"") 
Performing Test HAVE_CXX_FP:PRECISE
Performing Test HAVE_CXX_FP:PRECISE - Success
Performing Test HAVE_C_FP:PRECISE
Performing Test HAVE_C_FP:PRECISE - Success
Performing Test HAVE_CPU_SSE3_SUPPORT (check file: cmake/checks/cpu_sse3.cpp)
Performing Test HAVE_CPU_SSE3_SUPPORT - Success
Performing Test HAVE_CPU_SSSE3_SUPPORT (check file: cmake/checks/cpu_ssse3.cpp)
Performing Test HAVE_CPU_SSSE3_SUPPORT - Success
Performing Test HAVE_CPU_SSE4_1_SUPPORT (check file: cmake/checks/cpu_sse41.cpp)
Performing Test HAVE_CPU_SSE4_1_SUPPORT - Success
Performing Test HAVE_CPU_POPCNT_SUPPORT (check file: cmake/checks/cpu_popcnt.cpp)
Performing Test HAVE_CPU_POPCNT_SUPPORT - Success
Performing Test HAVE_CPU_SSE4_2_SUPPORT (check file: cmake/checks/cpu_sse42.cpp)
Performing Test HAVE_CPU_SSE4_2_SUPPORT - Success
Performing Test HAVE_CXX_ARCH:AVX (check file: cmake/checks/cpu_fp16.cpp)
Performing Test HAVE_CXX_ARCH:AVX - Success
Performing Test HAVE_CXX_ARCH:AVX2 (check file: cmake/checks/cpu_avx2.cpp)
Performing Test HAVE_CXX_ARCH:AVX2 - Success
Performing Test HAVE_CXX_ARCH:AVX512 (check file: cmake/checks/cpu_avx512.cpp)
Performing Test HAVE_CXX_ARCH:AVX512 - Success
Performing Test HAVE_CPU_BASELINE_FLAGS
Performing Test HAVE_CPU_BASELINE_FLAGS - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_1
Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_1 - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_2
Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_2 - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_FP16
Performing Test HAVE_CPU_DISPATCH_FLAGS_FP16 - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX2
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX2 - Success
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX512_SKX
Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX512_SKX - Success
Performing Test HAVE_CXX_W15240
Performing Test HAVE_CXX_W15240 - Success
Performing Test HAVE_C_W15240
Performing Test HAVE_C_W15240 - Success
Looking for malloc.h
Looking for malloc.h - found
Looking for _aligned_malloc
Looking for _aligned_malloc - found
Check if the system is big endian
Searching 16 bit integer
Looking for sys/types.h
Looking for sys/types.h - found
Looking for stdint.h
Looking for stdint.h - found
Looking for stddef.h
Looking for stddef.h - found
Check size of unsigned short
Check size of unsigned short - done
Using unsigned short
Check if the system is big endian - little endian
Looking for fseeko
Looking for fseeko - not found
Check size of off64_t
Check size of off64_t - failed
libjpeg-turbo: VERSION = 2.1.0, BUILD = opencv-4.5.4-libjpeg-turbo
Check size of size_t
Check size of size_t - done
Check size of unsigned long
Check size of unsigned long - done
Looking for include file intrin.h
Looking for include file intrin.h - found
Looking for assert.h
Looking for assert.h - found
Looking for fcntl.h
Looking for fcntl.h - found
Looking for inttypes.h
Looking for inttypes.h - found
Looking for io.h
Looking for io.h - found
Looking for limits.h
Looking for limits.h - found
Looking for memory.h
Looking for memory.h - found
Looking for search.h
Looking for search.h - found
Looking for string.h
Looking for string.h - found
Performing Test C_HAS_inline
Performing Test C_HAS_inline - Success
Check size of signed short
Check size of signed short - done
Check size of unsigned short
Check size of unsigned short - done
Check size of signed int
Check size of signed int - done
Check size of unsigned int
Check size of unsigned int - done
Check size of signed long
Check size of signed long - done
Check size of signed long long
Check size of signed long long - done
Check size of unsigned long long
Check size of unsigned long long - done
Check size of unsigned char *
Check size of unsigned char * - done
Check size of ptrdiff_t
Check size of ptrdiff_t - done
Looking for memmove
Looking for memmove - not found
Looking for setmode
Looking for setmode - found
Looking for strcasecmp
Looking for strcasecmp - not found
Looking for strchr
Looking for strchr - found
Looking for strrchr
Looking for strrchr - found
Looking for strstr
Looking for strstr - found
Looking for strtol
Looking for strtol - found
Looking for strtol
Looking for strtol - found
Looking for strtoull
Looking for strtoull - found
Looking for lfind
Looking for lfind - found
Performing Test HAVE_SNPRINTF
Performing Test HAVE_SNPRINTF - Success
Check if the system is big endian
Searching 16 bit integer
Using unsigned short
Check if the system is big endian - little endian
Performing Test HAVE_C_STD_C99
Performing Test HAVE_C_STD_C99 - Failed
Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources
OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.5.4-openjp2-2.4.0
Check if the system is big endian
Searching 16 bit integer
Using unsigned short
Check if the system is big endian - little endian
Looking for stdlib.h
Looking for stdlib.h - found
Looking for stdio.h
Looking for stdio.h - found
Looking for math.h
Looking for math.h - found
Looking for float.h
Looking for float.h - found
Looking for time.h
Looking for time.h - found
Looking for stdarg.h
Looking for stdarg.h - found
Looking for ctype.h
Looking for ctype.h - found
Looking for stdint.h
Looking for stdint.h - found
Looking for inttypes.h
Looking for inttypes.h - found
Looking for strings.h
Looking for strings.h - not found
Looking for sys/stat.h
Looking for sys/stat.h - found
Looking for unistd.h
Looking for unistd.h - not found
Looking for include file malloc.h
Looking for include file malloc.h - found
Looking for _aligned_malloc
Looking for _aligned_malloc - found
Looking for posix_memalign
Looking for posix_memalign - not found
Looking for memalign
Looking for memalign - not found
OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.4.0"")
CMake Error at 3rdparty/ippicv/ippicv.cmake:1:
  Parse error.  Expected a command name, got unquoted argument with text
  ""|"".
Call Stack (most recent call first):
  cmake/OpenCVFindIPP.cmake:258 (include)
  cmake/OpenCVFindLibsPerf.cmake:12 (include)
  CMakeLists.txt:727 (include)


Configuring incomplete, errors occurred!
See also ""D:/opencv-4.5.4/opencv/cpu_build/CMakeFiles/CMakeOutput.log"".
See also ""D:/opencv-4.5.4/opencv/cpu_build/CMakeFiles/CMakeError.log"".
```
please help me, thank you very much!"
opencv/opencv,2022-07-18 22:07:10,question,Error: Null pointer (NULL window: 'Test Program') in cvGetPropWindowAutoSize_W32,"##### System information (version)

- OpenCV => 4.5
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017


##### Detailed description

When I take WindowPropertyFlags that no work after opencv 4.5, but is normal work in opencv4.4.

Tips:Error: Null pointer (NULL window: 'Test Program') in cvGetPropWindowAutoSize_W32

Example Code:
```
#include <opencv2/opencv.hpp>
#include <iostream>
#include <opencv2/highgui/highgui_c.h>

using namespace cv;
using namespace std;

int main()
{
	VideoCapture capture(0);   

	while(1)
    {
		Mat frame;
		capture >> frame;
		if(frame.empty())
			break;
        namedWindow(""Test Program"",WINDOW_AUTOSIZE); 
		imshow(""Test Program"",frame);   
		waitKey(30); 

        if( getWindowProperty(""Test Program"",WND_PROP_AUTOSIZE) != 1)
            break;
     }
    capture.release();
    destroyAllWindows();
    return 0;
}
```"
opencv/opencv,2022-07-10 01:27:37,question,Why don't rectangle box in opencv4.6?,"
- OpenCV => 4.6
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017


- OpenCV => :grey_question:


##### Detailed description

I just try inference yolov5s.onnx by opencv4.6, but the opencv 4.6 don't rectangle box?

But the opencv 4.5.5 is normal.

That is inference API have changed about opencv4.6?

This is infer code exp:
https://blog.csdn.net/qq_26696715/article/details/123524128
"
opencv/opencv,2022-06-28 05:34:32,question,<class 'cv2.VideoCapture'> returned a result with an error set and my camera and videos can not be read,"import cv2

cv2.namedWindow('video', cv2.WINDOW_NORMAL)
cv2.resizeWindow('video', 640, 480)

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()

    cv2.imshow('video', frame)

    key = cv2.waitKey(10)
    if key & 0Xff == ord('q'):
        print('退出成功')
        break

cap.read()
cv2.destroyAllWindows()

My code is above and the error shows as follows:

D:\\Python\\anaconda\\envs\\opencv\\python.exe C:/Users/yyl47/Desktop/pythonProject/test.py cv2.error: invalid stoull argument

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\\Users\\yyl47\\Desktop\\pythonProject\\test.py"", line 5, in <module>
    cap = cv2.VideoCapture(0)
SystemError: <class 'cv2.VideoCapture'> returned a result with an error set

Process finished with exit code 1
Anyone can help me?My python version is 3.9, the operating system is windows10 and it is a laptop, the camera works well in other app. "
opencv/opencv,2022-06-24 11:38:57,question,Where are the build instructions for opencv 4.6,"Sorry, I tried signing up to https://forum.opencv.org to ask this question but did not receive an activation email.
Sorry again, if this is a stupid question but I cannot find any build instructions for opencv 4.6. Could someone advise please?

Marc
"
opencv/opencv,2022-06-21 15:02:34,question,"cv::norm(...,...,cv::NORM_L2SQR) is 10x slower than calling cv::normL2Sqr(...)","Used Version: OpenCV 4.5.4 and OpenCV 4.6.0

Calling cv::norm is 10x slower than calling cv::normL2Sqr directly.

I tested measured the time with 2.5 Million float vectors with 512 dimensions.

cv::normL2Sqr:     308ms
cv::norm:            3092ms

Im wondering how there can be such an performance loss.

There is also no avx optimization in opencv for cv::norm, but i found some implementations:

https://github.com/anas-899/l2_distance_SIMD/blob/master/Source.cpp
https://gist.github.com/matsui528/583925f88fcb08240319030202588c74 

Maybe somebody with avx knowledge wants to take this over?"
opencv/opencv,2022-06-14 11:13:13,question,!(roi & image_roi).empty() in function error ,"![image](https://user-images.githubusercontent.com/89961242/173564496-d7a85d24-80f6-4151-9f6f-42a7ffc5e46f.png)

now i'm having this error while making opencv KCF_tracker.

any help plz?"
opencv/opencv,2022-06-09 12:51:55,question,No codec available in Video I/O.,"<!--
##### System information (version)
<!-- Example
- OpenCV => 4.6
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2019
-->
- OpenCV => 4.5.4 && 4.6
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2019
##### Detailed description
Hi,

I'm trying to save video with `cv::VideoWriter` on either windows and unix system. Everything is working fine except for one particular computer (Win10, the two other windows computer are OK) where no codecs or apiPreference are working (`bool cv::VideoWriter::open(...)` outputs `false` for every combination, on OpenCV 4.5.4). So I tried to rebuild OpenCV 4.6 and it outputs 
a weird message saying that no Video I/O is available...
I[cmake_output.txt](https://github.com/opencv/opencv/files/8870383/cmake_output.txt)

```
  Media I/O: 
    ZLib:                        build (ver 1.2.12)
    JPEG:                        build-libjpeg-turbo (ver 2.1.2-62)
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         build (ver 1.6.37)
    TIFF:                        build (ver 42 - 4.2.0)
    JPEG 2000:                   build (ver 2.4.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES

  Video I/O:
    DC1394:                      NO
    FFMPEG:                      NO
      avcodec:                   NO
      avformat:                  NO
      avutil:                    NO
      swscale:                   NO
      avresample:                NO
    GStreamer:                   NO
    DirectShow:                  NO
    Media Foundation:            NO
      DXVA:                      NO
```

I retried the same steps on two others Windows computer, and they show YES to everything (except DC1394). Can you help me to understand why OpenCV cannot detect DirectShow and what's wrong with my setup (I assume it's here natively on a windows computer)?

Thanks!

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->
`cmake-gui`

##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [x] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-06-07 12:07:59,question,Cannot choose which qt to use,"As the title says.

With qt5 alone installed opencv-4.5.5 compiles just fine finding what it needs for qt5.
However with qt5 and 6 installed the configuration stage defaults to qt6. I have not seen a way in the scripts to jiggle things back to using qt5.

Obviously my issue is telling opencv which qt I want it to use.

I have qt5 installed in /usr/include/qt5, /usr/lib/qt5, etc and qt6 installed in /usr/lib/qt6 and /usr/include/qt6, etc, that is they all live in their own areas.

Looking at  #20499, #20183 and a few others they don't appear to take in account the ability to choose. 

Choice would be appreciated.

BTW, in my case since it picks up qt6 instead, the compile fails here;

[ 54%] Building CXX object modules/calib3d/CMakeFiles/opencv_calib3d.dir/undistort.avx2.cpp.o
[ 54%] Linking CXX shared library ../../lib/libopencv_calib3d.so
[ 54%] Built target opencv_calib3d
[ 54%] Automatic MOC for target opencv_cvv
[ 54%] Built target opencv_cvv_autogen
[ 54%] Building CXX object modules/cvv/CMakeFiles/opencv_cvv.dir/opencv_cvv_autogen/mocs_compilation.cpp.o
In file included from /usr/src/opencv-4.5.5/opencv-oosb/modules/cvv/opencv_cvv_autogen/MXUWEOXILK/moc_call_tab.cpp:10,
                 from /usr/src/opencv-4.5.5/opencv-oosb/modules/cvv/opencv_cvv_autogen/mocs_compilation.cpp:2:
/usr/src/opencv-4.5.5/opencv-oosb/modules/cvv/opencv_cvv_autogen/MXUWEOXILK/../../../../../opencv_contrib-4.5.5/modules/cvv/src/gui/call_tab.hpp:4:10: fatal error: QString: No such file or directory
    4 | #include <QString>
      |          ^~~~~~~~~
compilation terminated.
make[2]: *** [modules/cvv/CMakeFiles/opencv_cvv.dir/build.make:76: modules/cvv/CMakeFiles/opencv_cvv.dir/opencv_cvv_autogen/mocs_compilation.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:4369: modules/cvv/CMakeFiles/opencv_cvv.dir/all] Error 2
make: *** [Makefile:166: all] Error 2


"
opencv/opencv,2022-06-02 06:11:03,question,how can i put opencv-tracker in yolov5?,"hi. i have to put opencv-tracker(maybe KCF) in Yolov5.

um, if you've used opencv-tracker before, you know about this.

i don't want to selectROI with my mouse. i just want to put detected bbox coordinates from detect.py
and i want to track every frames in video.
![image](https://user-images.githubusercontent.com/89961242/171564459-bd06c7e8-e65d-40b3-b084-d6ff3510a676.png)

so this is my code (this is plots.py)


i'm having
error: (-5:Bad argument) One or more matrix operands are empty. in function 'checkOperandsExist'
error. plz help me. i want how to wirte the code. 

"
opencv/opencv,2022-05-19 01:45:50,question,TypeError: imwrite() takes 2 positional arguments but 3 were given,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<-- Example
- OpenCV => 4.5.5
- Operating System / Platform => Docker (Ubuntu 20.04)
- Compiler => GCC 9.3.0
- python 3.9.7
-->


##### Detailed description

I am trying to save jpg image with quality 80 by opencv. Here is the python script I tried:


`cv2.imwrite(isdocker.DOCKER_PREFIX + IM.path + IM.name, IM.data, [(int(cv2.IMWRITE_JPEG_QUALITY),80)])`

It pops up with an error 'TypeError: imwrite() takes 2 positional arguments but 3 were given'. I know a similar post in [[here](https://stackoverflow.com/questions/48629370/python-error-imwrite-takes-at-most-3-arguments-4-given)]. But I think its different issue. I have read the [documentation](https://docs.opencv.org/4.x/d8/d6a/group__imgcodecs__flags.html#ga292d81be8d76901bff7988d18d2b42ac) from opencv and it seems you have changed something. Please help me.

"
opencv/opencv,2022-05-17 02:53:44,question,"when i use libopencv_java3.so in android projetc,and run Android 7.0 phone，Phone crashes automatically after 25 seconds","i use android ndk  Cross-platform call c++，
use  
```
  cv::GaussianBlur(images.fimOrig, fimseg, cv::Size(0, 0), 1.0, 1.0, 2);
```

Phone crashes automatically after 25 seconds
i do not reason，can you help me why?
"
opencv/opencv,2022-05-14 16:12:17,question,"USB设备：不能打开设备，请检查设备是否安装。x:0,y:0","USB设备：不能打开设备，请检查设备是否安装。x:0,y:0"
opencv/opencv,2022-05-08 16:13:10,question,opencv,was ist die Unterschied zwischen opencv und openai
opencv/opencv,2022-05-07 05:36:32,question,how to convert a cv::Mat to ffmpeg AVFrame?,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

<!-- your description -->

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-05-03 14:10:45,question,How to remove distortion in opencv.js,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

<!-- your description -->

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-04-23 15:20:08,question,Multiple values appear when binary images are imwrite as JPG,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

- OpenCV-python => 4.5.5
- Operating System / Platform => Windows 64 Bit
- Python version: 3.8.13

##### Detailed description

I binarized an image and saved it as JPG. When I re-read the image, There are more than two values. PNG images don't have this problem.

##### Steps to reproduce

```python
img_path = r""temp.tif""
png_path = r""temp.png""
jpg_path = r""temp.jpg""
```
```python
gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
print(""gray shape: {}, values: {}"".format(gray.shape, np.unique(gray)))
```
output:

gray shape: (125, 125), values: [191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226
 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244
 245 246 247 248 249 250 251 252 253 254 255]

```python
_, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
print(""thresh shape: {}, values: {}"".format(thresh.shape, np.unique(thresh)))
cv2.imwrite(jpg_path, thresh)
cv2.imwrite(png_path, thresh)
jpg = cv2.imread(jpg_path, cv2.IMREAD_UNCHANGED)
png = cv2.imread(png_path, cv2.IMREAD_UNCHANGED)
print(""jpg shape: {}, values: {}"".format(jpg.shape, np.unique(jpg)))
print(""png shape: {}, values: {}"".format(png.shape, np.unique(png)))
```
output:

thresh shape: (125, 125), values: [  0 255]
jpg shape: (125, 125), values: [  0   **1   2   3   4   5 250 251 252 253 254** 255]
png shape: (125, 125), values: [  0 255]


##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
       This is where I found a similar problem on StackOverflow: python - Saving and Reading Binary Image - Stack Overflow : <https://stackoverflow.com/questions/48023127/saving-and-reading-binary-image>
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [x] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-04-20 12:11:28,question,[ 14%] Built target opencv_cudaarithm make: *** [Makefile:166: all] Error 2  in ubantu20.4  cuda 11.2 cudnn 8.2.1,"hi:
   when I  install opencv 4.5.5  on ubuntu20.4 with gpu.but i get some error.
  Determining if the include file sys/videoio.h exists failed with the following output:
```
Change Dir: /home/anthony/Downloads/opencvgpu/opencv/build/CMakeFiles/CMakeTmp

Run Build Command(s):/usr/bin/make -f Makefile cmTC_17c3f/fast && /usr/bin/make  -f CMakeFiles/cmTC_17c3f.dir/build.make CMakeFiles/cmTC_17c3f.dir/build
make[1]: Entering directory '/home/anthony/Downloads/opencvgpu/opencv/build/CMakeFiles/CMakeTmp'
Building C object CMakeFiles/cmTC_17c3f.dir/CheckIncludeFile.c.o
/usr/bin/cc   -fsigned-char -ffast-math -W -Wall -Wreturn-type -Waddress -Wsequence-point -Wformat -Wformat-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fopenmp  -O3 -DNDEBUG  -DNDEBUG -fPIE -o CMakeFiles/cmTC_17c3f.dir/CheckIncludeFile.c.o -c /home/anthony/Downloads/opencvgpu/opencv/build/CMakeFiles/CMakeTmp/CheckIncludeFile.c
In file included from /home/anthony/Downloads/opencvgpu/opencv/build/CMakeFiles/CMakeTmp/CheckIncludeFile.c:1:0:
/usr/include/sys/videoio.h:45:10: fatal error: opencv2/core/core_c.h: No such file or directory
 #include ""opencv2/core/core_c.h""
          ^~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
make[1]: *** [CMakeFiles/cmTC_17c3f.dir/build.make:78: CMakeFiles/cmTC_17c3f.dir/CheckIncludeFile.c.o] Error 1
make[1]: Leaving directory '/home/anthony/Downloads/opencvgpu/opencv/build/CMakeFiles/CMakeTmp'
make: *** [Makefile:127: cmTC_17c3f/fast] Error 2




when I made it ,error  is 

 14%] Linking CXX shared library ../../lib/libopencv_cudawarping.so
[ 14%] Built target opencv_cudawarping
[ 14%] Building CXX object modules/cudaarithm/CMakeFiles/opencv_cudaarithm.dir/src/arithm.cpp.o
[ 14%] Building CXX object modules/cudaarithm/CMakeFiles/opencv_cudaarithm.dir/src/core.cpp.o
[ 14%] Building CXX object modules/cudaarithm/CMakeFiles/opencv_cudaarithm.dir/src/element_operations.cpp.o
[ 14%] Building CXX object modules/cudaarithm/CMakeFiles/opencv_cudaarithm.dir/src/lut.cpp.o
[ 14%] Building CXX object modules/cudaarithm/CMakeFiles/opencv_cudaarithm.dir/src/reductions.cpp.o
[ 14%] Linking CXX shared library ../../lib/libopencv_cudaarithm.so
[ 14%] Built target opencv_cudaarithm
make: *** [Makefile:166: all] Error 2
```"
opencv/opencv,2022-04-14 09:12:11,question,"What is the difference between ""import cv2"" and ""import cv2.cv2""","I have two questions.
one: the difference between `import cv2` and `import cv2.cv2`
the other: cv2.error: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-memyuvq3\\opencv\\modules\\highgui\\src\\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'

In response to question 2, I have installed opencv-python and opencv-contrib-python
"
opencv/opencv,2022-04-13 20:00:00,question,Python Warped prespective ,"- OpenCV => 4.0.1
- Operating System / Platform => Windows 64 Bit
- Compiler => PyCharm 2021.3.1

##### Detailed description

I need to implement a warp perspective, and the following error happens:

Traceback (most recent call last):
  File ""C:/Users/Diogo Alpendre/OneDrive - IPLeiria/Para arrumar/Documentos/GitHub/T22_AD_Detection/perspective transormation.py"", line 22, in <module>
    result = cv2.warpPerspective(frame, pts1, (640, 480))
cv2.error: OpenCV(4.0.1) C:\\ci\\opencv-suite_1573470242804\\work\\modules\\imgproc\\src\\imgwarp.cpp:2927: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'cv::warpPerspective'

[ WARN:0] terminating async callback

Process finished with exit code 1 

##### Steps to reproduce

<!-- my code
    import cv2
import numpy as np

# Turn on Laptop's webcam
cap = cv2.VideoCapture(0)
width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
print(width, height)
while True:

    ret, frame = cap.read()

    # Locate points of the documents
    # or object which you want to transform
    pts1 = np.float32([[320, 360], [960, 360],
                       [0, 480], [640, 480]])
    pts2 = np.float32([[0, 0], [640, 0],
                       [0, 720], [640, 480]])

    # Apply Perspective Transform Algorithm
    matrix = cv2.getPerspectiveTransform(pts1, pts2)
    result = cv2.warpPerspective(frame, pts1, (640, 480))

    cv2.imshow('frame', frame)  # Initial Capture
    cv2.imshow('frame1', result)  # Transformed Capture

    if cv2.waitKey(24) == 27:
        break

cap.release()
cv2.destroyAllWindows()
-->"
opencv/opencv,2022-04-11 02:55:58,question,libopencv_core.a(matrix.cpp.o) is referenced by DSO?,"
- OpenCV => 4.2
- Operating System / Platform => linux 64 Bit
- Compiler =>cmake

##### Detailed description

<!-- your description -->

##### I build opencv by staticly and install it to /usr/local/opencv_aarch64 (for cross compile),but when I build my project,It report:
[100%] Linking CXX executable snpe-sample2
/usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: snpe-sample2: hidden symbol `_ZN2cv3MatC1Eiii' in /home/white/build_aarch64/install/lib/libopencv_core.a(matrix.cpp.o) is referenced by DSO
/usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: 最后的链结失败: 错误的值
collect2: error: ld returned 1 exit status
CMakeFiles/snpe-sample2.dir/build.make:132: recipe for target 'snpe-sample2' failed
make[2]: *** [snpe-sample2] Error 1
CMakeFiles/Makefile2:104: recipe for target 'CMakeFiles/snpe-sample2.dir/all' failed
make[1]: *** [CMakeFiles/snpe-sample2.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2

all thing is right ,but Linking is wrong,
and when I re-build opencv to SHARED LIBS,it's nothing happend,however,I don't want to use SHARED LIBS
how should I do?


"
opencv/opencv,2022-04-04 10:34:20,question,How to contriube opencv if the algorithm needs gram-shmidt process?,"hey, I want to contribute an algorithm to the library. 
 
I just read that I can't use extra dependencies. 

Do I have to write all the math operations from scratch (like gram-Shmidt and so on) for the algorithm? 

thanks.
"
opencv/opencv,2022-04-01 23:52:25,question,videocapture hardware acceleration seem not working,"Hi 

 Am using ubuntu and trying to use the hardware acceleration in my python code of Video Capture. i had compiled a opencv 4.5.4 ensuring cuda is on. 

  cap = cv2.VideoCapture(<rtsp address>, cv2.CAP_FFMPEG,(cv2.CAP_PROP_HW_ACCELERATION, cv2.VIDEO_ACCELERATION_ANY ))

 However, when i do a nvidia-smi dmon command, i noted that the nvdec is not processing anything. it works well for command line e.g. ffmpeg -hwaccel  ....

  May i check is there any working example that i can refer to  in ubuntu environment? Thank you

 


  "
opencv/opencv,2022-03-31 02:27:06,question, error: cannot declare variable ‘dflow’ to be of abstract type ‘cv::cuda::FarnebackOpticalFlow’      cv::cuda::FarnebackOpticalFlow dflow;,"opencv version3.4.5
```
    // Create the optical flow object
    cv::cuda::FarnebackOpticalFlow dflow;

    dflow.numLevels = numLevels;
    dflow.pyrScale = pyrScale;
    dflow.fastPyramids = fastPyramids;
    dflow.winSize = winSize;
    dflow.numIters = numIters;
    dflow.polyN = polyN;
    dflow.polySigma = polySigma;
```
error: cannot declare variable ‘dflow’ to be of abstract type ‘cv::cuda::FarnebackOpticalFlow’      cv::cuda::FarnebackOpticalFlow dflow;"
opencv/opencv,2022-03-24 18:29:40,question,Building opencv.js,"From:
`git clone https://github.com/opencv/opencv.git`

When I build with:
` python3 ./opencv/platforms/js/build_js.py ./build_wasm --build_wasm`

It generates opencv.js but I must replace in the latest lines:
```
  return cv.ready
}
```
by:
```
  return cv
}
```
to use it. What is wrong ?"
opencv/opencv,2022-03-17 18:11:44,question,cv::dnn::DNN_BACKEND_CUDA is not supporting error.,"- OpenCV => 4.5.5
- Operating System / Platform => Windows 64 Bit
- Compiler =>Visual Studio 2019

##### Detailed description
I have built OpenCV with CUDA enabled successfully, and works well, but when I am using CUDA with DNN model, it throws the following error:

> the error: OpenCV(4.5.5-dev) D:\\opencv\\opencv\\modules\\dnn\\src\\dnn.cpp:3266: error: (-213:The function/feature is not implemented) Layer ""detection_out"" of type """" unsupported on OpenCV backend in function 'cv::dnn::dnn4_v20211220::Net::Impl::forwardLayer'

Code:

```
cv::dnn::Net net = cv::dnn::Net::readFromModelOptimizer(xml, bin);
// OR
//cv::dnn::Net net = cv::dnn::readNet(bin, xml);

// This is working
//net.setPreferableBackend(cv::dnn::DNN_BACKEND_INFERENCE_ENGINE);
//net.setPreferableTarget(cv::dnn::DNN_TARGET_OPENCL);
// This is not working
net.setPreferableBackend(cv::dnn::DNN_BACKEND_CUDA);
net.setPreferableTarget(cv::dnn::DNN_TARGET_CUDA);
```
"
opencv/opencv,2022-03-17 06:55:53,question,build opencv for ios in mac,"Environment:
- OpenCV => 4.x
- Operating System / Platform => mac monterey(12.3)
- Compiler => xcode 13.3
- python 3.8.9


Fllowing the document, when execute command ""python opencv/platform/ios/build_framework.py build_ios"", error occur!!


Error log:

CompileC /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/Objects-normal/armv7/tokenizer.o /Users/xuhx3/lenovo/opencv_env/opencv/3rdparty/protobuf/src/google/protobuf/io/tokenizer.cc normal armv7 c++ com.apple.compilers.llvm.clang.1_0.compiler (in target 'libprotobuf' from project 'OpenCV')

    cd /Users/xuhx3/lenovo/opencv_env/opencv

    export LANG\\=en_US.US-ASCII

    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang -x c++ -target armv7-apple-ios8.0 -fmessage-length\\=307 -fdiagnostics-show-note-include-stack -fmacro-backtrace-limit\\=0 -fcolor-diagnostics -Wno-trigraphs -fpascal-strings -O3 -Wno-missing-field-initializers -Wno-missing-prototypes -Wno-return-type -Wno-non-virtual-dtor -Wno-overloaded-virtual -Wno-exit-time-destructors -Wno-missing-braces -Wparentheses -Wswitch -Wno-unused-function -Wno-unused-label -Wno-unused-parameter -Wno-unused-variable -Wunused-value -Wno-empty-body -Wno-uninitialized -Wno-unknown-pragmas -Wno-shadow -Wno-four-char-constants -Wno-conversion -Wno-constant-conversion -Wno-int-conversion -Wno-bool-conversion -Wno-enum-conversion -Wno-float-conversion -Wno-non-literal-null-conversion -Wno-objc-literal-conversion -Wno-shorten-64-to-32 -Wno-newline-eof -Wno-c++11-extensions -DCMAKE_INTDIR\\=\\""Release-iphoneos\\"" -DHAVE_PTHREAD\\=1 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS15.4.sdk -fstrict-aliasing -Wdeprecated-declarations -Winvalid-offsetof -Wno-sign-conversion -Wno-infinite-recursion -Wno-move -Wno-comma -Wno-block-capture-autoreleasing -Wno-strict-prototypes -Wno-range-loop-analysis -Wno-semicolon-before-method-body -fembed-bitcode -I/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/lib/Release/include -isystem /Users/xuhx3/lenovo/opencv_env/opencv/3rdparty/protobuf/src -isystem /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos -I/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/DerivedSources-normal/armv7 -I/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/DerivedSources/armv7 -I/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/DerivedSources -F/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/lib/Release -fembed-bitcode -fsigned-char -W -Wall -Wreturn-type -Wnon-virtual-dtor -Waddress -Wsequence-point -Wformat -Wformat-security -Wstrict-prototypes -Winit-self -Wpointer-arith -Wuninitialized -Wno-delete-non-virtual-dtor -Wno-unnamed-type-template-args -Wno-comment -fdiagnostics-show-option -Qunused-arguments -Wno-semicolon-before-method-body -fvisibility\\=hidden -fvisibility-inlines-hidden -Wno-deprecated -Wno-missing-prototypes -Wno-missing-declarations -Wno-shadow -Wno-unused-parameter -Wno-unused-local-typedefs -Wno-sign-compare -Wno-sign-promo -Wno-undef -Wno-tautological-undefined-compare -Wno-ignored-qualifiers -Wno-extra -Wno-unused-function -Wno-unused-const-variable -Wno-shorten-64-to-32 -Wno-invalid-offsetof -Wno-enum-compare-switch -Wno-suggest-override -Wno-inconsistent-missing-override -Wno-implicit-fallthrough -Wno-array-bounds -DNDEBUG -DNDEBUG -fPIC -std\\=c++11 -MMD -MT dependencies -MF /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/Objects-normal/armv7/tokenizer.d --serialize-diagnostics /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/Objects-normal/armv7/tokenizer.dia -c /Users/xuhx3/lenovo/opencv_env/opencv/3rdparty/protobuf/src/google/protobuf/io/tokenizer.cc -o /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/Objects-normal/armv7/tokenizer.o

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/3rdparty/protobuf/OpenCV.build/Release-iphoneos/libprotobuf.build/Objects-normal/armv7/tokenizer.dia:1:1: warning: Could not read serialized diagnostics file: error(""Failed to open diagnostics file"") (in target 'libprotobuf' from project 'OpenCV')



/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'ade' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'libjpeg-turbo' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'libpng' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'opencv_objc' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'gen_opencv_objc_source' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'ALL_BUILD' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'opencv_world' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'zlib' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'quirc' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'gen_opencv_objc_source_ios' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'ZERO_CHECK' from project 'OpenCV')

/Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/OpenCV.xcodeproj: warning: The iOS deployment target 'IPHONEOS_DEPLOYMENT_TARGET' is set to 8.0, but the range of supported deployment target versions is 9.0 to 15.4.99. (in target 'libprotobuf' from project 'OpenCV')

** BUILD FAILED **





The following build commands failed:

    PhaseScriptExecution Generate\\ CMakeFiles/dephelper/gen_opencv_objc_source_ios /Users/xuhx3/lenovo/opencv_env/build_ios/build/build-armv7-iphoneos/modules/objc_bindings_generator/OpenCV.build/Release-iphoneos/gen_opencv_objc_source_ios.build/Script-624AC68AC6F221E65CFC8166.sh (in target 'gen_opencv_objc_source_ios' from project 'OpenCV')

(1 failure)

============================================================

ERROR: Command '['xcodebuild', 'BITCODE_GENERATION_MODE=bitcode', 'IPHONEOS_DEPLOYMENT_TARGET=8.0', 'ARCHS=armv7', '-sdk', 'iphoneos', '-configuration', 'Release', '-parallelizeTargets', '-jobs', '8', '-target', 'ALL_BUILD', 'build']' returned non-zero exit status 65.

============================================================

Traceback (most recent call last):

  File ""opencv/platforms/ios/build_framework.py"", line 181, in build

    self._build(outdir)

  File ""opencv/platforms/ios/build_framework.py"", line 139, in _build

    self.buildOne(target[0], target[1], main_build_dir, cmake_flags)

  File ""opencv/platforms/ios/build_framework.py"", line 318, in buildOne

    execute(buildcmd + [""-target"", ""ALL_BUILD"", ""build""], cwd = builddir)

  File ""/Users/xuhx3/lenovo/opencv_env/opencv/platforms/apple/cv_build_utils.py"", line 13, in execute

    retcode = check_call(cmd, cwd = cwd)

  File ""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/subprocess.py"", line 364, in check_call

    raise CalledProcessError(retcode, cmd)

subprocess.CalledProcessError: Command '['xcodebuild', 'BITCODE_GENERATION_MODE=bitcode', 'IPHONEOS_DEPLOYMENT_TARGET=8.0', 'ARCHS=armv7', '-sdk', 'iphoneos', '-configuration', 'Release', '-parallelizeTargets', '-jobs', '8', '-target', 'ALL_BUILD', 'build']' returned non-zero exit status 65.
"
opencv/opencv,2022-03-15 14:12:12,question,setMouseCallback gives Null Pointer error in opencv 4.5.3 and later versions,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.5.5
- Operating System / Platform => Windows 64 Bit
- Compiler => Installed using opencv-contrib-python version=4.5.5.64
-->

- OpenCV => 4.5.5
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio  --  Installed using opencv-contrib-python version=4.5.5.64

##### Detailed description

Using setMouseCallback gives following error in opencv-contrib-python=4.5.5.64:
`cv2.error: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:2563: error: (-27:Null pointer) NULL window: 'frame' in function 'cvSetMouseCallback'`

I tested the following versions:
4.5.1.48  - good
4.5.2.54  - good
4.5.3.56  - error
4.5.4.58  - error
4.5.5.54  - error

Error seems to have appeared in opencv version 4.5.3.

Build information for last known good version and current version included below:

4.5.2.54

<details>

```
General configuration for OpenCV 4.5.2 =====================================
  Version control:               4.5.2
  Extra modules:
    Location (extra):            C:/Users/runneradmin/AppData/Local/Temp/pip-req-build-1hfhc_rd/opencv_contrib/modules
    Version control (extra):     4.5.2
  Platform:
    Timestamp:                   2021-06-07T08:54:10Z
    Host:                        Windows 10.0.17763 AMD64
    CMake:                       3.20.2
    CMake generator:             Visual Studio 14 2015 Win64
    CMake build tool:            MSBuild.exe
    MSVC:                        1900
    Configuration:               Debug Release
  CPU/HW features:
    Baseline:                    SSE SSE2 SSE3
      requested:                 SSE3
    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2
      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
      SSE4_1 (15 files):         + SSSE3 SSE4_1
      SSE4_2 (1 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
      FP16 (0 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
      AVX (4 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
      AVX2 (29 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
  C/C++:
    Built as dynamic libs?:      NO
    C++ standard:                11
    C++ Compiler:                C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe  (ver 19.0.24245.0)
    C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP  /MT /O2 /Ob2 /DNDEBUG 
    C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP  /MTd /Zi /Ob0 /Od /RTC1 
    C Compiler:                  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
    C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MT /O2 /Ob2 /DNDEBUG 
    C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MTd /Zi /Ob0 /Od /RTC1 
    Linker flags (Release):      /machine:x64  /NODEFAULTLIB:atlthunk.lib /INCREMENTAL:NO  /NODEFAULTLIB:libcmtd.lib /NODEFAULTLIB:libcpmtd.lib /NODEFAULTLIB:msvcrtd.lib
    Linker flags (Debug):        /machine:x64  /NODEFAULTLIB:atlthunk.lib /debug /INCREMENTAL  /NODEFAULTLIB:libcmt.lib /NODEFAULTLIB:libcpmt.lib /NODEFAULTLIB:msvcrt.lib
    ccache:                      NO
    Precompiled headers:         YES
    Extra dependencies:          wsock32 comctl32 gdi32 ole32 setupapi ws2_32
    3rdparty dependencies:       ittnotify libprotobuf ade libjpeg-turbo libwebp libpng libtiff libopenjp2 IlmImf zlib quirc ippiw ippicv
  OpenCV modules:
    To be built:                 aruco bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
    Disabled:                    freetype world
    Disabled by dependency:      -
    Unavailable:                 alphamat cnn_3dobj cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv hdf java julia matlab ovis python2 sfm ts viz
    Applications:                -
    Documentation:               NO
    Non-free algorithms:         NO
  Windows RT support:            NO
  GUI: 
    Win32 UI:                    YES
    VTK support:                 NO
  Media I/O: 
    ZLib:                        build (ver 1.2.11)
    JPEG:                        build-libjpeg-turbo (ver 2.0.6-62)
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         build (ver 1.6.37)
    TIFF:                        build (ver 42 - 4.2.0)
    JPEG 2000:                   build (ver 2.4.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES
  Video I/O:
    DC1394:                      NO
    FFMPEG:                      YES (prebuilt binaries)
      avcodec:                   YES (58.91.100)
      avformat:                  YES (58.45.100)
      avutil:                    YES (56.51.100)
      swscale:                   YES (5.7.100)
      avresample:                YES (4.0.0)
    GStreamer:                   NO
    DirectShow:                  YES
    Media Foundation:            YES
      DXVA:                      YES
  Parallel framework:            Concurrency
  Trace:                         YES (with Intel ITT)
  Other third-party libraries:
    Intel IPP:                   2020.0.0 Gold [2020.0.0]
           at:                   C:/Users/runneradmin/AppData/Local/Temp/pip-req-build-1hfhc_rd/_skbuild/win-amd64-3.9/cmake-build/3rdparty/ippicv/ippicv_win/icv
    Intel IPP IW:                sources (2020.0.0)
              at:                C:/Users/runneradmin/AppData/Local/Temp/pip-req-build-1hfhc_rd/_skbuild/win-amd64-3.9/cmake-build/3rdparty/ippicv/ippicv_win/iw
    Lapack:                      NO
    Eigen:                       NO
    Custom HAL:                  NO
    Protobuf:                    build (3.5.1)
  OpenCL:                        YES (NVD3D11)
    Include path:                C:/Users/runneradmin/AppData/Local/Temp/pip-req-build-1hfhc_rd/opencv/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load
  Python 3:
    Interpreter:                 C:/hostedtoolcache/windows/Python/3.9.5/x64/python.exe (ver 3.9.5)
    Libraries:                   C:/hostedtoolcache/windows/Python/3.9.5/x64/libs/python39.lib (ver 3.9.5)
    numpy:                       C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-xfmlpevn/overlay/Lib/site-packages/numpy/core/include (ver 1.19.3)
    install path:                python
  Python (for build):            C:/hostedtoolcache/windows/Python/2.7.18/x64/python.exe
  Java:                          
    ant:                         NO
    JNI:                         C:/hostedtoolcache/windows/Java_Adopt_jdk/8.0.292-10/x64/include C:/hostedtoolcache/windows/Java_Adopt_jdk/8.0.292-10/x64/include/win32 C:/hostedtoolcache/windows/Java_Adopt_jdk/8.0.292-10/x64/include
    Java wrappers:               NO
    Java tests:                  NO
  Install to:                    C:/Users/runneradmin/AppData/Local/Temp/pip-req-build-1hfhc_rd/_skbuild/win-amd64-3.9/cmake-install
-----------------------------------------------------------------
```

</details>


4.5.5.64

<details>

```
General configuration for OpenCV 4.5.5 =====================================
  Version control:               4.5.5
  Extra modules:
    Location (extra):            D:/a/opencv-python/opencv-python/opencv_contrib/modules
    Version control (extra):     4.5.5
  Platform:
    Timestamp:                   2022-03-04T06:38:45Z
    Host:                        Windows 10.0.17763 AMD64
    CMake:                       3.22.2
    CMake generator:             Visual Studio 14 2015
    CMake build tool:            MSBuild.exe
    MSVC:                        1900
    Configuration:               Debug Release
  CPU/HW features:
    Baseline:                    SSE SSE2 SSE3
      requested:                 SSE3
    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2
      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
      SSE4_1 (16 files):         + SSSE3 SSE4_1
      SSE4_2 (1 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
      FP16 (0 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
      AVX (4 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
      AVX2 (31 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
  C/C++:
    Built as dynamic libs?:      NO
    C++ standard:                11
    C++ Compiler:                C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe  (ver 19.0.24245.0)
    C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP  /MT /O2 /Ob2 /DNDEBUG 
    C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /MP  /MTd /Zi /Ob0 /Od /RTC1 
    C Compiler:                  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
    C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MT /O2 /Ob2 /DNDEBUG 
    C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MTd /Zi /Ob0 /Od /RTC1 
    Linker flags (Release):      /machine:x64  /NODEFAULTLIB:atlthunk.lib /INCREMENTAL:NO  /NODEFAULTLIB:libcmtd.lib /NODEFAULTLIB:libcpmtd.lib /NODEFAULTLIB:msvcrtd.lib
    Linker flags (Debug):        /machine:x64  /NODEFAULTLIB:atlthunk.lib /debug /INCREMENTAL  /NODEFAULTLIB:libcmt.lib /NODEFAULTLIB:libcpmt.lib /NODEFAULTLIB:msvcrt.lib
    ccache:                      NO
    Precompiled headers:         YES
    Extra dependencies:          wsock32 comctl32 gdi32 ole32 setupapi ws2_32
    3rdparty dependencies:       libprotobuf ade ittnotify libjpeg-turbo libwebp libpng libtiff libopenjp2 IlmImf zlib quirc ippiw ippicv
  OpenCV modules:
    To be built:                 aruco barcode bgsegm bioinspired calib3d ccalib core datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
    Disabled:                    world
    Disabled by dependency:      -
    Unavailable:                 alphamat cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev cvv freetype hdf java julia matlab ovis python2 sfm ts viz
    Applications:                -
    Documentation:               NO
    Non-free algorithms:         NO
  Windows RT support:            NO
  GUI:                           WIN32UI
    Win32 UI:                    YES
    VTK support:                 NO
  Media I/O: 
    ZLib:                        build (ver 1.2.11)
    JPEG:                        build-libjpeg-turbo (ver 2.1.2-62)
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         build (ver 1.6.37)
    TIFF:                        build (ver 42 - 4.2.0)
    JPEG 2000:                   build (ver 2.4.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES
  Video I/O:
    DC1394:                      NO
    FFMPEG:                      YES (prebuilt binaries)
      avcodec:                   YES (58.134.100)
      avformat:                  YES (58.76.100)
      avutil:                    YES (56.70.100)
      swscale:                   YES (5.9.100)
      avresample:                YES (4.0.0)
    GStreamer:                   NO
    DirectShow:                  YES
    Media Foundation:            YES
      DXVA:                      YES
  Parallel framework:            Concurrency
  Trace:                         YES (with Intel ITT)
  Other third-party libraries:
    Intel IPP:                   2020.0.0 Gold [2020.0.0]
           at:                   D:/a/opencv-python/opencv-python/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/icv
    Intel IPP IW:                sources (2020.0.0)
              at:                D:/a/opencv-python/opencv-python/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/iw
    Lapack:                      NO
    Eigen:                       NO
    Custom HAL:                  NO
    Protobuf:                    build (3.19.1)
  OpenCL:                        YES (NVD3D11)
    Include path:                D:/a/opencv-python/opencv-python/opencv/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load
  Python 3:
    Interpreter:                 C:/hostedtoolcache/windows/Python/3.6.8/x64/python.exe (ver 3.6.8)
    Libraries:                   C:/hostedtoolcache/windows/Python/3.6.8/x64/libs/python36.lib (ver 3.6.8)
    numpy:                       C:/hostedtoolcache/windows/Python/3.6.8/x64/lib/site-packages/numpy/core/include (ver 1.13.3)
    install path:                python/cv2/python-3
  Python (for build):            C:/hostedtoolcache/windows/Python/2.7.18/x64/python.exe
  Java:                          
    ant:                         NO
    JNI:                         C:/hostedtoolcache/windows/Java_Temurin-Hotspot_jdk/8.0.322-6/x64/include C:/hostedtoolcache/windows/Java_Temurin-Hotspot_jdk/8.0.322-6/x64/include/win32 C:/hostedtoolcache/windows/Java_Temurin-Hotspot_jdk/8.0.322-6/x64/include
    Java wrappers:               NO
    Java tests:                  NO
  Install to:                    D:/a/opencv-python/opencv-python/_skbuild/win-amd64-3.6/cmake-install
-----------------------------------------------------------------
```

</details>

##### Steps to reproduce
Python version=3.9
Test code:

```
import cv2


def mouse_callback(event, x, y, flags, param):
    if event == cv2.EVENT_LBUTTONDBLCLK:
        print(x, y)


cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print(""Couldn't open camera"")

while True:
    ret, frame = cap.read()
    cv2.setMouseCallback('frame', mouse_callback)
    cv2.imshow('frame', frame)
    if cv2.waitKey(10) & 0xFF == ord(""q""):
        break

```

##### Issue submission checklist

 - [✔️  ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ✔️ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [✔️  ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ✔️ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-03-12 13:14:20,question,"Building OpenCV on Windows with VS, OpenGL, GStreamer: Missing libraries in project files","##### System information (version)
- OpenCV => 4.5.5
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2019

##### Detailed description

modul opencv_videoio: there is to add gstaudio-1.0.lib (GStreamer audio) to the linked libraries, linker missing symbols from this

modul opencv_highgui: there is to add opengl32.lib (OpenGL on Windows) to the linked libraries, linker missing symbols from this

##### Steps to reproduce

Create OpenCV 4.5.5 Visual Project Files per CMAKE with OpenGL and GStreamer support, build library
"
opencv/opencv,2022-03-07 09:59:40,question,CAP_PROP_POS_MSEC to frame_number,"Hi !
I'm given some timestamps for a video.
I want to do in place changes to a frame on that particular timestamp of a video.
How can I achieve that?

For reading the frame at a given timestamp :
```
V = cv2.VideoCapture(videoFile)
V.set(cv2.CAP_PROP_POS_MSEC, row['frame_timestamp'] * 1e3)
_, frame = V.read()
```

How to write at a particular timestamp?"
opencv/opencv,2022-03-03 21:24:54,question,OpenCV 4.5.5 not building with OpenVINO-Toolkit 2021.4.2,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.5.5-dev
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2019
-->

- OpenCV => 4.5.5-dev
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2019

##### Detailed description
I am trying to build OpenCV with Inference-Engine support, but what I experienced is:

- Here in CMake GUI, I cannot find the **WITH_INF_ENGINE** flag as before there was this flag, but in the latest source which I downloaded there is no this flag. But I found another flag which is: **WITH_OPENVINO**, I don't know if this is a replacement for the **WITH_INF_ENGINE** flag, anyway.
- When I use the flag **WITH_OPENVINO**, then it shows: `OpenVINO_DIR-NOTFOUND `as well as` InferenceEngine_DIR-NOTFOUND`, but I have tried to direct this path to the following paths, but no one was working:

1. C:\\Program Files (x86)\\Intel\\openvino_2021\\inference_engine
2. C:\\Program Files (x86)\\Intel\\openvino_2021
3. C:\\Program Files (x86)\\Intel\\openvino_2021\\deployment_tools\\inference_engine

So now what should I do to solve this problem?

**NOTE:** I have installed OpenVINO-Toolkit as it was instructed on the official page, and also I am not new with OpenCV and OpenVINO before it was built successfully, but in new releases, it does not."
opencv/opencv,2022-03-02 14:11:27,question,cv::VideoCapture  set cv::CAP_PROP_FPS is not working,"##### System information (version)
- OpenCV => 4.5.4
- Operating System / Platform => Ubuntu20.04
- Compiler => Qt Creator 4.11.0 (GCC 9.3.0, 64 bit)

##### Detailed description

I am using OpenCV library to capture the images from web cameras, and I am able to do that with different capture format (cv::CAP_PROP_FOURCC).
But I am facing an issue with setting up frame per second value. Here is the sample code I tried:
```
#include ""opencv2/videoio.hpp""

cv::VideoCapture capture;
capture.open(""/dev/video0"");

bool success = capture.isOpened();
if (success) {
capture.set(cv::CAP_PROP_FPS, 25);
}
```
but after setting up the fps value 25, the camera still shows the fps value as 30.
I did some googling before creating this task, but didn't find any useful information. So I wanted to know anything I am missing or is there any other methods to setup the fps.

Thanks in advance.


##### Steps to reproduce

Please try to use some sample code to setup the fps for web cameras, like:
```
#include ""opencv2/videoio.hpp""

cv::VideoCapture capture;
capture.open(""/dev/video0"");

bool success = capture.isOpened();
if (success) {
capture.set(cv::CAP_PROP_FPS, 25);
}
```

##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [x] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->

"
opencv/opencv,2022-02-22 07:03:40,question,Does support torch.matmul?,"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => :grey_question:
- Operating System / Platform => :grey_question:
- Compiler => :grey_question:

##### Detailed description

<!-- your description -->

##### Steps to reproduce

<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [ ] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [ ] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [ ] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-02-15 15:39:00,question,Issue using functions from opencv shared libraries cross compiled for android,"Hi Team,
I was trying to build and execute a cpp code that links few opencv shared libraries that I had cross compiled for android but while trying to utilize the namespace cv or trying to use a function of opencv, it says undeclared identifier. I did try this documentation here: https://developer.android.com/ndk/guides/prebuilts but was unable to get it working. Also I referred to another stackoverflow question for reference here: [OpenCV with Android NDK Undefined References](https://stackoverflow.com/questions/14649710/opencv-with-android-ndk-undefined-references) as well. Any guidance on how to link them and import opencv functions properly which I am probably missing out here would be really helpful.

**trial_onnx.cpp file**
```
#include <iostream>
#include <fstream>
#include <cstring>
#include <opencv2/ml/ml.hpp>
#include <opencv2/dnn/dnn.hpp>
#include <opencv2/imgcodecs/imgcodecs.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/core/core.hpp>
#include <opencv2/core/mat.hpp>

    
#include ""trial_onnx.h""

using namespace std;


void execute_main() {
    std::cout << ""Hello World""<<std::endl;

    cv::Mat mat1;
    
}
```

**trial_onnx.h file**
```

#ifdef __cplusplus
extern ""C"" {
#endif // __cplusplus

void execute_main();
#ifdef __cplusplus
}
#endif // __cplusplus
```

**Android.mk file**

```
LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE := opencv_ml
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_ml.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)

LOCAL_MODULE := opencv_dnn
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_dnn.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)
#LOCAL_PATH := $(call my-dir)


LOCAL_MODULE := opencv_imgcodecs
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_imgcodecs.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)

#LOCAL_PATH := $(call my-dir)

LOCAL_MODULE := opencv_imgproc
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_imgproc.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)

#LOCAL_PATH := $(call my-dir)

LOCAL_MODULE := opencv_core
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_core.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)

LOCAL_MODULE := opencv_highgui
LOCAL_SRC_FILES := /home/ubuntu/trial/opencv-4.5.5-android-sdk/OpenCV-android-sdk/sdk/native/libs/arm64-v8a/libopencv_highgui.so
include $(PREBUILT_SHARED_LIBRARY)
include $(CLEAR_VARS)

LOCAL_SHARED_LIBRARIES = opencv_ml opencv_dnn opencv_imgcodecs opencv_highgui opencv_imgproc opencv_core opencv


LOCAL_ARM_MODE := arm
    

LOCAL_MODULE   := libtrial


LOCAL_SRC_FILES := inc/trial_onnx.h src/trial_onnx.cpp 

LOCAL_C_INCLUDES := ${LOCAL_PATH}/inc

LOCAL_LDLIBS += -llog -ldl
```
**Output**

```
[arm64-v8a] Install        : libopencv_core.so => libs/arm64-v8a/libopencv_core.so
[arm64-v8a] Install        : libopencv_dnn.so => libs/arm64-v8a/libopencv_dnn.so
[arm64-v8a] Install        : libopencv_highgui.so => libs/arm64-v8a/libopencv_highgui.so
[arm64-v8a] Install        : libopencv_imgcodecs.so => libs/arm64-v8a/libopencv_imgcodecs.so
[arm64-v8a] Install        : libopencv_imgproc.so => libs/arm64-v8a/libopencv_imgproc.so
[arm64-v8a] Install        : libopencv_ml.so => libs/arm64-v8a/libopencv_ml.so
[arm64-v8a] Compile++      : trial <= trial_onnx.cpp
[arm64-v8a] SharedLibrary  : lib_trial.so
./obj/local/arm64-v8a/objs/trial/src/trial_onnx.o: In function `execute_main':
/home/ubuntu//trial/./src/trial_onnx.cpp:19: undefined reference to `cv::Mat::Mat()'
/home/ubuntu/trial/./src/trial_onnx.cpp:21: undefined reference to `cv::Mat::~Mat()'
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [obj/local/arm64-v8a/lib_trial.so] Error 1
```
If I skip using cv:: in the cpp file the following outputs pops up, this does make sense but its able to identify the cv::Mat type is present in core/mat.hpp then why not identify cv::mat in other case baffles me:
```
    [arm64-v8a] Install        : libopencv_core.so => libs/arm64-v8a/libopencv_core.so
    [arm64-v8a] Install        : libopencv_dnn.so => libs/arm64-v8a/libopencv_dnn.so
    [arm64-v8a] Install        : libopencv_highgui.so => libs/arm64-v8a/libopencv_highgui.so
    [arm64-v8a] Install        : libopencv_imgcodecs.so => libs/arm64-v8a/libopencv_imgcodecs.so
    [arm64-v8a] Install        : libopencv_imgproc.so => libs/arm64-v8a/libopencv_imgproc.so
    [arm64-v8a] Install        : libopencv_ml.so => libs/arm64-v8a/libopencv_ml.so
    [arm64-v8a] Compile++      : trial <= trial_onnx.cpp
    [arm64-v8a] SharedLibrary  : lib_trial.so
./src/trial_onnx.cpp:19:5: error: unknown type name 'Mat'; did you mean
      'cv::Mat'?
    Mat mat1;
    ^~~
    cv::Mat

./opencv2/core/mat.hpp:801:18: note: 'cv::Mat' declared here
class CV_EXPORTS Mat
                 ^
1 error generated.
```"
opencv/opencv,2022-02-09 04:54:27,question,Issue linking OpenCV's Shared library with Android.mk,"Hi OpenCV Team
I have cross compiled opencv for android and was trying to link the shared libraries generated with a simple script, but somehow the linking seems to have an issue. I tried linking static library as well (commenting the shared library part) but again facing the same issue. Any help will be appreciated. Thank you. 

Android.mk file contents

```
LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE   := libtrial
LOCAL_ARM_MODE := arm

LOCAL_SRC_FILES := inc/trial_onnx.h
LOCAL_SRC_FILES := src/trial_onnx.cpp

include $(CLEAR_VARS)
LOCAL_MODULE := opencv_core
LOCAL_SRC_FILES := cross_compiled/libopencv_core.so
include $(PREBUILT_SHARED_LIBRARY)

#include $(CLEAR_VARS) 
#LOCAL_MODULE := opencv_core
#LOCAL_SRC_FILES := libopencv_core.a
#include $(PREBUILT_STATIC_LIBRARY)

LOCAL_C_INCLUDES := ${LOCAL_PATH}/inc
LOCAL_LDLIBS += -landroid -llog -ldl
include $(BUILD_SHARED_LIBRARY)
```
trial_onnx.cpp contents

```
#include <iostream>

#include ""wasabeef_onnx.h""
#include ""stdlib.h""
using namespace std;

void execute_main() {
    std::cout << ""Hello World!"";

    const std::string filename1 = ""Input.raw"";

    float *input1 = new float[320*320*3];
       
    
    cv::Mat mat1;
}
```
trial_onnx.h contents

```
#ifdef __cplusplus
extern ""C"" {
#endif // __cplusplus

void execute_main();
#ifdef __cplusplus
}
#endif // __cplusplus
```
Upon building, its getting terminated with the log as below- already defined: 
![image](https://user-images.githubusercontent.com/30999857/153124335-cc5a452a-6ec1-4d10-bb82-57a94f1a38c3.png)

"
opencv/opencv,2022-02-01 20:08:21,question,OpenCV CUDA gives low FPS,"- OpenCV => 4.5.2
- Operating System / Platform => Ubuntu 20.04 
- Compiler => cmake
-GPU => NVIDIA Corporation / GeForce RTX 3050 Ti Laptop GPU/PCIe/SSE2
-CPU => AMD® Ryzen 5 5600h with radeon graphics × 12
Hello everyone. Firstly, this topic is not about either OpenCV or CUDA. This problem is about me but I did not find answer anywhere. I built OpenCV 4.5.2 with CUDA 11.2 and CuDNN 8.1.1. I installed built successfully but I can not get enough FPS from this built. For example, when I run my code with CPU it gives me 15FPS when I use CUDA backend it gives me 12 FPS. Where did I miss? What do I have to do?

I am using very basic yolov3 object detection algorithm. Here are my files. It is about traffic signs. You can test it with stop sign etc. (Not all signs are included)
https://www.mediafire.com/file/uqj6mw5tbauagq2/yolov3_object_detection.zip/file
Here are my files. 

Here are my opencv getBuildInformation output
`General configuration for OpenCV 4.5.2 =====================================
  Version control:               unknown

  Extra modules:
    Location (extra):            /home/rota/Downloads/opencv_contrib-4.5.2/modules
    Version control (extra):     unknown

  Platform:
    Timestamp:                   2022-01-31T20:33:31Z
    Host:                        Linux 5.13.0-27-generic x86_64
    CMake:                       3.16.3
    CMake generator:             Unix Makefiles
    CMake build tool:            /usr/bin/make
    Configuration:               RELEASE

  CPU/HW features:
    Baseline:                    SSE SSE2 SSE3
      requested:                 SSE3
    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX
      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
      SSE4_1 (17 files):         + SSSE3 SSE4_1
      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
      AVX2 (31 files):           + SSSE3 Sis:pr is:open SE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
      AVX512_SKX (7 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX

  C/C++:
    Built as dynamic libs?:      YES
    C++ standard:                11
    C++ Compiler:                /usr/bin/c++  (ver 9.3.0)
    C++ flags (Release):         -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG
    C++ flags (Debug):           -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG
    C Compiler:                  /usr/bin/cc
    C flags (Release):           -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG
    C flags (Debug):             -fsigned-char -ffast-math -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG
    Linker flags (Release):      -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a   -Wl,--gc-sections -Wl,--as-needed  
    Linker flags (Debug):        -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a   -Wl,--gc-sections -Wl,--as-needed  
    ccache:                      NO
    Precompiled headers:         NO
    Extra dependencies:          m pthread cudart_static dl rt nppc nppial nppicc nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cudnn cufft -L/usr/local/cuda-11.2/lib64 -L/usr/lib/x86_64-linux-gnu
    3rdparty dependencies:

  OpenCV modules:
    To be built:                 aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann freetype fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto
    Disabled:                    cudacodec world
    Disabled by dependency:      -
    Unavailable:                 alphamat cnn_3dobj cvv hdf java julia matlab ovis python2 sfm viz
    Applications:                tests perf_tests apps
    Documentation:               NO
    Non-free algorithms:         YES

  GUI: 
    GTK+:                        YES (ver 3.24.20)
      GThread :                  YES (ver 2.64.6)
      GtkGlExt:                  NO
    OpenGL support:              NO
    VTK support:                 NO

  Media I/O: 
    ZLib:                        /usr/lib/x86_64-linux-gnu/libz.so (ver 1.2.11)
    JPEG:                        /usr/lib/x86_64-linux-gnu/libjpeg.so (ver 80)
    WEBP:                        build (ver encoder: 0x020f)
    PNG:                         /usr/lib/x86_64-linux-gnu/libpng.so (ver 1.6.37)
    TIFF:                        /usr/lib/x86_64-linux-gnu/libtiff.so (ver 42 / 4.1.0)
    JPEG 2000:                   build (ver 2.4.0)
    OpenEXR:                     build (ver 2.3.0)
    HDR:                         YES
    SUNRASTER:                   YES
    PXM:                         YES
    PFM:                         YES

  Video I/O:
    DC1394:                      NO
    FFMPEG:                      NO
      avcodec:                   NO
      avformat:                  NO
      avutil:                    NO
      swscale:                   NO
      avresample:                NO
    GStreamer:                   YES (1.16.2)
    v4l/v4l2:                    YES (linux/videodev2.h)

  Parallel framework:            pthreads

  Trace:                         YES (with Intel ITT)

  Other third-party libraries:
    Intel IPP:                   2020.0.0 Gold [2020.0.0]
           at:                   /home/rota/Downloads/opencv-4.5.2/build/3rdparty/ippicv/ippicv_lnx/icv
    Intel IPP IW:                sources (2020.0.0)
              at:                /home/rota/Downloads/opencv-4.5.2/build/3rdparty/ippicv/ippicv_lnx/iw
    VA:                          NO
    Lapack:                      NO
    Eigen:                       NO
    Custom HAL:                  NO
    Protobuf:                    build (3.5.1)

  NVIDIA CUDA:                   YES (ver 11.2, CUFFT CUBLAS FAST_MATH)
    NVIDIA GPU arch:             86
    NVIDIA PTX archs:

  cuDNN:                         YES (ver 8.1.1)

  OpenCL:                        YES (no extra features)
    Include path:                /home/rota/Downloads/opencv-4.5.2/3rdparty/include/opencl/1.2
    Link libraries:              Dynamic load

  Python 3:
    Interpreter:                 /usr/bin/python3 (ver 3.8.10)
    Libraries:                   /usr/lib/x86_64-linux-gnu/libpython3.8.so (ver 3.8.10)
    numpy:                       /usr/local/lib/python3.8/dist-packages/numpy/core/include (ver 1.22.1)
    install path:                /usr/lib/python3/dist-packages/cv2/python-3.8

  Python (for build):            /usr/bin/python2.7

  Java:                          
    ant:                         NO
    JNI:                         NO
    Java wrappers:               NO
    Java tests:                  NO

  Install to:                    /usr/local
-----------------------------------------------------------------`"
opencv/opencv,2022-01-26 03:41:00,question,cv2.error: OpenCV(4.5.4-pre) /home/pi/opencv/modules/core/src/persistence.cpp:682: error: (-5:Bad argument) Input file is invalid in function 'open',"I want to use opencv(cv2) to read the .tflite file but it cannot open.
Is the cv2 can only read .yml file?
How can I read the .tflite file succesfully?
or can I change the tflite to yml?
I can't find the way.
![image](https://user-images.githubusercontent.com/75157697/151100271-f747ade1-8ca5-4066-a7b1-a5740bb0a79f.png)
"
opencv/opencv,2022-01-24 03:26:21,question,include gapi always equires openvino dll?,"##### System information (version)
- OpenCV =4.5.4
- Operating System / Platform => Windows 64 Bit
- OpenCV package from  => https://github.com/opencv/opencv/releases/download/4.5.4/opencv-4.5.4-openvino-dldt-2021.4.1-vc16-avx2.7z


##### Detailed description

I apply OpenCV GAPI with the following simple image processing. 
```
cv::gapi::convertTo();
cv::gapi::resize();
cv::gapi::blur();
cv::gapi::mul;
```

After compiling the c++ application, it requires the following dll to run:
```
inference_engine.dll
ngraph.dll
opencv_dnn454.dll
opencv_features2d454.dll
opencv_flann454.dll
inference_engine_transformations.dll
```

Are these dlls really used in such a simple GAPI graph?
"
opencv/opencv,2022-01-14 10:08:24,question,Videoio Capgstreamer introducing 0.5-1.0 s latecy,"#### Videoio Capgstreamer introducing 0.5-1.0 s latecy

##### Versions
- OpenCV => 4.4
- Operating System / Platform => Linux tegra R32.2 (Ubuntu 18.04 )/ NV XavierAgx (ARM64) 
- Compiler => gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
- Module => videoio
- File => capgstreamer.cpp
- Gstreamer version => 1.14.5

##### Detailed description

Gstreamer capture through appsink introduces high latency on frame retrieval. Average latencies of 0.5 s with 1 s peak found with cpu load less than 80%. Latency tested using glass to glass method. Pipeline is:
- Emitter (videosrc ! omxh264enc ! rtph264pay ! udpsink)
- Receiver (udpsrc !  nvv4l2dec  !  appsink)

appsink is read with a dedicated thread to flush the pipeline

When changing appsink for display glass to glass latency falls to <80 ms.

Seems like a buffering problem or bottleneck on gst sample retrieve but i am not sure. Video doesn't seem to be downsampled and latency remains lowering framerate.


"
opencv/opencv,2022-01-14 02:35:01,question,Please ask how to solve this problem,"![-190d273a8698af7b](https://user-images.githubusercontent.com/81922395/149441718-b09ce807-934b-44f4-82fe-374a6bc84365.jpg)
Please ask how to solve this problem"
opencv/opencv,2022-01-13 02:00:48,question,Status -66: CL_INVALID_COMPILER_OPTIONS -cl-no-subgroup-ifp -D AMD_DEVICE,"my computer has two GPU
![image](https://user-images.githubusercontent.com/38007233/149252041-31d8f5ec-4114-437e-8a04-05ad7ecdec02.png)
how can I use my INTEL GPU accelerate my project, AMD GPU is slower
![image](https://user-images.githubusercontent.com/38007233/149252415-401b8b23-586d-457d-b319-cd35d7220897.png)
"
opencv/opencv,2022-01-06 18:03:39,question,CMake configuration fails for building OpenCV 4.5.5 with ONNX,"Using cmake-gui to build OpenCV with ONNX results in configuration report stating ""ONNX: NO"".

##### System information
- OpenCV => 4.5.5-dev
- ONNX => microsoft/onnxruntime v1.10.0
- Operating System  => Ubuntu 20.04 LTS (Linux 5.10.16.3-microsoft-standard-WSL2 x86_64)
- CMake => cmake-3.22.1-linux-x86_64
- CMake build tool & generator=> ninja 1.10.0

##### Steps to reproduce

These are the options in the CMake configuration:

> ONNXRT_ROOT_DIR: /opt/opencv455/onnxruntime
> ORT_INCLUDE: /opt/opencv455/onnxruntime/include/onnxruntime/core/session 
> WITH_ONNX: checked

##### Detailed description

This is my folder structure (opencv and onnxruntime were git cloned in there):

/opt/opencv455/
> build
>> 3rdparty
>> CMakeCache.txt
>> CMakeDownloadLog.txt
>> CMakeFiles
>> CMakeVars.txt
>> CPackConfig.cmake
>> CPackSourceConfig.cmake
>> OpenCVConfig-version.cmake
>> OpenCVConfig.cmake
>> apps
>> cmake_uninstall.cmake
>> configured
>> custom_hal.hpp
>> cv_cpu_config.h
>> cvconfig.h
>> data
>> doc
>> include
>> modules
>> opencv2
>> opencv_data_config.hpp
>> opencv_python_tests.cfg
>> opencv_tests_config.hpp
>> python_loader
>> setup_vars.sh
>> test-reports
>> tmp
>> unix-install
>> version_string.tmp

> onnxruntime
>> CITATION.cff
>> CODEOWNERS
>> CONTRIBUTING.md
>> LICENSE
>> NuGet.config
>> README.md
>> ThirdPartyNotices.txt
>> VERSION_NUMBER
>> build.amd64.1411.bat
>> build.bat
>> build.sh
>> cgmanifests
>> cmake
>> csharp
>> dockerfiles
>> docs
>> include
>> java
>> js
>> objectivec
>> onnxruntime
>> ort.wprp
>> orttraining
>> package
>> packages.config
>> requirements-dev.txt
>> requirements-doc.txt
>> requirements-training.txt
>> requirements.txt.in
>> samples
>> server
>> setup.py
>> tools
>> winml

> opencv
>> 3rdparty
>> CMakeLists.txt
>> CONTRIBUTING.md
>> COPYRIGHT
>> LICENSE
>> README.md
>> SECURITY.md
>> apps
>> cmake
>> data
>> doc
>> include
>> modules
>> platforms
>> samples

When generating, this is the output:

> Detected processor: x86_64
> Could NOT find PythonInterp (missing: PYTHON_EXECUTABLE) (Required is at least version ""2.7"")
> Looking for ccache - not found
> Cleaning INTERNAL cached variable: ZLIB_LIBRARY
> Cleaning INTERNAL cached variable: ZLIB_INCLUDE_DIR
> Could NOT find ZLIB (missing: ZLIB_LIBRARY ZLIB_INCLUDE_DIR) (Required is at least version ""1.2.3"")
> Cleaning INTERNAL cached variable: JPEG_LIBRARY
> Cleaning INTERNAL cached variable: JPEG_INCLUDE_DIR
> Could NOT find JPEG (missing: JPEG_LIBRARY JPEG_INCLUDE_DIR) 
> libjpeg-turbo: VERSION = 2.1.2, BUILD = opencv-4.5.5-dev-libjpeg-turbo
> Cleaning INTERNAL cached variable: TIFF_LIBRARY
> Cleaning INTERNAL cached variable: TIFF_INCLUDE_DIR
> Could NOT find TIFF (missing: TIFF_LIBRARY TIFF_INCLUDE_DIR) 
> Cleaning INTERNAL cached variable: WEBP_LIBRARY
> Cleaning INTERNAL cached variable: WEBP_INCLUDE_DIR
> Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources
> OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.5.5-dev-openjp2-2.4.0
> OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.4.0"")
> Cleaning INTERNAL cached variable: PNG_LIBRARY
> Cleaning INTERNAL cached variable: PNG_INCLUDE_DIR
> Could NOT find PNG (missing: PNG_LIBRARY PNG_PNG_INCLUDE_DIR) 
> libva: missing va.h header (VA_INCLUDE_DIR)
> found Intel IPP (ICV version): 2020.0.0 [2020.0.0 Gold]
> at: /opt/opencv455/build/3rdparty/ippicv/ippicv_lnx/icv
> found Intel IPP Integration Wrappers sources: 2020.0.0
> at: /opt/opencv455/build/3rdparty/ippicv/ippicv_lnx/iw
> Could not find OpenBLAS include. Turning OpenBLAS_FOUND off
> Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off
> Could NOT find Atlas (missing: Atlas_CBLAS_INCLUDE_DIR Atlas_CLAPACK_INCLUDE_DIR Atlas_CBLAS_LIBRARY Atlas_BLAS_LIBRARY Atlas_LAPACK_LIBRARY) 
> Could NOT find BLAS (missing: BLAS_LIBRARIES) 
> Could NOT find LAPACK (missing: LAPACK_LIBRARIES) 
>     Reason given by package: LAPACK could not be found because dependency BLAS could not be found.
> 
> Could NOT find JNI (missing: JAVA_AWT_LIBRARY JAVA_JVM_LIBRARY JAVA_INCLUDE_PATH JAVA_INCLUDE_PATH2 JAVA_AWT_INCLUDE_PATH) 
> VTK is not found. Please set -DVTK_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file
> Allocator metrics storage type: 'long long'
> Registering hook 'INIT_MODULE_SOURCES_opencv_dnn': /opt/opencv455/opencv/modules/dnn/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake
> opencv_dnn: filter out cuda4dnn source code
> Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.rvv.cpp
> imgcodecs: OpenEXR codec is disabled in runtime. Details: https://github.com/opencv/opencv/issues/21326
> highgui: using builtin backend: NONE
> 
> General configuration for OpenCV 4.5.5-dev =====================================
>   Version control:               4.5.5-20-ga1143c4ea0
> 
>   Platform:
>     Timestamp:                   2022-01-06T17:03:47Z
>     Host:                        Linux 5.10.16.3-microsoft-standard-WSL2 x86_64
>     CMake:                       3.22.1
>     CMake generator:             Ninja
>     CMake build tool:            /usr/bin/ninja
>     Configuration:               Release
> 
>   CPU/HW features:
>     Baseline:                    SSE SSE2 SSE3
>       requested:                 SSE3
>     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX
>       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX
>       SSE4_1 (18 files):         + SSSE3 SSE4_1
>       SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2
>       FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX
>       AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX
>       AVX2 (33 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2
>       AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX
> 
>   C/C++:
>     Built as dynamic libs?:      YES
>     C++ standard:                11
>     C++ Compiler:                /usr/bin/c++  (ver 9.3.0)
>     C++ flags (Release):         -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG
>     C++ flags (Debug):           -fsigned-char -W -Wall -Werror=return-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Wsuggest-override -Wno-delete-non-virtual-dtor -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG
>     C Compiler:                  /usr/bin/cc
>     C flags (Release):           -fsigned-char -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG
>     C flags (Debug):             -fsigned-char -W -Wall -Werror=return-type -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Wno-comment -Wimplicit-fallthrough=3 -Wno-strict-overflow -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG
>     Linker flags (Release):      -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a   -Wl,--gc-sections -Wl,--as-needed  
>     Linker flags (Debug):        -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a   -Wl,--gc-sections -Wl,--as-needed  
>     ccache:                      NO
>     Precompiled headers:         NO
>     Extra dependencies:          dl m pthread rt
>     3rdparty dependencies:
> 
>   OpenCV modules:
>     To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo stitching ts video videoio
>     Disabled:                    python_bindings_generator python_tests world
>     Disabled by dependency:      -
>     Unavailable:                 java python2 python3
>     Applications:                tests perf_tests apps
>     Documentation:               NO
>     Non-free algorithms:         NO
> 
>   GUI:                           NONE
>     GTK+:                        NO
>     VTK support:                 NO
> 
>   Media I/O: 
>     ZLib:                        zlib (ver 1.2.11)
>     JPEG:                        libjpeg-turbo (ver 2.1.2-62)
>     WEBP:                        build (ver encoder: 0x020f)
>     PNG:                         build (ver 1.6.37)
>     TIFF:                        build (ver 42 - 4.2.0)
>     JPEG 2000:                   build (ver 2.4.0)
>     OpenEXR:                     build (ver 2.3.0)
>     HDR:                         YES
>     SUNRASTER:                   YES
>     PXM:                         YES
>     PFM:                         YES
> 
>   Video I/O:
>     DC1394:                      NO
>     FFMPEG:                      NO
>       avcodec:                   NO
>       avformat:                  NO
>       avutil:                    NO
>       swscale:                   NO
>       avresample:                NO
>     GStreamer:                   NO
>     v4l/v4l2:                    YES (linux/videodev2.h)
> 
>   Parallel framework:            pthreads
> 
>   Trace:                         YES (with Intel ITT)
> 
>   Other third-party libraries:
>     Intel IPP:                   2020.0.0 Gold [2020.0.0]
>            at:                   /opt/opencv455/build/3rdparty/ippicv/ippicv_lnx/icv
>     Intel IPP IW:                sources (2020.0.0)
>               at:                /opt/opencv455/build/3rdparty/ippicv/ippicv_lnx/iw
>     VA:                          NO
>     Lapack:                      NO
>     Eigen:                       NO
>     Custom HAL:                  NO
>     Protobuf:                    build (3.19.1)
> 
>   OpenCL:                        YES (no extra features)
>     Include path:                /opt/opencv455/opencv/3rdparty/include/opencl/1.2
>     Link libraries:              Dynamic load
> 
>   ONNX:                          NO
> 
>   Python (for build):            /usr/bin/python3
> 
>   Java:                          
>     ant:                         NO
>     JNI:                         NO
>     Java wrappers:               NO
>     Java tests:                  NO
> 
>   Install to:                    /usr/local
> -----------------------------------------------------------------
> 
> Configuring done



##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with question without real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only latest release for each branch.
   The ticket is closed, if the problem is not reproduced with modern version.
   -->"
opencv/opencv,2022-01-05 09:19:59,question,"bug compiling opencv-4.5.5 on raspios_bullseye64 with ""/usr/ include/c++/10/complex""","Hi,
After installing opencv-4.5.5 on raspios64 with libcamera and gstreamer in place, I wanted to compile the example https://github.com/Qengineering/Libcamera-OpenCV-RPi-Bullseye-64OS/archive/refs/heads /main.zip. I had an error on including opencv (first line of main.cpp) with ""/usr/include/c++/10/complex, line 1314"" if I remember correctly. I uninstalled 4.5.5 to put 4.5.4 which does not include this file and had no problems compiling."
opencv/opencv,2023-09-09 03:35:34,feature,JavaScript: include LUT support,"resolves #22751

- [x] Validated with emsdk 1.39.0

```
force_builders=Custom
build_image:Docs=docs-js:18.04
build_image:Custom=javascript
buildworker:Custom=linux-1,linux-4,linux-f1
```"
opencv/opencv,2023-08-15 09:17:43,feature,Get Mat in a blob,"### Describe the feature and motivation

I think it could be usefull to get a specific Mat in a blob to read or write data

### Additional context

Code could be 

```
/**
* return a specific Mat in a blob.
* if dims blob is less or equal  to 2 (N rows x M columns) blob is return
* if dims blob is 3 blob is blob is (H x N x M array) mat at (coord(0),0, 0) is returned.
* if dims blob is r blob is blob is (T x H x N x M array) mat at (coord(0), coord(1), 0, 0) is returned.
* */
Mat getMatInBlob(Mat blob, vector<int> coord, int posChannel = DNN_LAYOUT_NCHW)
{

}
```"
opencv/opencv,2023-07-26 11:52:21,feature,G-API: Support CUDA & TensoRT Execution Providers for ONNXRT Backend,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-19 14:22:37,feature,G-API: Support OpenVINO Execution Provider for ONNXRT Backend,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-07-11 07:51:43,feature,core: add broadcast,"Should work as [`np.broadcast_to`](https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html).

## Benchmarks

Results on M1:

| src | dst | v1 (mean, ms) | v2 (mean, ms) |
| - | - | - | - |
| [1, 10, 100, 1000]   | [1000, 10, 100, 1000] | 124.79 | 63.41 |
| [1000, 1, 100, 1000] | [1000, 10, 100, 1000] | 866.13 | 89.96 |
| [1000, 10, 1, 1000]  | [1000, 10, 100, 1000] | 231.30 | 93.71 |
| [1000, 10, 100, 1]   | [1000, 10, 100, 1000] | 3798.67 | 100.02 |

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake


```
force_builders=Win32,Linux32,Linux AVX2
```"
opencv/opencv,2023-07-01 17:18:55,feature,License for CNN models of wechat_qrcode ? ,"### Describe the feature and motivation

Hi, 
"" Recently opencv add to opencv_contrib ""CNN models for wechat_qrcode
module, including the detector model and the super scale model."" 
https://github.com/WeChatCV/opencv_3rdparty

CNN models is about ""Object Detection using CNNs""
https://docs.opencv.org/4.x/d2/da2/tutorial_dnn_objdetect.html

I proposed add the models 
https://src.fedoraproject.org/rpms/opencv/pull-request/21#

The lack of information of the License of these ""models"" , make me
think that I should ask to Fedora legal, if we can bundle these binaries that are
in a 3rdparty repo ?
 
In the opencv_contrib code, we have this License information
https://github.com/opencv/opencv_contrib/blob/4.x/modules/wechat_qrcode/LICENSE ""


After write above, Fedora Legal team or others suggested  to me to make a pull request  to add the same License in the 3rdparty repo and here it is : 

https://github.com/WeChatCV/opencv_3rdparty/pull/1

"
opencv/opencv,2023-06-20 10:23:15,feature,How to use opencv to erase text from images and restore the background,"### Describe the feature and motivation

How to use opencv to erase text from images and restore the background

### Additional context

How to use opencv to erase text from images and restore the background"
opencv/opencv,2023-06-14 11:25:35,feature,Consider half pixel mode in ONNX resize,"### Pull Request Readiness Checklist

resolves https://github.com/opencv/opencv/issues/23745

merge with extra: https://github.com/opencv/opencv_extra/pull/1070

![image](https://github.com/opencv/opencv/assets/25801568/d817afce-ccb4-4332-9bed-8490892cad22)

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-06-10 19:07:48,feature,Type stubs: Make `cv2.typing` type aliases comments actual docstrings,"### Describe the feature and motivation

The comments in `cv2/typing/__init__.pyi` would be beneficial as actual docstrings:

![image](https://github.com/opencv/opencv/assets/1350584/1766afca-9ce4-4ad0-8739-6a562cb0b377)
vs
![image](https://github.com/opencv/opencv/assets/1350584/e550eb03-265d-413d-8a3f-ecd900f3e4b4)


### Additional context

This may be useful in other stubs too where there's comments, **as long as it doesn't override existing docstrings**"
opencv/opencv,2023-06-07 11:46:46,feature,Check type in blobFromImages and blobFromImagesWithParams,"### Describe the feature and motivation

Everything is described here https://forum.opencv.org/t/how-to-use-blobfromimageswithparams/13430

### Additional context

I propose to check InputArraysOfArray type  in blobFromImages and blobFromImageWithParams: 

    if (images_.kind() != _InputArray::STD_VECTOR_MAT && images_.kind() != _InputArray::STD_ARRAY_MAT &&
        images_.kind() != _InputArray::STD_VECTOR_VECTOR) {
        String error_message = ""The data is expected as InputArray::STD_VECTOR_MAT (a std::vector<Mat>) or _InputArray::STD_VECTOR_VECTOR (a std::vector< std::vector<...> >)."";
        CV_Error(Error::StsBadArg, error_message);
    }
"
opencv/opencv,2023-06-05 00:55:17,feature,"Do you have any plans to add a generic pre-trained model to OpenCV, just like using a DNN pre-trained model?","### Describe the feature and motivation

such as Lara,AIGC and Segment-Anything,and so on?

### Additional context

_No response_"
opencv/opencv,2023-05-26 10:41:43,feature,Import and export np.float16 in Python,"### Pull Request Readiness Checklist

* Also, fixes `cv::norm` with `NORM_INF` and `CV_16F`

resolves https://github.com/opencv/opencv/issues/23687

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-17 14:24:16,feature,SoftmaxInt8 is implemented but ONNX importer does not parse QLinearSoftmax,"### Describe the feature and motivation

[`QLinearSoftmax`](https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md#com.microsoft.QLinearSoftmax) is an operator from domain `com.microsoft`, which is not part of the standard ONNX opset. We have `SoftmaxInt8` implemented in https://github.com/opencv/opencv/blob/4.x/modules/dnn/src/int8layers/softmax_layer.cpp and `QLinearSoftmax` should be mapped to `SoftmaxInt8`, but there is no parser for `QLinearSoftmax` in the [ONNX importer](https://github.com/opencv/opencv/blob/4.x/modules/dnn/src/onnx/onnx_importer.cpp).

Model is attached here [qlinearsoftmax.zip](https://github.com/opencv/opencv/files/11499121/qlinearsoftmax.zip). Network arch is as below:

![image](https://github.com/opencv/opencv/assets/17219438/97e8b034-16c6-4ab7-9060-f8abefc341ca)



### Additional context

_No response_"
opencv/opencv,2023-05-15 11:19:53,feature,LSTM ONNX Layout Attribute Support ,"### Explanation
This PR contains necessary changes to support `layout` attribute. This attributes is present in [ONNX](https://github.com/onnx/onnx/blob/main/docs/Operators.md#lstm) and [Torch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#lstm) (in touch it is name as `batch_first=True`) libraries. When `layout = 1` input to LSTM layer is expected to have batch dimension first -> `[batch_size, sequence_length, features]` vs `layout = 0` - default `[sequence_length, batch_size, features]`

resolves https://github.com/opencv/opencv/issues/23602

### Test Data
Test data and data generator for PR located here [#1063](https://github.com/opencv/opencv_extra/pull/1063)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-11 07:28:52,feature,Build DNN without Protobuf,"### Pull Request Readiness Checklist

DNN module can be built without Protobuf for Darknet, TFLite, OpenVINO, Torch (not PyTorch) models.

```
cmake \\
    -DCMAKE_BUILD_TYPE=Release \\
    -DBUILD_LIST=dnn \\
    -DWITH_PROTOBUF=OFF \\
    -DWITH_OPENCL=OFF

7.1M    lib/libopencv_dnn.so.4.7.0
```


```
cmake \\
    -DCMAKE_BUILD_TYPE=Release \\
    -DBUILD_LIST=dnn \\
    -DWITH_OPENCL=OFF

3.9M    lib/libopencv_dnn.so.4.7.0
```

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-05-09 12:52:44,feature,[G-API] Implement OpenVINO 2.0 backend,"### Pull Request Readiness Checklist

Implemented basic functionality for `OpenVINO` 2.0 G-API backend.

#### Overview
- [x] Implement `Infer` kernel with some of essential configurable parameters + IR/Blob models format support.
- [ ] Implement the rest of kernels: `InferList`, `InferROI`, `Infer2` + other configurable params (e.g reshape)
- [x] Asyncrhonous execution support
- [ ] Remote context support

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-19 14:51:39,feature,OpenCV DNN poor performance on Web,"### Descripe the feature and motivation

Hello! I have C++ code that I compile using Emscripten and deploy it for browser usage. Now I have new feature to release and it involves running inference on small lightweight network.

I decided to use OpenCV DNN for that since I already have OpenCV dependency and OpenCV DNN has a very convenient public API. On desktop my network runs incredibly fast, just below 1 ms. But when I put my code on the Web, network runs around 50-60 times slower.

I decomposed an issue and I noticed that the problem is in convolution blocks. When linear blocks are processed lighting fast by OpenCV DNN on WASM, very small and simple convolution blocks take roughly `45-60 ms` to process.

Is there any way to improve OpenCV DNN performance on WASM?

### Additional context

_No response_"
opencv/opencv,2023-04-10 09:56:28,feature,[G-API] Handle meta from multiple inputs in IE backend,"### Overview

Since `IE backend` implements ""streaming""  [`run(IInput&, IOutput&)`](https://github.com/opencv/opencv/blob/4.x/modules/gapi/src/compiler/gislandmodel.cpp#L387)method it must take care of propagating meta from inputs.

The handling implemented the same way as for the default `GIslandModel::run`: https://github.com/opencv/opencv/blob/4.x/modules/gapi/src/compiler/gislandmodel.cpp#L436-L440
Since `post` is done in `PostOutputs` / `PostOutputsList` callback functions, meta collected and moved to `IECallContext`.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-04-06 06:14:53,feature,Cannot load yolov8n-pose ONNX model,"### Descripe the feature and motivation

Ultralytics has just released Yolov8-pose estimation models. But I cannot load them in OpenCV DNN. It'd be really nice to support their models - I'm forced to use onnxruntime.

Here is the model I exported from their `yolov8n-pose.pt` with default parameters: [yolov8n-pose.onnx](https://www.dropbox.com/s/ihotlm7vf4i1l8d/yolov8n-pose.onnx?dl=1)

The error I'm getting:

```
  File ""/home/dizcza/Projects/Airtouch/edgeai-yolov5/onnx_inference/yolo_pose_onnx_inference.py"", line 148, in inference_video
    net = cv2.dnn.readNet(model_path)
cv2.error: OpenCV(4.7.0) /io/opencv/modules/dnn/src/onnx/onnx_importer.cpp:1073: error: (-2:Unspecified error) in function 'handleNode'
> Node [Reshape@ai.onnx]:(onnx_node!/model.22/dfl/Reshape) parse error: OpenCV(4.7.0) /io/opencv/modules/dnn/src/layers/reshape_layer.cpp:109: error: (-215:Assertion failed) total(srcShape, srcRange.start, srcRange.end) == maskTotal in function 'computeShapeByReshapeMask'
```

### Additional context

I'm marking the issue as a feature request cause I'm aware of ONNX not fully supported by OpenCV DNN module."
opencv/opencv,2023-03-29 13:16:37,feature,imgcodecs: tiff: Support to encode for CV_32S with compression params,"Fix https://github.com/opencv/opencv/issues/23416

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-03-24 10:48:01,feature,dnn: Support more operators in CANN backend,"This PR adds the support of following layers:

- [x] Sub
- [x] PRelu
- [x] DeConv
- [x] Also warn users if backend is switched back to default if some of the layers are not supported.
- [ ] [Dropped] LSTM: some hacks (adding layers) were introduced which makes it even harder to build the graph for CANN backend.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-03-20 15:13:31,feature,Load Image in tutorials,"### Descripe the feature and motivation

Some images are in tutorials but not in samples/data. findfiles cannot be used source code 
example doc/tutorials/imgproc/histograms/back_projection/images

This [sample ](https://github.com/opencv/opencv/blob/4.x/samples/cpp/tutorial_code/Histograms_Matching/calcBackProject_Demo1.cpp#L26-L31)could be changed : 
```
int main( int argc, char* argv[] )
{
    //! [Read the image]
    CommandLineParser parser( argc, argv, ""{@input |Back_Projection_Theory0.jpg  | input image}"" );
    samples::addSamplesDataSearchSubDirectory(""./doc/tutorials/imgproc/histograms/back_projection/images"");
    Mat src = imread(samples::findFile(parser.get<String>( ""@input"" )) );

```
Is it a good idea?

### Additional context

_No response_"
opencv/opencv,2023-02-24 14:51:05,feature,Support VideoCapture CAP_PROP_AUTO_WB and CV_CAP_PROP_WHITE_BALANCE_BLUE_U for DShow,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [OK] I agree to contribute to the project under Apache 2 License.
- [OK] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [OK] The PR is proposed to the proper branch
- [OK] There is a reference to the original bug report and related work
https://github.com/opencv/opencv/issues/19621
https://github.com/opencv/opencv/issues/21408

### Before apply this pull request console output.

before AWB setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: -1

after AWB disable setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: -1

after AWB enable setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: -1

after Manual WB(and Disable AWB) setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: -1

### After apply this pull request console output.

before AWB setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: 0

after AWB disable setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 4000
CAP_PROP_AUTO_WB: 0

after AWB enable setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 4000
CAP_PROP_AUTO_WB: 1

after Manual WB(and Disable AWB) setting
CAP_PROP_WHITE_BALANCE_BLUE_U: 2000
CAP_PROP_AUTO_WB: 0

### Test Code
[OpenCvVideoCapTest.zip](https://github.com/opencv/opencv/files/10825399/OpenCvVideoCapTest.zip)
"
opencv/opencv,2023-02-18 22:23:53,feature,"Added QR_Code data flip support, flip and retry after first ECC failure","**Merge with extra**: https://github.com/opencv/opencv_extra/pull/1046

Fixes  #23249, fixes  #23155, fixes #20724

Added quirc_flip() method to horizontally flip the data.
When the decoder fails to ECC the data we flip the image, and try one more time.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-02-17 01:31:03,feature,Add detect qr with aruco,"Using Aruco to detect finder patterns to search QR codes.

TODO (in next PR):
- add single QR detect (update `detect()` and `detectAndDecode()`)
- need reduce full enumeration of finder patterns
- need add finder pattern info to `decode` step
- need to merge the pipeline of the old and new algorithm

[Current results:](https://docs.google.com/spreadsheets/d/1ufKyR-Zs-IGXwvqPgftssmTlceVjiQX364sbrjr2QU8/edit#gid=1192415584)
+20% total detect, +8% total decode in OpenCV [QR benchmark](https://github.com/opencv/opencv_benchmarks/tree/develop/python_benchmarks/qr_codes) 

![res1](https://user-images.githubusercontent.com/22337800/231228556-191d3eae-a318-44e1-af99-e7d420bf6248.png)


78.4% detect, 58.7% decode vs 58.5 detect, 50.5% decode in default

[main.py.txt](https://github.com/opencv/opencv/files/10762369/main.py.txt)

![res2](https://user-images.githubusercontent.com/22337800/231229123-ed7f1eda-159a-444b-a3ff-f107d8eb4a20.png)


add new info to [google docs](https://docs.google.com/spreadsheets/d/1ufKyR-Zs-IGXwvqPgftssmTlceVjiQX364sbrjr2QU8/edit?usp=sharing)


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-02-10 09:51:43,feature,Proposal: Delay load option for OpenCV modules?,"### Descripe the feature and motivation

In #23187 it was discussed to optionally enable a delay load option of OpenCV's modules on Windows. This means that modules with heavy dependencies - for example the Python bindings with enabled CUDA support and contrib build - could delay load their dependencies. This yields a more flexible library distribution process by allowing the user to only distribute the actually needed instead of all modules, i.e. ignore unused DLLs.

As we recently worked on integrating the delay load option for CUDA libraries (cf. #22675 and #23209), it is relatively straightforward to integrate this: Define the `/DELAYLOAD:opencv_<module><version>.dll` flag for all enabled modules (exepct core as this is always required) and introduce a CMake variable `OPENCV_ENABLE_DELAYLOAD` that controls whether these flags are set or not.

Still, we wanted to propose / discuss this feature before implementing it. If you think this is a useful feature, the main question is in which CMake script it should be implemented.

### Additional context

_No response_"
opencv/opencv,2023-02-04 16:15:04,feature,Add GELU layer for vision transformers,"This PR adds the CPU and OCL kernels of GELU and GELU-Approximation layers.

Merge with https://github.com/opencv/opencv_extra/pull/1044

References:

- https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py
- https://pytorch.org/docs/stable/generated/torch.nn.GELU.html

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-01-30 13:26:05,feature,added argument to print notice in `roiSelector.cpp`,"Related Issue : https://github.com/opencv/opencv/issues/23175

I've added a printNotice argument to `selectROI` (and it's overload) and `selectROIs` functions.
I've also updated the function declarations in `highgui.hpp`.
Tested by building locally.


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-01-23 13:44:39,feature,Adding HEVC/H265 FourCC support to MSMF video writer,"Adding HEVC/H265 FourCC support to MSMF video writer. I have verified it with my own video input stream, and it works well on my workstation.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-01-19 11:28:30,feature,ChArUco pre460 pattern support,"Add support for certain ChArUco board patterns as they had been generated with OpenCV contrib version prior 4.6.0.

The pull request adds a `setLegacyParameter(bool)` method to the Charuco class to allow switching to the old board design as described in https://github.com/opencv/opencv/issues/23152.

Default setting for this parameter is `false´ to remain compatible with OpenCV contrib 4.6.0 and OpenCV 4.7.0.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
- [X] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [X] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2023-01-16 02:02:24,feature,Draw Multiple Boxes in one shot,"### Descripe the feature and motivation

With the current development of a lot of Object detection models, sometimes drawing many boxes like 100 or 200 boxes could be time-consuming in python. 

I wonder if Opencv Would like to consider a function to draw many boxes in one shot. So it will speed up the process instead of drawing one by one using for loop. 

### Additional context

_No response_"
opencv/opencv,2023-01-07 14:39:37,feature,"Python bindings: Overload resolution failed, ""data type = 17"", should be called out as numpy array of `dtype=object`","### Describe the feature and motivation

Overload resolution should check a numpy array's `dtype` for being `object`, and in that case emit a useful error message like

> numpy array dtype is `object`, which is not supported. Check array contents.

### Additional context

Beginners often have trouble with `imread()`. It will not throw an exception but return `None`, silently, and most tutorials don't bother checking for that. Further processing may produce numpy arrays of `dtype=object`.

```python
img = None # cv.imread(""does_not_exist.png"")
res = np.hstack([img, img]) # put side by side
cv.imwrite(""foo.png"", res)
```
([Source](https://stackoverflow.com/questions/75039808/cv2-error-opencv4-7-0-1-error-5bad-argument-in-function-imwrite-img))

Calling `imwrite()` with a numpy array of `dtype=object` will throw this mysterious error that says:
```text
cv2.error: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'imwrite'
> Overload resolution failed:
>  - img data type = 17 is not supported
>  - Expected Ptr<cv::UMat> for argument 'img'
```
(behavior exists with v4.7.0 and v4.6.0 at least)

**This does not help.** Data type 17 would decode to be `CV_8SC3`, which this obviously *is not*. Mentioning the `Ptr<cv::UMat>` strikes me as irrelevant since the overload resolution already accepted the numpy array, merely complains about its element type."
opencv/opencv,2023-01-02 17:21:31,feature,Update USAC,"### Pull Request Readiness Checklist

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake

"
opencv/opencv,2022-12-28 05:52:48,feature,dnn: add layer normalization for vision transformers,"Merge with https://github.com/opencv/opencv_extra/pull/1032

- [x] add layer norm onnx parser
- [x] add layer norm impl
- [x] add layer norm onnx simplifier for both cases of constants being Constant and Initializer
- [x] add test model generation code for layer_norm_expanded and layer_norm_expanded_initializer

Benchmark:

| Layer | Mean (ms) | Median (ms) | Min (ms) |
| - | - | - | - |
| layer norm expanded | 0.43 | 0.42 | 0.40 |
| layer norm (this pr) | 0.02 | 0.02 | 0.01 |

*: tested with size 1x50x768 on Apple M1.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake

```
force_builders=Linux OpenCL
```"
opencv/opencv,2022-12-14 08:24:53,feature,Switch to new OpenVINO API after 2022.1 release,"* Use OpenVINO Runtime API (aka Tensor API or API 2.0) which has been introduces in 2022.1
* Remove `InferenceEngine::DataPtr` completely
* Internal dynamism feature works only with a new API. Actual for Faster-* family models and with layers such as NonZero, Top-k and others. This feature may improve performance.

- [x] Tested with OpenVINO 2022.1 ([CI](http://pullrequest.opencv.org/buildbot/builders/precommit_custom_linux/builds/100158))
- [x] Tested with OpenVINO 2021.4 ([CI](http://pullrequest.opencv.org/buildbot/builders/precommit_custom_linux/builds/100157))
- [ ] Tested with old OpenVINO (for example, 2020.3) - (**alalek**: older versions are not supported on 4.x branch)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake

```
force_builders=Custom

Xbuild_image:Custom=ubuntu-openvino-2021.4.2:20.04
build_image:Custom=ubuntu-openvino-2022.1.0:20.04

test_modules:Custom=dnn,python2,python3,java,gapi,video

buildworker:Custom=linux-1
# disabled due high memory usage: test_opencl:Custom=ON
test_opencl:Custom=ON
test_bigdata:Custom=1
test_filter:Custom=*

allow_multiple_commits=1
```"
opencv/opencv,2022-12-12 10:38:30,feature,added cv::hasNonZero(),"`cv::hasNonZero()` is semantically equivalent to (`cv::countNonZero()>0`) but stops parsing the image when a non-zero value is found, for a performance gain

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake

This pull request might be refused, but I submit it to know if further work is needed or if I just stop working on it.
The idea is only a performance gain vs `countNonZero()>0` at the cost of more code.

Reasons why it might be refused :

- this is just more code
- the execution time is ""unfair""/""unpredictable"" since it depends on the position of the first non-zero value
- the user must be aware that default search is from first row/col to last row/col and has no way to customize that, even if his use case lets him know where a non zero could be found
- the PR in its current state is using, for the ocl implementation, a mere `countNonZero()>0` ; there is not much sense in trying to break early the ocl kernel call when non-zero is encountered. So the ocl implementation does not bring any improvement.
- there is no IPP function that can help (`countNonZero()` is based in `ippCountInRange`)
- the PR in its current state might be slower than a call to `countNonZero()>0` in some cases (see ""challenges"" below)

Reasons why it might be accepted :

- the performance gain is huge on average, if we consider that ""on average"" means ""non zero in the middle of the image""
- the ""missing"" IPP implementation is replaced by an ""Open-CV universal intrinsics"" implementation
- the PR in its current state is almost always faster than a call to `countNonZero()>0`, is only slightly slower in the worst cases, and not even for all matrices

**Challenges**
The worst case is either an all-zero matrix, or a non-zero at the very last position.  In such a case, the `hasNonZero()` implementation will parse the whole matrix like `countNonZero()` would do. But we expect the performance to be the same in this case. And `ippCountInRange` is hard to beat !
There is also the case of very small matrices (<=32x32...) in 8b, where the SIMD can be hard to feed.

For all cases but the worse, my custom `hasNonZero()` performs better than `ippCountInRange()`
For the worst case, my custom `hasNonZero()` performs better than `ippCountInRange()` *except for large matrices of type CV_32S or CV_64F* (but surprisingly, not CV_32F).
The difference is small, but it exists (and I don't understand why).
For very small CV_8U matrices `ippCountInRange()` seems unbeatable.

Here is the code that I use to check timings

```

  //test cv::hasNonZero() vs (cv::countNonZero()>0) for different matrices sizes, types, strides...
  {
    cv::setRNGSeed(1234);
    const std::vector<cv::Size> sizes = {{32, 32}, {64, 64}, {128, 128}, {320, 240}, {512, 512}, {640, 480}, {1024, 768}, {2048, 2048}, {1031, 1000}};
    const std::vector<int> types = {CV_8U, CV_16U, CV_32S, CV_32F, CV_64F};
    const size_t iterations = 1000;
    for(const cv::Size& size : sizes)
    {
      for(const int type : types)
      {
        for(int c = 0 ; c<2 ; ++c)
        {
          const bool continuous = !c;
          for(int i = 0 ; i<4 ; ++i)
          {
            cv::Mat m = continuous ? cv::Mat::zeros(size, type) : cv::Mat(cv::Mat::zeros(cv::Size(2*size.width, size.height), type), cv::Rect(cv::Point(0, 0), size));
            const bool nz = (i <= 2);
            const unsigned int nzOffsetRange = 10;
            const unsigned int nzOffset = cv::randu<unsigned int>()%nzOffsetRange;
            const cv::Point pos = 
              (i == 0) ? cv::Point(nzOffset, 0) :
              (i == 1) ? cv::Point(size.width/2-nzOffsetRange/2+nzOffset, size.height/2) :
              (i == 2) ? cv::Point(size.width-1-nzOffset, size.height-1) :
              cv::Point(0, 0);
            std::cout << ""============================================================"" << std::endl;
            std::cout << ""size:"" << size << ""  type:"" << type << ""  continuous = "" << (continuous ? ""true"" : ""false"") << ""  iterations:"" << iterations << ""  nz="" << (nz ? ""true"" : ""false"");
            std::cout << ""  pos="" << ((i == 0) ? ""begin"" : (i == 1) ? ""middle"" : (i == 2) ? ""end"" : ""none"");
            std::cout << std::endl;
            cv::Mat mask = cv::Mat::zeros(size, CV_8UC1);
            mask.at<unsigned char>(pos) = 0xFF;
            m.setTo(cv::Scalar::all(0));
            m.setTo(cv::Scalar::all(nz ? 1 : 0), mask);
            std::vector<bool> results;
            std::vector<double> timings;

            {
              bool res = false;
              auto ref = cv::getTickCount();
              for(size_t k = 0 ; k<iterations ; ++k)
                res = cv::hasNonZero(m);
              auto now = cv::getTickCount();
              const bool error = (res != nz);
              if (error)
                printf(""!!ERROR!!\\r\\n"");
              results.push_back(res);
              timings.push_back(1000.*(now-ref)/cv::getTickFrequency());
            }
            {
              bool res = false;
              auto ref = cv::getTickCount();
              for(size_t k = 0 ; k<iterations ; ++k)
                res = (cv::countNonZero(m)>0);
              auto now = cv::getTickCount();
              const bool error = (res != nz);
              if (error)
                printf(""!!ERROR!!\\r\\n"");
              results.push_back(res);
              timings.push_back(1000.*(now-ref)/cv::getTickFrequency());
            }

            const size_t bestTimingIndex = (std::min_element(timings.begin(), timings.end())-timings.begin());
            if ((bestTimingIndex != 0) || (std::find_if_not(results.begin(), results.end(), [&](bool r) {return (r == nz);}) != results.end()))
            {
              std::cout << ""cv::hasNonZero\\t\\t=>"" << results[0] << ((results[0] != nz) ? ""  ERROR"" : """") << ""   perf:"" << timings[0] << ""ms => "" << (iterations/timings[0]*1000) << "" im/s"" << ((bestTimingIndex == 0) ? "" * "" : """") << std::endl;
              std::cout << ""cv::countNonZero\\t=>"" << results[1] << ((results[1] != nz) ? ""  ERROR"" : """") << ""   perf:"" << timings[1] << ""ms => "" << (iterations/timings[1]*1000) << "" im/s"" << ((bestTimingIndex == 1) ? "" * "" : """") << std::endl;
            }
          }
        }
      }
    }
  }

```

Here is a report of this benchmark (it only reports timings when `cv::countNonZero()` is faster)
My CPU is an Intel Core I7 4790 @ 3.60Ghz

```

============================================================
size:[32 x 32]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
cv::hasNonZero          =>1   perf:0.353764ms => 2.82674e+06 im/s
cv::countNonZero        =>1   perf:0.282044ms => 3.54555e+06 im/s *
============================================================
size:[32 x 32]  type:0  continuous = false  iterations:1000  nz=true  pos=end
cv::hasNonZero          =>1   perf:0.610478ms => 1.63806e+06 im/s
cv::countNonZero        =>1   perf:0.283182ms => 3.5313e+06 im/s *
============================================================
size:[32 x 32]  type:0  continuous = false  iterations:1000  nz=false  pos=none
cv::hasNonZero          =>0   perf:0.630115ms => 1.58701e+06 im/s
cv::countNonZero        =>0   perf:0.282044ms => 3.54555e+06 im/s *
============================================================
size:[32 x 32]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:5  continuous = false  iterations:1000  nz=true  pos=end
cv::hasNonZero          =>1   perf:0.607347ms => 1.64651e+06 im/s
cv::countNonZero        =>1   perf:0.467037ms => 2.14116e+06 im/s *
============================================================
size:[32 x 32]  type:5  continuous = false  iterations:1000  nz=false  pos=none
cv::hasNonZero          =>0   perf:0.618162ms => 1.6177e+06 im/s
cv::countNonZero        =>0   perf:0.468175ms => 2.13595e+06 im/s *
============================================================
size:[32 x 32]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[32 x 32]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[32 x 32]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[32 x 32]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[32 x 32]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[64 x 64]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[64 x 64]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[64 x 64]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[64 x 64]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[128 x 128]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[128 x 128]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[128 x 128]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[128 x 128]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[320 x 240]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[320 x 240]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[320 x 240]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[320 x 240]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[512 x 512]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[512 x 512]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[512 x 512]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[512 x 512]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[640 x 480]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[640 x 480]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[640 x 480]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[640 x 480]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1024 x 768]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1024 x 768]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1024 x 768]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1024 x 768]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:4  continuous = true  iterations:1000  nz=true  pos=end
cv::hasNonZero          =>1   perf:895.381ms => 1116.84 im/s
cv::countNonZero        =>1   perf:882.569ms => 1133.06 im/s *
============================================================
size:[2048 x 2048]  type:4  continuous = true  iterations:1000  nz=false  pos=none
cv::hasNonZero          =>0   perf:899.53ms => 1111.69 im/s
cv::countNonZero        =>0   perf:870.894ms => 1148.24 im/s *
============================================================
size:[2048 x 2048]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[2048 x 2048]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:6  continuous = true  iterations:1000  nz=true  pos=end
cv::hasNonZero          =>1   perf:2018.92ms => 495.313 im/s
cv::countNonZero        =>1   perf:1966.37ms => 508.552 im/s *
============================================================
size:[2048 x 2048]  type:6  continuous = true  iterations:1000  nz=false  pos=none
cv::hasNonZero          =>0   perf:2005.87ms => 498.537 im/s
cv::countNonZero        =>0   perf:1992.78ms => 501.812 im/s *
============================================================
size:[2048 x 2048]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[2048 x 2048]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[2048 x 2048]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[2048 x 2048]  type:6  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:0  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:0  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:0  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:0  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:0  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:0  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:0  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:0  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:2  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:2  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:2  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:2  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:2  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:2  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:2  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:2  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:4  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:4  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:4  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:4  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:4  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:4  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:4  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:4  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:5  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:5  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:5  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:5  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:5  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:5  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:5  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:5  continuous = false  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:6  continuous = true  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:6  continuous = true  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:6  continuous = true  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:6  continuous = true  iterations:1000  nz=false  pos=none
============================================================
size:[1031 x 1000]  type:6  continuous = false  iterations:1000  nz=true  pos=begin
============================================================
size:[1031 x 1000]  type:6  continuous = false  iterations:1000  nz=true  pos=middle
============================================================
size:[1031 x 1000]  type:6  continuous = false  iterations:1000  nz=true  pos=end
============================================================
size:[1031 x 1000]  type:6  continuous = false  iterations:1000  nz=false  pos=none
done

```
"
opencv/opencv,2022-12-07 13:41:02,feature,Support one-time audio video reading,"added to gsrtreamer:
- audio + video capturing
- added audio video samples synchronization

<cut/>

```
# Linux OpenCL
force_builders=linux,docs,Linux x64 Debug,Custom

build_image:Linux OpenCL=ubuntu:20.04
buildworker:Linux OpenCL=linux-4

build_image:Custom=gstreamer:16.04
buildworker:Custom=linux-1
```
"
opencv/opencv,2022-12-05 09:14:35,feature,Timeouts support for GStreamer backend,"Address https://github.com/opencv/opencv/issues/22868
Used the same defaults as it's done for FFmpeg

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake


```
force_builders=Custom
build_image:Custom=gstreamer:16.04
buildworker:Custom=linux-1
```"
opencv/opencv,2022-11-30 22:58:03,feature,videoio/FFmpeg: added CV_16UC1 read/write support,"Partially resolves #10623
Replaces #12284 and #12290

* _VideoCapture_: we can now disable BGR conversion using parameter for the FFmpeg backend and return image in original format, straight from decoder. However this feature has many limitations, for example it is not possible to return multiplanar images yet - new interface or additional processing is necessary for this. So I added a warning with some details which will be printed when this feature is enabled. Basically 8UC1 and 16UC1 can now be returned from _VideoCapture_ in addition to BGR.
* _VideoWriter_: we can set depth using parameter during object initialization. It can be 8U or 16U. In combination with _isColor_  it gives us three supported formats: 8UC3, 8UC1 and 16UC1; and one unsupported: 16UC3. 16UC1 can be handled by the `FFV1` codec.
* Several debug and warning log messages have been added for convenience and safety
* The new test writes a video and then reads it back, frames are not compared with the original, only format and count is verified. Unsupported format combination is also checked in this test which is not very good, but I believe is fine for now.
* FFV1 codec/container has been added to generic ffmpeg and videoio tests (regular BGR)

Note: some other codecs had _hidden_ support for 16UC1, but I did not test them (PNG and RAWVIDEO).

Note: originally I wanted to modify #12290 - add a test, fix minor issues, but it turned out that I had to rewrite 90% of changes made in that PR, so I decided to publish it as a brand new one.

```
buildworker:Custom=linux-4
build_image:Custom=ffmpeg-master
```"
opencv/opencv,2022-11-27 09:37:48,feature,DNN: let MatMul can work when both two inputs are const,"**Merge with extra**: https://github.com/opencv/opencv_extra/pull/1021

This PR is try to solve the `MatMul` node problem of https://github.com/opencv/opencv/issues/22859

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-27 00:37:10,feature,Support ffmpeg command line syntax with FFMPEG backend,"### Describe the feature and motivation

To support complex stream specifiers for VideoWriter/VideoCapture, conversion options and filter graphs with the FFMPEG-backend, I would like to introduce a syntax similar (or identical?) to what ffmpeg (https://github.com/FFmpeg/FFmpeg/blob/master/fftools/ffmpeg_opt.c) offers.

A simple example of what i have in mind:
 ```c++
 VideoCapture(""-v trace -f v4l2src -input_format mjpeg -i /dev/video0 -vf scale=320:240"", CAP_FFMPEG) cap;
 ```
 
 Before I implement this:
 * Is it a good idea? 
 * What should be the limits?

I am (sadly) aware I can't simply use the ffmpeg code.

### Additional context

_No response_"
opencv/opencv,2022-11-24 02:23:02,feature,dnn: add batched nms,"Resolves #22851.

Merge with https://github.com/opencv/opencv_extra/pull/1020.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-22 23:06:28,feature,Feature request within USAC framework: allow setting PolishingMethod to NonePolisher,"Within USAC framework, a final refinement of the best estimated model is always done using all the inliers found. However, in the case that only the inliers need to be known (or when a non-refined model suffices), this last refinement is not needed.

The motivation behind this resides in infering the previous quantities (inliers and/or non-refined model) faster. The increase in execution speed can be significant. For instance, when solving the Perspective-n-Point problem, USAC uses DLS ([J. Hesch, 2011](http://hesch.io/_files/Joel_Hesch_ICCV11.pdf)) as the non-minimal solver which has significant execution time w.r.t. the number of points (e.g. see Fig. 2 of ([S. Urban, 2016](https://arxiv.org/pdf/1607.08112.pdf))). In this specific case, this also would allow to consider different PnP methods for the last refinement.

Within my little knowledge of USAC's implementation, I think that this could be done by allowing to set  `PoilishingMethod` to `NonePolisher` (defined [here](https://github.com/opencv/opencv/blob/2273af0166f5c4eb825802a187501874d20256f8/modules/calib3d/src/usac.hpp#L11)), since this is checked when deciding to polish the model:
https://github.com/opencv/opencv/blob/456137fa24d94fd515b2d9814fef894363de7a75/modules/calib3d/src/usac/ransac_solvers.cpp#L354

Which I believe the previous condition will always hold, since the Polishing method is hard coded here:
https://github.com/opencv/opencv/blob/456137fa24d94fd515b2d9814fef894363de7a75/modules/calib3d/src/usac/ransac_solvers.cpp#L606-L607
and seems to not be reassigned after.

Thanks in advance,"
opencv/opencv,2022-11-18 03:47:37,feature,"can opencv support to push video streams to video server, like rtsp/rtmp... ?","### Descripe the feature and motivation

nowadays, video live is popular, opencv can receive rtsp/rtmp/usb .etc，but it cloudn't support to transform/push video stream to video server. it's possible to add this new feature?

### Additional context

_No response_"
opencv/opencv,2022-11-17 01:45:00,feature,Is it possible to pass in multiple conf_thres in cv::dnn::NMSBoxes？,"### Descripe the feature and motivation

Now I want to pass in multiple conf_thres for each label, set different conf_thres  for different labels,I found this feature to be implemented in the python version,but I can't find anything related to the C++ version,I wonder if this is achievable and how to do?

### Additional context

_No response_"
opencv/opencv,2022-11-15 06:34:12,feature,dnn: support ONNX Tile,"Fixes #22789

Merge with https://github.com/opencv/opencv_extra/pull/1017

ONNX doc for Tile:
- Opset 1: 3 inputs (https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Tile-1)
- Opset 6: 2 inputs (https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Tile-6)
- Opset 13: 2 inputs (https://github.com/onnx/onnx/blob/main/docs/Operators.md#Tile)
- Opset 13 is basically the same as Opset 6 for inference.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-11 12:23:42,feature,OpenCV cannot import ONNX model: step >= 1 in function 'cv::dnn::SliceLayerImpl::SliceLayerImpl',"### System Information

OpenCV => Python opencv-python-rolling 4.6.0.20221022
Operating System / Platform Windows 10 64 bit
Python =>3.10.4

### Detailed description

I converted the DecoupledSeg from PaddleSeg to ONNX:

https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.6/configs/decoupled_segnet

This fails to load in OpenCV 4.6 pre-release:

```
[ERROR:0@0.552] global D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp (1050) cv::dnn::dnn4_v20220524::ONNXImporter::handleNode DNN/ONNX: ERROR during processing node with 5 inputs and 1 outputs: [Slice]:(onnx_node!p2o.Slice.4) from domain='ai.onnx'
Traceback (most recent call last):
  File ""d:\\Local\\devel\\Python\\OpenCV\\dnn_segmentation_paddle_decoupledseg_cityscapes\\inference.py"", line 121, in <module>
    model = cv2.dnn.readNet(model_path)
cv2.error: OpenCV(4.6.0-dev) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onnx\\onnx_importer.cpp:1069: error: (-2:Unspecified error) in function 'cv::dnn::dnn4_v20220524::ONNXImporter::handleNode'
> Node [Slice@ai.onnx]:(onnx_node!p2o.Slice.4) parse error: OpenCV(4.6.0-dev) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\slice_layer.cpp:182: error: (-215:Assertion failed) step >= 1 in function 'cv::dnn::SliceLayerImpl::SliceLayerImpl'
```

### Steps to reproduce

```
model = cv2.dnn.readNet(model_path)
```

Find the ONNX file here:
https://drive.google.com/file/d/1qIVxTS2lZpN4fapN3yvnYrZ3x0g4rD4w/view?usp=sharing

### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [X] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-11-10 14:03:56,feature,OpenEXR encoder: add capability to set the DWA compression level,"extracted from #21324

(cherry picked from commit aa276ece66265dcf07af5f840933377d2106c871)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-10 10:57:12,feature,opencv dnn net = cv2.dnn.readNetFromONNX(w),"### System Information

Python 3.8.0
OpenCV version: '4.6.0'
OS : Windows10

### Detailed description

I trained the Yolov5 model on my own custom dataset.
I converted "".pt"" file to ONNX file. The ONNX model I created works fine.
I converted ONNX file to Quantize ONNX file. It cannot read the file ""yolov5n_quant.onnx"" that I created.
The error is as follows.

onnx for ONNX OpenCV DNN inference...
[ERROR:0@2.773] global D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\onn_importer.cpp (1021) cv::dnn::dnn4_v20220524::ONNXImporter::handleNode DNN/ONNX: Euring processing node with 1 inputs and 3 outputs: [DynamicQuantizeLinear]:(onnx_nages_QuantizeLinear) from domain='ai.onnx'
Traceback (most recent call last):
  File ""g:/yolov5-master/detect.py"", line 252, in <module>
    main(opt)
  File ""g:/yolov5-master/detect.py"", line 247, in main
    run(**vars(opt))
  File ""C:\\Users\\anaconda3\\envs\\yolov5\\lib\\site-packages\\torch\\autograd\\gradpy"", line 28, in decorate_context
 __init__
    net = cv2.dnn.readNetFromONNX(w)
cv2.error: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\o> Node [DynamicQuantizeLinear@ai.onnx]:(onnx_node!images_QuantizeLinear) parse errscale not found in const blobs in function 'cv::dnn::dnnscale not found in const b4_v20220524::ONNXImporter::getBlob' 

### Steps to reproduce

Model Input:

![image](https://user-images.githubusercontent.com/82284108/201072433-5a3f55d0-53c1-4df6-bb19-a49397a2656c.png)

Model Output:
![image](https://user-images.githubusercontent.com/82284108/201072647-6cf2c137-fb62-4e5a-8905-14e4e6b5b722.png)





### Issue submission checklist

- [X] I report the issue, it's not a question
- [X] I checked the problem with documentation, FAQ, open issues, forum.opencv.org, Stack Overflow, etc and have not found any solution
- [X] I updated to the latest OpenCV version and the issue is still there
- [ ] There is reproducer code and related data files (videos, images, onnx, etc)"
opencv/opencv,2022-11-07 08:11:17,feature,NPnP solver,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-04 02:56:26,feature,DNN: Add New API blobFromImageParam,"The purpose of this PR:

1. Add new API `blobFromImageParam` to extend `blobFromImage` API. It can support the different datalayout (NCHW or NHWC), and letter_box.
2. ~~`blobFromImage` can output `CV_16F`~~

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-11-01 07:55:16,feature,G-API Expose all imgproc operations to python,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake


### Motivation
Python bindings are available since a long time, but only a few operations (`operation::on(...)` wrappers) were exposed.
There was a great plan to implement feature in python parser that could automatically detect `G-API` operation (via `GAPI_TYPED_KERNEL` macro or so) and expose it into python, but this functionality is more about giving an opportunity to user to implement python `kernels` for already existing in G-API operations. 

This PR is going to expose just `operation::on` wrappers to python in order to give user everything that is available from c++, because now, for developers who don't build from source and change the code only available a small amount of functionality.
Since a lot of developers use `opencv` from `pip` let's expose it once and forever.

Great example of G-API usage: https://github.com/xiong-jie-y/g_api_examples
TODO list:
- [x] Expose `core`
- [x] Expose `imgproc`
- [ ] Expose `video` 
- [ ] Expose `stereo` 
- [ ] Other stuff (e.g constant initialization for `G-Type`'s & some compiler args) 

### Problems
Some of `imgproc` operations are still not exposed because of complexity of wrapping their return types to python.

* `GArray<GArray<Point>> findContours(...)` - 2 overloads
* `std::tuple<GArray<GArray<Point>>,GArray<Vec4i>> findContoursH(...)` - 2 overloads
* `GOpaque<Vec4f> fitLine2D(...)` - 4 overloads 
* `GOpaque<Vec6f> fitLine3D(...)` - 4 overloads
* `GMatP NV12toRGBp(...)`
* `GMatP NV12toBGRp(...)`
* ` GMatP resizeP(...)`

Issue: https://github.com/opencv/opencv/issues/22736"
opencv/opencv,2022-10-27 00:41:29,feature,Introduce libavdevice to make v4l2 available to the ffmpeg backend,"### Descripe the feature and motivation

You can follow my struggle to get ffmpeg to capture from a v4l2 device while accepting my precise options, here: https://github.com/opencv/opencv/issues/22622
By including libavdevice and calling ```avdevice_register_all();``` in my own code i was finally able to do it. Anyway, i think that call belongs into the ffmpeg backend (```void CvCapture_FFMPEG::init()```?).

### Additional context

_No response_"
opencv/opencv,2022-10-26 21:57:42,feature,log a debug message if a capture backend is generally available but isn't capabable of either capture by index or capture by filename,"### Descripe the feature and motivation

I am working on a few issues pertaining hw accelerated videoio and I experimented with the different backends and settings to get my application to capture from v4l2 while using vaapi for decoding among a few other things.
Anyway it took me considerable time to realize that e.g. the ffmpeg backend doesn't support selecting the capture device by index.
I would like to have at least debug message in place to alert the user to as why capturing failed.

### Additional context

```c++
VideoCapture cap(0, CAP_FFMPEG);
```
The above line fails without a hint on why."
opencv/opencv,2022-10-13 09:36:02,feature,dnn: add the CANN backend,"merge with adding a wiki page: https://gist.github.com/fengyuentau/083f7f339592545c1f1d2c1fde6a53dc

## Benchmark

Environment (provided by Ascend):
1. CPU: 20-core aarch64, freq unknown
2. NPU: Ascend 310

Time is in millisecond. The time of first run is excluded.

| Model | CPU backend | CANN backend (this PR) | Native CANN |
| - | - | - | - |
| PP-ResNet50 | 69.74 | 3.29 | 2.51 |
| MobileNetV1 | 6.60 | 1.21 | 0.85 |
| YOLOX | 265.90 | 12.80 | 9.11 |
| AlexNet | 13.19 | 3.82 | 3.36 |

**Notes**:
1. `PP-ResNet50`, `MobileNetV1` and `YOLOX` are from https://github.com/opencv/opencv_zoo.
1. `Native CANN` stands for loading and inferring a ATC-converted OM model with CANN interfaces. ATC is the model conversion from ONNX/TF/CAFFE to OM tool provided by CANN. `Native CANN` only measures the time of `aclmdlExecute`.
1. `CANN backend (this PR)` stands for loading ONNX/TF/CAFFE models with opencv parsers and inferring the OM model built by CANN backend. `CANN backend (this PR)` measures the time of `net.forward()`, including `aclmdlExecute` and some other overheads. Models built by CANN backend and converted by ATC have the same time of `aclmdlExecute`.
1. Differences between the implementations of this PR and the previous one:
    - This PR: use `aclgrphBuildModel` to fully optimize the graph and use `aclmdlExecute` to forward the graph directly.
    - Previous: use `aclopCompileAndExecute` to call and run operator on the fly.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-10-08 06:47:08,feature,DNN: need support of the GreaterOrEqual operator from ONNX,"### Descripe the feature and motivation

There is a contributor trying to contribute a model to opencv zoo. Link to the pull request: https://github.com/opencv/opencv_zoo/pull/82. And this model requires the support of GreaterOrEqual operator from ONNX.

### Additional context

There might be other operators that are not supported in the current version of OpenCV. Need to generate a list of operators to check whether there are other unsupported operators."
opencv/opencv,2022-09-27 04:29:37,feature,DNN: add enableWinograd API for Net,"Two task in the PR:
1. add `enableWinograd` API for Net
2. disable Winograd branch in try quantize func.

Winograd Convolution will effect the accuracy, which may lead to more errors in the calibration process of DNN on-fly-quantize (`net.quantize()`). This PR wants to disable the Winograd branch in convolution computation.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-09-23 13:23:08,feature,Enable issue template chooser,"This pull request enables template chooser with templates for bug report and feature request. See my fork for the effect: https://github.com/fengyuentau/opencv/issues/new/choose. Note these changes only take effect if they are put in the default branch.

Github documentation for configuring issue templates: https://docs.github.com/en/communities/using-templates-to-encourage-useful-issues-and-pull-requests/configuring-issue-templates-for-your-repository#configuring-the-template-chooser

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-09-15 11:44:17,feature,Stereo Calibration: Return rotation and transformation vectors for each calibration object,"### Motivation for feature

Extension of stereoCalibrate function for both, pinhole and fisheye model, to return the translation and rotation vectors between each calibration object and the coordinate system of the first camera of the stereo pair. This feature is helpful to evaluate the individual image pairs used for calibration. This feature is in particular interesting for the fisheye model since its stereoCalibrate function has not provided a parameter to obtain the reprojection errors per stereo image pair. With these per-view transformations available, it is now possible to calculate these reprojection errors. In addition, for the pinhole model, it is also interesting to not only get the per-view rms errors, but also statistics within one view.
For the implementation, the single camera calibration methods were used as example and all previous available function calls should still work.

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-09-12 14:19:38,feature,Extracted matches_confindece_thresh as stitching matcher parameter.,"Fixes #11084

Replacement for https://github.com/opencv/opencv/pull/19955/
### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-09-11 21:44:24,feature,ffmpeg/4.x: update FFmpeg wrapper 2022.09,"**Merge with 3rdparty**: https://github.com/opencv/opencv_3rdparty/pull/73

- FFmpeg 4.4.2
- added AV1 support through aom 3.4.0: https://aomedia.googlesource.com/aom/+/refs/tags/v3.4.0
- use Ubuntu 18.04 => 20.04 as build image

resolves #11389

previous update: https://github.com/opencv/opencv/pull/22034

```
force_builders=Win64,Win32
```"
opencv/opencv,2022-09-09 14:40:17,feature,Android Video Writter support for H264 / H265,"Currently on Android only MJPG encoder.
I would be nice to support more formats (H264 / H265).

##### System information (version)
- OpenCV => 4.x
- Operating System / Platform => Android
- Compiler => clang

##### Detailed description

Usually Android devices have hardware accelerated encoders for H264/H265.
I would be great to have from in OpenCV.

##### Issue submission checklist

 - [x] I report the issue, it's not a question
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
 - [x] I updated to the latest OpenCV version and the issue is still there
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
"
opencv/opencv,2022-08-30 20:57:27,feature,G-API: Introduce abstract base classes for GExecutor and GStreamingExecutor,"The classes are `GAbstractExecutor` and `GAbstractStreamingExecutor`, respectively.

The idea behind this simple refactoring is to introduce other execution policies (implemented by executors) in the near future.

Note: this PR replaces obsolete #21762 and #21816 -- which now can be closed.

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-08-26 08:09:17,feature,tiff support 4-bit palette,"Requires https://github.com/opencv/opencv_extra/pull/999

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake

opencv_extra=4-bit_palette_color"
opencv/opencv,2022-08-21 19:20:57,feature,darknet_io.cpp:250: error: (-212:Parsing error) Unsupported activation: silu in function 'setActivation',"<!--
If you have a question rather than reporting a bug please go to https://forum.opencv.org where you get much faster responses.
If you need further assistance please read [How To Contribute](https://github.com/opencv/opencv/wiki/How_to_contribute).

This is a template helping you to create an issue which can be processed as quickly as possible. This is the bug reporting section for the OpenCV library.
-->

##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => 4.6.0
- Operating System / Platform => Ubuntu 20.04
- Compiler => gcc10

##### Detailed description

Using Opencv 4.6.0 and the new yolov7 .weights and config (.cfg) from https://github.com/WongKinYiu/yolov7/tree/darknet causes the error -> cv2/opencv-4.6.0/modules/dnn/src/darknet/darknet_io.cpp:250: error: (-212:Parsing error) Unsupported activation: silu in function 'setActivation'

weights -> https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.weights
config -> https://github.com/WongKinYiu/yolov7/blob/darknet/cfg/yolov7x_darknet.cfg
alternate config -> https://github.com/WongKinYiu/yolov7/blob/darknet/cfg/yolov7x.cfg

##### Steps to reproduce
```
# python3.9 code
import cv2
## ommited irrelevant code, the readNet method is where the issue occurs.
model_input_file = ""path/to/yolov7x.weights""
model_config_file = ""path/to/yolov7x_darknet.cfg"" or ""path/to/yolov7x.cfg""
try:
    net = cv2.dnn.readNet(model_input_file, model_config_file)
except Exception as e:
    print(f""exception when loading in weights and config -> {e}"")
#### code continues
```
<!-- to add code example fence it with triple backticks and optional file extension
    ```.cpp
    // C++ code example
    ```
 or attach as .txt or .zip file
-->

##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [x] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-08-10 11:58:44,feature,Remove asymmetric padding in Conv layer since it is supported in CPU backend,"In https://github.com/opencv/opencv/pull/21910, fast convolution is introduced with support for asymmetric padding in calculation. So there is no need to keep a separate padding layer for asymmetric padding in convolution layer.

## Regression test for convolution layer with asymmetric padding:

https://github.com/opencv/opencv/blob/d09cc0f30c708ca06ad1a05f5b39f6368d986fad/modules/dnn/test/test_onnx_importer.cpp#L118-L122

OpenVINO, Default CPU (CPU, OCL, Tengine), Vulkan, CUDA, WebNN are registered for this test.

## Checks on different backends
- [x] OpenCV CPU
- [x] OpenCL
- [x] OpenVINO (should be good since it builds its padding from pads_begin & pads_end, see code [here](https://github.com/opencv/opencv/blob/a1d5565e65548bfbb9cafccf3016eca7994cfd85/modules/dnn/src/layers/convolution_layer.cpp#L838-L839) and [here](https://github.com/opencv/opencv/blob/a1d5565e65548bfbb9cafccf3016eca7994cfd85/modules/dnn/src/layers/convolution_layer.cpp#L846-L847))
- [x] CUDA
- [ ] ~Vulkan~ I am getting segmentation fault regardless my changes
- [ ] WebNN (should be good since it builds its padding from pads_begin & pads_end, see code [here](https://github.com/opencv/opencv/blob/a1d5565e65548bfbb9cafccf3016eca7994cfd85/modules/dnn/src/layers/convolution_layer.cpp#L936-L939))
- [x] Tengine (failed at `Test_ONNX_layers.Convolution_variable_weight_bias/0` but should not be related to this patch)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-08-06 07:01:47,feature,Changes separated from Mat 1D support in core #18594,"Changes separated from #18594

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-07-20 02:26:59,feature,DNN: FP16 support on Convolution 2D,"## FP16 support on ARM platform
This PR proposes to support FP16 backend in Convolution. 
For now, we only support FP16 at ARM aarch64.

In addition to adding fp16, I also added `seperateIm2col` optimization in this patch.

## How to use FP16 to speed up convolution?
```
Net net = readNet(modelPath);
net.setPreferableTarget(DNN_TARGET_CPU_FP16);
net.setInput(blob);
Mat output = net.forward();
```


### TODO List
| Task | Status | Remarks |
|:-------:|:--------:|:------------:|
| Convolution 2D FP16 | :heavy_check_mark: | Done |
| Winograd FP16 | Because the current modification has reached 2k lines, winograd fp16 will be completed in the next PR. |  |
| Accuracy Test | :heavy_check_mark: | Done |
| Performance Test | :heavy_check_mark: | Done |
| Compiler bug | :heavy_check_mark: | Done |

### Speed Test for FP 16.

**Test on M1 chip, 4 threads.**

| Model Name | FP32 (Conv+Wino) | Conv(FP16) + Wino(FP 32) |
|:-------:|:--------:|:------------:|
| ReseNet 50 | 26.0 ms | **18.05 ms** (25% speed up)|
| MobileNet V2 | 4.17 ms | **3.09 ms (29% speed up)** |

### Speed Test for `seperateIm2col` trick on X86.
**Test on AMD 5600x, 12 threads.**
| Model Name | 4.x | Patch |
|:-------:|:--------:|:------------:|
| MobileNet V2 | 5.6 ms | **3.0 ms (46% speed up)** |

### Performance Test

#### Performance Test of X86 platform: AMD 5600X, with `-perf_threas=1`
|Name of Test|4.x|patch|patch vs 4.x (x-factor)|
|---|:-:|:-:|:-:|
|Name of Test|4.x 0|fp16pr final|fp16pr final vs 4.x 0 (x-factor)|
|---|:-:|:-:|:-:|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 2, 19}, OCN=2, G=2, S=2, P=(1, 1), BIAS, OCV/CPU)|0.001|0.001|1.00|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 2, 25}, OCN=2, G=2, P=(2, 2), PM=SAME, OCV/CPU)|0.001|0.001|1.03|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 6, 10}, OCN=6, PM=VALID, BIAS, OCV/CPU)|0.001|0.001|0.92|
|conv3d::Conv3D::(GFLOPS=0.000, K=[1 x 1 x 1], IN={1, 4, 9, 10, 10}, OCN=4, S=[1 x 1 x 2], P=(1, 1) x (1, 1) x (1, 1), PM=VALID, OCV/CPU)|0.002|0.003|0.95|
|conv3d::Conv3D::(GFLOPS=0.000, K=[1 x 1 x 1], IN={1, 8, 1, 10, 10}, OCN=8, G=8, P=(1, 1) x (1, 1) x (1, 1), BIAS, OCV/CPU)|0.006|0.006|1.00|
|conv3d::Conv3D::(GFLOPS=0.000, K=[3 x 3 x 3], IN={1, 2, 19, 19, 19}, OCN=2, G=2, S=[2 x 2 x 2], P=(1, 1) x (1, 1) x (1, 1), BIAS, OCV/CPU)|0.045|0.033|1.39|
|conv3d::Conv3D::(GFLOPS=0.000, K=[3 x 4 x 2], IN={1, 4, 8, 10, 10}, OCN=4, G=4, S=[1 x 2 x 1], BIAS, OCV/CPU)|0.011|0.009|1.17|
|conv3d::Conv3D::(GFLOPS=0.001, K=[3 x 3 x 3], IN={1, 2, 25, 19, 19}, OCN=2, G=2, S=[1 x 2 x 2], P=(2, 2) x (2, 2) x (2, 2), PM=SAME, OCV/CPU)|0.109|0.078|1.39|
|conv3d::Conv3D::(GFLOPS=0.002, K=[3 x 1 x 4], IN={1, 14, 5, 10, 10}, OCN=14, PM=SAME, OCV/CPU)|0.040|0.042|0.94|
|conv3d::Conv3D::(GFLOPS=0.006, K=[5 x 5 x 5], IN={1, 4, 50, 19, 19}, OCN=4, S=[2 x 2 x 2], P=(1, 1) x (1, 1) x (1, 1), PM=VALID, OCV/CPU)|0.326|0.342|0.95|
|conv3d::Conv3D::(GFLOPS=0.027, K=[3 x 3 x 3], IN={1, 6, 10, 38, 50}, OCN=6, PM=VALID, BIAS, OCV/CPU)|0.580|0.589|0.99|
|conv3d::Conv3D::(GFLOPS=0.030, K=[5 x 5 x 5], IN={1, 6, 19, 19, 19}, OCN=6, G=2, OCV/CPU)|1.293|1.382|0.94|
|conv3d::Conv3D::(GFLOPS=0.045, K=[7 x 7 x 7], IN={1, 2, 38, 38, 38}, OCN=2, S=[1 x 2 x 1], OCV/CPU)|3.590|3.710|0.97|
|conv3d::Conv3D::(GFLOPS=0.053, K=[3 x 3 x 3], IN={1, 10, 98, 10, 10}, OCN=10, PM=SAME, OCV/CPU)|1.120|1.191|0.94|
|conv3d::Conv3D::(GFLOPS=0.071, K=[7 x 7 x 7], IN={1, 6, 15, 19, 19}, OCN=6, S=[2 x 1 x 1], P=(3, 3) x (3, 3) x (3, 3), PM=SAME, BIAS, OCV/CPU)|2.576|2.872|0.90|
|conv3d::Conv3D::(GFLOPS=0.093, K=[5 x 5 x 5], IN={1, 4, 40, 75, 75}, OCN=4, S=[2 x 2 x 2], OCV/CPU)|4.599|4.670|0.98|
|conv3d::Conv3D::(GFLOPS=0.116, K=[5 x 5 x 5], IN={1, 2, 21, 75, 100}, OCN=2, BIAS, OCV/CPU)|9.230|9.582|0.96|
|conv3d::Conv3D::(GFLOPS=1.267, K=[5 x 5 x 5], IN={1, 3, 75, 75, 100}, OCN=3, PM=SAME, BIAS, OCV/CPU)|65.946|69.381|0.95|
|conv3d::Conv3D::(GFLOPS=1.343, K=[3 x 3 x 3], IN={1, 11, 9, 150, 200}, OCN=11, PM=VALID, BIAS, OCV/CPU)|18.915|19.289|0.98|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 512, 26, 26}, OCN=256, OCV/CPU)|1.404|1.457|0.96|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 1024, 13, 13}, OCN=512, OCV/CPU)|2.060|1.501|1.37|
|conv::Conv::(GFLOPS=0.178, K=[1 x 1], IN={1, 256, 52, 52}, OCN=128, OCV/CPU)|1.409|1.464|0.96|
|conv::Conv::(GFLOPS=0.210, K=[1 x 1], IN={1, 576, 38, 50}, OCN=96, PM=SAME, BIAS, OCV/CPU)|1.793|1.838|0.98|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 128, 56, 56}, OCN=32, P=[1 x 1], OCV/CPU)|1.207|1.199|1.01|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 256, 14, 14}, OCN=256, P=[1 x 1], OCV/CPU)|1.277|1.275|1.00|
|conv::Conv::(GFLOPS=0.280, K=[1 x 1], IN={1, 576, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|2.319|2.370|0.98|
|conv::Conv::(GFLOPS=0.302, K=[3 x 3], IN={1, 64, 64, 64}, OCN=64, PM=SAME, OCV/CPU)|1.351|1.346|1.00|
|conv::Conv::(GFLOPS=0.357, K=[1 x 1], IN={1, 64, 208, 208}, OCN=64, OCV/CPU)|3.520|3.612|0.97|
|conv::Conv::(GFLOPS=0.420, K=[3 x 3], IN={1, 96, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|1.876|1.880|1.00|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 128, 40, 40}, OCN=128, PM=SAME, OCV/CPU)|1.981|1.995|0.99|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 256, 20, 20}, OCN=256, PM=SAME, OCV/CPU)|2.620|2.627|1.00|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 512, 10, 10}, OCN=512, PM=SAME, OCV/CPU)|4.202|4.123|1.02|
|conv::Conv::(GFLOPS=0.561, K=[3 x 3], IN={1, 128, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|2.429|2.445|0.99|
|conv::Conv::(GFLOPS=0.624, K=[3 x 3], IN={1, 128, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|2.591|2.576|1.01|
|conv::Conv::(GFLOPS=0.701, K=[3 x 3], IN={1, 128, 38, 50}, OCN=160, PM=SAME, BIAS, OCV/CPU)|3.005|2.998|1.00|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 64, 104, 104}, OCN=64, P=[1 x 1], OCV/CPU)|3.515|3.532|1.00|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 128, 52, 52}, OCN=128, P=[1 x 1], OCV/CPU)|3.115|3.134|0.99|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 256, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU)|3.937|3.899|1.01|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 512, 13, 13}, OCN=512, P=[1 x 1], OCV/CPU)|5.533|5.471|1.01|
|conv::Conv::(GFLOPS=0.830, K=[3 x 3], IN={1, 64, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU)|3.472|3.464|1.00|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 192, 38, 38}, OCN=192, PM=SAME, OCV/CPU)|4.302|4.322|1.00|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 384, 19, 19}, OCN=384, PM=SAME, OCV/CPU)|6.100|6.035|1.01|
|conv::Conv::(GFLOPS=1.022, K=[3 x 3], IN={1, 576, 19, 19}, OCN=273, PM=SAME, BIAS, OCV/CPU)|6.580|6.484|1.01|
|conv::Conv::(GFLOPS=1.112, K=[3 x 3], IN={1, 512, 10, 10}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU)|9.741|9.634|1.01|
|conv::Conv::(GFLOPS=1.181, K=[3 x 3], IN={1, 64, 160, 200}, OCN=128, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU)|10.131|10.156|1.00|
|conv::Conv::(GFLOPS=1.182, K=[3 x 3], IN={1, 32, 320, 400}, OCN=64, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU)|12.391|12.350|1.00|
|conv::Conv::(GFLOPS=1.195, K=[9 x 9], IN={1, 32, 240, 320}, OCN=3, P=[4 x 4], BIAS, OCV/CPU)|91.074|87.893|1.04|
|conv::Conv::(GFLOPS=1.196, K=[3 x 3], IN={1, 384, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU)|5.903|5.903|1.00|
|conv::Conv::(GFLOPS=1.210, K=[3 x 3], IN={1, 32, 256, 256}, OCN=32, PM=SAME, OCV/CPU)|6.890|6.794|1.01|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 64, 75, 75}, OCN=192, PM=SAME, BIAS, OCV/CPU)|5.160|5.131|1.01|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 96, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU)|4.970|5.036|0.99|
|conv::Conv::(GFLOPS=1.248, K=[3 x 3], IN={1, 256, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|5.045|5.015|1.01|
|conv::Conv::(GFLOPS=1.258, K=[3 x 3], IN={1, 1280, 10, 10}, OCN=546, PM=SAME, BIAS, OCV/CPU)|11.583|11.343|1.02|
|conv::Conv::(GFLOPS=1.261, K=[3 x 3], IN={1, 192, 38, 50}, OCN=192, PM=SAME, BIAS, OCV/CPU)|5.348|5.320|1.01|
|conv::Conv::(GFLOPS=1.416, K=[3 x 3], IN={1, 128, 62, 82}, OCN=128, BIAS, OCV/CPU)|5.357|5.396|0.99|
|conv::Conv::(GFLOPS=1.500, K=[3 x 3], IN={1, 128, 64, 84}, OCN=128, BIAS, OCV/CPU)|6.050|6.006|1.01|
|conv::Conv::(GFLOPS=1.586, K=[3 x 3], IN={1, 128, 66, 86}, OCN=128, BIAS, OCV/CPU)|5.952|5.953|1.00|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 26, 26}, OCN=512, P=[1 x 1], OCV/CPU)|8.014|8.014|1.00|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 52, 52}, OCN=512, S=[2 x 2], P=[1 x 1], OCV/CPU)|12.472|12.577|0.99|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 13, 13}, OCN=1024, P=[1 x 1], OCV/CPU)|10.803|10.655|1.01|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 26, 26}, OCN=1024, S=[2 x 2], P=[1 x 1], OCV/CPU)|18.429|13.405|1.37|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 104, 104}, OCN=128, P=[1 x 1], OCV/CPU)|6.659|6.647|1.00|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 208, 208}, OCN=128, S=[2 x 2], P=[1 x 1], OCV/CPU)|14.192|13.819|1.03|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 52, 52}, OCN=256, P=[1 x 1], OCV/CPU)|6.045|6.068|1.00|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 104, 104}, OCN=256, S=[2 x 2], P=[1 x 1], OCV/CPU)|12.742|12.828|0.99|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 208, 208}, OCN=64, P=[1 x 1], OCV/CPU)|8.046|7.773|1.04|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 416, 416}, OCN=64, S=[2 x 2], P=[1 x 1], OCV/CPU)|17.440|17.192|1.01|
|conv::Conv::(GFLOPS=1.659, K=[3 x 3], IN={1, 960, 10, 10}, OCN=960, PM=SAME, OCV/CPU)|15.418|14.972|1.03|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, G=128, P=[1 x 1], BIAS, OCV/CPU)|0.430|0.430|1.00|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, PM=SAME, OCV/CPU)|6.692|6.663|1.00|
|conv::Conv::(GFLOPS=1.675, K=[3 x 3], IN={1, 128, 68, 88}, OCN=128, BIAS, OCV/CPU)|6.350|6.347|1.00|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, G=256, P=[1 x 1], BIAS, OCV/CPU)|0.267|0.265|1.01|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, PM=SAME, OCV/CPU)|7.755|7.558|1.03|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, G=512, P=[1 x 1], BIAS, OCV/CPU)|0.203|0.202|1.00|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|10.663|10.576|1.01|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, PM=SAME, OCV/CPU)|10.827|10.614|1.02|
|conv::Conv::(GFLOPS=1.766, K=[3 x 3], IN={1, 128, 70, 90}, OCN=128, BIAS, OCV/CPU)|7.049|6.947|1.01|
|conv::Conv::(GFLOPS=1.859, K=[3 x 3], IN={1, 128, 72, 92}, OCN=128, BIAS, OCV/CPU)|6.900|6.901|1.00|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, G=1024, P=[1 x 1], BIAS, OCV/CPU)|0.165|0.165|1.00|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, PM=SAME, OCV/CPU)|17.953|17.251|1.04|
|conv::Conv::(GFLOPS=1.954, K=[3 x 3], IN={1, 128, 74, 94}, OCN=128, BIAS, OCV/CPU)|7.430|7.320|1.01|
|conv::Conv::(GFLOPS=1.995, K=[9 x 9], IN={1, 3, 320, 400}, OCN=32, P=[4 x 4], BIAS, OCV/CPU)|22.187|21.705|1.02|
|conv::Conv::(GFLOPS=2.052, K=[3 x 3], IN={1, 128, 76, 96}, OCN=128, BIAS, OCV/CPU)|8.349|8.126|1.03|
|conv::Conv::(GFLOPS=2.100, K=[3 x 3], IN={1, 144, 75, 75}, OCN=144, PM=SAME, OCV/CPU)|8.273|8.297|1.00|
|conv::Conv::(GFLOPS=2.153, K=[3 x 3], IN={1, 128, 78, 98}, OCN=128, BIAS, OCV/CPU)|8.169|8.094|1.01|
|conv::Conv::(GFLOPS=2.156, K=[3 x 3], IN={1, 576, 19, 19}, OCN=576, PM=SAME, OCV/CPU)|13.602|13.359|1.02|
|conv::Conv::(GFLOPS=2.255, K=[3 x 3], IN={1, 128, 80, 100}, OCN=128, BIAS, OCV/CPU)|8.633|8.584|1.01|
|conv::Conv::(GFLOPS=2.719, K=[3 x 3], IN={1, 96, 256, 256}, OCN=96, S=[2 x 2], PM=SAME, OCV/CPU)|29.339|28.897|1.02|
|conv::Conv::(GFLOPS=3.319, K=[3 x 3], IN={1, 128, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|13.000|12.920|1.01|
|conv::Conv::(GFLOPS=3.321, K=[3 x 3], IN={1, 64, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|14.262|13.319|1.07|
|conv::Conv::(GFLOPS=3.398, K=[7 x 7], IN={1, 128, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU)|27.453|27.253|1.01|
|conv::Conv::(GFLOPS=3.407, K=[3 x 3], IN={1, 512, 19, 19}, OCN=1024, D=[6 x 6], P=[6 x 6], BIAS, OCV/CPU)|32.052|27.269|1.18|
|conv::Conv::(GFLOPS=3.408, K=[3 x 3], IN={1, 256, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|15.363|15.208|1.01|
|conv::Conv::(GFLOPS=4.247, K=[3 x 3], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU)|18.543|18.434|1.01|
|conv::Conv::(GFLOPS=4.247, K=[5 x 5], IN={1, 144, 128, 128}, OCN=144, S=[2 x 2], PM=SAME, OCV/CPU)|39.114|37.954|1.03|
|conv::Conv::(GFLOPS=4.566, K=[7 x 7], IN={1, 172, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU)|36.271|36.972|0.98|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 256, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|19.262|19.427|0.99|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 512, 46, 46}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|19.298|19.349|1.00|
|conv::Conv::(GFLOPS=4.994, K=[3 x 3], IN={1, 128, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|20.261|19.847|1.02|
|conv::Conv::(GFLOPS=4.997, K=[3 x 3], IN={1, 64, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|21.867|21.525|1.02|
|conv::Conv::(GFLOPS=5.780, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, S=[2 x 2], PM=SAME, OCV/CPU)|51.756|49.979|1.04|
|conv::Conv::(GFLOPS=6.116, K=[3 x 3], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU)|28.133|27.060|1.04|
|conv::Conv::(GFLOPS=6.118, K=[3 x 3], IN={1, 144, 128, 128}, OCN=144, PM=SAME, OCV/CPU)|25.035|24.980|1.00|
|conv::Conv::(GFLOPS=6.637, K=[3 x 3], IN={1, 256, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|25.858|25.821|1.00|
|conv::Conv::(GFLOPS=6.638, K=[3 x 3], IN={1, 128, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|27.313|27.149|1.01|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 150, 200}, OCN=192, PM=SAME, BIAS, OCV/CPU)|28.219|28.111|1.00|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 300, 300}, OCN=64, P=[1 x 1], BIAS, OCV/CPU)|46.025|46.674|0.99|
|conv::Conv::(GFLOPS=6.814, K=[3 x 3], IN={1, 512, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|30.220|29.446|1.03|
|conv::Conv::(GFLOPS=8.025, K=[3 x 3], IN={1, 1024, 19, 19}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU)|49.410|48.708|1.01|
|conv::Conv::(GFLOPS=9.986, K=[3 x 3], IN={1, 512, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|38.203|38.001|1.01|
|conv::Conv::(GFLOPS=9.987, K=[3 x 3], IN={1, 256, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|39.961|39.021|1.02|
|conv::Conv::(GFLOPS=9.989, K=[3 x 3], IN={1, 128, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|48.685|47.075|1.03|
|conv::Conv::(GFLOPS=9.993, K=[3 x 3], IN={1, 64, 368, 368}, OCN=64, P=[1 x 1], BIAS, OCV/CPU)|75.114|72.586|1.03|
|conv::Conv::(GFLOPS=10.087, K=[3 x 3], IN={1, 576, 38, 50}, OCN=512, PM=SAME, BIAS, OCV/CPU)|41.222|41.144|1.00|
|conv::Conv::(GFLOPS=10.701, K=[3 x 3], IN={1, 512, 38, 38}, OCN=804, P=[1 x 1], BIAS, OCV/CPU)|46.220|46.353|1.00|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 240, 64, 64}, OCN=240, PM=SAME, OCV/CPU)|98.201|98.771|0.99|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU)|100.106|96.971|1.03|
|conv::Conv::(GFLOPS=16.987, K=[5 x 5], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU)|146.977|140.445|1.05|
|conv::Conv::(GFLOPS=23.122, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, PM=SAME, OCV/CPU)|198.618|194.665|1.02|


#### Performance Test of ARM platform: apple M1, with `-perf_threas=1`

Min (ms)

|Name of Test|4.x|patch|4.x vs patch (x-factor)|
|---|:-:|:-:|:-:|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 2, 19}, OCN=2, G=2, S=2, P=(1, 1), BIAS, OCV/CPU)|0.001|0.001|1.07|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 2, 25}, OCN=2, G=2, P=(2, 2), PM=SAME, OCV/CPU)|0.001|0.001|1.10|
|conv1d::Conv1D::(GFLOPS=0.000, K=[3], IN={1, 6, 10}, OCN=6, PM=VALID, BIAS, OCV/CPU)|0.002|0.002|0.97|
|conv3d::Conv3D::(GFLOPS=0.000, K=[1 x 1 x 1], IN={1, 4, 9, 10, 10}, OCN=4, S=[1 x 1 x 2], P=(1, 1) x (1, 1) x (1, 1), PM=VALID, OCV/CPU)|0.003|0.003|0.84|
|conv3d::Conv3D::(GFLOPS=0.000, K=[1 x 1 x 1], IN={1, 8, 1, 10, 10}, OCN=8, G=8, P=(1, 1) x (1, 1) x (1, 1), BIAS, OCV/CPU)|0.009|0.009|1.00|
|conv3d::Conv3D::(GFLOPS=0.000, K=[3 x 3 x 3], IN={1, 2, 19, 19, 19}, OCN=2, G=2, S=[2 x 2 x 2], P=(1, 1) x (1, 1) x (1, 1), BIAS, OCV/CPU)|0.027|0.030|0.90|
|conv3d::Conv3D::(GFLOPS=0.000, K=[3 x 4 x 2], IN={1, 4, 8, 10, 10}, OCN=4, G=4, S=[1 x 2 x 1], BIAS, OCV/CPU)|0.008|0.007|1.07|
|conv3d::Conv3D::(GFLOPS=0.001, K=[3 x 3 x 3], IN={1, 2, 25, 19, 19}, OCN=2, G=2, S=[1 x 2 x 2], P=(2, 2) x (2, 2) x (2, 2), PM=SAME, OCV/CPU)|0.066|0.072|0.91|
|conv3d::Conv3D::(GFLOPS=0.002, K=[3 x 1 x 4], IN={1, 14, 5, 10, 10}, OCN=14, PM=SAME, OCV/CPU)|0.090|0.054|1.68|
|conv3d::Conv3D::(GFLOPS=0.006, K=[5 x 5 x 5], IN={1, 4, 50, 19, 19}, OCN=4, S=[2 x 2 x 2], P=(1, 1) x (1, 1) x (1, 1), PM=VALID, OCV/CPU)|0.328|0.409|0.80|
|conv3d::Conv3D::(GFLOPS=0.027, K=[3 x 3 x 3], IN={1, 6, 10, 38, 50}, OCN=6, PM=VALID, BIAS, OCV/CPU)|0.659|0.697|0.95|
|conv3d::Conv3D::(GFLOPS=0.030, K=[5 x 5 x 5], IN={1, 6, 19, 19, 19}, OCN=6, G=2, OCV/CPU)|1.266|1.403|0.90|
|conv3d::Conv3D::(GFLOPS=0.045, K=[7 x 7 x 7], IN={1, 2, 38, 38, 38}, OCN=2, S=[1 x 2 x 1], OCV/CPU)|3.550|4.145|0.86|
|conv3d::Conv3D::(GFLOPS=0.053, K=[3 x 3 x 3], IN={1, 10, 98, 10, 10}, OCN=10, PM=SAME, OCV/CPU)|1.188|1.375|0.86|
|conv3d::Conv3D::(GFLOPS=0.071, K=[7 x 7 x 7], IN={1, 6, 15, 19, 19}, OCN=6, S=[2 x 1 x 1], P=(3, 3) x (3, 3) x (3, 3), PM=SAME, BIAS, OCV/CPU)|2.683|3.236|0.83|
|conv3d::Conv3D::(GFLOPS=0.093, K=[5 x 5 x 5], IN={1, 4, 40, 75, 75}, OCN=4, S=[2 x 2 x 2], OCV/CPU)|4.491|5.501|0.82|
|conv3d::Conv3D::(GFLOPS=0.116, K=[5 x 5 x 5], IN={1, 2, 21, 75, 100}, OCN=2, BIAS, OCV/CPU)|8.916|10.181|0.88|
|conv3d::Conv3D::(GFLOPS=1.267, K=[5 x 5 x 5], IN={1, 3, 75, 75, 100}, OCN=3, PM=SAME, BIAS, OCV/CPU)|69.995|72.296|0.97|
|conv3d::Conv3D::(GFLOPS=1.343, K=[3 x 3 x 3], IN={1, 11, 9, 150, 200}, OCN=11, PM=VALID, BIAS, OCV/CPU)|22.531|23.139|0.97|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 512, 26, 26}, OCN=256, OCV/CPU)|2.239|1.933|1.16|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 512, 26, 26}, OCN=256, OCV/CPU_FP16)|-|1.010|-|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 1024, 13, 13}, OCN=512, OCV/CPU)|3.134|2.068|1.52|
|conv::Conv::(GFLOPS=0.177, K=[1 x 1], IN={1, 1024, 13, 13}, OCN=512, OCV/CPU_FP16)|-|1.062|-|
|conv::Conv::(GFLOPS=0.178, K=[1 x 1], IN={1, 256, 52, 52}, OCN=128, OCV/CPU)|1.918|1.920|1.00|
|conv::Conv::(GFLOPS=0.178, K=[1 x 1], IN={1, 256, 52, 52}, OCN=128, OCV/CPU_FP16)|-|1.014|-|
|conv::Conv::(GFLOPS=0.210, K=[1 x 1], IN={1, 576, 38, 50}, OCN=96, PM=SAME, BIAS, OCV/CPU)|2.340|2.352|0.99|
|conv::Conv::(GFLOPS=0.210, K=[1 x 1], IN={1, 576, 38, 50}, OCN=96, PM=SAME, BIAS, OCV/CPU_FP16)|-|1.247|-|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 128, 56, 56}, OCN=32, P=[1 x 1], OCV/CPU)|1.116|1.111|1.00|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 128, 56, 56}, OCN=32, P=[1 x 1], OCV/CPU_FP16)|-|1.114|-|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 256, 14, 14}, OCN=256, P=[1 x 1], OCV/CPU)|1.116|1.112|1.00|
|conv::Conv::(GFLOPS=0.231, K=[3 x 3], IN={1, 256, 14, 14}, OCN=256, P=[1 x 1], OCV/CPU_FP16)|-|1.113|-|
|conv::Conv::(GFLOPS=0.280, K=[1 x 1], IN={1, 576, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|3.067|3.085|0.99|
|conv::Conv::(GFLOPS=0.280, K=[1 x 1], IN={1, 576, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU_FP16)|-|1.622|-|
|conv::Conv::(GFLOPS=0.302, K=[3 x 3], IN={1, 64, 64, 64}, OCN=64, PM=SAME, OCV/CPU)|1.153|1.187|0.97|
|conv::Conv::(GFLOPS=0.302, K=[3 x 3], IN={1, 64, 64, 64}, OCN=64, PM=SAME, OCV/CPU_FP16)|-|1.150|-|
|conv::Conv::(GFLOPS=0.357, K=[1 x 1], IN={1, 64, 208, 208}, OCN=64, OCV/CPU)|4.804|4.849|0.99|
|conv::Conv::(GFLOPS=0.357, K=[1 x 1], IN={1, 64, 208, 208}, OCN=64, OCV/CPU_FP16)|-|2.922|-|
|conv::Conv::(GFLOPS=0.420, K=[3 x 3], IN={1, 96, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|1.463|1.469|1.00|
|conv::Conv::(GFLOPS=0.420, K=[3 x 3], IN={1, 96, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU_FP16)|-|1.459|-|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 128, 40, 40}, OCN=128, PM=SAME, OCV/CPU)|1.577|1.580|1.00|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 128, 40, 40}, OCN=128, PM=SAME, OCV/CPU_FP16)|-|1.580|-|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 256, 20, 20}, OCN=256, PM=SAME, OCV/CPU)|1.826|1.818|1.00|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 256, 20, 20}, OCN=256, PM=SAME, OCV/CPU_FP16)|-|1.817|-|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 512, 10, 10}, OCN=512, PM=SAME, OCV/CPU)|6.541|5.081|1.29|
|conv::Conv::(GFLOPS=0.472, K=[3 x 3], IN={1, 512, 10, 10}, OCN=512, PM=SAME, OCV/CPU_FP16)|-|2.809|-|
|conv::Conv::(GFLOPS=0.561, K=[3 x 3], IN={1, 128, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU)|1.912|1.919|1.00|
|conv::Conv::(GFLOPS=0.561, K=[3 x 3], IN={1, 128, 38, 50}, OCN=128, PM=SAME, BIAS, OCV/CPU_FP16)|-|1.919|-|
|conv::Conv::(GFLOPS=0.624, K=[3 x 3], IN={1, 128, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|1.961|1.971|0.99|
|conv::Conv::(GFLOPS=0.624, K=[3 x 3], IN={1, 128, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|1.961|-|
|conv::Conv::(GFLOPS=0.701, K=[3 x 3], IN={1, 128, 38, 50}, OCN=160, PM=SAME, BIAS, OCV/CPU)|2.317|2.329|0.99|
|conv::Conv::(GFLOPS=0.701, K=[3 x 3], IN={1, 128, 38, 50}, OCN=160, PM=SAME, BIAS, OCV/CPU_FP16)|-|2.322|-|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 64, 104, 104}, OCN=64, P=[1 x 1], OCV/CPU)|2.920|2.947|0.99|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 64, 104, 104}, OCN=64, P=[1 x 1], OCV/CPU_FP16)|-|2.924|-|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 128, 52, 52}, OCN=128, P=[1 x 1], OCV/CPU)|2.467|2.466|1.00|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 128, 52, 52}, OCN=128, P=[1 x 1], OCV/CPU_FP16)|-|2.496|-|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 256, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU)|3.028|2.997|1.01|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 256, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU_FP16)|-|2.986|-|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 512, 13, 13}, OCN=512, P=[1 x 1], OCV/CPU)|4.353|4.355|1.00|
|conv::Conv::(GFLOPS=0.798, K=[3 x 3], IN={1, 512, 13, 13}, OCN=512, P=[1 x 1], OCV/CPU_FP16)|-|4.355|-|
|conv::Conv::(GFLOPS=0.830, K=[3 x 3], IN={1, 64, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU)|2.762|2.793|0.99|
|conv::Conv::(GFLOPS=0.830, K=[3 x 3], IN={1, 64, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU_FP16)|-|2.797|-|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 192, 38, 38}, OCN=192, PM=SAME, OCV/CPU)|3.428|3.226|1.06|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 192, 38, 38}, OCN=192, PM=SAME, OCV/CPU_FP16)|-|3.223|-|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 384, 19, 19}, OCN=384, PM=SAME, OCV/CPU)|3.967|3.957|1.00|
|conv::Conv::(GFLOPS=0.958, K=[3 x 3], IN={1, 384, 19, 19}, OCN=384, PM=SAME, OCV/CPU_FP16)|-|3.960|-|
|conv::Conv::(GFLOPS=1.022, K=[3 x 3], IN={1, 576, 19, 19}, OCN=273, PM=SAME, BIAS, OCV/CPU)|4.806|4.387|1.10|
|conv::Conv::(GFLOPS=1.022, K=[3 x 3], IN={1, 576, 19, 19}, OCN=273, PM=SAME, BIAS, OCV/CPU_FP16)|-|4.366|-|
|conv::Conv::(GFLOPS=1.112, K=[3 x 3], IN={1, 512, 10, 10}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU)|14.509|11.756|1.23|
|conv::Conv::(GFLOPS=1.112, K=[3 x 3], IN={1, 512, 10, 10}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|6.510|-|
|conv::Conv::(GFLOPS=1.181, K=[3 x 3], IN={1, 64, 160, 200}, OCN=128, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU)|13.718|13.287|1.03|
|conv::Conv::(GFLOPS=1.181, K=[3 x 3], IN={1, 64, 160, 200}, OCN=128, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU_FP16)|-|7.190|-|
|conv::Conv::(GFLOPS=1.182, K=[3 x 3], IN={1, 32, 320, 400}, OCN=64, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU)|15.133|14.853|1.02|
|conv::Conv::(GFLOPS=1.182, K=[3 x 3], IN={1, 32, 320, 400}, OCN=64, S=[2 x 2], P=[1 x 1], BIAS, OCV/CPU_FP16)|-|8.671|-|
|conv::Conv::(GFLOPS=1.195, K=[9 x 9], IN={1, 32, 240, 320}, OCN=3, P=[4 x 4], BIAS, OCV/CPU)|41.928|43.328|0.97|
|conv::Conv::(GFLOPS=1.195, K=[9 x 9], IN={1, 32, 240, 320}, OCN=3, P=[4 x 4], BIAS, OCV/CPU_FP16)|-|38.072|-|
|conv::Conv::(GFLOPS=1.196, K=[3 x 3], IN={1, 384, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU)|4.409|4.428|1.00|
|conv::Conv::(GFLOPS=1.196, K=[3 x 3], IN={1, 384, 26, 26}, OCN=256, P=[1 x 1], OCV/CPU_FP16)|-|4.427|-|
|conv::Conv::(GFLOPS=1.210, K=[3 x 3], IN={1, 32, 256, 256}, OCN=32, PM=SAME, OCV/CPU)|6.144|5.363|1.15|
|conv::Conv::(GFLOPS=1.210, K=[3 x 3], IN={1, 32, 256, 256}, OCN=32, PM=SAME, OCV/CPU_FP16)|-|5.368|-|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 64, 75, 75}, OCN=192, PM=SAME, BIAS, OCV/CPU)|3.926|3.932|1.00|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 64, 75, 75}, OCN=192, PM=SAME, BIAS, OCV/CPU_FP16)|-|3.938|-|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 96, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU)|3.920|3.915|1.00|
|conv::Conv::(GFLOPS=1.245, K=[3 x 3], IN={1, 96, 75, 100}, OCN=96, PM=SAME, BIAS, OCV/CPU_FP16)|-|3.950|-|
|conv::Conv::(GFLOPS=1.248, K=[3 x 3], IN={1, 256, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|3.767|3.764|1.00|
|conv::Conv::(GFLOPS=1.248, K=[3 x 3], IN={1, 256, 46, 46}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|3.762|-|
|conv::Conv::(GFLOPS=1.258, K=[3 x 3], IN={1, 1280, 10, 10}, OCN=546, PM=SAME, BIAS, OCV/CPU)|19.959|13.875|1.44|
|conv::Conv::(GFLOPS=1.258, K=[3 x 3], IN={1, 1280, 10, 10}, OCN=546, PM=SAME, BIAS, OCV/CPU_FP16)|-|7.781|-|
|conv::Conv::(GFLOPS=1.261, K=[3 x 3], IN={1, 192, 38, 50}, OCN=192, PM=SAME, BIAS, OCV/CPU)|3.951|3.955|1.00|
|conv::Conv::(GFLOPS=1.261, K=[3 x 3], IN={1, 192, 38, 50}, OCN=192, PM=SAME, BIAS, OCV/CPU_FP16)|-|3.969|-|
|conv::Conv::(GFLOPS=1.416, K=[3 x 3], IN={1, 128, 62, 82}, OCN=128, BIAS, OCV/CPU)|4.050|4.034|1.00|
|conv::Conv::(GFLOPS=1.416, K=[3 x 3], IN={1, 128, 62, 82}, OCN=128, BIAS, OCV/CPU_FP16)|-|4.093|-|
|conv::Conv::(GFLOPS=1.500, K=[3 x 3], IN={1, 128, 64, 84}, OCN=128, BIAS, OCV/CPU)|4.923|4.506|1.09|
|conv::Conv::(GFLOPS=1.500, K=[3 x 3], IN={1, 128, 64, 84}, OCN=128, BIAS, OCV/CPU_FP16)|-|4.509|-|
|conv::Conv::(GFLOPS=1.586, K=[3 x 3], IN={1, 128, 66, 86}, OCN=128, BIAS, OCV/CPU)|4.759|4.476|1.06|
|conv::Conv::(GFLOPS=1.586, K=[3 x 3], IN={1, 128, 66, 86}, OCN=128, BIAS, OCV/CPU_FP16)|-|4.447|-|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 26, 26}, OCN=512, P=[1 x 1], OCV/CPU)|6.079|5.628|1.08|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 26, 26}, OCN=512, P=[1 x 1], OCV/CPU_FP16)|-|5.625|-|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 52, 52}, OCN=512, S=[2 x 2], P=[1 x 1], OCV/CPU)|19.843|17.523|1.13|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 256, 52, 52}, OCN=512, S=[2 x 2], P=[1 x 1], OCV/CPU_FP16)|-|8.917|-|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 13, 13}, OCN=1024, P=[1 x 1], OCV/CPU)|8.334|8.247|1.01|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 13, 13}, OCN=1024, P=[1 x 1], OCV/CPU_FP16)|-|8.246|-|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 26, 26}, OCN=1024, S=[2 x 2], P=[1 x 1], OCV/CPU)|23.164|18.199|1.27|
|conv::Conv::(GFLOPS=1.595, K=[3 x 3], IN={1, 512, 26, 26}, OCN=1024, S=[2 x 2], P=[1 x 1], OCV/CPU_FP16)|-|9.305|-|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 104, 104}, OCN=128, P=[1 x 1], OCV/CPU)|5.184|5.178|1.00|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 104, 104}, OCN=128, P=[1 x 1], OCV/CPU_FP16)|-|5.149|-|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 208, 208}, OCN=128, S=[2 x 2], P=[1 x 1], OCV/CPU)|17.990|18.103|0.99|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 64, 208, 208}, OCN=128, S=[2 x 2], P=[1 x 1], OCV/CPU_FP16)|-|9.777|-|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 52, 52}, OCN=256, P=[1 x 1], OCV/CPU)|4.831|4.522|1.07|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 52, 52}, OCN=256, P=[1 x 1], OCV/CPU_FP16)|-|4.523|-|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 104, 104}, OCN=256, S=[2 x 2], P=[1 x 1], OCV/CPU)|17.328|17.319|1.00|
|conv::Conv::(GFLOPS=1.596, K=[3 x 3], IN={1, 128, 104, 104}, OCN=256, S=[2 x 2], P=[1 x 1], OCV/CPU_FP16)|-|8.948|-|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 208, 208}, OCN=64, P=[1 x 1], OCV/CPU)|5.944|5.961|1.00|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 208, 208}, OCN=64, P=[1 x 1], OCV/CPU_FP16)|-|5.936|-|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 416, 416}, OCN=64, S=[2 x 2], P=[1 x 1], OCV/CPU)|19.811|20.064|0.99|
|conv::Conv::(GFLOPS=1.598, K=[3 x 3], IN={1, 32, 416, 416}, OCN=64, S=[2 x 2], P=[1 x 1], OCV/CPU_FP16)|-|11.705|-|
|conv::Conv::(GFLOPS=1.659, K=[3 x 3], IN={1, 960, 10, 10}, OCN=960, PM=SAME, OCV/CPU)|22.398|17.686|1.27|
|conv::Conv::(GFLOPS=1.659, K=[3 x 3], IN={1, 960, 10, 10}, OCN=960, PM=SAME, OCV/CPU_FP16)|-|9.859|-|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, G=128, P=[1 x 1], BIAS, OCV/CPU)|0.416|0.416|1.00|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, G=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|0.417|-|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, PM=SAME, OCV/CPU)|5.356|5.110|1.05|
|conv::Conv::(GFLOPS=1.660, K=[3 x 3], IN={1, 128, 75, 75}, OCN=128, PM=SAME, OCV/CPU_FP16)|-|5.114|-|
|conv::Conv::(GFLOPS=1.675, K=[3 x 3], IN={1, 128, 68, 88}, OCN=128, BIAS, OCV/CPU)|5.092|4.748|1.07|
|conv::Conv::(GFLOPS=1.675, K=[3 x 3], IN={1, 128, 68, 88}, OCN=128, BIAS, OCV/CPU_FP16)|-|4.754|-|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, G=256, P=[1 x 1], BIAS, OCV/CPU)|0.260|0.229|1.13|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, G=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|0.229|-|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, PM=SAME, OCV/CPU)|5.872|5.460|1.08|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 256, 38, 38}, OCN=256, PM=SAME, OCV/CPU_FP16)|-|5.460|-|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, G=512, P=[1 x 1], BIAS, OCV/CPU)|0.161|0.161|1.00|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, G=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|0.161|-|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|7.176|7.175|1.00|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|7.162|-|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, PM=SAME, OCV/CPU)|7.174|7.185|1.00|
|conv::Conv::(GFLOPS=1.704, K=[3 x 3], IN={1, 512, 19, 19}, OCN=512, PM=SAME, OCV/CPU_FP16)|-|7.157|-|
|conv::Conv::(GFLOPS=1.766, K=[3 x 3], IN={1, 128, 70, 90}, OCN=128, BIAS, OCV/CPU)|5.400|5.180|1.04|
|conv::Conv::(GFLOPS=1.766, K=[3 x 3], IN={1, 128, 70, 90}, OCN=128, BIAS, OCV/CPU_FP16)|-|5.201|-|
|conv::Conv::(GFLOPS=1.859, K=[3 x 3], IN={1, 128, 72, 92}, OCN=128, BIAS, OCV/CPU)|5.330|5.188|1.03|
|conv::Conv::(GFLOPS=1.859, K=[3 x 3], IN={1, 128, 72, 92}, OCN=128, BIAS, OCV/CPU_FP16)|-|5.177|-|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, G=1024, P=[1 x 1], BIAS, OCV/CPU)|0.115|0.115|1.00|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, G=1024, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|0.115|-|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, PM=SAME, OCV/CPU)|26.156|20.222|1.29|
|conv::Conv::(GFLOPS=1.888, K=[3 x 3], IN={1, 1024, 10, 10}, OCN=1024, PM=SAME, OCV/CPU_FP16)|-|11.203|-|
|conv::Conv::(GFLOPS=1.954, K=[3 x 3], IN={1, 128, 74, 94}, OCN=128, BIAS, OCV/CPU)|5.627|5.543|1.02|
|conv::Conv::(GFLOPS=1.954, K=[3 x 3], IN={1, 128, 74, 94}, OCN=128, BIAS, OCV/CPU_FP16)|-|5.506|-|
|conv::Conv::(GFLOPS=1.995, K=[9 x 9], IN={1, 3, 320, 400}, OCN=32, P=[4 x 4], BIAS, OCV/CPU)|27.925|27.741|1.01|
|conv::Conv::(GFLOPS=1.995, K=[9 x 9], IN={1, 3, 320, 400}, OCN=32, P=[4 x 4], BIAS, OCV/CPU_FP16)|-|17.217|-|
|conv::Conv::(GFLOPS=2.052, K=[3 x 3], IN={1, 128, 76, 96}, OCN=128, BIAS, OCV/CPU)|6.359|6.062|1.05|
|conv::Conv::(GFLOPS=2.052, K=[3 x 3], IN={1, 128, 76, 96}, OCN=128, BIAS, OCV/CPU_FP16)|-|6.048|-|
|conv::Conv::(GFLOPS=2.100, K=[3 x 3], IN={1, 144, 75, 75}, OCN=144, PM=SAME, OCV/CPU)|6.559|6.322|1.04|
|conv::Conv::(GFLOPS=2.100, K=[3 x 3], IN={1, 144, 75, 75}, OCN=144, PM=SAME, OCV/CPU_FP16)|-|6.280|-|
|conv::Conv::(GFLOPS=2.153, K=[3 x 3], IN={1, 128, 78, 98}, OCN=128, BIAS, OCV/CPU)|6.412|6.200|1.03|
|conv::Conv::(GFLOPS=2.153, K=[3 x 3], IN={1, 128, 78, 98}, OCN=128, BIAS, OCV/CPU_FP16)|-|6.197|-|
|conv::Conv::(GFLOPS=2.156, K=[3 x 3], IN={1, 576, 19, 19}, OCN=576, PM=SAME, OCV/CPU)|9.167|8.624|1.06|
|conv::Conv::(GFLOPS=2.156, K=[3 x 3], IN={1, 576, 19, 19}, OCN=576, PM=SAME, OCV/CPU_FP16)|-|8.626|-|
|conv::Conv::(GFLOPS=2.255, K=[3 x 3], IN={1, 128, 80, 100}, OCN=128, BIAS, OCV/CPU)|6.755|6.491|1.04|
|conv::Conv::(GFLOPS=2.255, K=[3 x 3], IN={1, 128, 80, 100}, OCN=128, BIAS, OCV/CPU_FP16)|-|6.520|-|
|conv::Conv::(GFLOPS=2.719, K=[3 x 3], IN={1, 96, 256, 256}, OCN=96, S=[2 x 2], PM=SAME, OCV/CPU)|35.664|34.752|1.03|
|conv::Conv::(GFLOPS=2.719, K=[3 x 3], IN={1, 96, 256, 256}, OCN=96, S=[2 x 2], PM=SAME, OCV/CPU_FP16)|-|20.260|-|
|conv::Conv::(GFLOPS=3.319, K=[3 x 3], IN={1, 128, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|9.514|9.414|1.01|
|conv::Conv::(GFLOPS=3.319, K=[3 x 3], IN={1, 128, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|9.462|-|
|conv::Conv::(GFLOPS=3.321, K=[3 x 3], IN={1, 64, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|10.631|9.963|1.07|
|conv::Conv::(GFLOPS=3.321, K=[3 x 3], IN={1, 64, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|9.935|-|
|conv::Conv::(GFLOPS=3.398, K=[7 x 7], IN={1, 128, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU)|37.465|36.798|1.02|
|conv::Conv::(GFLOPS=3.398, K=[7 x 7], IN={1, 128, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU_FP16)|-|19.569|-|
|conv::Conv::(GFLOPS=3.407, K=[3 x 3], IN={1, 512, 19, 19}, OCN=1024, D=[6 x 6], P=[6 x 6], BIAS, OCV/CPU)|38.157|36.157|1.06|
|conv::Conv::(GFLOPS=3.407, K=[3 x 3], IN={1, 512, 19, 19}, OCN=1024, D=[6 x 6], P=[6 x 6], BIAS, OCV/CPU_FP16)|-|18.902|-|
|conv::Conv::(GFLOPS=3.408, K=[3 x 3], IN={1, 256, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|10.356|10.401|1.00|
|conv::Conv::(GFLOPS=3.408, K=[3 x 3], IN={1, 256, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|10.360|-|
|conv::Conv::(GFLOPS=4.247, K=[3 x 3], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU)|12.641|12.150|1.04|
|conv::Conv::(GFLOPS=4.247, K=[3 x 3], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU_FP16)|-|12.162|-|
|conv::Conv::(GFLOPS=4.247, K=[5 x 5], IN={1, 144, 128, 128}, OCN=144, S=[2 x 2], PM=SAME, OCV/CPU)|50.545|50.505|1.00|
|conv::Conv::(GFLOPS=4.247, K=[5 x 5], IN={1, 144, 128, 128}, OCN=144, S=[2 x 2], PM=SAME, OCV/CPU_FP16)|-|27.950|-|
|conv::Conv::(GFLOPS=4.566, K=[7 x 7], IN={1, 172, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU)|54.233|49.603|1.09|
|conv::Conv::(GFLOPS=4.566, K=[7 x 7], IN={1, 172, 46, 46}, OCN=128, P=[3 x 3], BIAS, OCV/CPU_FP16)|-|26.515|-|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 256, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|13.779|12.968|1.06|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 256, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|12.984|-|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 512, 46, 46}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|15.809|15.329|1.03|
|conv::Conv::(GFLOPS=4.993, K=[3 x 3], IN={1, 512, 46, 46}, OCN=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|15.433|-|
|conv::Conv::(GFLOPS=4.994, K=[3 x 3], IN={1, 128, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|14.563|14.527|1.00|
|conv::Conv::(GFLOPS=4.994, K=[3 x 3], IN={1, 128, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|14.480|-|
|conv::Conv::(GFLOPS=4.997, K=[3 x 3], IN={1, 64, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|16.714|16.484|1.01|
|conv::Conv::(GFLOPS=4.997, K=[3 x 3], IN={1, 64, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|16.362|-|
|conv::Conv::(GFLOPS=5.780, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, S=[2 x 2], PM=SAME, OCV/CPU)|77.832|65.729|1.18|
|conv::Conv::(GFLOPS=5.780, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, S=[2 x 2], PM=SAME, OCV/CPU_FP16)|-|32.065|-|
|conv::Conv::(GFLOPS=6.116, K=[3 x 3], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU)|21.903|20.386|1.07|
|conv::Conv::(GFLOPS=6.116, K=[3 x 3], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU_FP16)|-|20.416|-|
|conv::Conv::(GFLOPS=6.118, K=[3 x 3], IN={1, 144, 128, 128}, OCN=144, PM=SAME, OCV/CPU)|20.405|18.148|1.12|
|conv::Conv::(GFLOPS=6.118, K=[3 x 3], IN={1, 144, 128, 128}, OCN=144, PM=SAME, OCV/CPU_FP16)|-|18.128|-|
|conv::Conv::(GFLOPS=6.637, K=[3 x 3], IN={1, 256, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|20.334|18.521|1.10|
|conv::Conv::(GFLOPS=6.637, K=[3 x 3], IN={1, 256, 75, 75}, OCN=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|18.495|-|
|conv::Conv::(GFLOPS=6.638, K=[3 x 3], IN={1, 128, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|21.527|19.584|1.10|
|conv::Conv::(GFLOPS=6.638, K=[3 x 3], IN={1, 128, 150, 150}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|19.630|-|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 150, 200}, OCN=192, PM=SAME, BIAS, OCV/CPU)|22.715|20.057|1.13|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 150, 200}, OCN=192, PM=SAME, BIAS, OCV/CPU_FP16)|-|20.068|-|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 300, 300}, OCN=64, P=[1 x 1], BIAS, OCV/CPU)|26.228|24.992|1.05|
|conv::Conv::(GFLOPS=6.641, K=[3 x 3], IN={1, 64, 300, 300}, OCN=64, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|24.957|-|
|conv::Conv::(GFLOPS=6.814, K=[3 x 3], IN={1, 512, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|21.524|21.581|1.00|
|conv::Conv::(GFLOPS=6.814, K=[3 x 3], IN={1, 512, 38, 38}, OCN=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|21.782|-|
|conv::Conv::(GFLOPS=8.025, K=[3 x 3], IN={1, 1024, 19, 19}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU)|34.094|31.964|1.07|
|conv::Conv::(GFLOPS=8.025, K=[3 x 3], IN={1, 1024, 19, 19}, OCN=1206, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|31.925|-|
|conv::Conv::(GFLOPS=9.986, K=[3 x 3], IN={1, 512, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU)|28.677|27.813|1.03|
|conv::Conv::(GFLOPS=9.986, K=[3 x 3], IN={1, 512, 46, 46}, OCN=512, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|27.808|-|
|conv::Conv::(GFLOPS=9.987, K=[3 x 3], IN={1, 256, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU)|31.274|27.892|1.12|
|conv::Conv::(GFLOPS=9.987, K=[3 x 3], IN={1, 256, 92, 92}, OCN=256, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|27.910|-|
|conv::Conv::(GFLOPS=9.989, K=[3 x 3], IN={1, 128, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU)|30.533|30.007|1.02|
|conv::Conv::(GFLOPS=9.989, K=[3 x 3], IN={1, 128, 184, 184}, OCN=128, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|30.089|-|
|conv::Conv::(GFLOPS=9.993, K=[3 x 3], IN={1, 64, 368, 368}, OCN=64, P=[1 x 1], BIAS, OCV/CPU)|39.837|38.312|1.04|
|conv::Conv::(GFLOPS=9.993, K=[3 x 3], IN={1, 64, 368, 368}, OCN=64, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|38.477|-|
|conv::Conv::(GFLOPS=10.087, K=[3 x 3], IN={1, 576, 38, 50}, OCN=512, PM=SAME, BIAS, OCV/CPU)|32.480|29.237|1.11|
|conv::Conv::(GFLOPS=10.087, K=[3 x 3], IN={1, 576, 38, 50}, OCN=512, PM=SAME, BIAS, OCV/CPU_FP16)|-|29.452|-|
|conv::Conv::(GFLOPS=10.701, K=[3 x 3], IN={1, 512, 38, 38}, OCN=804, P=[1 x 1], BIAS, OCV/CPU)|33.544|32.832|1.02|
|conv::Conv::(GFLOPS=10.701, K=[3 x 3], IN={1, 512, 38, 38}, OCN=804, P=[1 x 1], BIAS, OCV/CPU_FP16)|-|32.784|-|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 240, 64, 64}, OCN=240, PM=SAME, OCV/CPU)|134.481|130.678|1.03|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 240, 64, 64}, OCN=240, PM=SAME, OCV/CPU_FP16)|-|70.134|-|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU)|127.930|126.530|1.01|
|conv::Conv::(GFLOPS=11.797, K=[5 x 5], IN={1, 480, 32, 32}, OCN=480, PM=SAME, OCV/CPU_FP16)|-|65.261|-|
|conv::Conv::(GFLOPS=16.987, K=[5 x 5], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU)|201.346|187.007|1.08|
|conv::Conv::(GFLOPS=16.987, K=[5 x 5], IN={1, 1152, 16, 16}, OCN=1152, PM=SAME, OCV/CPU_FP16)|-|91.525|-|
|conv::Conv::(GFLOPS=23.122, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, PM=SAME, OCV/CPU)|252.038|245.587|1.03|
|conv::Conv::(GFLOPS=23.122, K=[5 x 5], IN={1, 672, 32, 32}, OCN=672, PM=SAME, OCV/CPU_FP16)|-|125.477|-|

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-07-14 10:13:52,feature,Unable to load model ONNX 4.5.5 or 4.6.0,"##### System information (version)

- OpenCV => 4.5.5/4.6.0
- Operating System / Platform => MacOS 12.4 (21F79) Apple M1 Pro
- Compiler => Visual Studio Code/ python /c++

##### Detailed description

```
python:

4.6.0
[ERROR:0@0.079] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp (1021) handleNode DNN/ONNX: ERROR during processing node with 3 inputs and 1 outputs: [Clip]:(onnx_node!StatefulPartitionedCall/center_net_mobile_net_v2fpn_feature_extractor/model_1/model/Conv1_relu/Relu6) from domain='ai.onnx'
Traceback (most recent call last):
  File ""/Users/tony/Documents/Works/project/nts/cpp/python/pose.py"", line 5, in <module>
    net = cv2.dnn.readNetFromONNX(""./model_pb.onnx"")  # 加载训练好的识别模型
cv2.error: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:1040: error: (-2:Unspecified error) in function 'handleNode'
> Node [Clip@ai.onnx]:(onnx_node!StatefulPartitionedCall/center_net_mobile_net_v2fpn_feature_extractor/model_1/model/Conv1_relu/Relu6) parse error: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/dnn/src/onnx/onnx_importer.cpp:1934: error: (-2:Unspecified error) in function 'void cv::dnn::dnn4_v20220524::ONNXImporter::parseClip(cv::dnn::dnn4_v20220524::LayerParams &, const opencv_onnx::NodeProto &)'
> >  (expected: 'node_proto.input_size() == 1'), where
> >     'node_proto.input_size()' is 3
> > must be equal to
> >     '1' is 1
> 
```

```
c++:
版本:4.5.5
[ERROR:0@2.046] global /tmp/opencv-20220225-83415-181ywau/opencv-4.5.5/modules/dnn/src/onnx/onnx_importer.cpp (909) handleNode DNN/ONNX: ERROR during processing node with 3 inputs and 1 outputs: [Clip]:(StatefulPartitionedCall/center_net_mobile_net_v2fpn_feature_extractor/model_1/model/Conv1_relu/Relu6:0) from domain='ai.onnx'
OpenCV(4.5.5) /tmp/opencv-20220225-83415-181ywau/opencv-4.5.5/modules/dnn/src/onnx/onnx_importer.cpp:928: error: (-2:Unspecified error) in function 'handleNode'
> Node [Clip@ai.onnx]:(StatefulPartitionedCall/center_net_mobile_net_v2fpn_feature_extractor/model_1/model/Conv1_relu/Relu6:0) parse error: OpenCV(4.5.5) /tmp/opencv-20220225-83415-181ywau/opencv-4.5.5/modules/dnn/src/onnx/onnx_importer.cpp:1613: error: (-2:Unspecified error) in function 'void cv::dnn::dnn4_v20211220::ONNXImporter::parseClip(cv::dnn::dnn4_v20211220::LayerParams &, const opencv_onnx::NodeProto &)'
> >  (expected: 'node_proto.input_size() == 1'), where
> >     'node_proto.input_size()' is 3
> > must be equal to
> >     '1' is 1
> 
```

##### Steps to reproduce

1. download model: https://tfhub.dev/google/movenet/singlepose/thunder/4?tf-hub-format=compressed
2. convert model : python -m tf2onnx.convert --saved-model ./movenet_singlepose_thunder_4 --output model_pb.onnx
3. load model the follow code:

```
# python
import cv2

print(cv2.__version__)
net = cv2.dnn.readNetFromONNX(""./model_pb.onnx"") 
image = cv2.imread(""person.jpeg"") 
blob = cv2.dnn.blobFromImage(image) 
net.setInput(blob)  
out = net.forward() 
```
```
#include <opencv2/opencv.hpp>
#include <iostream>
#include <stdexcept>

using namespace cv;
using namespace cv::dnn;
using namespace std;

int main() try {
    system(""color F0"");

    cout << ""version:"" << cv::getVersionString() << endl;

    string model = ""../model/model_pb.onnx"";
    Net net = dnn::readNet(model);
    if(net.empty()){
        cout << ""确认是否输入了空的模型文件"" << endl;
        return -1;
    }
    vector<string> layerNames = net.getLayerNames();
    for (size_t i = 0; i < layerNames.size(); i++)
    {
        int ID = net.getLayerId(layerNames[i]);
        Ptr<Layer> layer = net.getLayer(ID);
        cout << ""网络层数"" << ID << "" 网络层名称 "" << layerNames[i] << "" 类型 "" << layer->type.c_str() << endl;
    }
    
    cout << ""ok"" << endl;
}catch(std::exception const &ex){
    cerr<<ex.what()<<endl;
}
```

##### Issue submission checklist
 - [ ] I report the issue, it's not a question
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
 - [x] I updated to the latest OpenCV version and the issue is still there
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
"
opencv/opencv,2022-07-10 21:12:30,feature,[GSoC 2022] spng encoder/decoder added as optional png codec,"**Merge with extra**: https://github.com/opencv/opencv_extra/pull/995

This pull request contains the implementation of PngEncoder using spng library. One thing that I am not sure about is the CMake configuration. I appreciate any help on CMake configuration

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-07-06 06:28:06,feature,Yolov4-csp inference on opencv 4.6.0 provides different results to native darknet inference.,"##### System information (version)

- OpenCV => 4.6.0
- Operating System / Platform => ubuntu 20.04
- Source => inference on python, opencv from pip3 install opencv-python

##### Detailed description

I have a yolov4-csp based OCR model trained from darknet (the model was trained for a short time, just for testing purposes), running inference on native darknet (https://github.com/AlexeyAB/darknet), gets acceptable results 

![darknet_inference](https://user-images.githubusercontent.com/105900664/177468957-2fb9b4d7-aa7d-4c27-8ab7-e709e8edebb0.jpg)

However running the same using opencv dnn gets results, which are quite different and not acceptable in this case.

![opencv_inference](https://user-images.githubusercontent.com/105900664/177469130-c0115bc5-9083-4238-87b5-3548d5073576.jpg)

As mentioned here (https://github.com/AlexeyAB/darknet/issues/8146#issuecomment-946251267), opencv 4.5.4 or greater should support yolov4-csp, This was why I tested it on the latest version 4.6.0.

##### Steps to reproduce

To reproduce I will provide google drive zip file with all the weights and python script for opencv inference. (https://drive.google.com/file/d/1e_JyalXZuZgunzx_GWEmGINy37xiyqpU/view?usp=sharing)
Below I will also provide the python inference script 

`
import cv2
import time

CONFIDENCE_THRESHOLD = 0.3
NMS_THRESHOLD = 0.4
COLORS = [(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]
print(""cv2 version"", cv2.__version__)
class_names = []
with open(""ocr-yolov4-csp/classes.txt"", ""r"") as f:
    class_names = [cname.strip() for cname in f.readlines()]

frame = cv2.imread(""ocr-yolov4-csp/test1.jpg"")

net = cv2.dnn.readNet(""ocr-yolov4-csp/yolov4-csp_best.weights"", ""ocr-yolov4-csp/yolov4-csp.cfg"")
#net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)

model = cv2.dnn_DetectionModel(net)
model.setInputParams(size=(512, 512), scale=1/255, swapRB=True)

classes, scores, boxes = model.detect(frame, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)
end = time.time()
start_drawing = time.time()
for (classid, score, box) in zip(classes, scores, boxes):
    color = COLORS[int(classid) % len(COLORS)]    
    label = ""%s : %f"" % (class_names[classid], score)
    cv2.rectangle(frame, box, color, 2)
    cv2.putText(frame, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

cv2.imwrite(""opencv_inference.jpg"", frame)

`


##### Issue submission checklist

 - [✓] I report the issue, it's not a question

 - [✓] I checked the problem with documentation, FAQ, open issues,

 - [✓] I updated to the latest OpenCV version and the issue is still there

 - [✓] There is reproducer code and related data files: videos, images, onnx, etc

"
opencv/opencv,2022-07-01 01:05:25,feature,[GSoC] New universal intrinsic backend for RVV,"This is a patch of my GSoC project whose goal is to make the existing Universal Intrinsic compatible with variable-length backends. Thereby improving the performance of the RISC-V Vector (RVV) backend.

In this patch,
1. We added a new rvv backend. Unlike the current implementation, we used the rvv type directly instead of the wrapper class, which will improve performance. Currently only the necessary functions are included, mainly v_setall, v_load, v_lut for compatibility with vx_XXX and v_min ,v_max for the MedianBlur example.
2. To be compatible with other backends that use wrapper classes, we had to modify the existing universal intrinsic interfaces, including Vtraits and function compatibility layers. The documentation has not been updated yet, we will update it when changes of API are finalized.
3. We modified some of the existing SIMD code to ensure compilation, mainly by adding the packaging of the CV_SIMD macro
4. We enabled the new rvv backend implementation for the Imgproc module's MedianBlur as an example of how to reuse existing UI-optimized code blocks. 

Run the opencv_test_imgproc, the test passed and result are promising: 7040 ms by new implentation vs. 42094 ms by current one.
`$ qemu-riscv64 -cpu rv64,x-v=true ./bin/opencv_test_imgproc --gtest_filter=""Imgproc_MedianBlur*""`


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ ] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-06-27 07:58:24,feature,V4l2 multi planar,"- V4L2: Add multi-planar capture support
- V4L2: fix data order of conversion from NV[12|21] to RGB (relates #12893)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake

```
force_builders=Custom
build_image:Custom=centos:7
buildworker:Custom=linux-f1
```"
opencv/opencv,2022-06-22 12:10:01,feature,Add symmetric circles board support to interactive calibration app,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [X] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-06-20 13:43:00,feature,Add option to force reopen camera in interactive calibration tool,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [X] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-06-14 15:34:25,feature,"Fix issue 22015, let Clip layer support 1-3 inputs","**Merge with extra**: https://github.com/opencv/opencv_extra/pull/980

This PR will fix the issue #22015 

The essence of the issue is that the `Clip layer` is supposed to accept 1-3 inputs, but the implementation here  is based on `ReLU6`, which only supports attributes as parameters.

https://github.com/opencv/opencv/blob/b19683eb41f425ec436193bd90a27f7637b4e8e1/modules/dnn/src/onnx/onnx_importer.cpp#L1935

Solution: When parsing the `Clip layer`, the `min` and `max` obtained from `node_proto.input()`, instead of from `layerParams`, are passed as parameters to the `ReLU6`. This allows the `Clip layer` to support 1-3 inputs."
opencv/opencv,2022-06-03 18:02:11,feature,apps/opencv_visualisation: configurable video codec,"Problem: the `model_visualization.avi` with `XVID` codec did (silently) not work whereas `model_visualization.mp4` with `H264` does work.

Proposed solution: make the video codec configurable (defaulting to existing behaviour) and error-log-and-exit if the output video file could not be opened.

#### Usage example

(The ""happy fish"" image does not have the 20x20 size of the ""profile face"" model but it seems nonetheless suitable for smoke test use.)

```
opencv_build/bin/opencv_visualisation \\
--image=opencv/samples/data/HappyFish.jpg \\
--model=opencv/data/haarcascades/haarcascade_profileface.xml \\
--data=temp_data/ \\
--ext=mp4 --fourcc=H264 --fps=42
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-05-29 22:14:37,feature,[request] New parameter to imwrite JPEG to disable chroma subsampling,"##### System information (version)
<!-- Example
- OpenCV => 4.2
- Operating System / Platform => Windows 64 Bit
- Compiler => Visual Studio 2017
-->

- OpenCV => all versions and OS's

##### Detailed description

`cv::imwrite()` has several JPEG quality-related parameters but I often miss a flag to *disable chroma subsampling*. This can make a significant difference in images with pure color details (e.g. with superimposed red, green or blue lines or circles where features have been found). Just setting JPEG quality to 100 does not help because standard chroma subsampling comes before and can ruin the image (chroma subsampling is good for real photographic content).

This is a normal JPEG (q 95) (uses YUV 4:2:0 subsampling). See the fuzzy borders on the red and magenta lines.
![q95-normal](https://user-images.githubusercontent.com/36175101/170862440-d3a49dc4-1ee4-4564-8af9-f5693ee0386f.jpg)

This is the same JPEG without chroma subsampling (uses RGB color space with no subsampling)
![q95-no_subsampling](https://user-images.githubusercontent.com/36175101/170862447-7efc42bf-20b6-4b1c-8d90-610d0bf28771.jpg)

The file grows a little, but can improve significantly in some kinds of images. A PNG would be better of course, but is usually much larger and slower to compress.

##### Implementation

I've looked at the code of `grfmt_jpeg.cpp` and libjpeg and I think I've found how to do this. It would be a very little change, with a new parameter that I'd call `IMWRITE_JPEG_FULL_COLOR` or `IMWRITE_JPEG_NO_SUBSAMPLE`. Possibly just adding something like this after the line `jpeg_set_defaults(&cinfo);` (and something more to get the flag value from the params list)

```.cpp
if (IMWRITE_JPEG_NO_SUBSAMPLE && channels > 1)
    jpeg_set_colorspace(&cinfo, JCS_RGB);
```

I have not tried yet. And I'm not sure I would know how to do it and create a pull request but I'll try.


##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-05-20 12:25:09,feature,Add python bindings for G-API onnx,"### Motivation
I think that the feature to run modle on onnxruntime is worth exposing to python G-API considering that

* Onnxruntime supports many ML runtimes
* Onnxruntime itself is very easy to install

This PR is the first step to expose onnxruntime features to python and support of execution providers will follow later.
This PR is draft, so let's have a discussion if necessary :slightly_smiling_face: 

### Remaining tasks
-  [x] Adding python unit test for onnx runtime
-  [x] Create the same preprocess as openvino one to be able to accept the input with same shape as openvino. Currently transposing and reshaping is necessary like [this](https://github.com/xiong-jie-y/g_api_examples/blob/main/low_light_enhancement_onnx/enhance_low_light.py#L25-L26) in the user code.
-> I will not implement this because it seems that the implementation in the onnx seems right from [this line](https://github.com/xiong-jie-y/opencv/blob/py_onnx/modules/gapi/src/backends/onnx/gonnxbackend.cpp#L227).

### Questions
- How do you manage models for testing in opencv? I coudln't find the model used in infer_ssd_onnx.cpp test.

### Example Code
https://github.com/xiong-jie-y/g_api_examples/blob/main/low_light_enhancement_onnx/enhance_low_light.py

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
There's no bug report.
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
I don't thik performance test is necessary this time, because it's just a binding.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-05-16 11:02:29,feature,Support asymmetric paddings for QConv in ONNX,"We have a model in https://github.com/opencv/opencv_zoo/pull/51, which has asymmetric paddings in convolution layers. The model is managed to be quantized, but the current version of opencv does not support asymmetric paddings in quantized int8 convolution layer. So this PR adds the asymmetric padding support for quantized int8 convolution layer in ONNX importer.

Test data: https://github.com/opencv/opencv_extra/pull/973

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-05-08 21:07:01,feature,imgproc: optimise local cost computation in IntelligentScissorsMB::buildMap,"When reading the code I noticed a `TODO(opt)` comment, this pull request is for that.

https://github.com/opencv/opencv/blob/4.5.5/modules/imgproc/src/intelligent_scissors.cpp#L628

```
float cost = cost_q + local_cost(q, r);  // TODO(opt): compute partially until cost < cost_r
```

### Notes

* https://github.com/opencv/opencv/pull/21941 and this pull request change the same non-test files. However, the changes themselves don't overlap i.e. there should be no merge conflicts.

* Based on https://github.com/opencv/opencv/wiki/Branches#which-branch-should-i-target-my-pr this pull request here could be for 3.4 instead of 4.x branch. However, it is for 4.x since 3.4 doesn't have the intelligent scissors code.

### Test run snippet

```
$ export OPENCV_TEST_DATA_PATH=../opencv_extra/testdata

$./bin/opencv_perf_imgproc --gtest_filter=""TestIntelligentScissorsMB*"" 
...
[ RUN      ] TestIntelligentScissorsMB_buildMap_setComputeFullLocalCost.buildMap_setComputeFullLocalCost/0, where GetParam() = false
...
[ PERFSTAT ]    (samples=10   mean=5459.27   median=5452.70   min=5410.39   stddev=34.39 (0.6%))
[       OK ] TestIntelligentScissorsMB_buildMap_setComputeFullLocalCost.buildMap_setComputeFullLocalCost/0 (54604 ms)
[ RUN      ] TestIntelligentScissorsMB_buildMap_setComputeFullLocalCost.buildMap_setComputeFullLocalCost/1, where GetParam() = true
...
[ PERFSTAT ]    (samples=10   mean=5635.78   median=5634.03   min=5621.81   stddev=9.53 (0.2%))
[       OK ] TestIntelligentScissorsMB_buildMap_setComputeFullLocalCost.buildMap_setComputeFullLocalCost/1 (56360 ms)
...
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [X] I agree to contribute to the project under Apache 2 License.
- [X] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [X] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-04-29 17:41:41,feature,Add undistortImagePoints function,"`cv::undistortPoints()` has unclear interface and additional functionality. New function computes only undistorted image points position

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-04-12 23:36:02,feature,"to extract frame type (I, P, B) info for better post-process without ffmpeg/ffprobe","##### System information (version)
- OpenCV => opencv-python 4.5.5
- Operating System / Platform => win10
- Compiler => pre-compiled

##### Detailed description
Currently, it always needs FFmpeg/FFprobe to extract frame type info before any future processing, but calling a new process is slow and complex for other codes.
I,P,B frame type info could be useful to do some scene-change detection.

But OpenCV currently misses the function to get that info, but it should be there when OpenCV reads the video files.
Could OpenCV enhance this feature?

Thanks
##### Steps to reproduce
N/A

##### Issue submission checklist

 - [x] I report the issue, it's not a question
   <!--
   OpenCV team works with forum.opencv.org, Stack Overflow and other communities
   to discuss problems. Tickets with questions without a real issue statement will be
   closed.
   -->
 - [x] I checked the problem with documentation, FAQ, open issues,
       forum.opencv.org, Stack Overflow, etc and have not found any solution
   <!--
   Places to check:
   * OpenCV documentation: https://docs.opencv.org
   * FAQ page: https://github.com/opencv/opencv/wiki/FAQ
   * OpenCV forum: https://forum.opencv.org
   * OpenCV issue tracker: https://github.com/opencv/opencv/issues?q=is%3Aissue
   * Stack Overflow branch: https://stackoverflow.com/questions/tagged/opencv
   -->
 - [x] I updated to the latest OpenCV version and the issue is still there
   <!--
   master branch for OpenCV 4.x and 3.4 branch for OpenCV 3.x releases.
   OpenCV team supports only the latest release for each branch.
   The ticket is closed if the problem is not reproduced with the modern version.
   -->
 - [ ] There is reproducer code and related data files: videos, images, onnx, etc
   <!--
   The best reproducer -- test case for OpenCV that we can add to the library.
   Recommendations for media files and binary files:
   * Try to reproduce the issue with images and videos in opencv_extra repository
     to reduce attachment size
   * Use PNG for images, if you report some CV related bug, but not image reader
     issue
   * Attach the image as an archive to the ticket, if you report some reader issue.
     Image hosting services compress images and it breaks the repro code.
   * Provide ONNX file for some public model or ONNX file with random weights,
     if you report ONNX parsing or handling issue. Architecture details diagram
     from netron tool can be very useful too. See https://lutzroeder.github.io/netron/
   -->
"
opencv/opencv,2022-04-05 17:42:17,feature,Python bindings for VideoCapture::waitAny,"related: https://github.com/opencv/opencv/pull/15100

Everything in `VideoCapture` has Python bindings except for the new `waitAny` facility. I think that would be generally useful to have in Python too.

if the bindings generation doesn't imply a regular signature, I'd propose

```python
waitAny(streams, timeoutNs=0) -> readyIndex
```
Where `streams` is a list of VideoCapture instances and `readyIndex` is a python _list_ of indices into `streams`. No need to preserve the boolean return value. It's only false if the list is empty.

NB: Docs are inconsistent in their naming. Single mention of ""streamReady"". ""readyIndex"" was probably intended. It's not a singular value either (""index""), it's a vector/list (indices)."
opencv/opencv,2022-03-29 08:56:24,feature, G-API: VPL Source turn on Linux CPU version,"Fix compilation VPL Streaming Source for Linux
Add stub for VAAPI acceleration which uses accel for decode but produces MediaFrame in CPU memory
Fix UT of existing functionalities

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake

### Build Configuration
```
force_builders=Custom,Custom Win,Custom Mac
build_gapi_standalone:Linux x64=ade-0.1.1f
build_gapi_standalone:Win64=ade-0.1.1f
build_gapi_standalone:Mac=ade-0.1.1f
build_gapi_standalone:Linux x64 Debug=ade-0.1.1f

buildworker:Custom=linux-1
build_gapi_standalone:Custom=ade-0.1.1f
build_image:Custom=onevpl-2021.6.0
test_opencl:Custom=OFF

Xbuild_image:Custom=ubuntu-openvino-2021.3.0:20.04
Xbuild_image:Custom Win=openvino-2021.2.0
build_image:Custom Mac=openvino-2021.2.0

test_modules:Custom=gapi,python2,python3,java
test_modules:Custom Win=gapi,python2,python3,java
test_modules:Custom Mac=gapi,python2,python3,java

build_image:Custom Win=gapi-onevpl-2021.6.0
buildworker:Custom Win=windows-3
build_contrib:Custom Win=OFF
```"
opencv/opencv,2022-03-18 07:04:06,feature,dnn: add plugin support for OpenVINO,"TODO:
- [x] Fix Windows build scripts for plugin: disable PCH
- [x] Fix Windows build scripts for plugin: .simd.hpp support
- [ ] (postpone) BUILD_PLUGIN -> OPENCV_BUILD_PLUGIN (backlog, to be done as separate PR - need to fix other modules too)
- [x] Myriad detection (NCS/NCS2) - 2021.4+ supports only NCS2


<cut/>

```
force_builders=Custom,Custom Win
Xforce_builders=linux,docs,Custom,Custom Win

Xbuild_image:Custom=ubuntu-openvino-2022.1.0:20.04
build_image:Custom=ubuntu-openvino-2021.4.2:20.04
Xbuild_image:Custom=ubuntu-openvino-2022.1.0.dev20220131:20.04
build_image:Custom Win=openvino-2021.4.2
build_image:Custom Mac=openvino-2021.4.2

test_modules:Custom=dnn,gapi,python2,python3,java
test_modules:Custom Win=dnn,gapi,python2,python3,java
test_modules:Custom Mac=dnn,gapi,python2,python3,java

buildworker:Custom=linux-1
# disabled due high memory usage: test_opencl:Custom=ON
test_opencl:Custom=ON
test_bigdata:Custom=1
test_filter:Custom=*

buildworker:Custom Win=windows-1
test_bigdata:Custom Win=1
test_filter:Custom Win=*
test_opencl:Custom Win=ON
build_contrib:Custom Win=OFF

allow_multiple_commits=1
```
"
opencv/opencv,2022-03-17 07:40:25,feature,Android: don't require deprecated tools,"Checking for these deprecated is no longer necessary, and infact broken on fresh Android SDK installs. Remove the check.

resolves #21735

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake


```
force_builders_only=Android armeabi-v7a,docs
```"
opencv/opencv,2022-03-08 15:59:34,feature,Add 10-12-14bit (integer) TIFF decoding support,"**Merge with extra**: https://github.com/opencv/opencv_extra/pull/962

Proposal for #21700

A (slow) unpacking step is inserted when the native bpp is not equal to the dst_bpp

Currently, I do not know if there can be several packing flavours in TIFF data.

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work

"
opencv/opencv,2022-03-06 03:40:12,feature,add apply softmax option to ClassificationModel,"Fix https://github.com/opencv/opencv/issues/21689

```cpp
// model doesn't contain softmax layer
cv::dnn::ClassificationModel classification_model = cv::dnn::ClassificationModel( model, config );

// not apply softmax post process (default)
bool apply_softmax = classification_model.getSoftmaxPostProcessing(); // false

// apply softmax post process
apply_softmax = true;
classification_model.setEnableSoftmaxPostProcessing( apply_softmax );

// if enable softmax post process is true, confidences range is [0.0-1.0]
// if enable softmax post process is false, confidences range is varies by model
auto [classid, confidence] = model.classify( image );
```

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [x] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-03-05 07:58:42,feature,G-API: Add VPP preproc CPU/GPU dispatcher,"Added VPP preproc Dispatcher/Facade implements the same IPreprocEngine interface and aggregates CPU & GPU preproc engine and switch implementation by actual MediaFrame type. 


### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake


### Build Configuration

```
force_builders=XCustom,Custom Win,Custom Mac
build_gapi_standalone:Linux x64=ade-0.1.1f
build_gapi_standalone:Win64=ade-0.1.1f
build_gapi_standalone:Mac=ade-0.1.1f
build_gapi_standalone:Linux x64 Debug=ade-0.1.1f

Xbuild_image:Custom=centos:7
Xbuildworker:Custom=linux-1
build_gapi_standalone:Custom=ade-0.1.1f

build_image:Custom=ubuntu-openvino-2021.3.0:20.04
Xbuild_image:Custom Win=openvino-2021.2.0
build_image:Custom Mac=openvino-2021.2.0

test_modules:Custom=gapi,python2,python3,java
test_modules:Custom Win=gapi,python2,python3,java
test_modules:Custom Mac=gapi,python2,python3,java

buildworker:Custom=linux-1
test_opencl:Custom=OFF
test_bigdata:Custom=1
test_filter:Custom=*

build_image:Custom Win=gapi-onevpl-2021.6.0
buildworker:Custom Win=windows-3
build_contrib:Custom Win=OFF
```"
opencv/opencv,2022-02-28 15:20:45,feature,Error while loading yolor_p6.onnx: expand op doesn't support multiple axes for constant input,"
I'm trying to load yolor after i converted it from pytorch to onnx. buti get this error:

Node [Expand@ai.onnx]:(815) parse error: OpenCV(4.5.5) /io/opencv/modules/dnn/src/onnx/onnx_importer.cpp:2389: error: (-213:The function/feature is not implemented) Expand op doesn't support multiple axes for constant input in function 'parseExpand'

my code :
`import cv2`
`onnx_model = cv2.dnn.readNetFromONNX('yolor_p6.onnx')`"
opencv/opencv,2022-02-14 09:12:14,feature,DNN: add depth2space and space2depth layer for onnx importer,"Hi, I implement the `depth2space` and `space2depth` layers by the `Reshape` and `Permute` Layer, instead of creating a New Layer.

Is it necessary to create a separate layer for `depth2space` and `space2depth`?

[Relate Issue](https://github.com/opencv/opencv/issues/17514)

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [x] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-01-29 13:47:06,feature,Fallback to vaCreateImage + vaPutImage when vaDeriveImage fails,"Per intel docs for libva, when vaDeriveImage fails vaCreateImage +
  vaPutImage should be tried. This is important as mesa with AMD HW
  will always fail because the image is interlaced, but the HW
  capability still exists

Fixes https://github.com/opencv/opencv/issues/21536

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ x ] I agree to contribute to the project under Apache 2 License.
- [ x ] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [ x ] The PR is proposed to the proper branch
- [ x ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ x ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-01-28 08:42:28,feature,Support downloading 3rdparty resources from Gitcode & Gitlab-style mirrors,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [x] The feature is well documented and sample code can be built with the project CMake


```
force_builders_only=linux,docs
```"
opencv/opencv,2022-01-26 09:00:19,feature,Add resize layer compatible with ONNX opset13 version,"**Merge with extra**: https://github.com/opencv/opencv_extra/pull/954

Opset13 has changed resize layer mapping. 
And there may the empty input in the resize node.
Happened when exporting the ONNX model with the 1.9 or later version of pytorch with opset 13, for example the ""Resize"" in [ AnimeGANv2 model](https://github.com/Kazuhito00/AnimeGANv2-ONNX-Sample/tree/main/model).
More details can be found in the [Test data](https://github.com/opencv/opencv_extra/pull/954).

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [x] The PR is proposed to the proper branch
- [ ] There is a reference to the original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake

```
opencv_extra=resize_onnx_opset13
```
"
opencv/opencv,2022-01-22 11:52:21,feature,Task: GCC 12 support,"Support compilation with GCC 12 and fix tests

- #21497 (persistence_base64.cpp compilation error `-Werror=address`) 
- #21465 (ppc64le compilation)
- #21988 (build warnings)"
opencv/opencv,2022-01-16 19:27:55,feature,AudioIO: add dnn speech recognition sample on C++,"### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [ ] I agree to contribute to the project under Apache 2 License.
- [ ] To the best of my knowledge, the proposed patch is not based on a code under GPL or other license that is incompatible with OpenCV
- [ ] The PR is proposed to proper branch
- [ ] There is reference to original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
"
opencv/opencv,2022-01-14 22:05:58,feature,Use modern OpenVINO package interface,"* new cmake options: `WITH_OPENVINO`, `OPENCV_GAPI_WITH_OPENVINO`, old options are deprecated
* old options will be ignored if new options have been set
* tested with current OpenVINO@_master_

Notes:
* OpenVINO version will be autodetected, can not be changed manually anymore (`INF_ENGINE_RELEASE`)
* not possible to disable ngraph integration when OpenVINO is enabled (is it necessary?)

Validation:
- [x] [build](http://pullrequest.opencv.org/buildbot/builders/precommit_custom_linux/builds/7205) with OpenVINO pre-release [2022.1.0.dev20220131](https://github.com/openvinotoolkit/openvino/releases/tag/2022.1.0.dev20220131)

<cut/>

```
force_builders=Custom,Custom Win,Custom Mac

Xbuild_image:Custom=ubuntu-openvino-2021.4.2:20.04
build_image:Custom=ubuntu-openvino-2022.1.0.dev20220131:20.04
build_image:Custom Win=openvino-2021.4.2
build_image:Custom Mac=openvino-2021.4.2

test_modules:Custom=dnn,gapi,python2,python3,java
test_modules:Custom Win=gapi,python2,python3,java
test_modules:Custom Mac=gapi,python2,python3,java

buildworker:Custom=linux-1
# disabled due high memory usage: test_opencl:Custom=ON
test_opencl:Custom=OFF
test_bigdata:Custom=1
test_filter:Custom=*
```"
opencv/opencv,2022-01-12 09:14:41,feature,TiffEncoder write support more depth type,"**Merge with extra**: https://github.com/opencv/opencv_extra/pull/955

### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or other license that is incompatible with OpenCV
- [ ] The PR is proposed to proper branch
- [ ] There is reference to original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake


```
opencv_extra=master
```"
opencv/opencv,2022-01-06 11:01:32,feature,"tiff need check TIFFTAG_SAMPLEFORMAT, should not always use unsigned.","### Pull Request Readiness Checklist

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or other license that is incompatible with OpenCV
- [ ] The PR is proposed to proper branch
- [ ] There is reference to original bug report and related work
- [ ] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake

```
force_builders=linux,win64,mac,docs
```"
